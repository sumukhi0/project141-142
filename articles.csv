id,index,timestamp,eventType,contentId,authorPersonId,authorSessionId,authorUserAgent,authorRegion,authorCountry,contentType,url,title,text,lang,total_events
0,3096,1487246811,CONTENT SHARED,-4029704725707465084,6013226412048763966,-6569695881431984742,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36",SP,BR,HTML,http://www.cnbc.com/2016/12/21/former-google-career-coach-shares-a-useful-visual-trick.html,former google career coach shares a visual trick for figuring out what to do with your life,"If you want 2017 to be an exciting year, design it that way. That's the advice of former Google career coach and job strategist Jenny Blake , who has helped more than a thousand people improve their work lives. She recommends creating a ""mind map,"" a visual diagram of your interests and goals. Drawing one doesn't take long and could help you figure out the next project, hobby or career change that will make the new year happier and more successful, Blake says. ""My favorite way to brainstorm creatively, whether it's about values or setting goals for the new year, is through mind maps,"" Blake tells CNBC. To make one, write down the year at the center of a piece of paper, and then draw spokes with different themes that are important to you. For example, your spokes could be business, personal life, health and fitness, fun, or skill building. From each of those themes, draw additional spokes to connect the themes to ways you want to improve or experiment in that area. Blake recommends you ask yourself: ""What's important to me about that? And what does success look like?"" For example, in ""personal life,"" you could write, ""Meet up with a friend at least once a month"" or ""Take a music class."" The visual trick works because it helps you see the different ways you can improve, Blake writes in her book "" Pivot: The Only Move That Matters Is Your Next One ."" ""With mind mapping you are taking pen to paper, going old-school,"" Blake says. And if you're like many professionals who sit too much and stare too long at a computer, drawing might actually be fun. ""The goal is to break out of linear thinking,"" Blake says. ""Go broad. Go big. Go sideways, and then experiment to see which of your ideas is most likely to lead to a resonant next step."" Video by Andrea Kramar.",en,433
1,1671,1467813728,CONTENT SHARED,-133139342397538859,4918484843075254252,-5701227433817087697,,,,HTML,http://gq.globo.com/Prazeres/Poder/Carreira/noticia/2016/06/novo-workaholic-trabalha-pratica-esportes-e-tem-tempo-para-familia-conheca.html,"novo workaholic trabalha, pratica esportes e tem tempo para a família. conheça","Novo workaholic não abre mão do esporte e da família (Foto: iStock) Se alguém, um dia, precisar reunir os personagens desta reportagem para uma rodada de negócios, fica aqui uma sugestão: marque o encontro na Vila Olímpica. Maratona, triatlo, judô, taekwondo e windsurfe são algumas das modalidades praticadas - em alguns casos com dedicação e resultados semiprofissionais - por este pequeno conjunto de executivos. Mas boa sorte com as agendas. Todos presidem empresas, obviamente com rotinas intensas, e dificilmente permitem que compromissos extracurriculares roubem o tempo sagrado para suas famílias ou, no caso dos solteiros, para os hobbies que fazem brilhar seus olhos. Representantes de uma geração que conviveu com chefes e colegas viciados em trabalho (e em certos momentos se confundiram com eles), esses profissionais fizeram do esporte a pedra angular para a construção de rotinas criativas. Sua dedicação a empresas tão diferentes entre si como P&G, Leo Burnett e Unisys não é avaliada pelo número de horas passadas no escritório, uma obsessão típica dos velhos baby boomers. ""Talvez porque eles não tivessem outros interesses na vida"", afirma Alberto Ogata, diretor técnico da Associação Brasileira de Qualidade de Vida (ABQV). Tampouco é medida pelos símbolos de status corporativo que conspiravam contra a saúde dos executivos, a começar pela vaga de estacionamento próxima ao elevador. Pelas novas regras com as quais eles estão subvertendo o jogo corporativo, o que vale são os resultados entregues e o modo como se dá essa entrega: com equilíbrio e foco invejáveis. É o que permite a altos executivos, em posições de destaque nas companhias que lideram, terem vidas saudáveis, interessantes e, quase sempre, divertidas. É o que faz deles a primeira geração dos chamados novos workaholics. O ponto de partida para essa mudança de paradigma encontra-se no Vale do Silício. O engravatado durão que demitia 10% de seus funcionários todo ano deu lugar ao ex-hippie. E as empresas passaram a trocar horários a cumprir por tarefas a executar Com idades entre 40 e 50 anos, eles estão no auge de suas carreiras, lideram multinacionais e têm vasta experiência internacional. São todos apaixonados pelos esportes que praticam, mas têm uma visão utilitária da atividade física. Em troca da disciplina férrea que tira a maioria deles da cama antes de o sol nascer, esperam benefícios (aplicáveis ao trabalho) como energia, serenidade e capacidade de executar impecavelmente sob pressão. Pragmaticamente, eles estão trocando quantidade por qualidade no trabalho porque entenderam que mais tempo no escritório não significa mais trabalho feito. Um estudo recente da Business Roundtable, uma associação de CEOs de grandes empresas nos EUA, concluiu que embora um pico concentrado de muitas horas de trabalho possa aumentar o rendimento em prazos curtos (fechamentos de trimestre, lançamento de produtos, etc.), longas jornadas sistemáticas tendem a diminuir a produtividade. Os pesquisadores constataram que pessoas trabalhando 60 horas por semana - ou 12 horas por dia - durante dois meses não produziram mais do que num regime tradicional de 40 horas semanais - ou oito horas diárias. Já uma semana de 80 horas provoca burnout em menos de um mês. Há raízes históricas para a transformação em curso. ""No início dos anos 80 tivemos as primeiras manifestações de executivos que se saturaram da busca pelo sucesso, justamente por terem percebido que o custo de alcançá-lo é imenso"", afirma o professor Esdras Vasconcellos, do Departamento de Psicologia Social e do Trabalho da Universidade de São Paulo. ""Naquela época, reduzir o ritmo era mais difícil. Então, alguns abdicaram da carreira executiva."" Ouviam-se casos anedóticos de executivos que largavam tudo para abrir uma pousada no Nordeste. No auge da onda workaholic, eles tiveram o insight que originou o que Vasconcellos chama de ecologia da vida. ""Esse novo pensamento não traz uma abdicação da vida civilizada, moderna, até certo ponto consumista"", pondera. ""Traz, sim, uma leveza simbolizada pela troca do tênis (longas partidas, alta competitividade) pela ioga (meditação, retiros espirituais)."" Geograficamente, o ponto de partida para essa mudança de paradigma encontra-se no Vale do Silício - não por acaso, na Califórnia. Volte aos anos 60. Ali floresceu todo o movimento hippie, toda a contracultura que influenciou São Francisco e região. ""Esses anos revolucionários romperam totalmente com valores antigos"", afirma Vasconcellos. Duas décadas depois, a Califórnia estava no epicentro do surgimento da informática, do computador pessoal como facilitador de processos de trabalho. Placas tectônicas do mundo corporativo se moveram durante o período de ascensão e estrelato de Steve Jobs. Da sua explosão como empreendedor, nos anos 80, à sua assimilação pelo establishment empresarial na primeira década do século 21 - que coincide com o caso de Jack Welch, até então o grande ícone do executivo vencedor e o protótipo do workaholic. O engravatado durão que demitia 10% de seus funcionários todo ano deu lugar ao ex-hippie com grande apetite por LSD, meditação e comida vegana. Era o início de uma revolução cultural. Pragmática. As empresas, ao perceber que a permissão para que as pessoas trabalhem com um pouco mais de leveza acaba trazendo resultados, decidiram relaxar. Trocaram horários a cumprir por tarefas a executar. Os ambientes de trabalho ganharam quadras de esporte, áreas de lazer, restaurantes gourmet. Se workaholic é o profissional que não consegue ou tem grande dificuldade de se desconectar do trabalho, o que importa não é o número de hobbies e atividades esportivas que ele encaixa na rotina. O desafio para esse executivo é o poder de se desligar Esse movimento ganhou impulso com uma mudança na cultura das empresas na década de 90, quando começa a implantação dos programas de qualidade de vida. A geração que estava, então, em início de carreira já recebeu esse novo modelo mental de autogerenciamento nos seus anos de formação. ""Esses executivos têm consciência elevada e tratam a saúde como elemento de sustentabilidade pessoal"", diz Carlos Legal, sócio da consultoria Legalas, especializada em educação corporativa. Parte deles abre mão voluntariamente de posições no topo das organizações. Todavia, há aqueles que querem tudo: chegar ao topo de suas empresas, porém sem sacrificar vida pessoal e vida familiar. Na visão de Vicky Bloch, uma das mais renomadas coachs de CEOs do país, o perfil predominante no topo das companhias brasileiras ainda é o do workaholic clássico. Profissionais atléticos são pontos fora da curva e não um retrato fiel da realidade do Brasil, onde 51% das mulheres e 47% dos homens hoje são sedentários. A obesidade é uma epidemia, sobretudo na base da pirâmide organizacional das empresas. Mas isso está mudando, e rapidamente. ""Estamos em transição"", nota Vicky. A mudança maior está se dando com a entrada no primeiro escalão das empresas de executivos na faixa de 35 a 45 anos. Pessoas que já são associadas ao que ela chama de geração flexível, no que diz respeito à sua relação com o tempo e o espaço em que o trabalho é executado. Gente mais saudável, sem dúvida, mas não necessariamente menos viciada em trabalho, pondera Vicky. Afinal, se workaholic é o sujeito que não consegue se desconectar do trabalho, o que importa não é o número de hobbies e atividades esportivas que ele encaixa na rotina. O desafio é o poder de se desligar. Alguns sintomas típicos do antigo vício em trabalho, é bem verdade, tornam-se mais raros na geração flexível - como os casamentos desfeitos. De acordo com as pesquisas de Bryan Robinson, psicoterapeuta, professor emérito da Universidade da Carolina do Norte e autor do livro Chained to the Desk (""Acorrentado à Mesa"", sem edição em português), casamentos com ao menos um cônjuge workaholic têm 40% mais riscos de acabar em divórcio. Contudo, se a cachaça do workaholic é o trabalho, a dependência, em muitos casos, continua presente. ""Só que hoje ele não trabalha apenas no escritório"", diz Vicky. ""Trabalha em casa, no carro, correndo."" O próximo desafio, então, talvez seja completar a transição desses novos workaholics em autênticos pós-workaholics. A troca da guarda geracional, sem dúvida, está fazendo sua parte. Basta conferir o depoimento de Ricardo Sangion, principal executivo da rede social Pinterest no Brasil. Mais jovem do time aqui reunido, ele se orgulha de dormir muito, trabalhar pouco, entregar resultados e ainda ter tempo para administrar um site, um bar e uma pousada na Bahia. Nem tudo, porém, se resume à idade. Valores antes reprimidos estão sendo integrados à vida executiva. ""A espiritualidade passa a ser importante"", exemplifica o professor Vasconcellos. E há o poderoso efeito demonstração. ""As empresas começam a perceber que até profissionais na posição de CEO já não estão mais disponíveis para alcançar resultados a qualquer preço"", observa o consultor Carlos Legal. Ao dizer que não estão dispostos a fazer qualquer tipo de sacrifício para chegar ao topo, esses profissionais aceleram o processo de mudança no próprio modo de funcionar das organizações.",pt,315
2,1814,1468867647,CONTENT SHARED,-6783772548752091658,4918484843075254252,-8995217520473210153,,,,HTML,http://www.caroli.org/livro-retrospectivas-divertidas/,livro: retrospectivas divertidas,"Neste livro, nós fornecemos um conjunto de ferramentas de atividades para transformar um grupo de pessoas em um time eficaz. Manter a diversão para os participantes e proporcionar um ambiente onde todos poderão refletir e discutir sem perder a diversão é fundamental para a melhoria contínua. Apresentaremos aqui um catálogo de atividades diversas, apropriadas, uma a uma, a diferentes contextos e equipes. Todas equipes devem fazer uma retrospectiva por semana, a menos que estejam sem tempo. Daí devem fazer duas!",pt,294
3,1317,1465484901,CONTENT SHARED,8657408509986329668,-8020832670974472349,838596071610016700,,,,HTML,https://medium.com/practical-blend/pull-request-first-f6bb667a9b6,pull request first - practical blend,"Pull request first After two years of working with pull requests a bit differently from the norm, I now take for granted that most developers submit pull requests only once their work is ""ready"" for review. Obviously... right? Or... not? There's a different approach I'd argue provides more transparency to your team, makes sure everyone is aligned and helps to keep a historical record of what decisions were made when developing a feature. Simply... Open a pull request before you write code.* Wait, what? Rather than treating pull requests solely as means for reviewing code, use the pull request as the master of record for the work related to a feature. This isn't a new idea - this is an approach our product team at bitHound practiced for nearly 2 years, inspired by GitHub itself ( Zach Holman's talk explains it). * And yes, technically GitHub requires at least one commit to open a pull request, so you have to write something first. There are several benefits that we saw from adopting this approach: Increased transparency Your Open Pull Requests view on GitHub becomes an instant snapshot of the current state of dev work in your company. Isn't that what the scrum board is for? Oh, sure it is - and once everyone races to move/write their stickies 30 seconds before standup, you'll get a similar view (for about 5 minutes). Specs as you go By using checkboxes in the pull request description , you can build out the current list of todos, what issues the pull request addresses, and update that as you uncover hidden gotchas during development. Those checkbox counts even bubble up to your pull request list for easy at-a-glance status updates. This allows other collaborators to know what the progress is on the feature, and let's them jump in at the appropriate spot. We'd often use these checkboxes to track issues we discovered in development rather than opening new tickets. Or, reference existing issues that will get closed when the pull request gets merged . Continuous review Why review when someone has ""finished"" only to realize they might have taken a wrong turn somewhere early in development? With open pull requests as you work, reviews can be done as the work is done. Sure this isn't always practical when everyone is super busy, but it works great when you're bringing a new team member on board, or just working on a doozy of a feature. It's helped catch problems before they become disasters. Great for asynchronous/remote work While our work style was very much centred around in-person discussion, we did have enough occasions where someone was working remotely (home, different country, whatever). I found that this is where the pull request was critical in understanding the state of the project. As long as you kept the pull request current, it didn't matter if you missed an earlier Slack or office discussion that changed its direction. What's the catch? Is everyone on board? Like any approach to organizing your work, it's only as good as the effort the team puts into it. So you need to keep each other honest to make sure pull requests are updated in a timely fashion. Does it scale? I don't know. Like any methodology, make sure it makes sense for you. When it stops working, tweak something or look for alternatives. What's the priority? Unlike a scrum board, there is no clear indication of progress as you can't reorder open pull requests (without the use of some extensions at least), but you can probably get away with a good labeling/milestone system that helps alleviates this.",en,294
4,588,1461629452,CONTENT SHARED,-6843047699859121724,7527226129639571966,-1297230017812472163,,,,HTML,https://medium.com/@jeffersoncn/ganhe-6-meses-de-acesso-ao-pluralsight-maior-plataforma-de-treinamento-online-dd2b1c9a22b9,"ganhe 6 meses de acesso ao pluralsight, maior plataforma de treinamento online","Ganhe 6 meses de acesso ao Pluralsight, maior plataforma de treinamento online Muitos não sabem, mas o Pluralsight é uma das melhores, senão a melhor, plataforma de treinamentos online. Com quase 5000 cursos nas áreas de desenvolvimento, infraestrutura, dados, entre outras e que são ministrados por mais de 800 especialistas, é fácil reconhecer o valor de se ter acesso a uma biblioteca tão rica de conhecimento como esta. Abaixo temos exemplos de alguns cursos dentre a lista dos mais populares da plataforma: Building a Web App with ASP.NET Core RC1, MVC 6, EF7 & AngularJS Angular 2: Getting Started ASP.NET Core 1.0 Fundamentals C# Fundamentals with Visual Studio 2015 Building Applications with ASP.NET MVC 4 JavaScript Best Practices ASP.NET MVC 5 Fundamentals TypeScript Fundamentals Docker for Web Developers Applying Functional Principles in C# ASP.NET in Multi-tenant App, Examples in MVC, ExtJS, and Angular Python Fundamentals Java Fundamentals: The Java Language Rapid JavaScript Training C# Best Practices: Improving on the Basics Design Patterns Library Getting Started with Entity Framework 6 Rebuilding Web Forms Applications in MVC Advanced JavaScript Building Your First Xamarin.Android App from Start to Store Building Web Applications with Node.js and Express 4.0 WCF End-to-End WPF and MVVM: Test Driven Development of ViewModels Se interessou por algum deles? E que tal ganhar uma assinatura gratuita de 6 meses com acesso completo à toda essa coleção de cursos do Pluralsight e começar agora a aprender aquelas habilidades que você vem buscado desenvolver? A Microsoft em parceria com o Pluralsight nos oferecem essa oportunidade! Quer saber como? Então vamos lá! Primeiramente precisamos nos inscrever no programa Visual Studio Dev Essentials utilizando uma conta Microsoft ( hotmail , live ou outlook ). O cadastro é simples e gratuito e você pode fazê-lo nesse link : 2. Agora que já temos acesso ao Visual Studio Dev Essentials , podemos então clicar em "" Get Code "" que fica próximo ao logo do Pluralsight . 3. Ao clicar em "" Get Code "" aparecerá em seu lugar "" Activate "", que ao ser clicado, vai nos redirecionar para a página de cadastro do Pluralsight . 4. No formulário de cadastro, o campo do código já estará preenchido, bastando entrar com as informações restantes antes de clicar em "" Activate Benefit "". 5. Pronto! Você pode agora aproveitar os seus 6 meses de estudos no Pluralsight gratuitamente. Dica extra pra quem tem condição de aluno verificada no Dreamspark Também é possível obter uma assinatura de 3 meses gratuitamente no Pluralsight através do Dreamspark . No catálogo de software do Dreamspark , ao rolar a página até o final e chegar na sessão "" Treinamento e Certificação "", clique no logo do Pluralsight ou acesse esse link . Selecione o idioma: English e clique em "" Obter chave "". No final da página há um link para o resgate do código que irá redirecionar para o cadastro no Pluralsight . Preencha os dados, ative o benefício e pronto! Agora você tem um acesso de 3 meses a todos os cursos do Pluralsight ! Espero que aproveitem bem os estudos! Até a próxima!",pt,281
5,2308,1473593226,CONTENT SHARED,-2358756719610361882,-9120685872592674274,5016857076790212194,,,,HTML,http://www.attps.com.br/cinco-motivos-para-investir-em-automacao-de-testes-e-reduzir-o-custo-do-erro/,custo do erro - cinco motivos para investir em automação de testes,"Atualmente, o custo de manutenção de software representa cerca de 50% do custo de uma empresa desenvolvedora de software. O Instituto de Pesquisa Triângulo (RTI), sediado no Estados Unidos, realizou um estudo para o Instituto Nacional de Padrões e Tecnologia (NIST) dos EUA, em 2002, para estimar o impacto do teste de software inadequado sobre a economia dos Estados Unidos. As suas conclusões foram que esses custos ou perdas financeiras ocasionadas por defeitos de software ficaram na ordem de US$ 59,5 bilhões, ou 0,06% do PIB dos EUA. Cerca de 37%, ou US$ 22 bilhões, deste custo poderia ser evitado caso fossem aplicadas técnicas de automação de testes de forma correta, com infrastrutura adequada e realizados de forma sistemática. Um estudo recente da Universidade de Cambridge mostra que o custo global com defeitos de software, nas atividades de encontro e remoção dos defeitos, cresce anualmente a uma taxa de US$ 312 bilhões de dólares. Este custo representa em média a metade do tempo de desenvolvimento de projetos de software. Os desenvolvedores, em média, gastam 50,1% do seu tempo em atividades que não são de desenvolvimento. Metade do tempo útil do programador é gasto em atividades de correção de defeitos. Diante destes custos elevadíssimos, destacamos CINCO MOTIVOS PARA INVESTIR EM AUTOMAÇÃO DE TESTES DE SOFTWARE PARA REDUZIR ERROS EM PRODUÇÃO. 1- Teste manual sozinho não consegue ter a mesma abrangência de execução dos testes automatizados Cada grupo de desenvolvimento de software testa seus próprios produtos, contudo software entregues sempre tem defeitos. Os analistas de teste se esforçam para pegá-los antes do produto ser lançado, mas os defeitos insistem em reaparecer, mesmo com os melhores processos de testes manuais. Automação de testes de software é a melhor maneira de aumentar a eficácia, a eficiência e a cobertura de execução do seu teste de software, aumentando assim a confiabilidade e a estabilidade dos sistemas em ambiente de produção. 2- Teste manual é executado por seres humanos. E seres humanos invariavelmente falham. Teste de software manual é uma atividade realizada por um ser humano sentado em frente ao computador, que de forma cuidadosa, passa por telas de aplicativos, tentando várias combinações de uso e de entrada, comparando os resultados com o comportamento esperado e evidenciando as suas observações. Testes manuais são repetidos por várias vezes durante os ciclos de desenvolvimento de alterações no código fonte e outras situações, como vários ambientes operacionais e configurações de hardware. Estas repetições de teste manuais podem ser falhas, cada pessoa pode interpretar uma ação ou resultado de forma diferente no momento da execução do teste ou ainda, se ausentar por diversos motivos, gerando atrasos ao projeto. 3- Teste automatizado é executado por robôs, 24 horas por dia, 7 dias por semana. Uma ferramenta de testes automatizados é capaz de reproduzir ações pré-gravadas e pré-definidas, comparar os resultados com o comportamento esperado e relatar o sucesso ou fracasso desses testes manuais para um analista de teste. Uma vez que os testes automatizados são criados, eles podem facilmente ser repetidos e estendidos, uma tarefa impossível com os testes manuais. Devido a isso, os gerentes experientes descobriram que o teste de software automatizado é um componente essencial dos projetos de desenvolvimento bem-sucedidos. 4- A cobertura de teste aumenta de forma significativa nos testes automatizados Teste de software automatizado pode aumentar a profundidade e o alcance de testes para ajudar a melhorar a qualidade do software. Testes longos e que muitas vezes são evitados durante o teste manual podem ser executados sem supervisão. Eles podem até mesmo ser executados em vários computadores com diferentes configurações. Teste de software automatizado pode olhar para dentro de um aplicativo e ver o conteúdo da memória, tabelas de dados, conteúdo de arquivos, comparar imagem de telas, e os estados internos do programa para determinar se o produto está se comportando conforme o esperado. Automação de teste pode facilmente executar milhares de diferentes casos de teste complexos durante cada teste, fornecendo uma cobertura que é impossível com testes manuais. 5- Teste automatizado de software economiza tempo e dinheiro Testes de software tem que ser repetido muitas vezes durante os ciclos de desenvolvimento, para garantir que a qualidade seja mantida em níveis aceitáveis. Cada código fonte alterado precisará ter testes de software repetido. Para cada versão do software liberada, pode ser necessário que seja testado em todos os sistemas operacionais e configurações de hardware. Fazer isto manualmente é uma tarefa muito dispendiosa e demorada. Uma vez criado, os testes automatizados podem ser executados uma e outra vez sem nenhum custo adicional, e eles são muito mais rápidos do que os testes manuais. Teste de software automatizado pode reduzir o tempo para executar testes repetitivos de dias para horas, com um nível de acertividade muito maior do que os seres humanos podem conseguir realizar. A economia de tempo é traduzida diretamente em redução de custos, aumento de qualidade e satisfação do cliente. Veja neste comparativo entre teste manual e teste automatizado. O custo inicial do teste automatizado é de fato maior, mas é um custo que se paga já a partir do quinto sprint. E o custo do teste manual continua a crescer de forma linear até não poder ser mais viável, onde passa a deixar de ser feito e irá gerar custos de manutenção corretiva, além de desgastes na relação com o cliente e perda de mercado do seu produto. O ROI (return of investment) dos testes automatizados sobre teste manual pode ser da ordem de 63% ao final de um ano. Algo espetacular, concorda? Por Rodrigo Almeida, Gerente de Qualidade de Software",pt,280
6,1902,1469678235,CONTENT SHARED,-8208801367848627943,-3390049372067052505,2045534933671019150,,,,HTML,http://www.geekwire.com/2016/ray-kurzweil-world-isnt-getting-worse-information-getting-better/,ray kurzweil: the world isn't getting worse - our information is getting better,"Ray Kurzweil, the author, inventor, computer scientist, futurist and Google employee, was the featured keynote speaker Thursday afternoon at Postback , the annual conference presented by Seattle mobile marketing company Tune. His topic was the future of mobile technology. In Kurzweil's world, however, that doesn't just mean the future of smartphones - it means the future of humanity. Continue reading for a few highlights from his talk. On the effect of the modern information era: People think the world's getting worse, and we see that on the left and the right, and we see that in other countries. People think the world is getting worse. ... That's the perception. What's actually happening is our information about what's wrong in the world is getting better. A century ago, there would be a battle that wiped out the next village, you'd never even hear about it. Now there's an incident halfway around the globe and we not only hear about it, we experience it. On the potential of human genomics: It's not just collecting what is basically the object code of life that is expanding exponentially. Our ability to understand it, to reverse-engineer it, to simulate it, and most importantly to reprogram this outdated software is also expanding exponentially. Genes are software programs. It's not a metaphor. They are sequences of data. But they evolved many years ago, many tens of thousands of years ago, when conditions were different. How technology will change humanity's geographic needs: We're only crowded because we've crowded ourselves into cities. Try taking a train trip across the United States, or Europe or Asia or anywhere in the world. Ninety-nine percent of the land is not used. Now, we don't want to use it because you don't want to be out in the boondocks if you don't have people to work and play with. That's already changing now that we have some level of virtual communication. We can have workgroups that are spread out. ... But ultimately, we'll have full-immersion virtual reality from within the nervous system, augmented reality. On connecting the brain directly to the cloud: We don't yet have brain extenders directly from our brain. We do have brain extenders indirectly. I mean this (holds up his smartphone) is a brain extender. ... Ultimately we'll put them directly in our brains. But not just to do search and language translation and other types of things we do now with mobile apps, but to actually extend the very scope of our brain. Why machines won't displace humans: We're going to merge with them, we're going to make ourselves smarter. We're already doing that. These mobile devices make us smarter. We're routinely doing things we couldn't possibly do without these brain extenders.",en,266
7,1942,1469992520,CONTENT SHARED,2581138407738454418,6756039155228175109,611377856408166257,,,,HTML,https://medium.com/@rdsubhas/10-modern-software-engineering-mistakes-bc67fbef4fc8,10 modern software over-engineering mistakes,"10 Modern Software Over-Engineering Mistakes Few things are guaranteed to increase all the time: Distance between stars, Entropy in the visible universe, and Fucking business requirements . Many articles say Dont over-engineer but don't say why or how. Here are 10 clear examples. Important Note: Some points below like ""Don't abuse generics"" are being misunderstood as ""Don't use generics at all"", ""Don't create unnecessary wrappers"" as ""Don't create wrappers at all"", etc. I'm only discussing over-engineering and not advocating cowboy coding. 1. Engineering is more clever than Business Engineers think we're the smartest people around because we build stuff. This first mistake often makes us over-engineer. But if we plan for 100 things, Business will always come up with the 101st thing we never thought of. If we solve 1,000 problems, they will come back with 10,000 problems. We think we have everything under control - but we have no clue what's headed our way. In my 15 year involvement with coding, I have never seen a single business ""converge"" on requirements. They only diverge. Its simply the nature of business and its not the business people's fault. TL;DR - The House (Business) Always Wins Tip: If you don't have time to go through the entire post, then this one point is enough. 2. Reusable Business Functionality When Business throws more and more functionality (as expected), we sometimes react like this: We try to group and generalize logic as much as possible. This is why most MVC systems end up in either Fat Models or Fat Controllers. But as we saw already, business requirements only diverge, they never converge. Instead, how should we have reacted: Shared logic and abstractions tend to stabilise over time in natural systems. They either stay flat or relatively go down as functionality gets broader. When the opposite happens, it creates systems that are Too big to fail (leading closer to the dreaded rewrite). Example: We created a User profile system for a previous client. Started with a CRUD controller with shared functionality because we assumed everything is going to be similar. But ended up with 13 different signup flows - initial social connection, a long signup form upon first entry, smaller and edit page sections, completely different looking profile page and so on and on - that it made very little sense to share stuff in the end. Similarly, an Order View and Order Edit flow ends up so inherently different from the actual Ordering flow. Try to vertically split business functionality first before splitting horizontally. This works in all cases - isolated services, trunk-based services, language-specific modules, etc. Also helps to switch from one form to another with ease. Otherwise it becomes increasingly complicated to change parts of the system. TL;DR - Prefer Isolating Actions than Combining Tip: Pick one external-facing action (Endpoint/Page/Job/etc) in your codebase. How many context switches does someone need to understand what's going on? 3. Everything is Generic (Sometimes goes together with previous point, but also seen applied individually in separate projects) Want to connect to a database? Lets write a Generic Adapter Query that database? Generic Query Pass it some params? Generic Params Build those params? Generic Builder Map the response? Generic Data Mapper Handle the user request? Generic Request Execute the whole thing? Generic Executor and so on and so on Sometimes Engineers get carried away. Instead of trying to solve the business problem, we waste our time trying to find the perfect abstractions. The answer is so simple. Designs are always playing catch up to changing real world requirements. So even if we found a perfect abstraction by miracle, it comes tagged with an expiry date because #1 - The House wins in the end. The best quality of a Design today is how well it can be undesigned. There is an amazing article on write code that is easy to delete, not easy to extend . TL;DR - Duplication is better than the wrong abstraction Conversely, Duplication is sometimes essential for the right abstraction. Because only when we see many parts of the system share ""similar"" code, a better shared abstraction emerges. The Quality of Abstraction is in the weakest link. Duplication exposes many use cases and makes boundaries clearer. Tip: Shared abstractions across services sometimes leads to Microservices ending up as a Distributed Monolith . 4. Shallow Wrappers The practice of wrapping every external library before using it. Unfortunately most wrappers we write are Shallow. We are juggling between delivering functionality and writing a good wrapper. So our wrappers are mostly tightly bound to the underlying library (in some cases being a 1:1 mirror, or doing 1/10th of what the original library does with 10x effort). If we change the underlying library later, usages of this wrapper everywhere usually end up having to be changed as well. Sometimes we also mix up business logic inside wrappers, making it neither a good wrapper nor a good business solution, but some kind of gluey layer in between. This is 2016. External libraries and clients have improved leaps and bounds. OSS Libraries are fantastic. They have high quality and well tested codebases written by awesome people, who have had dedicated, focused time writing it. Most have clear, testable, instrumentable APIs, allowing us to follow the standard pattern of Initialize - Instrument - Implement. TL;DR - Wrappers are an exception, not the norm. Don't wrap good libraries for the sake of wrapping Tip: Creating an ""agnostic"" wrapper is no laughing matter. ""Swap out library"" comes from a mindset of ""Configurability"" which is covered in detail in the ""<X>-ity"" section later. 5. Applying Quality like a Tool Blindly applying Quality concepts (like changing all variables to ""private final"", writing an interface for all classes, etc) is NOT going to make code magically better. Check Enterprise FizzBuzz (or Hello World ). It has a gazillion code. In the micro-level each class follows SOLID principles, uses all sorts of great Design patterns (factory, builder, strategy, etc) and coding techniques (generics, enums, etc). It gets high Code quality ratings from CQM tools. But if we take a step back, this prints Fizz Buzz . TL;DR - Always take a step back and look at the macro picture Conversely, automated CQM tools are good at tracking Test coverage, but can't tell whether we are testing the right thing. A benchmark tool can track performance, but can't tell whether stuff runs parallel or sequential. Only a Human has to look at the big picture. Which takes us to... 5.1. Sandwich Layers Lets take a concise, closely bound action and split it into 10 or 20 sandwiched layers, where none of the individual layers make any sense without the whole. Because we want to apply the concept of ""Testable code"", or ""Single Responsibility Principle"", or something. In the Past  - this was done by a chain of Inheritance. A extends B extends C extends D and so on. Now  - People do the exact same thing, except they make each class have an interface/implementation and inject it into the next layer, because duh SOLID. Concepts like SOLID came up in response to abuse of Inheritance and other OOP concepts. Most engineers are unaware of where/why these concepts came from, but just end up following the memo. TL;DR - Concepts need shift in Mindset. Cannot be applied blindly like tools. Learn a different language and try the other mindset of doing things. That makes a fundamentally better developer. Pouring old wine in a new labelled bottle doesn't work for concepts. We never have to tear apart a clear design in the name of applying a concept. 6. Overzealous Adopter Syndrome Discovered Generics. Now even a simple ""HelloWorldPrinter"" becomes ""HelloWorldPrinter<String,Writer>"". Don't use Generics when its obvious that a problem handles only specific data types, or when normal type signatures are enough. Discovered Strategy Pattern. Every ""if"" condition is now a strategy. Why? Discovered how to write a DSL. Gonna use DSLs everywhere. I don't know... Used Mocks. Gonna mock every single object I'm testing. how to even... Metaprogramming is awesome, let me use it everywhere describe why... Enums/Extension Methods/Traits/whatever are awesome, let me use it everywhere this is wrong. TL;DR - TL;DRs should not be used everywhere 7. <X>-ity Configurability Security Scalability Maintainability Extensibility ... Vague. Unchallenged. Hard to argue against. FUD. Example 1: Lets build a CMS for our forms for ""Extensibility"". Business people can add new fields easily. Result: Business people never used it. When they had to, they would have a developer sit right beside them and do it. Maybe all we needed was a simple Developer guide to add a new field in few hours, instead of a point-and-click interface? Example 2: Lets design a big Database layer for easy ""Configurability"". We should be able to switch database in a single Magic file. Result: In 10 years, I've seen only one business make serious effort to completely swap a fully vested database. And when it happened, the ""Magic file"" did not help. There was so much operational work. Incompatibilities and gaps in functionality. And the client asked us to switch ""one half"" of our models to the new NoSQL database. We tore our hair apart - our Magic toggle was a single point of change, but this was cross-cutting. In today's world, we're well past a point where there is no way to design a single configurable layer for modern document/KV stores (e.g. Redis/ CouchDB/ DynamoDB/ etc). Not even SQL Databases like Postgres/ HSQLDB/ SQLite are compatible for that matter. Either you completely dumb down your data layer (and struggle with delivering functionality), or acknowledge the database as part of your solution (e.g. postgres geo/json features) and throw away configurability guilt. Your stack is as much part of your solution as your code. When you let go of this vague X-ity, better solutions start to emerge. e.g. You can now break data access vertically (small DAOs for each action) instead of horizontally (magic configurable layer), or even pick and choose different data stores for different functionality [micro] services style. Example 3: We built an OAuth system for enterprise clients. For the Internal administrators - we were asked to use a secondary Google OAuth system. Because Security. If someone hacks our OAuth, business didn't want them to get access to admin credentials. Google OAuth is more secure, and who can argue against ""more security"" at any point? Result: If someone really wanted to hack into our system, they don't have to go through the OAuth layer. We had many vulnerabilities lying around. e.g. they could have just done privilege escalation. So all that effort of supporting two different OAuth user profiles and systems everywhere had little to no returns in securing our system, compared to properly securing our base first. TL;DR - Don't let <X>-ities go unchallenged. Clearly define and evaluate the Scenario/Story/Need/Usage. Tip: Ask a simple question - ""What's an example story/scenario?"" - and then dig deep on that scenario. This exposes flaws in most <X>-ities. 8. In House ""Inventions"" It feels cool in the beginning. But these are most common sources of Legacy in few years. Some examples: In-house libraries (HTTP, mini ORM/ODM, Caching, Config, etc) In-house frameworks (CMS, Event Streaming, Concurrency, Background Jobs, etc) In-house tools (Buildchains, Deployment tools, etc) Things that are missed: It takes a lot of skills and deep understanding of the problem domain. A ""Service runner"" library needs expertise of how daemons work, process management, I/O redirection, PID files and so on. A CMS is not just about rendering fields with a datatype - it has inter-field dependencies, validations, wizards, generic renderers and so on. Even a simple ""retry"" library is not so simple. There is a constant effort required to keeping this going. Even a tiny open source library takes a lot of time to maintain. If you open source it, nobody cares. Except the original starters and people paid to work on it. The original starters will eventually move away with the ""Inventor of X"" tag in their résumé. Contributing to existing frameworks takes up more time NOW. But creating an ""invention"" takes up considerably more time going forward. TL;DR - Reuse. Fork. Contribute. Reconsider. Finally, if really pushed to go ahead, do it only with an Internal OSS mindset. Fight with existing competition. Work to convince even internal people to use this. Don't take it for granted since you are an insider. 9. Following the Status Quo Once something is implemented in a certain way, everyone implicitly starts building on top of it. Nobody questions the status quo. Working code is considered ""the right way"". Even in cases where it was never intended, people go all the way around to even slightly fit into what's existing. A Healthy System churns. An Unhealthy system is additive-only. Areas of code that don't see commits for a long time are smells. We are expected to keep every part of the system churning. Here is a wonderful article explaining this in detail . How teams iterate vs How they should, Every single day: TL;DR - Refactoring is part of each and every story. No code is untouchable 10. Bad Estimation Frequently we see really good teams/coders end up producing shit. We see their codebase and wonder ""WTF, was this really developed by that team/person I thought was awesome?"" Quality needs time and not just skill. And smart developers frequently overestimate their capability. Finally they end up taking ugly hacks to finish stuff on a self-committed suicide timeline. TL;DR - Bad Estimation destroys Quality even before a single line of code is written If you made it this far, thanks! Reminder that I'm only discussing over-engineering and not advocating cowboy coding. Further links:",en,255
8,1724,1468247818,CONTENT SHARED,-1297580205670251233,534764222466712491,-9077035980881675856,,,,HTML,https://viagensdealline.com/2013/08/20/a-minha-viagem-a-maternidade/,a minha viagem à maternidade #tetodomundo,"Já fazia uma semana, desde o dia 26 de dezembro, que eu não parava de sentir uma fraqueza horrível pelo corpo, um embrulho constante nas entranhas e um mau humor insuportável. O mundo todo tinha perdido suas cores, eu estava com uma sensação fortíssima de morte iminente. Era primeiro de janeiro de 1999, toda a minha família se reunia num resort no interior de São Paulo para comemorar a passagem do ano. Mas aquele réveillon eu não comemorei com eles, eu estava de cama. Havia dormido desde o dia 30 de dezembro e não conseguia levantar para nada. Quando minha mãe passou pelo meu quarto naquela manhã de ano novo, eu implorei a ela: - Mãe, eu estou falando muito sério com você. Eu preciso ir para um hospital urgente! Eu tenho certeza que algo muito grave está acontecendo comigo. Por favor, mãe, me ajuda! Mesmo diante de minha urgência hospitalar e de meu claro desespero, minha mãe, que é uma médica pediatra, respondeu: - Fica calma, minha filha... Olha, não tem nenhum hospital ou clínica aqui perto. Amanhã bem cedinho a gente já vai embora para casa e eu prometo que assim que chegar em Brasília a gente te leva direto para o hospital, está bem? Descansa só mais esse dia de hoje que amanhã tudo vai se esclarecer. ... E que você tenha o ano mais feliz da sua vida, minha filha! Não adiantava continuar insistindo. Eu ali no meu leito de morte e minha mãe sem tirar aquele sorriso insuportável do rosto. Mas eu estava tão fraca que não conseguia nem sentir vontade de esconjurá-la. Fiz a viagem de volta para Brasília como pude, aos trancos, buracos e barrancos. Sonhando com o prometido de que alguém me levaria para o hospital assim que chegasse e que algum médico me daria a cura para aquela sensação terrível que consumia meu corpo por completo. Triste ilusão. Assim que chegamos, ao invés de hospital, minha mãe recomendou a Leonardo, o meu namorado que estava viajando comigo, que passássemos direto numa farmácia e comprássemos um teste de gravidez. TESTE DE QUÊ??? Eu estava fisica e psicologicamente abalada demais para rir de uma palhaçada tão sem graça como aquela. Eu vinha de uma depressão crônica de vários anos, havia ficado um bom tempo sem nem menstruar por causa da doença. Eu comia bem, mas há muito que só emagrecia. Já estava pele e osso, sempre me considerando uma pessoa muito debilitada. Até nem tomava mais anticoncepcional porque as pílulas só pioravam o meu estado de saúde, tanto físico quanto mental. ""Eu nem me considero uma mulher completamente adulta ainda"" - pensava eu. ""Sou magra demais e não tenho nenhuma maturidade para ser mãe. Que óbvio que eu não estou grávida!"". Aceitei fazer a porcaria do teste só para que minha mãe tirasse aquele sorrisinho impertinente do rosto e me levasse finalmente direto para uma emergência. Fui até o banheiro igual a um zumbi, levando na mão o pacotinho com o tal do teste. Leonardo ficou esperando do lado de fora calado, com aquela cara de quem há dias não tinha a menor idéia do que estava acontecendo comigo. Fiz o meu xixi e o papelzinho indicador ficou completamente rosa. Dei uma olhada nas instruções para saber o que aquilo significava e a bula era bastante clara. Era simples demais. Azul, teste negativo. Rosa, teste positivo. Como assim positivo? O quê diabos isso significa?? Mostrei para o Leonardo e eu nunca vou esquecer o tamanho do sorriso que ele deu. Mas eu mesma, na minha cabeça não tinha nenhum pensamento muito claro passando por ali. Talvez desespero seja isso mesmo... Eu estava em choque, catatônica. Então de repente, não tinha mais nada embaixo dos meus pés. As coisas, parece que começavam a rodar na minha frente. Me deu uma vontade imensa de voltar atrás, de descobrir a fórmula para viajar no tempo e reverter todo aquele pesadelo absurdo. Como assim positivo?? Eu estou no meio de uma porção de planos para o futuro, para a minha carreira, para o meu crescimento! Inclusive, a minha lista de coisas para fazer esse ano está gigante! Não existe a menor possibilidade de aparecer uma criança a essa altura do campeonato. Eu tenho ZERO de estabilidade, sem condição nenhuma de sustentar um filho ainda por cima! A primeira imagem que veio à minha cabeça era do paredão de concreto de um viaduto escuro e mal cuidado. Numa noite de lua clara, onde eu estava sentada embaixo de uma tenda de papelão com aquela criança no meu colo, coberta só com um pano sujo qualquer. Lembro-me de ter visto a fome claramente naquela imagem. Um cansaço extremo. Um abandono absoluto da vida. Vou acabar de baixo da ponte... Pensei em quem seria o primeiro a morrer, se seria eu ou o bebê... Era óbvio que eu não iria conseguir sustentar aquela criança. Esse era o fim. ....... Lição número um da maternidade, e talvez a maior de todas as lições: A natureza é perfeita e implacável. E a nossa arrogância é uma total estupidez. Gente, eu já tinha 24 anos de idade! Como é que eu podia ser uma pessoa tão despreparada e imatura assim? Mas eu era. Os jovens de hoje são totalmente superprotegidos. Tanto fisica quanto psicologicamente, aos 24 anos qualquer um já está para lá de pronto para ter um filho. Como é que eu não sabia disso?? Quanta arrogância a minha, de achar que eu não estava pronta... Levaram-se pouquíssimos dias para eu começar a compreender o tamanho da sabedoria da natureza. Existe sim vida após o resultado positivo de um teste de farmácia. Dias se passaram e eu ainda não havia morrido! Na verdade, exatamente nada tinha mudado na minha rotina até então. ""Interessante..."" - pensei eu. Tudo ainda era muito confuso, eu ainda estava muito zonza. Mas assim que a ""catatonisse"" passou, lá por volta da primeira semana depois de tomar ciência dessa condição chamada gravidez, eu começava a perceber que um amor muito forte já brotava dentro de mim. Uma coisa muito diferente de tudo aquilo que eu havia sentido antes. Uma coisa bem mais profunda, parece que mais pura... Alguns outros poucos dias mais e então tudo na minha cabeça já mudava completamente. Assim que a maior de todas as responsabilidades bate a sua porta, você amadurece na marra. Ou talvez, só pára de segurar uma maturidade que na verdade sempre esteve ali, mas que nunca tinha sido preciso utilizá-la. ...... As lições da maternidade são diárias e diversas. Infelizmente, no caso da minha gravidez não foram só os três primeiros meses de mal estar e enjôo não. Eu passei todos os noves meses me sentindo um lixo. E minha mãe falou que com ela aconteceu o mesmo, tanto quando estava grávida de mim, quando de meu irmão. Eu me sentia tão mal, que nem tenho muitas lembranças do dia que fui fazer a ecografia para descobrir o sexo do bebê. Lembro só de o Léo ter ficado muito contente com o resultado. Mas para mim mesma, nada daquilo tinha a menor importância. Já por outro lado, também por causa da gravidez, Léo e eu decidimos nos casar. Um evento para mais de duzentos familiares e amigos. Foi o dia que eu me senti mais linda em toda a minha vida. Maquiagem, cabelo, unhas, sapato, hormônios a flor da pele... Tudo! Uma produção impecável. Lembro-me perfeitamente do exato momento da abertura do grande portal da igreja, lotada. Eu vinha com um véu de cinco metros de comprimento todo bordado, saindo gloriosamente de um Chevy Camaro esporte, vermelho cor de rubi. O meu vestido branco era deslumbrante, do jeito que toda a noiva deve se achar. Eu escolhi o estilo ""Julieta de Shakespeare"", porque no fundo eu sou uma romântica e também para aproveitar e já disfarçar a barriguinha que àquela altura estava para lá de saliente. Eu achei que meu coração fosse parar quando a música alta e majestal começou a tocar só por causa da minha presença. Havia uma multidão de olhos arregalados em minha direção. Eu vinha de braços dados com o meu pai, que naquele dia sentia um orgulho muito grande de mim. Podia ver meu irmão lá na frente, no altar, com seu lindo sorriso no rosto. E minha mãe com minha única avó ainda viva, magníficas! E já chorando, lógico. A festa foi muito simples, mas grande e extremamente alegre, lotada de pessoas que nos desejavam uma felicidade absoluta. Senti a presença muito forte de um primo amado que eu tinha perdido há pouco tempo, eu sabia que ele estava ali compartilhando aquele momento comigo. Mas voltando à história da gravidez, bem no dia do meu casamento os meus peitos estavam gigantes de tanto leite. Lindos, redondos e naturais. Para quem nunca teve peito nenhum, essa com certeza é uma das boas lembranças de quando se está grávida. Lembro-me de ter comprado umas coisinhas aqui e ali, de ter montado o quartinho do bebê com móveis doados pela família e reformados por mim com a ajuda da minha mãe. Por sinal, apoio sempre total na minha vida e diário durante aquela fase tão difícil. Fiz eu mesma a pátina do bercinho, da cômoda e do armário. Lembro-me de ter bordado quadrinhos infantis para pendurar na parede; do dia que estava em Goiânia comprando todo o enxoval; e lembro-me também de um churrasco na beira do lago que fizemos para barganhar fraldas descartáveis dos amigos (foto abaixo). Enchemos um armário inteiro só de fraldas. Mas lembro também de infecção urinária, azia, estresse, gases doloridos, estrias, aftas, inchaço, de no último mês estar com um barrigão tão grande que não conseguia mais ver nem meus pés nem minhas pernas, de estar sentido uma dificuldade enorme para respirar, comer, dormir, trabalhar, tomar banho, andar, mexer, sentar, levantar, conversar, pensar, sorrir, existir, ... O tamanho que estava ficando a minha barriga nas últimas semanas daquela contagem regressiva começou a me dar pânico. Só não surtei de vez porque na verdade quando não se respira direito, o raciocínio vai lá para baixo, e quando não se dá conta de pensar, isso inclui também não pensar besteira. Simplesmente vegetei, apavorada. ........ No dia 20 de agosto daquele mesmo ano, no exato dia do aniversário de 20 anos do meu irmão mais novo, eu acordei as 6 da manhã com uma dor intestinal meio chatinha, fui ao banheiro algumas vezes mas nada acontecia... Mimada como sempre fui, já liguei logo para a minha mamãezinha para pedir ajuda. - Mãe, que remédio eu posso tomar para dor de barriga? Resolvi perguntar, porque uma das lições sobre gravidez que a gente já aprende desde o começo, é que não se pode mais botar qualquer coisa para dentro de você. Tudo tem regra. - Quando isso começou, minha filha? - ""respondeu"" minha mãe com outra pergunta. - Tem menos de uma hora - disse eu. - Me deixa falar com o Léo. - Oi, Dona Cleusa... - disse Leonardo pegando o telefone. - Léo, vai começando a preparar uma malinha com as coisas dela e do bebê. Nós vamos aqui ligar para o obstetra para saber como ele quer proceder. Peguei o telefone da mão do Léo e disse já sem paciência: - Mãe, que parte da frase ""Eu estou só com uma dor de barriga"" você não está entendendo? - Filha... Contração é igual à dor de barriga. Essa foi a primeira lição daquele dia que seria longo... O recado do meu obstetra, que estava em meio a sua caminhada matinal, era para que eu aguardasse ainda mais algumas horas e entrasse em contato com ele lá pelo meio da manhã para reportar a evolução do caso. Eu acho que em menos de 15 minutos depois daquela conversa apavorante, eu já tinha feito a tal da malinha e já estava dentro do carro obrigando o Léo a me dirigir até o hospital no grito. Eu estava em um completo estado de pânico. Já no hospital foi quando eu comecei a sentir as primeiras contrações para valer. Muito curtas ainda, mas já dilacerantes. O meu obstetra chegou logo em seguida e me examinou de prontidão. Falou que estava tudo indo muito bem e que eu tinha que esperar dilatar mais não sei o quê, do não sei o quê lá. Foi quando então que eu implorei, por tudo o que era mais sagrado, confesso que num tom muito mais alto do que ele merecia, para que ele tirasse imediatamente aquela criança de dentro de mim. A minha barriga estava gigante e eu tinha muita certeza na minha cabeça que nunca que eu iria conseguir colocar tudo aquilo ali para fora sozinha. O Doutor Sebastião é um santo! Ele aguentou todos os meus pitis daquele dia com toda a serenidade de um grande amigo e de um profissional de extrema competência. Ele sabia que o meu bebê não tinha virado de cabeça para baixo, que estava com três voltas do cordão umbilical em seu pescoço e que o meu caso teria que ser Cesária de qualquer forma. Ele não me deixou sofrer absolutamente mais nada. Já deitada numa maca em direção ao centro cirúrgico, num movimento desconcertante, encarando aquele teto pálido e frio, cheio de luzes passando rapidamente pelos meus olhos, sem ter a mínima noção do que me aguardava lá na frente, foi então que eu senti um pânico ainda maior. O maior de todos na minha vida até então. Eu me lembro de estar chorando como uma louca desesperada. Só me acalmei quando o meu anjo da guarda veio, segurou na minha mão e falou bem baixinho ao meu ouvido: ""Pode se acalmar agora, eu estou aqui com você"". Era o meu primo Evandro, de apelido Deco, assistente do meu parto aquele dia. A sala que me colocaram estava vazia, só eu e ele ali. Ele me sentou na maca com o seu sorriso doce, sua voz mansa e seu tato de médico sábio. Depois chegou outro cara de branco, também com um sorriso, passou a mão pelas minhas costas através daquela camisola de doente, com a abertura atrás, em que eu estava vestida. Eu senti uma picadinha bem de leve na região da coluna. Deco ainda segurava a minha mão calmamente. O outro doutor fazia mais algumas coisas ali por trás que eu não tinha a menor idéia do que era porque eu não conseguia ver nada. Muito depois que eu fui saber que ele estava enfiando aquele agulhão gigante da anestesia peridural na minha espinha. Mas eu não sentia nada por causa da anestesia prévia que ele tinha me dado. Obrigada doutor. Eu implorava muito para o Deco me colocar para dormir urgentemente. Ele me perguntou se eu não ia querer ver o meu bebê assim que nascesse. Eu então surtei de vez. Só de cogitar a possibilidade de ver sangue, o meu coração já disparava para a boca. ""Deco, me coloca para dormir agora! Pelo amor de Deus!!!"" Ele então consentiu. Me deitou de volta na maca, embaixo de uma carrossel de luzes fortíssimas e começou a preparar umas coisinhas por ali, até que alguém botou um grande pano branco na minha frente, entre minha cabeça e minha barriga. Deco então falou: - Já vou te colocar para dormir em um minuto. - Eu não quero ver sangue! - eu dizia com terror. - Você não vai ver nada. Confie em mim. Eu senti um pequeno estalinho na barriga e bem rapidamente depois um jovem doutor saiu de trás do pano branco, levantou a minha mão, fez alguma coisa por ali e bem simpaticamente me pediu que contasse até dez. - Um... Eu não me lembro do dois. Também muito depois, fiquei sabendo que naquele momento em que eu estava crente que eles nem tinham começado a cirurgia ainda, na verdade já haviam aberto a minha barriga toda. Na minha cabeça, cirurgia era uma coisa que demorava horas e horas, mas a realidade é que a minha Cesária não durou nem trinta minutos. Por várias razões médicas, durante o parto a mãe só pode ficar inconsciente após a criança sair do útero. Aquele estalinho que eu senti já era o bebê se descolando de mim através do corte apertado. Tudo foi muito rápido. Eu dormi no segundo antes de ouvir o meu filho chorar. ........ Acordei e meus olhos se relutaram a abrir. Estava encostada na parede de um corredor do hospital, ainda deitada numa maca. Fiquei inconsciente por mais ou menos uma hora depois do parto. Quando minha visão desembaçou, a primeira coisa que apareceu na minha frente foi o sorriso enorme do meu pai. - Ooooooooooi! - ele falou. - Como você está se sentindo? - Eu sei lá... - respondi. Estava ainda sonolenta. Uma enfermeira (eu acho) me transportou até o quarto em que eu iria passar a noite. Só sei que em menos de uma hora, o efeito da anestesia começou a diminuir, e foi então que eu comecei a sentir a maior dor de todo esse mundo. A parte de baixo da minha barriga começou a latejar um pouquinho, depois foi piorando, e em questão de minutos era como se estivesse uma faca alucinada serrando a minha barriga de um lado para o outro sem parar, bem no lugar dos pontos e da faixa branca do curativo. Lá estava eu aos berros de novo. Já mandaram chamar logo o meu ""personal"" anjo da guarda. Deco, como todo anjo que se preze, aplicou na minha veia alguma droga sobrenatural, que teve o poder de acabar completamente com toda aquela dor que eu estava sentindo. Ele botou a mão no vidrinho do soro que escorria em mim, e em questão de milésimos de segundos uma grande onda percorreu todo o meu corpo, do primeiro fiozinho de cabelo até a pontinha da unha do meu pé. Uma grande onda de paz. Eu sorri de verdade pela primeira vez depois de longos nove meses. Na verdade, eu gargalhei! Foi bom demais. Logo então, meio que de repente, a porta do quarto se abriu. E nesse momento a visão mais maravilhosa do mundo surgiu magicamente à minha frente. Era um príncipe todo de branco. O príncipe mais encantado de todos os príncipes. ""O cavaleiro errante enviado pelo Universo para resgatar a minha vida das trevas da imaturidade. O salvador abençoado que veio inundar o vazio da minha existência com o beijo de seu amor eterno."" Ele tinha os olhinhos fechados, estavam vermelhos. A mãozinha era cheia de dedinhos. Que coisa MAIS LINDA! Mastercard que me perdoe, mas ver o rosto de seu filho pela primeira vez não tem preço. É o cataclisma de meses e meses de ansiedade pensando em como seria aquela imagem. O coração vai a mil. Mesmo quando injustamente ele é a cara do pai e não puxou nada de você, nada disso tem importância. ""O MEU filho é MUITO LINDO!"" - repetia eu sem conseguir parar. Acho que era uma mistura do efeito das drogas anestésicas com a visão do paraíso, tudo ao mesmo tempo. Veja a foto abaixo dos meus dois anjos juntinhos! Como que um dia passou pela minha cabeça que eu pudesse não estar pronta para esse momento? Quanta arrogância... Muito obrigada por essa grande lição, minha mãe natureza. ......... Hoje o meu filho faz quatorze anos. Eu ainda tento entender o que foi que eu fiz para merecer um filho tão perfeito. O bebê mais lindo, a criança mais doce, o menino mais inteligente, o pré-adolescente mais meigo, o companheiro mais amoroso, o futuro homem mais sensível que existe. Eu nunca pensei muito em ter filhos, mas sempre que imaginava como eles seriam, nem de longe passava pela minha cabeça que seria algo tão maravilhoso como o meu Luiz. O maior amor de toda a minha vida. Feliz aniversário, meu filho amado. Escrevi esse post de presente para você.",pt,253
9,2095,1471520397,CONTENT SHARED,-1633984990770981161,2195040187466632600,1000828687449358272,,,,HTML,https://medium.com/@roxrogge/ux-ou-ui-4c0a1bcb4b83,ux ou ui?,"UX ou UI? Tenho escutado essa pergunta com frequência e sempre me dói muito respondê-la. Há um bom tempo vemos vários comentários sobre UX Designer e UI Designer sem que as pessoas saibam, de fato, o que faz cada um desses profissionais. Mas antes de falarmos sobre as diferenças vamos rapidamente refletir e voltar uns 15 anos. Parece muito tempo, não? Mas você parou para pensar como passou rápido e como foi nossa evolução tecnológica? Em 2002, quando minha filha tinha um ano, eu e meu marido já estávamos preocupados com a segurança dela na adolescência. Um dia estávamos conversando e ele olhou para o seu incrível celular mega blaster, um motorola V120 do Guga, com rádio FM e gravador, e disse: - Não se preocupe, quando nossa filha tiver 15 anos o celular já vai ter câmera e vamos conseguir vê-la em real-time onde ela estiver! E não é que ele estava certo? Evoluiu muito rápido. E como você acha que aconteceu essa evolução? Foi apenas pela busca da beleza do produto ou pela necessidade do usuário? Nessa época as siglas UX e UI não eram tão populares para definir cargos: éramos todos Designers e fomos trabalhando mesmo sem uma denominação específica e muitos sem perceber o que já estavam fazendo. A necessidade do usuário e os avanços tecnológicos criaram uma demanda especializada. Mesmo que a experiência do usuário sempre tenha existido, ela começou a ganhar uma definição nos anos 90, quando Don Norman disse em uma entrevista: ""Eu inventei o termo experiência do usuário porque achava que interface do usuário e usabilidade eram muito restritos, eu queria cobrir todos os aspectos da experiência de uma pessoa com o sistema, incluindo design industrial, gráficos, a interface, a interação física e o manual. Desde então o termo tem se espalhado amplamente..."" [Don Norman / meados de 1990] Chegamos em um tempo em que existem milhares de apps no celular, smart watch, tablet, smart tv, IoT [internet das coisas] e a todo momento surgem novos produtos que usam interfaces e precisam de uma boa experiência para o produto ter vida no mercado. Nesta última década a área de design se desenvolveu com muita rapidez, e por isso evoluiu também o número de especialidades na profissão e as formas de denominá-las: Design, Web Design, Web-Dev, Design visual, Design de interface, IxD, Design de Interação, Design de Produto, UI, UI Design, UI/UX Design, Usabilidade, UX, UX Design, Design da Experiência do usuário, UX Strategy, UCD, Design Centrado no Usuário, Design de Aplicação, Design de Navegação, User Experience Researcher, Design Industrial, Design da Informação, AI, Information Architect, Arquiteto da Informação, Data-Informed Design, Data-Driven Design, Data-""whatever"" Design, Service Design, só para mencionar algumas. Tem design para tudo hoje em dia! As funções se tornam disciplinas isoladas entre suas especializações. E com isso há dúvidas na área e muitas pessoas não sabem que caminho seguir. Então afinal, qual a diferença entre UX e UI? De forma simplificada , please? UX, User Experience, Experiência do Usuário UX é toda a pesquisa para uma melhor interação e comportamento do site/app/produto/marca. Pense em pesquisa, testes, experiência, emoções, conteúdo, hierarquia, fluxos... UI, User Interface, Interface de Usuário UI é o visual do site/app/produto/marca. Pense em interação, harmonia visual, fonte, respiro, tipografia, cores, formas... Podemos dizer que em um determinado projeto, o UX pesquisa e testa como o produto vai dar prazer ao usuário, e o UI cuida da interface, a parte visual do produto. Mas e aí? UX é parte de UI? Ou UI é parte de UX? Eles andam de mãos dadas desde sempre. É impossível pensar um projeto sem pesquisa, sem saber quem vai usar o produto, sem pensar nas frustrações e motivações dos usuários. É o UX que direciona a escolha da UI. E a UI também determina a experiência. Usando uma metáfora simples: você não pode ir à partida de futebol no estádio de um time usando uma camiseta da mesma cor do time rival. Tem que verificar e pesquisar! Da mesma forma, um aplicativo não pode ter as cores do concorrente. ... Enfim, experiência e interface são multidisciplinares, envolvendo os aspectos da psicologia, neurociência, antropologia, ciência da computação, design gráfico, design industrial e ciência cognitiva. É preciso pesquisar, desenvolver empatia, e usar métricas. Sem pesquisa não há uma boa experiência do usuário e sem design não existirá uma boa interface, e sem os dois não existirá um produto inovador. As disciplinas não podem ser tratadas isoladamente. Seria tipo avião sem asa, fogueira sem brasa, futebol sem bola ou Piu-Piu sem Frajola! Como comentei no post anterior, sou apaixonada pela experiência do usuário e amo design. Sinceramente não consigo separar UX & UI e acredito que todo profissional de design já tenha os dois incorporados em seu trabalho, mesmo que não perceba. E minha filha tem um belo celular com câmera, com UI e UX dignos de 2016!",pt,249
10,2351,1473971457,CONTENT SHARED,3367026768872537336,-3535274684588209118,-6495023147849651556,,,,HTML,http://br.blog.trello.com/melhore-a-comunicacao-na-empresa/,seja esperto no trabalho: melhore a comunicação na empresa com 12 robôs,"Seja Esperto no Trabalho: Melhore a Comunicação na Empresa Com 12 Bots ""Eu odeio ferramentas que me ajudam a trabalhar com mais eficiência e auxiliam a melhorar a comunicação na empresa"", disse absolutamente ninguém, em momento algum. Entram em cena seus próprios minions. Robôs (ou bots, em inglês) são incríveis para a produtividade. Eles podem automatizar tarefas que você teria que fazer por sua conta, deixando tempo livre para focar nas coisas que realmente precisam do seu toque de humanidade. Robôs também te livram daquela sobrecarga de novos apps. Em vez de baixar mais um aplicativo para fazer uma tarefa específica - e acabar com um monte de novas notificações para verificar o tempo todo - você pode turbinar a utilidade dos aplicativos que já tem. É a mesma coisa que personalizar seu uso do Trello com uma miscelânea de aplicativos de terceiros e Power-Ups . Robôs baseados em textos podem ajudar a melhorar a comunicação na empresa e com sua equipe, ou engajar os usuários nos aplicativos que você (e eles) já usam - aplicativos de chat como o Slack, redes sociais como Twitter e WhatsApp, e até mesmo com seu e-mail ou com o ecossistema do seu smartphone. Você também pode usar os robôs para se manter nos apps e plataformas que mais usa, diminuindo as chances de seu cérebro ""dar uma viajada"" com essa constante mudança de contexto . Tem robôs para quase tudo no trabalho e para todas as plataformas que você usa no dia a dia. Se a repercussão sobre isso já chegou até você, mas não teve uma chance de experimentar, aqui tem alguns robôs para você se cadastrar e começar a melhorar a comunicação na empresa, poupar tempo e trabalho imediatamente: Robôs de Comunicação na Empresa 1. Slackbot Evidentemente esta lista começa com o robô do Slack, o Salckbot , um dos melhores robôs da nossa seleção. O Slackbot poderia ser facilmente classificado como um ""assistente pessoal robótico"", mas uma vez que ele vive na plataforma de mensagens do Slack, sua função principal é ajudar você e sua equipe a melhorar a comunicação na empresa e trabalhar com mais produtividade. Tem um monte de maneiras de programar o Slackbot, e o Slack até fornece um API para criar ""Slack bots"" (não confunda com o próprio Slackbot) para rodar dentro do Slack. Você pode começar criando respostas automáticas personalizadas para aquelas perguntas mais comuns que te tomam tempo, tais como ""Qual a senha do wi-fi?"" ou ""Que horas é a nossa reunião semanal de equipe, mesmo?"". 2. Robô Standup Muitas empresas e equipes usam reuniões diárias de atualização para alinhar o time sobre o andamento dos projetos atuais e superar possíveis obstáculos juntos. Se você trabalha com sua equipe a distância , ou tem membros que usam horários diferentes, ter um robô que automaticamente inicia, toca e resume reuniões diárias de alinhamento pode soar com um sonho. O Standup gera atualizações automáticas no Slack que não exigem reuniões cara a cara em tempo real, todo dia ou toda semana. Sem mais desculpas para não melhorar a comunicação na empresa! 3. Drift 2.0 O novo Driftbot permite que se respondam consultas no site usando inteligência artificial para ajudar a direcionar as perguntas certas para as pessoas certas, melhorando a comunicação na empresa para que ninguém desperdice tempo nem esforços. A integração Drift + Slack permite falar com usuários do site sem ter que sair do Slack, poupando o tempo que se perde mudando de uma plataforma para outra. Robôs ajudando humanos a interagir com outros seres humanos é mais do que suficiente para aquecer as válvulas de um coração artificial. Assistentes Pessoais ""Robóticos"" 4. x.ai Conheça a x.ai Amy, a toda poderosa assistente pessoal robótica que agenda seus compromissos quando você manda um e-mail para ela. Quando você recebe um pedido de reunião, você pode simplesmente mandar um e-mail para amy@x.ai e a robô automaticamente faz o vai e volta de e-mails ""Eu estou disponível entre as 2:00 e as 4:00, nos dias ímpares, e nunca antes das 9:00 na segunda terça-feira"" para agendar as reuniões. Municiada com sua agenda de compromissos e preferência de horários, Amy faz toda programação de um jeito realmente muito legal. 5. Jarvis Esta corujinha adorável é seu novo companheiro, um robô para o Facebook Messenger que vai te mandar lembretes de tarefas baseado nos pedidos que você faz naturalmente, em linguagem comum. Se você prefere não falar em voz alta ao celular para dar instruções ao Siri, Google ou Cortana para lembrá-lo de atualizar prazos de vencimento em seus cartões Trello toda quinta às 10:00 - ou se você trabalha com Mídias Sociais, ou mesmo é alguém que passa muito tempo no Messenger - assistentes pessoais baseados em texto podem fazer uma tonelada de sentido. 6. WorkLife Para muita gente, principalmente quem trabalha a distância e a galera de tecnologia, abrir o Slack é uma das primeiras coisas que você faz quando vai começar a trabalhar. O Robô WorkLife Slack garante que você não tenha que abrir seu aplicativo de agenda ou seus e-mails para saber onde você vai e com quem você tem reuniões naquele dia. O robô te manda toda manhã um resumo de suas próximas reuniões, inclusive tudo que precisa saber sobre qual linha usar para um ""conference call"" e os assuntos que serão tratados. Esse é o tipo de coisa que eu gosto de ver em um robô! Robôs de Dados e Desenvolvimento 7. Hubot O Hubot começou a vida como um robô interno do GitHub, mas agora está disponível em código aberto. Anunciado como ""o robô da sua empresa"", ele vem com alguns scripts pré-programados para ajudar em coisas como tradução e postagem de imagens, mas também pode ser personalizada com uma tonelada de scripts criados colaborativamente ou que você mesmo codificou. 8. Baremetrics bot Se você ama métricas e construir empresas SaaS , você provavelmente ama Baremetrics - e você provavelmente também vai amar o robô Métricas Baremetrics e Slack ! Esta integração envia informações cruciais sobre os clientes diretamente para o Slack, assim você nunca vai ter que abrir o Baremetrics em separado para verificar novas inscrições, LTV (Life Time Value), MRR (Receita Recorrente Mensal) ou taxas de churn (cancelamento) novamente. 9. Statsbot Como as melhores invenções, o Statsbot surgiu do desejo de seus criadores de solucionar um problema próprio. Eles se viam constantemente indo aos seus painéis de controle do Mixpanel , tirando uma foto de um gráfico ou tabela e depois anexando no Slack em resposta a perguntas ou para explicar uma opinião para a equipe. Então eles criaram o Statsbot, um robô de métricas que mantém você e sua equipe atualizados sobre as métricas do Google Analytics, Mixpanel e mais, direto no Slack, melhorando muito a comunicação na empresa. Robôs Para Produtividade Pessoal e da Equipe 10. Tomatobot O método de Pomodoro é um jeito bem legal de dividir seu dia em períodos mais produtivos e focados. Com tantas pessoas adotando essa ferramenta de trabalho baseada no tempo, não é de se admirar que haja um robô para isso. Apropriadamente chamado de Robô de Produtividade Tomatobot , ele envia seus avisos de tempo diretamente para o seu canal do Slack. Você também pode identificar mensagens específicas como distrações ou digitar o que terminou - no final de cada Pomodoro, o Tomatobot vai te mostrar as mensagens marcadas como distrações para você poder vê-las e também tudo que conseguiu fazer. 11. Ace Quantos galhos um castor poderia quebrar para você se ele fosse um robô que gerencia inteligentemente sua produtividade no Slack, como o Robô de Produtividade Ace pode fazer? Este inteligente robozinho (e seu avatar de castor) acompanha despesas, tarefas, votações e muito mais com facilidade, tudo no Slack. Pauleira esse negócio, hein? 12. Trello para Slack Se você é um usuário ativo tanto do Slack quanto do Trello, você provavelmente não conseguiria sequer contar quantas vezes por dia você transfere informação de um para o outro. Trello para Slack permite que você crie instantaneamente cartões Trello de dentro dos canais Slack (até com GIFs de gatinhos no meio do post!), atualizar cartões, anexar cartões a conversas, incluir pessoas em cartões e muito mais. Mais do que apenas comandos, este robô fornece vários botões para tarefas comuns do Trello, assim você pode ir fazendo o que precisa direto, sem mudar de ferramenta. Deu para entender agora? Com uma legião de robozinhos (seus minions!), só esperando para melhorar a comunicação na empresa, você vai ser muito mais produtivo, eficiente e cada vez mais admirado por todos! Postagens Relacionadas",pt,247
11,1162,1464708954,CONTENT SHARED,2857117417189640073,22763587941636338,8000922562339244508,,,,HTML,https://sprintstories.com/running-gv-sprints-inside-corporates-learn-from-my-mistakes-526f67c1960f?gi=e381d0ad2b1,running gv sprints inside corporates - learn from my mistakes - sprint stories,"Running GV sprints inside corporates - learn from my mistakes GV (Google Ventures) has just released Sprint , a book about their design sprint process. This seems like a good time to share what I've learned from facilitating more than 30 sprints with corporates here in New Zealand. I hope my experience will help you avoid some of the pitfalls. Where I've learned what I've learned I'm a graphic, UI, UX, interaction, product designer/manager ... I really don't know what to call myself anymore. After several years as a lead designer, I worked as a UX consultant from 2012 until the end of 2015. Through this work, I quickly realized that consulting was broken. We could deliver great work that the client loved, but when we walked out the door everything went back to normal. Our ideals of lean thinking, rapid iteration and user feedback walked out with us. We started experimenting with working on-site with the client running, Lean UX and GV sprints. Finally, we started making a difference ... but there were problems... Running a sprint on an obstacle course Running the sprint process was very challenging in corporate environments - like running a sprint on an obstacle course. Many of the problems we came across are interrelated and point to issues that are common, if not universal, to 'corporates'. I also made my own mistakes and, learning from it all, I've found better ways. The issues I faced, the mistakes I made, and what I learned... 1. The war surrounding the War Room The drama that occurs when you take over a space in a corporate office for more than an hour, let alone a week, can increase to stunning levels. Faced with this loss of precious meeting-room space, I've seen other teams turn hostile, bookings sabotaged and angry people standing over a poor PA's desk, demanding to know why the room was fully booked. I've seen someone literally stomping mad, pointing at us through the glass wall and ranting furiously to the colleague beside them. (We just kept our heads down and kept drawing.) Solutions Here's some of things I did to reduce the drama: Straight to the top . I asked the CEO's PA to book the meeting room in the CEO's name. No one complained. Hide it. We removed the room from the booking system and covered up the glass wall with large foam boards. Move it. Teams have favorite meeting rooms. If you book it for a week you'll annoy them for that week. If it stretches into two, then drama is imminent. Instead, move to a different meeting room each week. Avoid the drama . Don't use a bookable meeting room. Find a space with at least one wall you can plant yourself beside. Get creative ... you're likely in a big building with all sorts of space. 2. Attendance chaos You've got a War Room, now you need a team. The Decider can make cameos, but the rest need to be available close to full-time for five days. In some companies you'll work with a passionate team who have been given permission to drop everything else. Freed of their obligations, they walk in with a clear mind and tackle the sprint with enthusiasm. But in other companies the team has only been encouraged to take part. Without that real permission, their to-do list, inbox and regular meetings are forever on their mind. Despite working on critical business problems, and even with the CEO's blessing, they will ask to pop in and out and attend other meetings, which is disruptive to both the team and the process. And then there are the ones who attend, but are forever focused on their laptop (that they refuse to give up)... Despite briefings and emphasizing this commitment, I found people likened it to a long meeting - 'Surely they don't need me the whole time?' Solutions If you want people full-time for five days straight you might have to compromise. Break it up . Instead of five days in a row, try for five days over a few weeks. Don't run a sprint for less than seven hours a day though, as if it's shorter people tend to not be entirely present as their mind is busy preparing for the rest of their day. Compress it. For one client I compressed the first three days of the sprint into two consecutive days. I then prepared the prototypes by myself and got the team together for an hour to provide feedback. After making the changes, I got the team to observe the user studies. By showing the clients how the process worked, they became more interested and invested, with the result that they actually started backfilling people so the team could truly spend five days straight on a sprint. Both of these are not ideal, but if it's the difference between running most of the process or none of it ... compromise this little bit. 3. Saboteurs in plain sight Even Daniel Burka , a design partner at GV, admits he was a little skeptical about the process when he did his first sprint - but after just one it all made sense. Each group that walks into the War Room will have a mix of people: those who understand the principles and are thrilled to be trying something different; those who are on the fence, like Daniel; and those who actually don't want to be there. It is simply counterproductive having people who don't want to be there, no matter how useful their subject-matter expertise. They will clash with you on every activity you lead the group in, because they don't want to do it. They will question the process, and will re-litigate and debate every point. Solutions 1. Ask them to opt in . Insist you meet all the attendees one-on-one in the weeks before the sprint and have a quick chat. Cover the process, the way the team works, and why the company is trying it out. Ask them to choose whether to take part or not, but also to agree to the way they'll work. Most people I've warmed up like this have entered the sprint and at least given it a go. 2. Let them be experts. Instead of booking them in for the whole week, instead get them to pop in on Monday so the team can interview them. Tease out their knowledge and then release them back into the wild. 4. Responsibility avoidance In the first sprints we ran, we didn't realize we were making a fundamental mistake: when you're responsible for facilitating a process, and coaching/training people in that process, you should not also be responsible for a deliverable. One key concept of the sprint process is to set time constraints to propel the team forward. However, negative behaviors such as re-litigation, too much talking and not following agreed principles will limit progress. This usually happens because the team members don't feel that they're actually responsible for delivering a result. They instead feel that they're there to be subject-matter experts, or simply take part. At the end of one of these early sprints, due to this slow progress the resulting prototype that was tested was not ready, and therefore the tests were not conclusive. Because I was technically responsible for delivering a result, I was then open to criticism, and so was the process. Solutions Do not agree to deliver anything. You are facilitating a process, but the team as a whole must be responsible for the outcome . There are some ways you can encourage this shared responsibility: Principles . Print off key principles that the team should agree to at the start of the sprint and stick them to the wall. Remind the team of these principles whenever they waiver. Everyone's in until the end . It's easy for prototyping to fall onto the people who can build a prototype. This often feels like a good time for others to run off and tackle their inboxes, leaving the deadline responsibility to one or two people. But instead, everyone should stay in the room. The rest of the team can work on other things if they have to, but if the prototypers ask for anything - to source images, write copy, even get coffee - the others should help them. Manage saboteurs in hiding . Some people may say all the right things and start off taking part, but later show signs that they are not compelled by the looming deadline. This often occurs when someone has felt that the agreed direction doesn't match their preference. Their behavior can become disruptive, usually through debate and re-litigation. At its worst, others join their cause and consensus is lost. As the facilitator, it's your job to remind them of the agreed principles and ask the Decider to reiterate the decisions. They then need to agree to the principles again and continue, or leave the room. 5. The corporate immune system and ambiguity The corporate immune system has been well-documented . Simply put, the 'body' of the corporate doesn't like change and the individuals responsible for it - especially if they're outsiders - and fights against it. The issues above are symptoms of this, but I found the immune system to be visible in many other ways. For example: The Decider was influenced mid-sprint by politics outside the room and wanted to re-litigate everything. We were blocked mid-sprint from talking to five customers for our user studies because a team wanted to control 'the message' the customers were hearing. A team decided the way we were working was not the [CompanyName] way. A team felt that the sprint team were making them look bad by uncovering customer problems that they were responsible for. They became very hostile. Then there is ambiguity . This is totally what you learn as you get older as a designer. I've been doing this for almost 20 years. And the older you get and the more you do this, the more you get confident that you just don't know. And then you look for a process where you can learn what's working and what's not working more quickly. That's why I like sprints so much. It takes off that weight of needing to be right. It's okay, you don't know, just admit you don't know. That's fine. - Daniel Burka Ambiguity is fine. That's what the process is for. Teams should embrace ambiguity, ask good questions, examine the problem, and ensure they're testing and learning the right things to remove ambiguity as fast as possible. Each sprint builds confidence and removes risk. However, in most corporates ambiguity is seen as weakness. These companies train people that they should always have the answer. People are even rewarded for coming up with plausible-sounding solutions quickly, even when ignoring potentially catastrophic assumptions. (Watch Claudia Batten's great short talk on Solutions Allies in high places . As one person, you can't begin to tackle an entire organization's culture. Ask the CEO (or the most senior person you have access to) to tackle these issues head on, and do your best to keep the sprint team focused on their deadlines. Show your progress . Invite teams in to see how you're working and present what has been learned so far. Results, progress, data and transparency are your best defense against the immune system. Get an internal spokesperson. If you're an outsider that has come in to do the facilitating, do not become the voice of the process and progress. Get the team members from inside the company to present to the rest - this will have more impact with the skeptics. Get out. When you try to move a project quickly inside one of these companies you'll discover these obstacles and behaviors. Changing individual human behaviors is hard, changing a group culture is insurmountable by a sprint facilitator. If the above options aren't working and the culture is too toxic to let the team deliver any results, walk away. embracing the squiggly line .) Even during show and tell at the end of the sprint, I've heard someone comment that it was 'five days wasted' as they already had the solution. At least when they're brash enough to say it out loud the team could provide evidence to defend themselves; however, in most cases this opinion is only shared away from the team. Like nothing else, ambiguity encourages the immune system to poison attempts at change. The sprint team is the key As a facilitator, you have to focus on your sprint team and can't get involved in any drama occurring outside of the room - even if there's foot-stomping and fury. It's the people you let inside the room that will make or break the process. But, because the process represents change, this can lead to uncertainty, even within the sprint team. Again, results, progress, data and transparency are your best defense. But to comfort the team from day one so you get those results, they need to be prepared. Sprint warm-ups This is the process I used to warm up the team for my last series of sprints to guarantee the best results: I interviewed several stakeholders early on and got a sense of some of the internal issues the company was struggling with. I sat down with each sprint participant and talked about a range of issues and got them to identify the ones that they had encountered. I talked them through the sprint process, going into detail in places where it combated the issues they had identified. I asked them whether they felt it was worth experimenting with the process. They always said yes. I repeated it with the group at the start of the sprint, focusing on the issues specific to that company, and how the process can help. Throughout the sprint I asked for permission to move onto each next step to avoid any tension between me as facilitator and the team. I created an atmosphere where the team corrected each other when people broke agreed principles. I focused on coaching individuals rather than the group as a whole. If someone seemed to be struggling I'd ensure I talked to them one on one. The corporate experiment As you will have gathered, I had some frustrating experiences while running sprints with corporate clients. In some cases, despite learning so much and the prototypes resulting in great customer feedback, projects were cancelled and everyone moved on. The sprint process was not designed with the corporate obstacle course in mind, and for a while I felt it could never work. However, I've come out the other side and I truly believe this process and its underlying principles are the key, even in these organizations. If I was a CEO of a company struggling to innovate or solve critical problems, I would encourage a group to run this process as soon as possible. Not to deliver a result, but to highlight all the obstacles that the leadership had become blind to. If the leaders are determined and don't hesitate, they may be able to clear the obstacles in time for the sprint team to achieve something big ... in just five days .",en,241
12,1875,1469487944,CONTENT SHARED,8224860111193157980,6013226412048763966,5651952480198335454,,,,HTML,http://www.jornaldoempreendedor.com.br/destaques/inspiracao/psicologa-de-harvard-diz-que-as-pessoas-julgam-voce-em-segundos-por-esses-criterios/,psicóloga de harvard diz que as pessoas julgam você em segundos por esses critérios | jornal do empreendedor,"As pessoas avaliam você em segundos, mas o que exatamente eles estão avaliando? A professora de Harvard Business School, Amy Cuddy vem estudando as primeiras impressões ao lado dos colegas psicólogos Susan Fiske e Peter Glick por mais de 15 anos, e descobriu padrões nessas interações. Em seu novo livro, ""Presença"", Cuddy diz que as pessoas respondem rapidamente duas perguntas quando eles te encontram pela primeira vez: Posso confiar nesta pessoa? Eu posso respeitar esta pessoa? Os psicólogos referem-se a estas dimensões como cordialidade e competência, respectivamente, e, idealmente, você quer ser percebido tendo ambos. Curiosamente, Cuddy diz que a maioria das pessoas, especialmente em um contexto profissional, acreditam que a competência é o fator mais importante . Afinal, eles querem provar que eles são inteligentes e talentosos o suficiente para lidar com o seu negócio. Mas, na verdade a cordialidade, ou confiabilidade, é o fator mais importante na forma como as pessoas avaliam você. ""De uma perspectiva evolucionária"", diz Cuddy, ""é mais crucial para a nossa sobrevivência saber se uma pessoa merece a nossa confiança."" Faz sentido quando você considera que para os homens das cavernas era mais importante descobrir se seu companheiro estava lá para matá-lo e roubar todos os seus bens ou se ele era competente o suficiente para construir um bom fogo com você. O novo livro de Cuddy explora formas para nos sentirmos mais confiantes. O livro já é best-seller na Amazon dos EUA e está aqui . Enquanto a competência é altamente valorizada, Cuddy diz que ela é avaliada apenas depois que a confiança é estabelecida. E, que se concentrar demais em exibir a sua força pode sair pela culatra. A competência é altamente valorizada mas é avaliada apenas depois que a confiança é... Click To Tweet Cuddy diz que estudantes de MBA estão muitas vezes tão preocupados em parecer inteligentes e competentes que isso pode levá-los a ignorar eventos sociais, não pedir ajuda, e geralmente parecer inacessível. ""Uma pessoa calorosa, confiável que também é forte provoca admiração, mas só depois que você estabelece a confiança é que sua força se torna um dom e não uma ameaça."" Estes overachievers podem se frustrar ao não receberem a oferta de emprego porque ninguém os conheceu melhor para confiar neles como pessoas. ""Se alguém que você está tentando influenciar não confiar em você, você não vai chegar muito longe. Na verdade, você pode até provocar suspeitas porque você parecer apenas um grande manipulador"", diz Cuddy. Amy fez um TED Talk muito interessante e está legendado abaixo: Artigo da Business Insider .",pt,236
13,2265,1473105755,CONTENT SHARED,7507067965574797372,-4028919343899978105,-3666565354944070559,,,,HTML,http://gizmodo.uol.com.br/disputa-tabs-vs-espacos/,um bilhão de arquivos mostram quem vence a disputa tabs vs. espaços entre programadores,"Esta é uma das maiores batalhas já travadas entre os programadores: você deveria usar a tecla tab ou cinco espaços ao recuar linhas em seu código-fonte? * Como aprender programação? * Aprenda mais sobre programação usando este baralho com trechos de código O debate, em última análise, se resume à forma como o código-fonte é exibido ao ser editado. A razão pela qual desenvolvedores se irritam é que o código fica desajeitado se o mesmo método não for usado em todo o arquivo. Isto pode se tornar especialmente complicado quando várias pessoas trabalham no mesmo projeto. O debate já se arrasta há tanto tempo que os programadores se referem a outros como ""gente de tab"" e ""gente de espaço"". Isso foi até mencionado em um episódio recente da série Silicon Valley , da HBO: Richard prefere tabs e fica irritado em ver Winnie - que trabalha para o Facebook - usando espaços. O breve relacionamento acaba por causa disso! O chileno Felipe Hoffa, que trabalha para o Google, decidiu encontrar um vencedor para este debate. Assim, ele analisou 1 bilhão de arquivos entre 14 diferentes linguagens de programação para descobrir qual método é realmente mais popular e, em seguida, publicou os resultados em um post no Medium . Os dados vieram de arquivos do GitHub armazenados no BigQuery . Ele removeu duplicatas e arquivos com menos de 10 linhas de código. O desenvolvedor também deu um voto por arquivo: assim, se o desenvolvedor usou abas e espaços, o voto foi para o método usado mais frequentemente. Por fim, os 400.000 repositórios principais foram classificados pelo número de estrelas que receberam no GitHub entre janeiro e maio de 2016. Eis os resultados: Como você pode notar a partir dos dados, o vencedor aqui são os espaços. Em toda grande linguagem de programação (fora C), os espaços foram mais utilizados nos arquivos mais populares no GitHub. Esta evidência é a mais definitiva em revelar se tabs ou espaços são mais populares no código-fonte. Mas por quê? Alguns dirão que é simples: os espaços serão exibidos da mesma forma em qualquer hardware ou software de visualização de texto, e são mais visíveis do que tabs. Enquanto isso, tabs são mais organizados e geram código-fonte mais enxuto. Sinto que o debate espaços vs. tabs ainda está longe de acabar. Imagens por HBO e Medium/@hoffa",pt,233
14,2000,1470418766,CONTENT SHARED,-6156751702010469220,3302556033962996625,769925043682591828,,,,HTML,https://blog.codinghorror.com/the-broken-window-theory/,the broken window theory,"In a previous entry , I touched on the broken window theory. You might be familiar with the Pragmatic Progammers' take on this : Don't leave ""broken windows"" (bad designs, wrong decisions, or poor code) unrepaired. Fix each one as soon as it is discovered. If there is insufficient time to fix it properly, then board it up. Perhaps you can comment out the offending code, or display a ""Not Implemented"" message, or substitute dummy data instead. Take some action to prevent further damage and to show that you're on top of the situation. We've seen clean, functional systems deteriorate pretty quickly once windows start breaking. There are other factors that can contribute to software rot, and we'll touch on some of them elsewhere, but neglect accelerates the rot faster than any other factor. That's excellent advice for programmers, but it's not the complete story. The broken window theory is based on an Atlantic Monthly article published in 1982. It's worth reading the article to get a deeper understanding of the human factors driving the theory: Second, at the community level, disorder and crime are usually inextricably linked, in a kind of developmental sequence. Social psychologists and police officers tend to agree that if a window in a building is broken and is left unrepaired, all the rest of the windows will soon be broken. This is as true in nice neighborhoods as in rundown ones. Window-breaking does not necessarily occur on a large scale because some areas are inhabited by determined window-breakers whereas others are populated by window-lovers; rather, one unrepaired broken window is a signal that no one cares, and so breaking more windows costs nothing. (It has always been fun.) Philip Zimbardo, a Stanford psychologist, reported in 1969 on some experiments testing the broken-window theory. He arranged to have an automobile without license plates parked with its hood up on a street in the Bronx and a comparable automobile on a street in Palo Alto, California. The car in the Bronx was attacked by ""vandals"" within ten minutes of its ""abandonment."" The first to arrive were a family--father, mother, and young son--who removed the radiator and battery. Within twenty-four hours, virtually everything of value had been removed. Then random destruction began--windows were smashed, parts torn off, upholstery ripped. Children began to use the car as a playground. Most of the adult ""vandals"" were well-dressed, apparently clean-cut whites. The car in Palo Alto sat untouched for more than a week. Then Zimbardo smashed part of it with a sledgehammer. Soon, passersby were joining in. Within a few hours, the car had been turned upside down and utterly destroyed. Again, the ""vandals"" appeared to be primarily respectable whites. Untended property becomes fair game for people out for fun or plunder and even for people who ordinarily would not dream of doing such things and who probably consider themselves law-abiding. Because of the nature of community life in the Bronx--its anonymity, the frequency with which cars are abandoned and things are stolen or broken, the past experience of ""no one caring""--vandalism begins much more quickly than it does in staid Palo Alto, where people have come to believe that private possessions are cared for, and that mischievous behavior is costly. But vandalism can occur anywhere once communal barriers--the sense of mutual regard and the obligations of civility--are lowered by actions that seem to signal that ""no one cares."" There's even an entire book on this subject . What's fascinating to me is that the mere perception of disorder-- even with seemingly irrelevant petty crimes like graffiti or minor vandalism -- precipitates a negative feedback loop that can result in total disorder: We suggest that ""untended"" behavior also leads to the breakdown of community controls. A stable neighborhood of families who care for their homes, mind each other's children, and confidently frown on unwanted intruders can change, in a few years or even a few months, to an inhospitable and frightening jungle. A piece of property is abandoned, weeds grow up, a window is smashed. Adults stop scolding rowdy children; the children, emboldened, become more rowdy. Families move out, unattached adults move in. Teenagers gather in front of the corner store. The merchant asks them to move; they refuse. Fights occur. Litter accumulates. People start drinking in front of the grocery; in time, an inebriate slumps to the sidewalk and is allowed to sleep it off. Pedestrians are approached by panhandlers. At this point it is not inevitable that serious crime will flourish or violent attacks on strangers will occur. But many residents will think that crime, especially violent crime, is on the rise, and they will modify their behavior accordingly. They will use the streets less often, and when on the streets will stay apart from their fellows, moving with averted eyes, silent lips, and hurried steps. ""Don't get involved."" For some residents, this growing atomization will matter little, because the neighborhood is not their ""home"" but ""the place where they live."" Their interests are elsewhere; they are cosmopolitans. But it will matter greatly to other people, whose lives derive meaning and satisfaction from local attachments rather than worldly involvement; for them, the neighborhood will cease to exist except for a few reliable friends whom they arrange to meet. Programming is insanely detail oriented, and perhaps this is why: if you're not on top of the details, the perception is that things are out of control, and it's only a matter of time before your project spins out of control. Maybe we should be sweating the small stuff .",en,221
15,3091,1487127440,CONTENT SHARED,1469580151036142903,-4465926797008424436,1603046601233039757,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.37 Safari/537.36",SP,BR,HTML,https://dev.to/raddikx/dont-document-your-code-code-your-documentation,don't document your code. code your documentation.,"This is one of the great discussions among developers: document or not document your code? Is it worth writing documentation in your code? I thought this topic was completely overcome and it was clear that, except some few occasions (implementing a public API), documentation was not necessary. Until I saw a code review where the reviewer pointed out the lack of documentation as an issue. Really? I was one of those who used to document my code... or at least I tried. I was so convinced that code had to be documented. As a backup or reminder for my future myself or any other developer luck enough to end up in my code. Although I always realized it was always out of date. And by then, I already wondered: what is the purpose of documenting the code if the documentation is always outdated? Until several years ago I read the book Clean Code . I saw it ""crystal clear"", there is no need to document your code if you code your documentation. With this I mean to use meaningful variable and method names. If the name of the member already tells you the information that is keeping and the name of the method tells you what the method is doing you can end up reading the code without the need to figure out or document what your code is doing. Extract as much code as you can to methods. Even if you end up having a method with only 3 or 4 lines. Each method should do one thing and only one thing. And the name must explain what it does. Each member of a class must have a name that only reading it you know which information you can find there. Same for variables and input parameters. Following this simple steps you can have a code you can read, having the documentation right in the same code. Yes, I know, there are those times you have to implement code with a complex algorithm or you are copying a code you found on Internet which might be complex, you might not understand and you might not extract in simple and meaningful methods. Yes, there are always exception. What do you think? Do you document or write the documentation in your code? This post was originally published in Medium",en,209
16,2629,1477329558,CONTENT SHARED,-4333957157636611418,-7496361692498935601,-4955259521885105809,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36",SP,BR,HTML,https://business.stackoverflow.com/blog/why-programmers-want-private-offices,why programmers want private offices,"Ask any of your employees or coworkers what they think makes a workplace attractive and you're bound to get a variety of results. Some may refer to the benefits, while others are more concerned about the standard working hours. Developers, however, often will bring up the physical work environment when asked this question. But why? The type of work that developers do day in and day out requires a space that's noiseless with minimal interruptions. Distractions such as phone calls, chatty coworkers, or constant questions from colleagues completely interrupt the developer's flow of work and kill their productivity. For them to perform optimally, they need space and quiet. It's as simple as that. Let's start off with a few findings from various workplace productivity studies. In the book Peopleware , writers Tom DeMarco and Timothy Lister discuss their public productivity survey, which showed that ""The top quartile of participants, those who did the exercise most rapidly and effectively, worked in a space that was substantially different from that of the bottom quartile. The top performers' space was quieter, more private, better protected from interruption, and larger."" Additionally, IBM and architect Gerald McCue studied the work habits of developers in their current workspaces, as well as mock-ups of proposed (more private) workspaces. They found that the minimum accommodation for each developer would be ""100 square feet of dedicated space per worker, 30 square feet of work surface per person, and noise protection in the form of enclosed offices."" Why Do Private Offices Matter for Job Candidates? When a developer hears about a new job (whether they applied themselves or were passively recruited), they usually have the luxury of being picky. Since there's so much demand for them - multiple jobs for every unique developer - they want to ensure they are making the right choice by choosing a company that fits their values and needs. For many developers, one of those values is a top-notch private workspace. Our CEO Joel Spolsky said it best -- ""Put yourself in the job candidate's shoes. Company number 1 shows you a big crowded room, with a bunch of desks shoved in tightly, lots of marketing guys shouting on the phone next to the programmers and a bunch of sales jocks shouting tasteless jokes. Company number 2 shows you through a quiet hallway, a sunlit, plush office with a window and a door that closes. All else being equal, which job are you going to take?"" But What If We Don't Have Permission/Room/Budget for Private Offices? When companies do a cost/benefit analysis for improving the working space, they often focus too much on the cost. This is likely because the costs are easy to calculate, while the benefits are not. The potential benefits, which could be things like increased productivity, reduced turnover rate, increased revenue, often would outweigh the associated costs if calculated properly. How does your company calculate these benefits? That's the million-dollar question. According to Peopleware , ""the entire cost of workspace for one developer is a small percentage of the salary paid to the developer. In general, it varies in the range from 6 to 16 percent. For a programmer working in company-owned space, you should expect to pay $15 directly to the worker for every dollar you spend on space and amenities. If you add the cost for employee benefits, the total investment in the worker could easily be 20 times the cost of his or her workplace."" While not every company can afford to implement private offices for their developers (especially startups with a small budget), it's certainly not impossible. Lots of companies have invested in their developers and cut costs in other areas to make it work, giving them a competitive advantage in the technical hiring market.",en,192
17,1216,1464879718,CONTENT SHARED,880612740433495828,6013226412048763966,3831181614891772761,,,,HTML,http://vocerh.uol.com.br/noticias/entrevista/o-que-voce-deve-fazer-para-se-tornar-um-lider-melhor.phtml,o que você deve fazer para se tornar um líder melhor?,"Para ser um grande coach, você deve fazer mais perguntas em vez de tentar dar todas as respostas Marcelo Nóbrega* Bom líder: executivos em posição gerencial sentem-se compelidos a rapidamente orientar suas equipes no que e como fazer | Crédito: Pexels O segredo de todo bom líder é ter uma equipe de sucesso - é o que dizem nove entre dez gurus da administração. E como armar uma equipe de sucesso? Dando coaching. E como ser um grande coach e fazer coaching bem feito? A resposta é surpreendentemente simples. Você deve fazer mais perguntas em vez de tentar dar todas as respostas. É o que ensina Michael Bungay Stainer no livro The Coaching Habit (ainda sem tradução para o português). Michael é consultor de empresas, coach e autor de vários livros. Em suas próprias palavras, seu trabalho é ajudar executivos a causar maior impacto trabalhando menos. De forma Inteligente, sagaz, articulada e bem-humorada, Michael apresenta as sete perguntas que transformarão a maneira como você lidera sua equipe. Executivos em posição gerencial sentem-se compelidos a rapidamente orientar suas equipes no que e como fazer. Se você ocupa uma posição gerencial, tenho certeza de que sua reação quando um colaborador lhe apresenta um problema é tentar resolvê-lo, dar conselhos, agir e apagar o incêndio. Afinal, você é o chefe e sabe mais, certo? O que você não percebe é que, agindo desta maneira, se torna um gargalo para o desempenho e aprendizado da equipe. Coaching e feedback frequentes são mais poderosos do que as avaliações de desempenho tradicionais que são realizadas apenas uma ou duas vezes por ano. Diante de tantos cursos, textos, propagandas, certificações etc ... é fácil ficar confuso com o que é coaching. Que tal começar lendo este livro? Coaching não é dar conselhos ou resolver problemas, mas saber fazer as perguntas certas para ajudar o coachee a encontrar sua maneira de superar um obstáculo. Dedique mais tempo a ouvir para que encontre ele mesmo suas soluções. O resultado valerá a pena. E você trabalhará menos. Como? Comece perguntando: ""Porque você veio me procurar?"" - assim você define o foco da conversa. Siga com: ""O que mais?"" - explore um pouco pois raramente a primeira resposta é a verdadeira razão. Depois foque novamente: ""Qual seu verdadeiro desafio?"" - a real dificuldade ficará explícita e, se houver, o seu papel na solução também. Esse é apenas o início de uma fórmula simples para transformar qualquer conversa. Aprenda com The Coaching Habit: coaching é uma atitude. E você pode ser coach de sua equipe, pares, chefe e de você mesmo. * Este artigo é de autoria de Marcelo Nóbrega (diretor de recursos humanos da Arcos Dourados) e não representa necessariamente a opinião da revista",pt,191
18,1557,1467123962,CONTENT SHARED,-5002383425685129595,-3390049372067052505,2471338745689678750,,,,HTML,http://www.mckinsey.com/global-themes/leadership/changing-change-management,changing change management,"Research tells us that most change efforts fail. Yet change methodologies are stuck in a predigital era. It's high time to start catching up. Change management as it is traditionally applied is outdated. We know, for example, that 70 percent of change programs fail to achieve their goals, largely due to employee resistance and lack of management support. We also know that when people are truly invested in change it is 30 percent more likely to stick. While companies have been obsessing about how to use digital to improve their customer-facing businesses, the application of digital tools to promote and accelerate internal change has received far less scrutiny. However, applying new digital tools can make change more meaningful-and durable-both for the individuals who are experiencing it and for those who are implementing it. The advent of digital change tools comes at just the right time. Organizations today must simultaneously deliver rapid results and sustainable growth in an increasingly competitive environment. They are being forced to adapt and change to an unprecedented degree: leaders have to make decisions more quickly; managers have to react more rapidly to opportunities and threats; employees on the front line have to be more flexible and collaborative. Mastering the art of changing quickly is now a critical competitive advantage. For many organizations, a five-year strategic plan-or even a three-year one-is a thing of the past. Organizations that once enjoyed the luxury of time to test and roll out new initiatives must now do so in a compressed period while competing with tens or hundreds of existing (and often incomplete) initiatives. In this dynamic and fast-paced environment, competitive advantage will accrue to companies with the ability to set new priorities and implement new processes quicker than their rivals. The power of digital to drive change Large companies are increasingly engaged in multiple simultaneous change programs, often involving scores of people across numerous geographies. While traditional workshops and training courses have their place, they are not effective at scale and are slow moving. B2C companies have unlocked powerful digital tools to enhance the customer journey and shift consumer behavior. Wearable technology, adaptive interfaces, and integration into social platforms are all areas where B2C companies have innovated to make change more personal and responsive. Some of these same digital tools and techniques can be applied with great effectiveness to change-management techniques within an organization. Digital dashboards and personalized messages, for example, can build faster, more effective support for new behaviors or processes in environments where management capacity to engage deeply and frequently with every employee is constrained by time and geography. Digitizing five areas in particular can help make internal change efforts more effective and enduring. 1. Provide just-in-time feedback The best feedback processes are designed to offer the right information when the recipient can actually act on it. Just-in-time feedback gives recipients the opportunity to make adjustments to their behavior and to witness the effects of these adjustments on performance. Consider the experience of a beverage company experiencing sustained share losses and stagnant market growth in a highly competitive market in Africa. The challenge was to motivate 1,000-plus sales representatives to sell with greater urgency and effectiveness. A simple SMS message system was implemented to keep the widely distributed sales reps, often on the road for weeks at a time, plugged into the organization. Each rep received two to three daily SMS messages with personalized performance information, along with customer and market insights. For example, one message might offer feedback on which outlets had placed orders below target; another would alert the rep to a situation that indicated a need for increased orders, such as special events or popular brands that were trending in the area. Within days of implementing the system, cross-selling and upselling rates increased to more than 50 percent from 4 percent, and within the first year, the solution delivered a $25 million increase in gross margin, which helped to swing a 1.5 percent market-share loss into a 1 percent gain. 2. Personalize the experience Personalization is about filtering information in a way that is uniquely relevant to the user and showing each individual's role in and contribution to a greater group goal. An easy-to-use system can be an effective motivator and engender positive peer pressure. This worked brilliantly for a rail yard looking to reduce the idle time of its engines and cars by up to 10 percent. It implemented a system that presented only the most relevant information to each worker at that moment, such as details on the status of a train under that worker's supervision, the precise whereabouts of each of the trains in the yard, or alerts indicating which train to work on. Providing such specific and relevant information helped workers clarify priorities, increase accountability, and reduce delays. 3. Sidestep hierarchy Creating direct connections among people across the organization allows them to sidestep cumbersome hierarchal protocols and shorten the time it takes to get things done. It also fosters more direct and instant connections that allow employees to share important information, find answers quickly, and get help and advice from people they trust. In the rail-yard example, a new digital communications platform was introduced to connect relevant parties right away, bypassing middlemen and ensuring that issues get resolved quickly and efficiently. For example, if the person in charge of the rail yard has a question about the status of an incoming train, he or she need only log into the system and tap the train icon to pose the question directly to the individuals working on that train. Previously, all calls and queries had to be routed through a central source. This ability to bridge organizational divides is a core advantage in increasing agility, collaboration, and effectiveness. 4. Build empathy, community, and shared purpose In increasingly global organizations, communities involved in change efforts are often physically distant from one another. Providing an outlet for colleagues to share and see all the information related to a task, including progress updates and informal commentary, can create an important esprit de corps . Specific tools are necessary to achieve this level of connectivity and commitment. Those that we have seen work well include shared dashboards, visualizations of activity across the team, ""gamification"" to bolster competition, and online forums where people can easily speak to one another (for example, linking a Twitter-like feed to a work flow or creating forums tied to leaderboards so people can easily discuss how to move up in the rankings). This approach worked particularly well with a leading global bank aiming to reduce critical job vacancies. The sourcing team made the HR process a shared experience, showing all stakeholders the end-to-end view-dashboards identifying vacancies; hiring requisitions made and approved; candidates identified, tested, and interviewed; offers made and accepted; and hire letters issued. This transparency and openness built a shared commitment to getting results, a greater willingness to deliver on one's own step in the process, and a greater willingness to help one another beyond functional boundaries. 5. Demonstrate progress Organizational change is like turning a ship: the people at the front can see the change but the people at the back may not notice for a while. Digital change tools are helpful in this case to communicate progress so that people can see what is happening in real time. More sophisticated tools can also show individual contributions toward the common goal. We have seen how this type of communication makes the change feel more urgent and real, which in turn creates momentum that can help push an organization to a tipping point where a new way of doing things becomes the way things are done. Digital tools and platforms, if correctly applied, offer a powerful new way to accelerate and amplify the ability of an organization to change. However, let's be clear: the tool should not drive the solution. Each company should have a clear view of the new behavior it wants to reinforce and find a digital solution to support it. The best solutions are tightly focused on a specific task and are rolled out only after successful pilots are completed. The chances of success increase when management actively encourages feedback from users and incorporates it to give them a sense of ownership in the process. About the author(s) Boris Ewenstein is a principal in McKinsey's Johannesburg office, where Wesley Smith is a consultant and Ashvin Sologar is an associate principal.",en,190
19,600,1461698179,CONTENT SHARED,6044362651232258738,6013226412048763966,4613953821207157764,,,,HTML,http://vocesa.uol.com.br/noticias/acervo/cinco-competencias-comportamentais-para-voce-ser-um-bom-lider.phtml,cinco competências comportamentais para você ser um bom líder,"escritório | Crédito: pixabay Por que algumas pessoas se tornam bem-sucedidas e outras não? Essa era a pergunta que incomodava Marcelo Veras, professor de planejamento de carreira e presidente da Inova Business School, de Campinas. Para ele, livros, workshops e cursos não eram o suficiente para responder à questão, que ouvia constantemente dos alunos em sala de aula. ""Queria saber o que as pessoas que de fato chegaram ao topo tinham a dizer sobre isso"", afirma. Foi assim que, em julho de 2006, começou uma pesquisa sobre o assunto. Procurou pessoas em posições importantes de liderança, como diretores nacionais, vice-presidentes e presidentes para fazer uma pergunta simples: ""Quais competências o trouxeram até aqui e como você definiria cada uma delas?"". Foram mais de 170 entrevistados desde então. A partir das respostas, Marcelo reuniu uma lista das principais habilidades apontadas pelos bem-sucedidos. Elas são divididas em três categorias: comportamentais (como agimos em relação a nós mesmos e às pessoas); técnicas (domínio da área de atuação e de competências básicas de linguagem e leitura); e de gestão, que, claro, têm a ver com nossa atitude na condição de líderes de pessoas e de negócios. Marcelo compara essas competências a um macarrão à bolonhesa. As competências técnicas são o espaguete, as comportamentais, o molho e o resultado final, as competências de gestão, são o macarrão à bolonhesa. ""No curto prazo, ter apenas algumas competências funciona, mas, no fim, só uma combinação sólida é que mantém os líderes em seus cargos"", diz. Ou seja, as habilidades fazem mais sentido quando combinadas entre si e usadas de forma coerente. E, claro, dificilmente alguém terá todas elas superdesenvolvidas, mas criará um conjunto sólido delas - a sua própria receita. ""Querer desenvolver todas as competências no mesmo grau é utopia"", diz Adriana Prates, presidente da Dasein Executive Search, consultoria de recrutamento, de Belo Horizonte. ""As pessoas são diferentes e vão se destacar por diferentes motivos."" O segredo é identificar quais são as mais importantes para você. Ter essa clareza nem sempre é fácil, até porque envolve aceitar as limitações que temos, além de um conhecimento aprofundado de si mesmo. Esse entendimento serve, inclusive, para ver quando vale mais melhorar os pontos fortes e deixar os fracos de lado. Tudo isso demanda saber escutar os outros e receber bem os feedbacks, além de criar o hábito de pensar sobre si mesmo. Para fazer isso, Silvana Mello, diretora da Lee Hecht Harrison, consultoria de transição de carreira, de São Paulo, propõe um exercício de autoanálise baseado em três dimensões. A primeira é tentar definir o que se busca em termos de carreira e vida no longo prazo. A segunda é entender por que você busca esses objetivos e o que motiva suas atitudes. A terceira é pensar como você fará para alcançar esses objetivos e que valores usará para chegar lá. ""Gosto desse modelo de tripé porque ele serve para buscar uma coerência no dia a dia e se obrigar a questionar sempre para onde você está indo e como"", diz Silvana. De fato, um dos principais fatores que determinam o sucesso de uma empreitada é a clareza sobre por que se está fazendo aquilo. Mas, além disso, é preciso olhar para fora e notar como você está se comportando em relação ao meio em que atua. Isso significa prestar atenção ao que está acontecendo e identificar quais as demandas implícitas e explícitas das empresas e de seus colegas. Normalmente, o melhor sinal de que é preciso desenvolver uma competência é quando você percebe que não é (ou não foi) capaz de lidar tão bem quanto gostaria com uma situação. Expor-se a diferentes cenários - dentro ou fora do trabalho - facilita esse aprendizado. ""Saia da rotina de vez em quando para perceber coisas novas"", diz Paula Chimenti, professora do Coppead, escola de negócios da Universidade Federal do Rio de Janeiro. É a melhor forma de perceber o que você ainda precisa melhorar e o que já tem de bom. E ter essa percepção é o que ajuda na motivação. ""É difícil desenvolver algo se você não sentiu a necessidade"", diz. Por isso, é preciso ter um olhar constantemente voltado à melhoria e ao crescimento pessoal para dar conta de notar seus pontos fortes e fracos. O desenvolvimento de competências não é um processo isolado, mas combinado a diversos fatores: seus objetivos e personalidade, a necessidade dos outros e o meio em que você quer crescer. ""A relação entre competência e o contexto é inseparável"", diz Roberto Aylmer, professor da Fundação Dom Cabral, de Minas Gerais. O que vai diferenciar cada um são as atitudes, ou seja, as competências comportamentais. Já as competências técnicas são obtidas por meio de estudo e aprendizado contínuo. As de gestão são como você combina as anteriores de forma a ser um líder bem-sucedido. A seguir você encontra os cinco ingredientes relacionados com o comportamento que mais levaram as pessoas ao sucesso. ""Você não tem como controlar os problemas, mas pode melhorar a forma como reage a eles"", diz Adriana Prates, da Dasein. Ter autonomia em relação aos sentimentos para escolher como vai se comportar faz parte do equilíbrio emocional. Para chegar a esse ponto, é preciso ser capaz de entender suas próprias emoções - que não devem ser suprimidas ou ignoradas, mas geridas. Assistir a si mesmo no dia a dia e perceber como você se sente e quais tipos de situação trazem determinadas reações é uma forma de melhorar essa habilidade. Saber, por exemplo, que você tende a ficar alterado com um tipo de cenário pode ajudá-lo a resolver o que causa aquilo e a monitorar ocasiões futuras. A resposta pode ser desde encontrar válvulas de escape, como um hobby, até fazer terapia ou mudar o ponto de vista. Se for difícil perceber onde estão seus pontos frágeis, peça a opinião de pessoas em quem você confia. ""É como exercício físico, não dá para fazer um tempo e depois parar"", diz Adriana. ""Ser flexível é aceitar o desconhecido"", diz Silvana Mello, da Lee Hecht Harrison. Sair da zona de conforto é difícil, mas essencial para ter flexibilidade para encarar coisas novas e mudar de ideia. ""Tenha em mente que sempre podemos crescer mais, e que para isso precisamos conhecer o novo"", afirma. Fora do trabalho, vale desenvolver essa característica sempre que possível, se colocando em situações diferentes. A experiência diversificada, aliás, o ajudará a perceber quando você deve ser mais firme e quando deve mudar de ideia. Ser humilde em relação ao quanto você mesmo sabe sobre as coisas é importante. ""Hoje as pessoas estão muito mimadas e pouco flexíveis"", diz Adriana. ""Essa é com certeza uma competência que fará toda diferença nas empresas."" Mostre essa habilidade tendo abertura a opiniões diferentes das suas e mantendo o debate focado em ideias, e não em pessoas. O objetivo deve ser conseguir o melhor para todos - e não ter razão sempre. 3. Comprometimento e Execução Dificilmente uma pessoa que deixa tarefas para a última hora e não consegue pensar nos resultados chegará ao topo. Mas, muitas vezes, é difícil manter um desempenho consistente, porque isso exige clareza sobre seus objetivos. ""Quando você vê no trabalho um meio para atingir um fim, é mais fácil encontrar o comprometimento"", diz Adriana. Se você não cumpre seus prazos com a eficiência e rapidez que deveria, reavalie o que o impede de se envolver com aquela tarefa. Aplique o mesmo comprometimento na vida pessoal: não se atrase para encontros e cumpra sua palavra. Como qualquer hábito, o senso de urgência funciona melhor quando entra na rotina. 4. Etiqueta pessoal e profissional ""Etiqueta é saber reduzir a dissonância, a diferença entre você e o ambiente"", diz Roberto Aylmer, da Fundação Dom Cabral. ""Quando a pessoa lê o ambiente e adota uma postura e linguagem compatíveis, pode ter uma grande força de integração."" É verdade que as primeiras impressões também têm a ver com a forma como nos portamos. Uma pessoa mais gentil passará uma impressão de maior credibilidade, ao mesmo tempo em que uma pessoa supercompetente mas pouco respeitosa causará uma péssima impressão. O jeito é prestar atenção em quem é admirado, estudar regras de etiqueta e, na dúvida, perguntar aos outros sobre qual a maneira apropriada de se vestir e de se comportar em certos ambientes. 5. Relacionamento e Network Saber trabalhar com os outros para um objetivo em comum é requisito básico, não importa a sua posição. Se você não faz ideia de como o que você fala pode ser recebido pelos outros, é mau sinal. ""É necessário entender o impacto que causa nas pessoas e não querer sempre que seu desejo prevaleça"", afirma Márcia Portazio, coordenadora do ESPM Carreiras, de São Paulo. E quem tem boa capacidade de relacionamento não faz discriminação e trata do mesmo modo o estagiário e o presidente - com atenção e respeito. Se você não está convencido sobre isso, pense também que as posições hoje podem rapidamente se inverter. O network precisa ser pensado a partir do que você oferecerá para o outro, e não só do que o outro pode trazer a você. Esta matéria faz parte da reportagem ""20 competências essenciais para você ser um bom líder"" Você S/A | Edição 212 | Março de 2016",pt,184
20,580,1461608361,CONTENT SHARED,5238119115012015307,-3390049372067052505,6068971232842203123,,,,HTML,https://hbr.org/2016/05/embracing-agile,embracing agile,"Idea in Brief The Problem Agile methods such as scrum, kanban, and lean development are spreading beyond IT to other functions. Although some companies are scoring big improvements in productivity, speed to market, and customer and employee satisfaction, others are struggling. The Root Cause Leaders don't really understand agile. As a result, they unwittingly continue to employ conventional management practices that undermine agile projects. The Solution Learn the basics of agile. Understand the conditions in which it does or doesn't work. Start small and let it spread organically. Allow ""master"" teams to customize it. Employ agile at the top. Destroy the barriers to agile behaviors. Agile innovation methods have revolutionized information technology. Over the past 25 to 30 years they have greatly increased success rates in software development, improved quality and speed to market, and boosted the motivation and productivity of IT teams. Now agile methodologies-which involve new values, principles, practices, and benefits and are a radical alternative to command-and-control-style management-are spreading across a broad range of industries and functions and even into the C-suite. National Public Radio employs agile methods to create new programming. John Deere uses them to develop new machines, and Saab to produce new fighter jets. Intronis, a leader in cloud backup services, uses them in marketing. C.H. Robinson, a global third-party logistics provider, applies them in human resources. Mission Bell Winery uses them for everything from wine production to warehousing to running its senior leadership group. And GE relies on them to speed a much-publicized transition from 20th-century conglomerate to 21st-century ""digital industrial company."" By taking people out of their functional silos and putting them in self-managed and customer-focused multidisciplinary teams, the agile approach is not only accelerating profitable growth but also helping to create a new generation of skilled general managers. The spread of agile raises intriguing possibilities. What if a company could achieve positive returns with 50% more of its new-product introductions? What if marketing programs could generate 40% more customer inquiries? What if human resources could recruit 60% more of its highest-priority targets? What if twice as many workers were emotionally engaged in their jobs? Agile has brought these levels of improvement to IT. The opportunity in other parts of the company is substantial. But a serious impediment exists. When we ask executives what they know about agile, the response is usually an uneasy smile and a quip such as ""Just enough to be dangerous."" They may throw around agile-related terms (""sprints,"" ""time boxes"") and claim that their companies are becoming more and more nimble. But because they haven't gone through training, they don't really understand the approach. Consequently, they unwittingly continue to manage in ways that run counter to agile principles and practices, undermining the effectiveness of agile teams in units that report to them. These executives launch countless initiatives with urgent deadlines rather than assign the highest priority to two or three. They spread themselves and their best people across too many projects. They schedule frequent meetings with members of agile teams, forcing them to skip working sessions or send substitutes. Many of them become overly involved in the work of individual teams. They talk more than listen. They promote marginal ideas that a team has previously considered and back-burnered. They routinely overturn team decisions and add review layers and controls to ensure that mistakes aren't repeated. With the best of intentions, they erode the benefits that agile innovation can deliver. Further Reading Innovation Digital Article IT's most famous idea didn't start in IT. Innovation is what agile is all about. Although the method is less useful in routine operations and processes, these days most companies operate in highly dynamic environments. They need not just new products and services but also innovation in functional processes, particularly given the rapid spread of new software tools. Companies that create an environment in which agile flourishes find that teams can churn out innovations faster in both those categories. From our work advising and studying such companies, we have discerned six crucial practices that leaders should adopt if they want to capitalize on agile's potential. 1. Learn How Agile Really Works Some executives seem to associate agile with anarchy (everybody does what he or she wants to), whereas others take it to mean ""doing what I say, only faster."" But agile is neither. (See the sidebar ""Agile Values and Principles."") It comes in several varieties, which have much in common but emphasize slightly different things. They include scrum, which emphasizes creative and adaptive teamwork in solving complex problems; lean development, which focuses on the continual elimination of waste; and kanban, which concentrates on reducing lead times and the amount of work in process. One of us (Jeff Sutherland) helped develop the scrum methodology and was inspired to do so in part by ""The New New Product Development Game,"" a 1986 HBR article coauthored by another of us (Hirotaka Takeuchi). Because scrum and its derivatives are employed at least five times as often as the other techniques, we will use its methodologies to illustrate agile practices. There are at least a dozen agile innovation methodologies, which share values and principles but differ in their emphases. Experts often combine various approaches. Here are three of the most popular forms and the contexts in which each works best. Guiding Principles Empower creative, cross-functional teams Visualize workflows and limit work in process Eliminate waste from the system as a whole Favorable Conditions for Adoption Creative cultures with high levels of trust and collaboration, or Radical innovation teams that want to change their working environment Process-oriented cultures that prefer evolutionary improvements with few prescribed practices Process-oriented cultures that prefer evolutionary improvements with overarching values but no prescribed practices Prescribed Roles Initiative owners responsible for rank ordering team priorities and delivering value to customers and the business Process facilitators who guide the work process Small, cross-functional, innovation teams None None Prescribed Work Rules Five events: Sprint planning to prepare for the next round of work Fixed time sprints of consistent duration (1-4 weeks) to create a potentially releasable product increment Daily stand-ups of 15 minutes to review progress and surface impediments Sprint reviews that inspect the new working increment Sprint retrospectives for the team to inspect and improve itself Three deliverables (or ""artifacts""): Portfolio backlog, a fluid and rank-ordered list of potential innovation features Sprint backlog, the subset of portfolio backlog items selected for completion in the next sprint Releasable working increments Start with what you do now Visualize workflows and stages Limit the work in process at each development stage Measure and improve cycle times None Approach to Cultural Change Quickly adopt minimally prescribed practices, even if they differ substantially from those in the rest of the organization Master prescribed practices and then adapt them through experimentation Respect current structures and processes Increase visibility into workflows Encourage gradual, collaborative changes Respect current structures and processes Stress agile values throughout the organization while minimizing organizational resistance Advantages Facilitates radical breakthroughs while (unlike skunkworks) retaining the benefits of operating as part of the parent organization Delivers the most valuable innovations earliest Rapidly increases team happiness Builds general management skills Avoids clashes with the parent organization's culture Maximizes the contributions of team members through flexible team structures and work cycles Facilitates rapid responses to urgent issues through flexible work cycles Optimizes the system as a whole and engages the entire organization Provides the ultimate flexibility in customizing work practices Challenges Leaders may struggle to prioritize initiatives and relinquish control to self-managing teams New matrix-management skills are required to coordinate dozens or hundreds of multi-disciplinary teams Fixed iteration times may not be suitable for some problems (especially those that arise on a daily basis) Some team members may be underutilized in certain sprint cycles Practitioners must figure out how best to apply most agile values and principles Wide variation in practices can complicate the prioritization of initiatives and coordination among teams When initiatives don't succeed, it can be hard to determine whether teams selected the wrong tools or used the right tools in the wrong ways Novices trying to change behaviors may find the lack of prescriptive methodologies frustrating Evolutionary improvements can make radical breakthroughs less likely and major improvements less rapid Leaders need to make the grind of continuously eliminating waste feel inspirational and fun Source Darrell K. Rigby, Jeff Sutherland, and Hirotaka Takeuchi From ""Embracing Agile,"" April 2016 The fundamentals of scrum are relatively simple. To tackle an opportunity, the organization forms and empowers a small team, usually three to nine people, most of whom are assigned full-time. The team is cross-functional and includes all the skills necessary to complete its tasks. It manages itself and is strictly accountable for every aspect of the work. The team's ""initiative owner"" (also known as a product owner) is ultimately responsible for delivering value to customers (including internal customers and future users) and to the business. The person in this role usually comes from a business function and divides his or her time between working with the team and coordinating with key stakeholders: customers, senior executives, and business managers. The initiative owner may use a technique such as design thinking or crowdsourcing to build a comprehensive ""portfolio backlog"" of promising opportunities. Then he or she continually and ruthlessly rank-orders that list according to the latest estimates of value to internal or external customers and to the company. The initiative owner doesn't tell the team who should do what or how long tasks will take. Rather, the team creates a simple road map and plans in detail only those activities that won't change before execution. Its members break the highest-ranked tasks into small modules, decide how much work the team will take on and how to accomplish it, develop a clear definition of ""done,"" and then start building working versions of the product in short cycles (less than a month) known as sprints. A process facilitator (often a trained scrum master) guides the process. This person protects the team from distractions and helps it put its collective intelligence to work. The process is transparent to everyone. Team members hold brief daily ""stand-up"" meetings to review progress and identify roadblocks. They resolve disagreements through experimentation and feedback rather than endless debates or appeals to authority. They test small working prototypes of part or all of the offering with a few customers for short periods of time. If customers get excited, a prototype may be released immediately, even if some senior executive isn't a fan, or others think it needs more bells and whistles. The team then brainstorms ways to improve future cycles and prepares to attack the next top priority. In 2001, 17 rebellious software developers (including Jeff Sutherland) met in Snowbird, Utah, to share ideas for improving traditional ""waterfall"" development, in which detailed requirements and execution plans are created up front and then passed sequentially from function to function. This approach worked fine in stable environments, but not when software markets began to change rapidly and unpredictably. In that scenario, product specifications were outdated by the time the software was delivered to customers, and developers felt oppressed by bureaucratic procedures. The rebels proposed four new values for developing software, described principles to guide adherence to those values, and dubbed their call to arms ""The Agile Manifesto."" To this day, development frameworks that follow these values and principles are known as agile techniques. Here is an adapted version of the manifesto: People Over Processes and Tools Projects should be built around motivated individuals who are given the support they need and trusted to get the job done. Teams should abandon the assembly-line mentality in favor of a fun, creative environment for problem solving, and should maintain a sustainable pace. Employees should talk face-to-face and suggest ways to improve their work environment. Management should remove impediments to easier, more fruitful collaboration. Working Prototypes Over Excessive Documentation Innovators who can see their results in real market conditions will learn faster, be happier, stay longer, and do more-valuable work. Teams should experiment on small parts of the product with a few customers for short periods, and if customers like them, keep them. If customers don't like them, teams should figure out fixes or move on to the next thing. Team members should resolve arguments with experiments rather than endless debates or appeals to authority. Respond to Change Rather Than Follow a Plan Most detailed predictions and plans of conventional project management are a waste of time and money. Although teams should create a vision and plan, they should plan only those tasks that won't have changed by the time they get to them. And people should be happy to learn things that alter their direction, even late in the development process. That will put them closer to the customer and make for better results. Customer Collaboration Over Rigid Contracts Time to market and cost are paramount, and specifications should evolve throughout the project, because customers can seldom predict what they will actually want. Rapid prototyping, frequent market tests, and constant collaboration keep work focused on what they will ultimately value. Compared with traditional management approaches, agile offers a number of major benefits, all of which have been studied and documented. It increases team productivity and employee satisfaction. It minimizes the waste inherent in redundant meetings, repetitive planning, excessive documentation, quality defects, and low-value product features. By improving visibility and continually adapting to customers' changing priorities, agile improves customer engagement and satisfaction, brings the most valuable products and features to market faster and more predictably, and reduces risk. By engaging team members from multiple disciplines as collaborative peers, it broadens organizational experience and builds mutual trust and respect. Finally, by dramatically reducing the time squandered on micromanaging functional projects, it allows senior managers to devote themselves more fully to higher-value work that only they can do: creating and adjusting the corporate vision; prioritizing strategic initiatives; simplifying and focusing work; assigning the right people to tasks; increasing cross-functional collaboration; and removing impediments to progress. 2. Understand Where Agile Does or Does Not Work Agile is not a panacea. It is most effective and easiest to implement under conditions commonly found in software innovation: The problem to be solved is complex; solutions are initially unknown, and product requirements will most likely change; the work can be modularized; close collaboration with end users (and rapid feedback from them) is feasible; and creative teams will typically outperform command-and-control groups. In our experience, these conditions exist for many product development functions, marketing projects, strategic-planning activities, supply-chain challenges, and resource allocation decisions. They are less common in routine operations such as plant maintenance, purchasing, sales calls, and accounting. And because agile requires training, behaviorial change, and often new information technologies, executives must decide whether the anticipated payoffs will justify the effort and expense of a transition. Conditions Favorable Unfavorable Market Environment Customer preferences and solution options change frequently. Market conditions are stable and predictable. Customer Involvement Close collaboration and rapid feedback are feasible. Customers know better what they want as the process progresses. Requirements are clear at the outset and will remain stable. Customers are unavailable for constant collaboration. Innovation Type Problems are complex, solutions are unknown, and the scope isn't clearly defined. Product specifications may change. Creative breakthroughs and time to market are important. Cross-functional collaboration is vital. Similar work has been done before, and innovators believe the solutions are clear. Detailed specifications and work plans can be forecast with confidence and should be adhered to. Problems can be solved sequentially in functional silos. Modularity of Work Incremental developments have value, and customers can use them. Work can be broken into parts and conducted in rapid, iterative cycles. Late changes are manageable. Customers cannot start testing parts of the product until everything is complete. Late changes are expensive or impossible. Impact of Interim Mistakes They provide valuable learning. They may be catastrophic. Agile innovation also depends on having a cadre of eager participants. One of its core principles is ""Build projects around motivated individuals. Give them the environment and support they need, and trust them to get the job done."" When the majority of a company, a function, or a team chooses to adopt agile methodologies, leaders may need to press the holdouts to follow suit or even replace them. But it's better to enlist passionate volunteers than to coerce resisters. OpenView Venture Partners, a firm that has invested in about 30 companies, took this path. Having learned about agile from some of the companies in its portfolio, Scott Maxwell, the firm's founder, began using its methodologies at the firm itself. He found that they fit some activities more easily than others. Agile worked well for strategic planning and marketing, for instance, where complex problems can often be broken into modules and cracked by creative multidisciplinary teams. That wasn't the case for selling: Any sales call can change a representative's to-do list on the spot, and it would be too complicated and time-consuming to reassemble the sales team, change the portfolio backlog, and reassign accounts every hour. Maxwell provided the companies in OpenView's portfolio with training in agile principles and practices and let them decide whether to adopt the approach. Some of them immediately loved the idea of implementing it; others had different priorities and decided to hold off. Intronis was one fan. Its marketing unit at the time relied on an annual plan that focused primarily on trade shows. Its sales department complained that marketing was too conservative and not delivering results. So the company hired Richard Delahaye, a web developer turned marketer, to implement agile. Under his guidance the marketing team learned, for example, how to produce a topical webinar in a few days rather than several weeks. (A swiftly prepared session on CryptoLocker malware attracted 600 registrants-still a company record.) Team members today continue to create calendars and budgets for the digital marketing unit, but with far less line-item detail and greater flexibility for serendipitous developments. The sales team is much happier. 3. Start Small and Let the Word Spread Large companies typically launch change programs as massive efforts. But the most successful introductions of agile usually start small. They often begin in IT, where software developers are likely to be familiar with the principles. Then agile might spread to another function, with the original practitioners acting as coaches. Each success seems to create a group of passionate evangelists who can hardly wait to tell others in the organization how well agile works. The adoption and expansion of agile at John Deere, the farm equipment company, provides an example. George Tome, a software engineer who had become a project manager within Deere's corporate IT group, began applying agile principles in 2004 on a low-key basis. Gradually, over several years, software development units in other parts of Deere began using them as well. This growing interest made it easier to introduce the methodology to the company's business development and marketing organizations. In 2012 Tome was working as a manager in the Enterprise Advanced Marketing unit of the R&D group responsible for discovering technologies that could revolutionize Deere's offerings. Jason Brantley, the unit head, was concerned that traditional project management techniques were slowing innovation, and the two men decided to see whether agile could speed things up. Tome invited two other unit managers to agile training classes. But all the terminology and examples came from software, and to one of the managers, who had no software background, they sounded like gibberish. Tome realized that others would react the same way, so he tracked down an agile coach who knew how to work with people without a software background. In the past few years he and the coach have trained teams in all five of the R&D group's centers. Tome also began publishing weekly one-page articles about agile principles and practices, which were e-mailed to anyone interested and later posted on Deere's Yammer site. Hundreds of Deere employees joined the discussion group. ""I wanted to develop a knowledge base about agile that was specific to Deere so that anyone within the organization could understand it,"" Tome says. ""This would lay the foundation for moving agile into any part of the company."" Using agile techniques, Enterprise Advanced Marketing has significantly compressed innovation project cycle times-in some cases by more than 75%. One example is the development in about eight months of a working prototype of a new ""machine form"" that Deere has not yet disclosed. ""If everything went perfectly in a traditional process,"" Brantley says, ""it would be a year and a half at best, and it could be as much as two and a half or three years."" Agile generated other improvements as well. Team engagement and happiness in the unit quickly shot from the bottom third of companywide scores to the top third. Quality improved. Velocity (as measured by the amount of work accomplished in each sprint) increased, on average, by more than 200%; some teams achieved an increase of more than 400%, and one team soared 800%. Success like this attracts attention. Today, according to Tome, in almost every area at John Deere someone is either starting to use agile or thinking about how it could be used. 4. Allow ""Master"" Teams to Customize Their Practices Japanese martial arts students, especially those studying aikido, often learn a process called shu-ha-ri. In the shu state they study proven disciplines. Once they've mastered those, they enter the ha state, where they branch out and begin to modify traditional forms. Eventually they advance to ri, where they have so thoroughly absorbed the laws and principles that they are free to improvise as they choose. Mastering agile innovation is similar. Before beginning to modify or customize agile, a person or team will benefit from practicing the widely used methodologies that have delivered success in thousands of companies. For instance, it's wise to avoid beginning with part-time assignment to teams or with rotating membership. Empirical data shows that stable teams are 60% more productive and 60% more responsive to customer input than teams that rotate members. Further Reading Managing People Feature The Toyota principles can be applied in operations involving expertise. Over time, experienced practitioners should be permitted to customize agile practices. For example, one principle holds that teams should keep their progress and impediments constantly visible. Originally, the most popular way of doing this was by manually advancing colored sticky notes from the ""to-do"" column to ""doing"" to ""done"" on large whiteboards (known as kanban boards). Many teams are still devoted to this practice and enjoy having nonmembers visit their team rooms to view and discuss progress. But others are turning to software programs and computer screens to minimize input time and allow the information to be shared simultaneously in multiple locations. A key principle guides this type of improvisation: If a team wants to modify particular practices, it should experiment and track the results to make sure that the changes are improving rather than reducing customer satisfaction, work velocity, and team morale. Spotify, the music-streaming company, exemplifies an experienced adapter. Founded in 2006, the company was agile from birth, and its entire business model, from product development to marketing and general management, is geared to deliver better customer experiences through agile innovation. But senior leaders no longer dictate specific practices; on the contrary, they encourage experimentation and flexibility as long as changes are consistent with agile principles and can be shown to improve outcomes. As a result, practices vary across the company's 70 ""squads"" (Spotify's name for agile innovation teams) and its ""chapters"" (the company term for functional competencies such as user interface development and quality testing). Although nearly every squad consists of a small cross-functional team and uses some form of visual progress tracking, ranked priorities, adaptive planning, and brainstorming sessions on how to improve the work process, many teams omit the ""burndown"" charts (which show work performed and work remaining) that are a common feature of agile teams. Nor do they always measure velocity, keep progress reports, or employ the same techniques for estimating the time required for a given task. These squads have tested their modifications and found that they improve results. 5. Practice Agile at the Top Some C-suite activities are not suited to agile methodologies. (Routine and predictable tasks-such as performance assessments, press interviews, and visits to plants, customers, and suppliers-fall into this category.) But many, and arguably the most important, are. They include strategy development and resource allocation, cultivating breakthrough innovations, and improving organizational collaboration. Senior executives who come together as an agile team and learn to apply the discipline to these activities achieve far-reaching benefits. Their own productivity and morale improve. They speak the language of the teams they are empowering. They experience common challenges and learn how to overcome them. They recognize and stop behaviors that impede agile teams. They learn to simplify and focus work. Results improve, increasing confidence and engagement throughout the organization. A number of companies have reallocated 25% or more of selected leaders' time from functional silos to agile leadership teams. These teams rank-order enterprisewide portfolio backlogs, establish and coordinate agile teams elsewhere in the organization to address the highest priorities, and systematically eliminate barriers to their success. Here are three examples of C-suites that took up agile: 1. Catching up with the troops. Systematic, a 525-employee software company, began applying agile methodologies in 2005. As they spread to all its software development teams, Michael Holm, the company's CEO and cofounder, began to worry that his leadership team was hindering progress. ""I had this feeling that I was saying, 'Follow me-I'm just behind you,'"" he told us. ""The development teams were using scrum and were doing things differently, while the management team was stuck doing things the same old-fashioned way""-moving too slowly and relying on too many written reports that always seemed out-of-date. So in 2010 Holm decided to run his nine-member executive group as an agile team. The team reprioritized management activities, eliminating more than half of recurring reports and converting others to real-time systems while increasing attention to business-critical items such as sales proposals and customer satisfaction. The group started by meeting every Monday for an hour or two but found the pace of decision making too slow. So it began having daily 20-minute stand-ups at 8:40 am to discuss what members had done the day before, what they would do that day, and where they needed help. More recently the senior team began to use physical boards to track its own actions and the improvements coming from the business units. Other functions, including HR, legal, finance, and sales, now operate in much the same way. 2. Speeding a corporate transition. In 2015 General Electric rebranded itself as a ""digital industrial company,"" with a focus on digitally enabled products. Part of the transformation involved creating GE Digital, an organizational unit that includes all 20,000-plus of the company's software-related employees. Brad Surak, who began his career as a software engineer and is now GE Digital's COO, was intimately familiar with agile. He piloted scrum with the leadership team responsible for developing industrial internet applications and then, more recently, began applying it to the new unit's management processes, such as operating reviews. Surak is the initiative owner, and an engineering executive is the scrum master. Together they have prioritized backlog items for the executive team to address, including simplifying the administrative process that teams follow to acquire hardware and solving knotty pricing issues for products requiring input from multiple GE businesses. Agile Alliance: For guides to agile practices, links to ""The Agile Manifesto,"" and training videos Scrum Alliance: For a ""Scrum Guide,"" conference presentations and videos, and the ""State of Scrum"" research report ScrumLab Open: For training presentations, videos, webinars, and published papers Annual State of Agile Survey: For key statistics such as usage rates, customer benefits, barriers to adoption and success, and specific practices used The scrum team members run two-week sprints and conduct stand-up meetings three times a week. They chart their progress on a board in an open conference room where any employee can see it. Surak says, ""It takes the mystery out of what executives do every day. Our people want to know if we are in tune with what they care about as employees."" The team collects employee happiness surveys, conducts root cause analysis on the impediments to working more effectively, and reports back to people throughout the organization, saying (in effect), ""We heard you. Here is how we will improve things."" Surak believes that this shows the organization that ""executives work in the same ways as engineers,"" increasing employee motivation and commitment to agile practices. 3. Aligning departments and functions on a common vision. Erik Martella, the vice president and general manager of Mission Bell Winery, a production facility of Constellation Brands, introduced agile and helped it spread throughout the organization. Leaders of each department served as initiative owners on the various agile teams within their departments. Those individual teams achieved impressive results, but Martella worried that their time was being spread too thin and that department and enterprise priorities weren't always aligned. He decided to pull department leaders into an executive agile team focused on the enterprise initiatives that held the greatest value and the greatest opportunity for cross-functional collaboration, such as increasing process flows through the warehouse. The team is responsible for building and continually refining the backlog of enterprise priorities, ensuring that agile teams are working on the right problems and have sufficient resources. Team members also protect the organization from pet projects that don't deserve high priority. For instance, shortly after Martella started implementing agile, he received an e-mail from a superior in Constellation's corporate office suggesting that the winery explore a personal passion of the sender. Previously, Martella might have responded, ""OK, we'll jump right on it."" Instead, he replied that the winery was following agile principles: The idea would be added to the list of potential opportunities and prioritized. As it happened, the executive liked the approach-and when he was informed that his suggestion had been assigned a low priority, he readily accepted the decision. Working on agile teams can also help prepare functional managers-who rarely break out of their silos in today's overspecialized organizations-for general management roles. It exposes them to people in other disciplines, teaches collaborative practices, and underscores the importance of working closely with customers-all essential for future leaders. 6. Destroy the Barriers to Agile Behaviors Research by Scrum Alliance, an independent nonprofit with 400,000-plus members, has found that more than 70% of agile practitioners report tension between their teams and the rest of the organization. Little wonder: They are following different road maps and moving at different speeds. Here's a telling example: A large financial services company we examined launched a pilot to build its next mobile app using agile methodologies. Of course, the first step was to assemble a team. That required a budget request to authorize and fund the project. The request went into the batch of submissions vying for approval in the next annual planning process. After months of reviews, the company finally approved funding. The pilot produced an effective app that customers praised, and the team was proud of its work. But before the app was released, it had to pass vulnerability testing in a traditional ""waterfall"" process (a protracted sequence in which the computer code is tested for documentation, functionality, efficiency, and standardization), and the queue for the process was long. Then the app had to be integrated into core IT systems-which involved another waterfall process with a six-to-nine-month logjam. In the end, the total time to release improved very little. Further Reading Human resource management Magazine Article The Toyota story has been intensively researched and painstakingly documented, yet what really happens inside the company remains a mystery. Here's new insight into the unspoken rules that give Toyota its competitive edge. Here are some techniques for destroying such barriers to agile: Get everyone on the same page. Individual teams focusing on small parts of large, complex problems need to see, and work from, the same list of enterprise priorities-even if not all the teams responsible for those priorities are using agile processes. If a new mobile app is the top priority for software development, it must also be the top priority for budgeting, vulnerability testing, and software integration. Otherwise, agile innovations will struggle in implementation. This is a key responsibility of an executive team that itself practices agile. Don't change structures right away; change roles instead. Many executives assume that creating more cross-functional teams will necessitate major changes in organizational structure. That is rarely true. Highly empowered cross-functional teams do, by definition, need some form of matrix management, but that requires primarily that different disciplines learn how to work together simultaneously rather than separately and sequentially. Name only one boss for each decision. People can have multiple bosses, but decisions cannot. In an agile operating model it must be crystal clear who is responsible for commissioning a cross-functional team, selecting and replacing team members, appointing the team leader, and approving the team's decisions. An agile leadership team often authorizes a senior executive to identify the critical issues, design processes for addressing them, and appoint a single owner for each innovation initiative. Other senior leaders must avoid second-guessing or overturning the owner's decisions. It's fine to provide guidance and assistance, but if you don't like the results, change the initiative owner-don't incapacitate him or her. Focus on teams, not individuals. Studies by the MIT Center for Collective Intelligence and others show that although the intelligence of individuals affects team performance, the team's collective intelligence is even more important. It's also far easier to change. Agile teams use process facilitators to continually improve their collective intelligence-for example, by clarifying roles, teaching conflict resolution techniques, and ensuring that team members contribute equally. Shifting metrics from output and utilization rates (how busy people are) to business outcomes and team happiness (how valuable and engaged people are) also helps, as do recognition and reward systems that weight team results higher than individual efforts. Lead with questions, not orders. General George S. Patton Jr. famously advised leaders never to tell people how to do things: ""Tell them what to do, and they will surprise you with their ingenuity."" Rather than give orders, leaders in agile organizations learn to guide with questions, such as ""What do you recommend?"" and ""How could we test that?"" This management style helps functional experts grow into general managers, and it helps enterprise strategists and organizations evolve from silos battling for power and resources into collaborative cross-functional teams. Agile innovation has revolutionized the software industry, which has arguably undergone more rapid and profound change than any other area of business over the past 30 years. Now it is poised to transform nearly every other function in every industry. At this point, the greatest impediment is not the need for better methodologies, empirical evidence of significant benefits, or proof that agile can work outside IT. It is the behavior of executives. Those who learn to lead agile's extension into a broader range of business activities will accelerate profitable growth. A version of this article appeared in the May 2016 issue (pp.40-48, 50) of Harvard Business Review .",en,182
21,1578,1467219707,CONTENT SHARED,8901449108040307914,-6786856227257648356,4843271338129443641,,,,HTML,https://www.infoq.com/br/presentations/monitoramento-em-tempo-real-com-elasticsearch-e-kibana,monitoramento em tempo real com elasticsearch e kibana,"Resumo Descubra o que acontece com o seu código em produção, com Elasticsearch, Kibana e Codahale Metrics. Nessa apresentação explicarei como indexar em uma base nosql uma quantidade massiva de informações sobre a utilização do seu sistema, proporcionando uma forma de antecipar problemas e responder rapidamente sobre anomalias no seu código, em tempo real. Minibiografia Arquiteto de software a mais de 9 anos, projetando e conduzindo soluções complexas em grandes clientes. Formado na Universidade Federal de Uberlândia em 2002, iniciou sua carreira na Ci&T em 2004 e, desde 2014, atua como responsável pelo time de arquitetos na vertical de Resources & Logistics, garantindo que as práticas de arquitetura e tecnologias corretas sejam aplicada nestes projetos. A comunidade de desenvolvimento DevIsland organizou novamente o DEVDAY, um evento para desenvolvedores de software. Na sua sexta edição, o DEVDAY 2015 reuniu aproximadamente 500 desenvolvedores. Mais de 20 profissionais reconhecidos pelo mercado nacional que compartilharam seus conhecimentos nas 20 palestras do evento. Foram abordados diversos assuntos focados em desenvolvimento de software ou que de alguma forma afetam a vida de um desenvolvedor.",pt,179
22,1245,1464962305,CONTENT SHARED,2372438485070148864,3636910968448833585,-7707155044672153170,,,,HTML,https://www.infoq.com/articles/Continuous-Delivery-Maturity-Model,the continuous delivery maturity model,"The principles and methods of Continuous Delivery are rapidly gaining recognition as a successful strategy for true business agility. For many organizations the question is no longer ""why?"", but rather ""how?"" How do you start with Continuous Delivery, and how do you transform your organization to ensure sustainable results. This Maturity Model aims to give structure and understanding to some of the key aspects you need to consider when adopting Continuous Delivery in your organization. Why a maturity model? Continuous Delivery is all about seeing the big picture, to consider all aspects that affect the ability to develop and release your software. For any non-trivial business of reasonable size this will unfortunately include quite a lot of steps and activities. The end-to-end process of developing and releasing software is often long and cumbersome, it involves many people, departments and obstacles which can make the effort needed to implement Continuous Delivery seem overwhelming. Where do we start? Do we have to do everything, what parts can we leave out? Where do we get the biggest and quickest effect? These are questions that inevitably will come up when you start looking at implementing Continuous Delivery. This is why we created the Continuous Delivery Maturity Model, to give structure and understanding to the implementation of Continuous Delivery and its core components. Inspiration for this model comes mainly from our combined experiences in several continuous delivery implementation projects, and also from selected books, papers and blog-posts on the subject where Jez Humble & David Farley's groundbreaking book Continuous Delivery and Eric Minick & Jeffrey Fredricks terrific whitepaper Enterprise Continuous Delivery Model are two that deserve special recognition. With this model we aim to be broader, to extend the concept beyond automation and spotlight all the key aspects you need to consider for a successful Continuous Delivery implementation across the entire organization. A structured approach The journey that started with the Agile movement a decade ago is finally getting a strong foothold in the industry. Business leaders now have begun to embrace the fact that there is a new way of thinking about software development. A paradigm shift, if you like, that will inevitably segment the industry into those who struggle to keep up with competition, and those who have the ability to stay agile and ahead of competition by having an IT organization that responds quickly and efficiently, and serves as a true business enabler. IT can once again start pushing innovation instead of restraining it by expensive, slow, unpredictable and outdated processes. There are many ways to enter this new era and here we will describe a structured approach to attaining the best results. While agile methodologies often are described to best grow from inside the organization we have found that this approach also has limitations. Some parts of the organization are not mature enough to adapt and consequently inhibit development, creating organizational boundaries that can be very hard to break down. The best way to include the whole organization in the change is to establish a solid platform with some important prerequisites that will enable the organization to evolve in the right direction. This platform includes adopting specific tools, principles, methods and practices that we have organized into five key categories, Culture & Organization, Design & Architecture, Build & Deploy, Test & Versification and Information & Reporting. S tructuring Continuous Delivery implementation into these categories that follows a natural maturity progression will give you a solid base for a fast transformation with sustainable results. The purpose of the maturity model is to highlight these five essential categories, and to give you an understanding of how mature your company is. Your assessment will give you a good base when planning the implementation of Continuous Delivery and help you identify initial actions that will give you the best and quickest effect from your efforts. The model will indicate which practices are essential, which should be considered advanced or expert and what is required to move from one level to the next. Five levels The model defines five maturity levels: base, beginner, intermediate, advanced and expert. The model is calibrated with a typical industry average around the base level where we see most organizations are at today. Some organizations have beginner maturity in some categories and below base (outside the model) in others but on average base-level is the typical starting point for many organizations. The intermediate level is what we consider a mature level of continuous delivery practices where you benefit from the larger effects. The advanced level includes practices that will bring substantial value and effect to a higher effort. The last maturity level, expert, includes practices that will be very valuable for some organizations that want or need to specialize in certain practices. The levels are not strict and mandatory stages that needs to be passed in sequence, but rather should serve as a base for evaluation and planning. It is however important to try to keep the overall maturity level fairly even and to keep in mind that big changes may cause skepticism and reluctance in the organization, so an incremental approach to moving through the levels is recommended. Five categories The model also defines five categories that represent the key aspects to consider when implementing Continuous Delivery. Each category has it's own maturity progression but typically an organization will gradually mature over several categories rather than just one or two since they are connected and will affect each other to a certain extent. Culture & Organization The organization and it's culture are probably the most important aspects to consider when aiming to create a sustainable Continuous Delivery environment that takes advantage of all the resulting effects. Base A typical organization will have, at base level, started to prioritize work in backlogs, have some process defined which is rudimentarily documented and developers are practicing frequent commits into version control. Beginner Moving to beginner level, teams stabilize over projects and the organization has typically begun to remove boundaries by including test with development. Multiple backlogs are naturally consolidated into one per team and basic agile methods are adopted which gives stronger teams that share the pain when bad things happen. Intermediate At the intermediate level you will achieve more extended team collaboration when e.g. DBA, CM and Operations are beginning to be a part of the team or at least frequently consulted by the team. Multiple processes are consolidated and all changes, bugs, new features, emergency fixes, etc, follow the same path to production. Decisions are decentralized to the team and component ownership is defined which gives teams the ability to build in quality and to plan for sustainable product and process improvements. Advanced At the advanced level, the team will have the competence and confidence it needs to be responsible for changes all the way to production. Continuous improvement mechanisms are in place and e.g. a dedicated tools team is set up to serve other teams by improving tools and automation. At this level, releases of functionality can be disconnected from the actual deployment, which gives the projects a somewhat different role. A project can focus on producing requirements for one or multiple teams and when all or enough of those have been verified and deployed to production the project can plan and organize the actual release to users separately. This separation of concerns will optimize the projects flexibility of packaging and releasing functionality and at the same time give the development teams better flow and speed since they don't have to stack up changes and wait for coordinated project releases. Expert At expert level some organizations choose to make a bigger effort and form complete cross functional teams that can be completely autonomous. With extremely short cycle time and a mature delivery pipeline, such organizations have the confidence to adopt a strict roll-forward only strategy to production failures. Design & Architecture The design and architecture of your products and services will have an essential impact on your ability to adopt continuous delivery. If a system is built with continuous delivery principles and a rapid release mind set from the beginning, the journey will be much smoother. However, an upfront complete redesign of the entire system is not an attractive option for most organizations, which is why we have included this category in the maturity model. Base A typical organization will have one or more legacy systems of monolithic nature in terms of development, build and release. Many organizations at the base maturity level will have a diversified technology stack but have started to consolidate the choice of technology and platform, this is important to get best value from the effort spent on automation. Beginner At beginner level, the monolithic structure of the system is addressed by splitting the system into modules. Modules give a better structure for development, build and deployment but are typically not individually releasable like components. Doing this will also naturally drive an API managed approach to describe internal dependencies and also influence applying a structured approach to manage 3rd party libraries. At this level the importance of applying version control to database changes will also reveal itself. Intermediate Moving to intermediate level will result in a solid architectural base for continuous delivery when adopting branch by abstraction and other techniques for feature hiding for the purpose of minimizing repository branching to enable true continuous integration. At this level the work with modularization will evolve into identifying and breaking out modules into components that are self-contained and separately deployed. At this stage it will also be natural to start migrating scattered and ad-hoc managed application and runtime configuration into version control and treat it as part of the application just like any other code. Advanced At the advanced level you will have split the entire system into self contained components and adopted a strict api-based approach to inter-communication so that each component can be deployed and released individually. With a mature component based architecture, where every component is a self-contained releasable unit with business value, you can achieve small and frequent releases and extremely short release cycles. At this level it is easy for the business to experiment with rapid release of new features, monitor and verify expected business results; therefore techniques to push and graph business metrics from the application becomes important and natural part of the overall design and architecture. Expert At expert level, some organizations will evolve the component based architecture further and value the perfection of reducing as much shared infrastructure as possible by also treating infrastructure as code and tie it to application components. The result is a system that is totally reproducible from source control, from the O/S and all the way up to application. Doing this enables you to reduce a lot of complexity and cost in other tools and techniques for e.g. disaster recovery that serves to ensure that the production environment is reproducible. Instead of having a separate process, disaster recovery is simply done by pushing out the last release from the pipeline like any other release. This together with virtualization gives extreme flexibility in setting up test and production environments with minimum manual effort. Build & Deploy Build and deployment is of course core to Continuous Delivery and this is where a lot of tools and automation come into the pipeline; this is what is most is commonly perceived when Continuous Delivery is discussed. At first glance a typical mature delivery pipeline can be very overwhelming; depending on how mature the current build and deployment process is in the organization, the delivery pipeline can be more or less complex. In this category we will describe a logical maturity progression to give structure and understanding to the different parts and levels it includes. Base At a base level you will have a code base that is version controlled and scripted builds are run regularly on a dedicated build server. The deployment process is manual or semi-manual with some parts scripted and rudimentarily documented in some way. Beginner> Beginner level introduces frequent polling builds for faster feedback and build artifacts are archived for easier dependency management. Tagging and versioning of builds is structured but manual and the deployment process is gradually beginning to be more standardized with documentation, scripts and tools. Intermediate At intermediate level, builds are typically triggered from the source control system on each commit, tying a specific commit to a specific build. Tagging and versioning of builds is automated and the deployment process is standardized over all environments. Built artifacts or release packages are built only once and are designed to be able to be deployed in any environment. The standardized deployment process will also include a base for automated database deploys (migrations) of the bulk of database changes, and scripted runtime configuration changes. A basic delivery pipeline is in place covering all the stages from source control to production. Advanced At this stage it might also become necessary to scale out the build to multiple machines for parallel processing and for specific target environments. Techniques for zero downtime deploys can be important to include in the automated process to gain better flexibility and to reduce risk and cost when releasing. At this level you might also explore techniques to automate the trailing part of more complex database changes and database migrations to completely avoid manual routines for database updates. Expert Expert practices will include zero touch continuous deployment to production where every commit can potentially make it all the way to production automatically. Another expert practice is taking the infrastructure as code and virtualization even further and making the build process produce not only deployment artifacts but complete virtual machines which are promoted down the pipeline all the way to production replacing the existing ones. Test & Verification Testing is without doubt very important for any software development operation and is an absolutely crucial part of a successful implementation of Continuous Delivery. Similar to Build & Deploy, maturity in this category will involve tools and automation. However, it is also important to constantly increase the test-coverage of the application to build up the confidence in speed with frequent releases. Usually test involves verifying expected functionality according to requirements in different ways but we also want to emphasize the importance of verifying the expected business value of released features. Base At the base stage in the maturity model a development team or organization will typically practice unit-testing and have one or more dedicated test environments separate from local development machines. This system and integration level testing is typically done by a separate department that conducts long and cumbersome test periods after development ""code freeze"". Beginner When moving to beginner level you will naturally start to investigate ways of gradually automating the existing manual integration testing for faster feedback and more comprehensive regression tests. For accurate testing the component should be deployed and tested in a production like environment with all necessary dependencies. Intermediate Beginner level lets you broaden the scope of your automatic tests to functional testing which includes bigger parts of the component than unit tests but will have ""mocked"" implementations of it's external dependencies, e.g. database or other backend services. These tests are especially valuable when working in a highly component based architecture or when good complete integration tests are difficult to implement or too slow to run frequently. At this level you will most likely start to look at gradually automating parts of the acceptance testing. While integration tests are component specific, acceptance tests typically span over several components and across multiple systems. Advanced Advanced practices include fully automatic acceptance tests and maybe also generating structured acceptance criteria directly from requirements with e.g. specification by example and domains specific languages. This means no manual testing or verification is needed to pass acceptance but typically the process will still include some exploratory testing that feeds back into automated tests to constantly improve the test coverage and quality. If you correlate test coverage with change traceability you can start practicing risk based testing for better value of manual exploratory testing. At the advanced level some organizations might also start looking at automating performance tests and security scans. Expert It might seem strange to state that verifying expected business result is an expert practice but this is actually something that is very rarely done as a natural part of the development and release process today. Verifying expected business value of changes becomes more natural when the organization, culture and tooling has reached a certain maturity level and feedback of relevant business metrics is fast and accessible. As an example the implementation of a new feature must also include a way to verify the expected business result by making sure the relevant metrics can be pulled or pushed from the application. The definition of done must also be extended from release to sometime later when business has analyzed the effects of the released feature or change.. Information & Reporting Any business process includes a specific information set which can be reported on; the process of developing and releasing software is no exception, and the set of information that is handled in this particular area would typically include concepts like component, requirement, version, developer, release, environment etc. In this category we want to show the importance of handling this information correctly when adopting Continuous Delivery. Information must e.g. be concise, relevant and accessible at the right time to the right persons in order to obtain the full speed and flexibility possible with Continuous Delivery. Apart from information directly used to fulfill business requirements by developing and releasing features, it is also important to have access to information needed to measure the process itself and continuously improve it. Base At the base level in this category it is important to establish some baseline metric for the current process, so you can start to measure and track. At this level reporting is typically done manually and on-demand by individuals. Interesting metrics can e.g. be cycle-time, delivery time, number of releases, number of emergency fixes, number of incidents, number of features per release, bugs found during integration test etc. Beginner At beginner level, you start to measure the process and track the metrics for a better understanding of where improvement is needed and if the expected results from improvements are obtained. Reporting at this stage would typically include static analysis of code and quality reports which might be scheduled so that the latest reports are always accessible to facilitate decisions on quality and where improvements are needed. Intermediate Moving to intermediate the level of automation requires you to establish a common information model that standardizes the meaning of concepts and how they are connected. This model will typically give answers to questions like; what is a component?; how are requirements related to changes and components? When this model is established not only can you automate build, test and deployment even further but you can also build in traceability and information transparency to the pipeline and e.g. automatically generate information like release notes and test plans. Automatic reporting and feedback on events is implemented and at this level it will also become natural to store historical reports connected to e.g. builds or other events. This gives management crucial information to make good decisions on how to adjust the process and optimize for e.g. flow and capacity. Advanced At advanced level when you have started to practice frequent releases and the tools, products and services are mature enough to e.g. push business metrics, a natural progression is to set up a graphing service where people can get specific real time information that is relevant to their specific need. At this level real time graphs and other reports will typically also include trends over time. As a complement to static code analysis and unit tests coverage reports you might also at this level start looking at dynamic test coverage and profiling information from production like runtime environments when e.g. running automatic integrations tests. Expert Moving to expert level in this category typically includes improving the real time information service to provide dynamic self-service useful information and customized dashboards. As a result of this you can also start cross referencing and correlating reports and metrics across different organizational boundaries,. This information lets you broaden the perspective for continuous improvement and more easy verify expected business results from changes. Jump start the journey Every company is unique and has its own specific challenges when it comes to changing the way things work, like implementing Continuous Delivery. This maturity model will give you a starting point and a base for planning the transformation of the company towards Continuous Delivery. After evaluating your organization according to the model you need to set the goals and identify which practices will give your organization the best outcomes. If there are practices you do not want to adopt you need to analyse the consequences of excluding them. It is also important to decide on an implementation strategy, you can e.g. start small using slack in the existing process to improve one thing at a time. However, from our experience you will have a better chance of a successful implementation if you jump start the journey with a dedicated project with a clear mandate and aggressive goals on e.g. reducing cycle time. A typical Continuous Delivery implementation project will need skills and competence according to the categories and practices in the maturity model in order to implement a solid platform of tools, methods and practices that the organization can continue to grow and improve on. (Click on the image to enlarge it) About the authors Andreas Rehn is an Enterprise Architect and a strong advocate for Continuous Delivery, DevOps, Agile and Lean methods in systems development. With extensive experience in many disciplines of software development and a solid understanding of process, information and management theories and practises; he is dedicated to help customers implement Continuous Delivery and transform their business by adopting new methods for efficient and modern software development. Tobias Palmborg , Believes that Continuous Delivery describes the vision that scrum, XP and the agile manifesto once set out to be. Continuous Delivery is not just about automating the release pipeline but how to get your whole change flow, from grain to bread ,in a state of the art shape. Former Head of Development at one of europes largest online gaming company. Tobias is currently implementing Continuous Delivery projects at several customers. Patrik Boström is one of the founders of Diabol. Senior developer and architect with experience in operations of large system. Strong believer that Continuous Delivery and DevOps is the natural step in the evolution of Agile and Lean movement. Wants to change the way we look at systems development today, moving it to the next level where we focus more time on developing features than doing manually repetitive tasks. Where we visualize and understand the path from idea to where it is released and brings business value.",en,178
23,1333,1465557971,CONTENT SHARED,-5148591903395022444,-2979881261169775358,6661249173876371595,,,,HTML,http://code.joejag.com/2016/anti-if-the-missing-patterns.html,anti-if: the missing patterns,"Around 10 years ago I encountered the anti-if campaign and found it to be an absurd concept. How on earth would you make a useful program without using an if statement? Preposterous. But then it gets you thinking. Do you remember that heavily nested code you had to understand last week? That kinda sucked right? If only there was a way to make it simpler. The anti-if campaign site is sadly low on practical advice. This post intends to remedy that with a collection of patterns you can adopt when the need arises. But first let's look at the problem that if statements pose. The problems of if statements The first problem with if statements is that they often make it easy to modify code in bad ways. Let's start with the birth of a new if statement: This isn't too bad at this point, but we've already given us some problems. When I read this code I have to check how CodeBlockA and CodeBlockB are modifying the same SharedState. This can be easy to read at first but can become difficult as the CodeBlocks grow and the coupling becomes more complicated. You'll often see the above CodeBlocks abused with further nested if statements and local returns. Making it hard to see what the business logic is through the routing logic. The second problem with if statements is when they are duplicated. This means means a domain concept is missing. It's all too easy to increase coupling by bringing things together than don't need to be. Making code harder to read and change. The third problem with if statements is that you have to simulate execution in your own head. You must beome a mini-computer. That's taking away from your mental energy, energy that would be better spent thinking about solving the problem, rather than how the intracate code branches weave together. I want to get to the point of telling you patterns we can do instead, but first a word of warning. Moderation in all things, especially moderation If statements usually make your code more complicated. But we don't want to outright ban them. I've seen some pretty heinous code created with the goal of removing all traces of if statements. We want to avoid falling into that trap. For each pattern we'll read about I'm going to give you a tolerance value for when to use it. A single if statement which isn't duplicated anywhere else is probably fine. It's when you have duplicated if statements that you want your spider sense to be tingling. At the outside of your code base, where you talk to the dangerous outside world, you are going to want to validate incoming responses and change your beahaviour accordingly. But inside our own codebases, where we behind those trusted gatekeepers, I think we have a great opportunity to use simple, richer and more powerful alternatives. Pattern 1: Boolean Params Context: You have a method that takes a boolean which alters its behaviour Problem: Any time you see this you actually have two methods bundled into one. That boolean represents an opportunity to name a concept in your code. Tolerance: Usually when you see this context you can work out at compile time which path the code will take. If that is the case then always use this pattern. Solution: Split the method into two new methods. Voilà, the if is gone. Pattern 2: Switch to Polymorphism Context: You are switching based on type. Problem: When we add a new type we have to remember to update the switch statement. Additionally the cohesion is suffering in this Bird class as multiple concepts of different birds are being added. Tolerance: A single switch on type is fine. It's when their are multiple switches then bugs can be introduced as a person adding a new type can forget to update all the switches that exist on this hidden type. There is an excellent write up on the 8thlight blog on this context. Solution: Use Polymorphism. Anyone introducing a new type cannot forget to add the associated behaviour, note: This example only has one method being switched on for brevity, it's more compelling when there are multiple switches Patten 3: NullObject/Optional over null passing Context: A outsider asked to understand the primary purpose of your code base answers with ""to check if things equal null"". Problem: Your methods have to check if they are being passed non null values. Tolerance: It's necessary to be defensive at the outer parts of your codebase, but being defensive inside your codebase probably means the code that you are writing is offensive. Don't write offensive code. Solution: Use a NullObject or Optional type instead of ever passing a null. An empty collection is a great alternative. Patten 4: Inline statements into expressions Context: You have an if statement tree that calculates a boolean expression. Problem: This code forces you to use your brain to simulate how a computer will step through your method. Tolerance: Very little. Code like this is easier to read on one line. Or broken into different parts. Solution: Simplify the if statements into a single expression. Pattern 5: Give a coping strategy Context: You are calling some other code, but you aren't sure if the happy path will suceceed. Problem: These sort of if statements multiply each time you deal with the same object or data structure. They have a hidden coupling where 'null' means someting. Other objects may return other magic values that mean no result. Tolerance: It's better to push this if statement into one place, so it isn't duplicated and we can remove the coupling on the empty object magic value. Solution: Give the code being called a coping strategy. Ruby's Hash#fetch is a good example which Java has copied . This pattern can be taken even further to remove exceptions . Happy hunting Hopefully you can use some of these patterns on the code you are working on just now. I find them useful when refactoring code to better understand it. Remember if statements aren't all evil. But we have a rich set of features in modern languages to use instead which we should take advantage of.",en,176
24,1511,1466729247,CONTENT SHARED,-4503975842879662368,8250502084456448386,970831914672612417,,,,HTML,http://backlinko.com/google-ranking-factors,google ranking factors: the complete list,"You probably already know that Google uses about 200 ranking factors in their algorithm... But what the heck are they? Well today you're in for a treat because I've put together a complete list. Some are proven. Some are controversial. Others are SEO nerd speculation. But they're all here . Bonus: Download a free checklist that will show you how to tap into the 10 most important Google ranking factors listed here. Domain Factors 1. Domain Age: In this video , Matt Cutts states that: ""The difference between a domain that's six months old versus one year old is really not that big at all."". In other words, they do use domain age...but it's not very important. 2. Keyword Appears in Top Level Domain: Doesn't give the boost that it used to, but having your keyword in the domain still acts as a relevancy signal. After all, they still bold keywords that appear in a domain name. 3. Keyword As First Word in Domain: A domain that starts with their target keyword has an edge over sites that either don't have the keyword in their domain or have the keyword in the middle or end of their domain. 4. Domain registration length: A Google patent states: ""Valuable (legitimate) domains are often paid for several years in advance, while doorway (illegitimate) domains rarely are used for more than a year. Therefore, the date when a domain expires in the future can be used as a factor in predicting the legitimacy of a domain"". 5. Keyword in Subdomain Name: Moz's 2011 panel agreed that a keyword appearing in the subdomain can boost rankings: 6. Domain History: A site with volatile ownership (via whois) or several drops may tell Google to ""reset"" the site's history, negating links pointing to the domain. 7. Exact Match Domain: EMDs may still give you an edge...if it's a quality site. But if the EMD happens to be a low-quality site, it's vulnerable to the EMD update : 8. Public vs. Private WhoIs: Private WhoIs information may be a sign of ""something to hide"". Matt Cutts is quoted as stating at Pubcon 2006: ""...When I checked the whois on them, they all had ""whois privacy protection service"" on them. That's relatively unusual. ...Having whois privacy turned on isn't automatically bad, but once you get several of these factors all together, you're often talking about a very different type of webmaster than the fellow who just has a single site or so."" 9. Penalized WhoIs Owner: If Google identifies a particular person as a spammer it makes sense that they would scrutinize other sites owned by that person. 10. Country TLD extension: Having a Country Code Top Level Domain (.cn, .pt, .ca) helps the site rank for that particular country...but limits the site's ability to rank globally. Page-Level Factors 11. Keyword in Title Tag: The title tag is a webpage's second most important piece of content (besides the content of the page) and therefore sends a strong on-page SEO signal. 12. Title Tag Starts with Keyword : According to Moz data , title tags that starts with a keyword tend to perform better than title tags with the keyword towards the end of the tag: 13. Keyword in Description Tag: Another relevancy signal. Not especially important now, but still makes a difference. 14. Keyword Appears in H1 Tag: H1 tags are a ""second title tag"" that sends another relevancy signal to Google, according to results from this correlation study : 15. Keyword is Most Frequently Used Phrase in Document: Having a keyword appear more than any other likely acts as a relevancy signal. 16. Content Length: Content with more words can cover a wider breadth and are likely preferred to shorter superficial articles. SERPIQ found that content length correlated with SERP position : 17. Keyword Density : Although not as important as it once was, keyword density is still something Google uses to determine the topic of a webpage. But going overboard can hurt you. 18. Latent Semantic Indexing Keywords in Content (LSI): LSI keywords help search engines extract meaning from words with more than one meaning (Apple the computer company vs. the fruit). The presence/absence of LSI probably also acts as a content quality signal. 19. LSI Keywords in Title and Description Tags: As with webpage content, LSI keywords in page meta tags probably help Google discern between synonyms. May also act as a relevancy signal. 20. Page Loading Speed via HTML: Both Google and Bing use page loading speed as a ranking factor. Search engine spiders can estimate your site speed fairly accurately based on a page's code and filesize. 21. Duplicate Content: Identical content on the same site (even slightly modified) can negatively influence a site's search engine visibility. 22. Rel=Canonical: When used properly , use of this tag may prevent Google from considering pages duplicate content. 23. Page Loading Speed via Chrome : Google may also use Chrome user data to get a better handle on a page's loading time as this takes into account server speed, CDN usage and other non HTML-related site speed signals. 24. Image Optimization: Images on-page send search engines important relevancy signals through their file name, alt text, title, description and caption. 25. Recency of Content Updates: Google Caffeine update favors recently updated content, especially for time-sensitive searches. Highlighting this factor's importance, Google shows the date of a page's last update for certain pages: 26. Magnitude of Content Updates : The significance of edits and changes is also a freshness factor. Adding or removing entire sections is a more significant update than switching around the order of a few words. 27. Historical Updates Page Updates: How often has the page been updated over time? Daily, weekly, every 5-years? Frequency of page updates also play a role in freshness. 28. Keyword Prominence : Having a keyword appear in the first 100-words of a page's content appears to be a significant relevancy signal. 29. Keyword in H2, H3 Tags : Having your keyword appear as a subheading in H2 or H3 format may be another weak relevancy signal. Moz's panel agrees: 30. Keyword Word Order: An exact match of a searcher's keyword in a page's content will generally rank better than the same keyword phrase in a different order. For example: consider a search for: ""cat shaving techniques"". A page optimized for the phrase ""cat shaving techniques"" will rank better than a page optimized for ""techniques for shaving a cat"". This is a good illustration of why keyword research is really, really important. 31. Outbound Link Quality : Many SEOs think that linking out to authority sites helps send trust signals to Google. 32. Outbound Link Theme: According to Moz , search engines may use the content of the pages you link to as a relevancy signal. For example, if you have a page about cars that links to movie-related pages, this may tell Google that your page is about the movie Cars, not the automobile. 33. Grammar and Spelling: Proper grammar and spelling is a quality signal, although Cutts gave mixed messages in 2011 on whether or not this was important. 34. Syndicated Content: Is the content on the page original? If it's scraped or copied from an indexed page it won't rank as well as the original or end up in their Supplemental Index . 35. Helpful Supplementary Content: According to a now-public Google Rater Guidelines Document , helpful supplementary content is an indicator of a page's quality (and therefore, Google ranking). Examples include currency converters, loan interest calculators and interactive recipes. 36. Number of Outbound Links: Too many dofollow OBLs may ""leak"" PageRank, which can hurt that page's rankings. 37. Multimedia: Images, videos and other multimedia elements may act as a content quality signal. 38. Number of Internal Links Pointing to Page: The number of internal links to a page indicates its importance relative to other pages on the site. 39. Quality of Internal Links Pointing to Page : Internal links from authoritative pages on domain have a stronger effect than pages with no or low PR. 40. Broken Links: Having too many broken links on a page may be a sign of a neglected or abandoned site. The Google Rater Guidelines Document uses broken links as one was to assess a homepage's quality. 41. Reading Level: There's no doubt that Google estimates the reading level of webpages. In fact, Google used to give you reading level stats: But what they do with that information is up for debate. Some say that a basic reading level will help you rank better because it will appeal to the masses. But others associate a basic reading level with content mills like Ezine Articles. 42. Affiliate Links : Affiliate links themselves probably won't hurt your rankings. But if you have too many, Google's algorithm may pay closer attention to other quality signals to make sure you're not a ""thin affiliate site"". 43. HTML errors/W3C validation : Lots of HTML errors or sloppy coding may be a sign of a poor quality site. While controversial, many in SEO think that WC3 validation is a weak quality signal. 44. Page Host's Domain Authority : All things being equal, a page on an authoritative domain will rank higher than a page on a domain with less authority. 45. Page's PageRank: Not perfectly correlated. But in general higher PR pages tend to rank better than low PR pages. 46. URL Length: Search Engine Journal notes that excessively long URLs may hurt search visibility. 47. URL Path : A page closer to the homepage may get a slight authority boost. 48. Human Editors: Although never confirmed, Google has filed a patent for a system that allows human editors to influence the SERPs. 49. Page Category: The category the page appears on is a relevancy signal. A page that's part of a closely related category should get a relevancy boost compared to a page that's filed under an unrelated or less related category. 50. WordPress Tags: Tags are WordPress-specific relevancy signal. According to Yoast.com : ""The only way it improves your SEO is by relating one piece of content to another, and more specifically a group of posts to each other"" 51. Keyword in URL : Another important relevancy signal. 52. URL String: The categories in the URL string are read by Google and may provide a thematic signal to what a page is about: 53. References and Sources: Citing references and sources, like research papers do, may be a sign of quality. The Google Quality Guidelines states that reviewers should keep an eye out for sources when looking at certain pages: ""This is a topic where expertise and/or authoritative sources are important..."". However, Google has denied that they use external links as a ranking signal. 54. Bullets and Numbered Lists: Bullets and numbered lists help break up your content for readers, making them more user friendly. Google likely agrees and may prefer content with bullets and numbers. 55. Priority of Page in Sitemap: The priority a page is given via the sitemap.xml file may influence ranking. 56. Too Many Outbound Links: Straight from the aforementioned Quality rater document: ""Some pages have way, way too many links, obscuring the page and distracting from the Main Content"" 57. Quantity of Other Keywords Page Ranks For: If the page ranks for several other keywords it may give Google an internal sign of quality. 58. Page Age: Although Google prefers fresh content, an older page that's regularly updated may outperform a newer page. 59. User Friendly Layout: Citing the Google Quality Guidelines Document yet again: ""The page layout on highest quality pages makes the Main Content immediately visible"" 60. Parked Domains : A Google update in December of 2011 decreased search visibility of parked domains. 61. Useful Content: As pointed out by Backlinko reader Jared Carrizales , Google may distinguish between ""quality"" and ""useful"" content . Site-Level Factors 62. Content Provides Value and Unique Insights: Google has stated that they're on the hunt for sites that don't bring anything new or useful to the table, especially thin affiliate sites. 63. Contact Us Page: The aforementioned Google Quality Document states that they prefer sites with an ""appropriate amount of contact information"". Supposed bonus if your contact information matches your whois info. 64. Domain Trust/TrustRank: Site trust - measured by how many links away your site is from highly-trusted seed sites - is a massively important ranking factor. You can read more about TrustRank here . 65. Site Architecture: A well put-together site architecture (especially a silo structure) helps Google thematically organize your content. 66. Site Updates: How often a site is updated - and especially when new content is added to the site - is a site-wide freshness factor. 67. Number of Pages: The number of pages a site has is a weak sign of authority. At the very least a large site helps distinguish it from thin affiliate sites. 68. Presence of Sitemap: A sitemap helps search engines index your pages easier and more thoroughly, improving visibility. 69. Site Uptime : Lots of downtime from site maintenance or server issues may hurt your ranking (and can even result in deindexing if not corrected). 70. Server Location : Server location may influence where your site ranks in different geographical regions. Especially important for geo-specific searches. 71. SSL Certificate : Google has confirmed that they index SSL certificates and that they use HTTPS as a ranking signal. 72. Terms of Service and Privacy Pages : These two pages help tell Google that a site is a trustworthy member of the internet. 73. Duplicate Meta Information On-Site : Duplicate meta information across your site may bring down all of your page's visibility. 74. Breadcrumb Navigation: This is a style of user-friendly site-architecture that helps users (and search engines) know where they are on a site: Both SearchEngineJournal.com and Ethical SEO Consulting claim that this set-up may be a ranking factor. 75. Mobile Optimized: Google's official stance on mobile is to create a responsive site. It's likely that responsive sites get an edge in searches from a mobile device. In fact, they now add "" Mobile friendly"" tags to sites that display well on mobile devices. Google also started penalizing site s in Mobile search that aren't mobile friendly 76. YouTube: There's no doubt that YouTube videos are given preferential treatment in the SERPs (probably because Google owns it ): In fact, Search Engine Land found that YouTube.com traffic increased significantly after Google Panda . 77. Site Usability: A site that's difficult to use or to navigate can hurt ranking by reducing time on site, pages viewed and bounce rate. This may be an independent algorithmic factor gleaned from massive amounts of user data. 78. Use of Google Analytics and Google Webmaster Tools: Some think that having these two programs installed on your site can improve your page's indexing. They may also directly influence rank by giving Google more data to work with (ie. more accurate bounce rate, whether or not you get referral traffic from your backlinks etc.). 79. User reviews/Site reputation: A site's on review sites like Yelp.com and RipOffReport.com likely play an important role in the algorithm. Google even posted a rarely candid outline of their approach to user reviews after an eyeglass site was caught ripping off customers in an effort to get backlinks. Backlink Factors 80. Linking Domain Age: Backlinks from aged domains may be more powerful than new domains. 81. # of Linking Root Domains: The number of referring domains is one of the most important ranking factors in Google's algorithm, as you can see from this chart from Moz (bottom axis is SERP position): 82. # of Links from Separate C-Class IPs: Links from separate class-c IP addresses suggest a wider breadth of sites linking to you. 83. # of Linking Pages : The total number of linking pages - even if some are on the same domain - is a ranking factor. 84. Alt Tag (for Image Links) : Alt text is an image's version of anchor text. 85. Links from .edu or .gov Domains : Matt Cutts has stated that TLD doesn't factor into a site's importance. However, that doesn't stop SEOs from thinking that there's a special place in the algo for .gov and .edu TLDs. 86. Authority of Linking Page: The authority (PageRank) of the referring page is an extremely important ranking factor. 87. Authority of Linking Domain : The referring domain's authority may play an independent role in a link's importance (ie. a PR2 page link from a site with a homepage PR3 may be worth less than a PR2 page link from PR8 Yale.edu). 88. Links From Competitors: Links from other pages ranking in the same SERP may be more valuable for a page's rank for that particular keyword. 89. Social Shares of Referring Page : The amount of page-level social shares may influence the link's value. 90. Links from Bad Neighborhoods: Links from ""bad neighborhoods"" may hurt your site . 91. Guest Posts: Although guest posting can be part of a white hat SEO campaign , links coming from guest posts - especially in an author bio area - may not be as valuable as a contextual link on the same page. 92. Links to Homepage Domain that Page Sits On: Links to a referring page's homepage may play special importance in evaluating a site's - and therefore a link's - weight. 93. Nofollow Links: One of the most controversial topics in SEO. Google's official word on the matter is: ""In general, we don't follow them."" Which suggests that they do... at least in certain cases. Having a certain % of nofollow links may also indicate a natural vs. unnatural link profile. 94. Diversity of Link Types: Having an unnaturally large percentage of your links come from a single source (ie. forum profiles, blog comments) may be a sign of webspam. On the other hand, links from diverse sources is a sign of a natural link profile. 95. ""Sponsored Links"" Or Other Words Around Link: Words like ""sponsors"", ""link partners"" and ""sponsored links"" may decrease a link's value. 96. Contextual Links: Links embedded inside a page's content are considered more powerful than links on an empty page or found elsewhere on the page. A good example of contextual links are backlinks from guestographics . 97. Excessive 301 Redirects to Page: Links coming from 301 redirects dilute some (or even all) PR, according to a Webmaster Help Video . 98. Backlink Anchor Text : As noted in this description of Google's original algorithm: ""First, anchors often provide more accurate descriptions of web pages than the pages themselves."" Obviously, anchor text is less important than before (and likely a webspam signal). But it still sends a strong relevancy signal in small doses. 99. Internal Link Anchor Text : Internal link anchor text is another relevancy signal, although probably weighed differently than backlink anchor text. 100. Link Title Attribution : The link title (the text that appears when you hover over a link) is also used as a weak relevancy signals. 101. Country TLD of Referring Domain: Getting links from country-specific top level domain extensions (.de, .cn, .co.uk) may help you rank better in that country. 102. Link Location In Content: Links in the beginning of a piece of content carry slightly more weight than links placed at the end of the content. 103. Link Location on Page: Where a link appears on a page is important. Generally, links embedded in a page's content are more powerful than links in the footer or sidebar area. 104. Linking Domain Relevancy: A link from site in a similar niche is significantly more powerful than a link from a completely unrelated site. That's why any effective SEO strategy today focuses on obtaining relevant links. 105. Page Level Relevancy: The Hilltop Algorithm states that link from a page that's closely tied to page's content is more powerful than a link from an unrelated page. 106. Text Around Link Sentiment: Google has probably figured out whether or not a link to your site is a recommendation or part of a negative review. Links with positive sentiments around them likely carry more weight. 107. Keyword in Title: Google gives extra love to links on pages that contain your page's keyword in the title (""Experts linking to experts"".) 108. Positive Link Velocity: A site with positive link velocity usually gets a SERP boost. 109. Negative Link Velocity: Negative link velocity can significantly reduce rankings as it's a signal of decreasing popularity. 110. Links from ""Hub"" Pages: Aaron Wall claims that getting links from pages that are considered top resources (or hubs) on a certain topic are given special treatment. 111. Link from Authority Sites: A link from a site considered an ""authority site"" likely pass more juice than a link from a small, microniche site. 112. Linked to as Wikipedia Source: Although the links are nofollow, many think that getting a link from Wikipedia gives you a little added trust and authority in the eyes of search engines. 113. Co-Occurrences: The words that tend to appear around your backlinks helps tell Google what that page is about . 114. Backlink Age: According to a Google patent , older links have more ranking power than newly minted backlinks. 115. Links from Real Sites vs. Splogs: Due to the proliferation of blog networks, Google probably gives more weight to links coming from ""real sites"" than from fake blogs. They likely use brand and user-interaction signals to distinguish between the two. 116. Natural Link Profile: A site with a ""natural"" link profile is going to rank highly and be more durable to updates. 117. Reciprocal Links: Google's Link Schemes page lists ""Excessive link exchanging"" as a link scheme to avoid. 118. User Generated Content Links: Google is able to identify links generated from UGC vs. the actual site owner. For example, they know that a link from the official WordPress.com blog at en.blog.wordpress.com is very different than a link from besttoasterreviews.wordpress.com. 119. Links from 301: Links from 301 redirects may lose a little bit of juice compared to a direct link. However, Matt Cutts says that a 301 is similar to a direct link. 120. Schema.org Microformats: Pages that support microformats may rank above pages without it. This may be a direct boost or the fact that pages with microformatting have a higher SERP CTR: 121. DMOZ Listed : Many believe that Google gives DMOZ listed sites a little extra trust. 122. TrustRank of Linking Site: The trustworthiness of the site linking to you determines how much ""TrustRank"" gets passed onto you. 123. Number of Outbound Links on Page: PageRank is finite. A link on a page with hundreds of OBLs passes less PR than a page with only a few OBLs. 124. Forum Profile Links: Because of industrial-level spamming, Google may significantly devalue links from forum profiles. 125. Word Count of Linking Content: A link from a 1000-word post is more valuable than a link inside of a 25-word snippet. 126. Quality of Linking Content: Links from poorly written or spun content don't pass as much value as links from well-written, multimedia-enhanced content. 127. Sitewide Links: Matt Cutts has confirmed that sitewide links are ""compressed"" to count as a single link. User Interaction 128. Organic Click Through Rate for a Keyword : Pages that get clicked more in CTR may get a SERP boost for that particular keyword. 129. Organic CTR for All Keywords : A page's (or site's) organic CTR for all keywords is ranks for may be a human-based, user interaction signal. 130. Bounce Rate: Not everyone in SEO agrees bounce rate matters, but it may be a way of Google to use their users as quality testers (pages where people quickly bounce is probably not very good). 131. Direct Traffic: It's confirmed that Google uses data from Google Chrome to determine whether or not people visit a site (and how often). Sites with lots of direct traffic are likely higher quality than sites that get very little direct traffic. 132. Repeat Traffic : They may also look at whether or not users go back to a page or site after visiting. Sites with repeat visitors may get a Google ranking boost. 133. Blocked Sites : Google has discontinued this feature in Chrome. However, Panda used this feature as a quality signal. 134. Chrome Bookmarks: We know that Google collects Chrome browser usage data . Pages that get bookmarked in Chrome might get a boost. 135. Google Toolbar Data: Search Engine Watch's Danny Goodwin reports that Google uses toolbar data as a ranking signal. However, besides page loading speed and malware, it's not known what kind of data they glean from the toolbar. 136. Number of Comments: Pages with lots of comments may be a signal of user-interaction and quality. 137. Dwell Time: Google pays very close attention to ""dwell time"": how long people spend on your page when coming from a Google search. This is also sometimes referred to as ""long clicks vs short clicks"". If people spend a lot of time on your site, that may be used as a quality signal. Special Algorithm Rules 138. Query Deserves Freshness: Google gives newer pages a boost for certain searches . 139. Query Deserves Diversity: Google may add diversity to a SERP for ambiguous keywords, such as ""Ted"", ""WWF"" or ""ruby"". 140. User Browsing History : Sites that you frequently visit while signed into Google get a SERP bump for your searches. 141. User Search History: Search chain influence search results for later searches . For example, if you search for ""reviews"" then search for ""toasters"", Google is more likely to show toaster review sites higher in the SERPs. 142. Geo Targeting: Google gives preference to sites with a local server IP and country-specific domain name extension. 143. Safe Search: Search results with curse words or adult content won't appear for people with Safe Search turned on. 144. Google+ Circles: Google shows higher results for authors and sites that you've added to your Google Plus Circles 145. DMCA Complaints: Google ""downranks"" pages with DMCA complaints . 146. Domain Diversity : The so-called "" Bigfoot Update "" supposedly added more domains to each SERP page. 147. Transactional Searches : Google sometimes displays different results for shopping-related keywords, like flight searches. 148. Local Searches: Google often places Google+ Local results above the ""normal"" organic SERPs. 149. Google News Box: Certain keywords trigger a Google News box: 150. Big Brand Preference: After the Vince Update , Google began giving big brands a boost for certain short-tail searches. 151. Shopping Results: Google sometimes displays Google Shopping results in organic SERPs: 152. Image Results: Google elbows our organic listings for image results for searches commonly used on Google Image Search. 153. Easter Egg Results: Google has a dozen or so Easter Egg results . For example, when you search for ""Atari Breakout"" in Google image search, the search results turn into a playable game (!). Shout out to Victor Pan for this one. 154. Single Site Results for Brands: Domain or brand-oriented keywords bring up several results from the same site . Social Signals 155. Number of Tweets: Like links, the tweets a page has may influence its rank in Google. 156. Authority of Twitter Users Accounts : It's likely that Tweets coming from aged, authority Twitter profiles with a ton of followers (like Justin Bieber) have more of an effect than tweets from new, low-influence accounts. 157. Number of Facebook Likes : Although Google can't see most Facebook accounts , it's likely they consider the number of Facebook likes a page receives as a weak ranking signal. 158. Facebook Shares: Facebook shares - because they're more similar to a backlink - may have a stronger influence than Facebook likes . 159. Authority of Facebook User Accounts: As with Twitter, Facebook shares and likes coming from popular Facebook pages may pass more weight. 160. Pinterest Pins: Pinterest is an insanely popular social media account with lots of public data. It's probably that Google considers Pinterest Pins a social signal. 161. Votes on Social Sharing Sites: It's possible that Google uses shares at sites like Reddit, Stumbleupon and Digg as another type of social signal. 162. Number of Google+1's: Although Matt Cutts gone on the record as saying Google+ has "" no direct effect "" on rankings, it's hard to believe that they'd ignore their own social network. 163. Authority of Google+ User Accounts: It's logical that Google would weigh +1's coming from authoritative accounts more than from accounts without many followers. 164. Known Authorship : In February 2013, Google CEO Eric Schmidt famously claimed: ""Within search results, information tied to verified online profiles will be ranked higher than content without such verification, which will result in most users naturally clicking on the top (verified) results."" Although the Google+ authorship program has been shut down , it's likely Google uses some form of authorship to determine influential content producers online (and give them a boost in rankings). 165. Social Signal Relevancy: Google probably uses relevancy information from the account sharing the content and the text surrounding the link. 166. Site Level Social Signals: Site-wide social signals may increase a site's overall authority, which will increase search visibility for all of its pages. Brand Signals 167. Brand Name Anchor Text: Branded anchor text is a simple - but strong - brand signal. 168. Branded Searches: It's simple: people search for brands. If people search for your site in Google (ie. ""Backlinko twitter"", Backlinko + ""ranking factors""), Google likely takes this into consideration when determining a brand. 169. Site Has Facebook Page and Likes: Brands tend to have Facebook pages with lots of likes. 170. Site has Twitter Profile with Followers: Twitter profiles with a lot of followers signals a popular brand. 171. Official Linkedin Company Page: Most real businesses have company Linkedin pages. 172. Employees Listed at Linkedin: Rand Fishkin thinks that having Linkedin profiles that say they work for your company is a brand signal. 173. Legitimacy of Social Media Accounts: A social media account with 10,000 followers and 2 posts is probably interpreted a lot differently than another 10,000-follower strong account with lots of interaction. 174. Brand Mentions on News Sites : Really big brands get mentioned on Google News sites all the time. In fact, some brands even have their own Google News feed on the first page: 175. Co-Citations : Brands get mentioned without getting linked to. Google likely looks at non-hyperlinked brand mentions as a brand signal. 176. Number of RSS Subscribers: Considering that Google owns the popular Feedburner RSS service , it makes sense that they would look at RSS Subscriber data as a popularity/brand signal. 177. Brick and Mortar Location With Google+ Local Listing: Real businesses have offices. It's possible that Google fishes for location-data to determine whether or not a site is a big brand. 178. Website is Tax Paying Business: Moz reports that Google may look at whether or not a site is associated with a tax-paying business. On-Site WebSpam Factors 179. Panda Penalty : Sites with low-quality content (particularly content farms ) are less visible in search after getting hit by a Panda penalty . 180. Links to Bad Neighborhoods: Linking out to ""bad neighborhoods"" - like pharmacy or payday loan sites - may hurt your search visibility. 181. Redirects: Sneaky redirects is a big no-no . If caught, it can get a site not just penalized, but de-indexed. 182. Popups or Distracting Ads: The official Google Rater Guidelines Document says that popups and distracting ads is a sign of a low-quality site. 183. Site Over-Optimization: Includes on-page factors like keyword stuffing, header tag stuffing, excessive keyword decoration. 184. Page Over-Optimization: Many people report that - unlike Panda - Penguin targets individual page (and even then just for certain keywords). 185. Ads Above the Fold : The "" Page Layout Algorithm "" penalizes sites with lots of ads (and not much content) above the fold. 186. Hiding Affiliate Links: Going too far when trying to hide affiliate links (especially with cloaking) can bring on a penalty. 187. Affiliate Sites: It's no secret that Google isn't the biggest fan of affiliates . And many think that sites that monetize with affiliate links are put under extra scrutiny. 188. Autogenerated Content: Google isn't a big fan of autogenerated content. If they suspect that your site's pumping out computer-generated content, it could result in a penalty or de-indexing. 189. Excess PageRank Sculpting: Going too far with PageRank sculpting - by nofollowing all outbound links or most internal links - may be a sign of gaming the system. 190. IP Address Flagged as Spam: If your server's IP address is flagged for spam, it may hurt all of the sites on that server . 191. Meta Tag Spamming: Keyword stuffing can also happen in meta tags. If Google thinks you're adding keywords to your meta tags to game the algo, they may hit your site with a penalty. Off Page Webspam Factors 192. Unnatural Influx of Links : A sudden (and unnatural) influx of links is a sure-fire sign of phony links. 193. Penguin Penalty: Sites that were hit by Google Penguin are significantly less visible in search. 194. Link Profile with High % of Low Quality Links: Lots of links from sources commonly used by black hat SEOs (like blog comments and forum profiles) may be a sign of gaming the system. 195. Linking Domain Relevancy: The famous analysis by MicroSiteMasters.com found that sites with an unnaturally high amount of links from unrelated sites were more susceptible to Penguin. 196. Unnatural Links Warning: Google sent out thousands of ""Google Webmaster Tools notice of detected unnatural links"" messages. This usually precedes a ranking drop, although not 100% of the time . 197. Links from the Same Class C IP : Getting an unnatural amount of links from sites on the same server IP may be a sign of blog network link building. 198. ""Poison"" Anchor Text: Having ""poison"" anchor text (especially pharmacy keywords) pointed to your site may be a sign of spam or a hacked site. Either way, it can hurt your site's ranking. 199. Manual Penalty: Google has been known to hand out manual penalties, like in the well-publicized Interflora fiasco . 200. Selling Links: Selling links can definitely impact toolbar PageRank and may hurt your search visibility. 201. Google Sandbox: New sites that get a sudden influx of links are sometimes put in the Google Sandbox , which temporarily limits search visibility. 202. Google Dance: The Google Dance can temporarily shake up rankings. According to a Google Patent , this may be a way for them to determine whether or not a site is trying to game the algorithm. 203. Disavow Tool: Use of the Disavow Tool may remove a manual or algorithmic penalty for sites that were the victims of negative SEO. 204. Reconsideration Request : A successful reconsideration request can lift a penalty. 205. Temporary Link Schemes: Google has (apparently) caught onto people that create - and quickly remove - spammy links. Also know as a temporary link scheme . ""How Can I Use This Information For My Site?"" I created a free step-by-step checklist that you can use to quickly apply the most important information from this post to your site. The checklist contains the 10 most important ranking factors on this list... ...and super-actionable strategies that you can use to get higher rankings and more traffic. Click below to download the free checklist:",en,176
25,1076,1464034990,CONTENT SHARED,-8742648016180281673,6644119361202586331,595221735013488469,,,,HTML,"http://revistaepoca.globo.com/Revista/Epoca/0,,EDG78613-8056-483,00.html",aposta na inovação,"RITUAL Bruno Guiçardi, sócio da Ci&T, e o pote de rolhas de champanhe. A cada cliente novo, uma garrafa aberta para comemorar Eles ainda não completaram 40 anos, não sabem o que é ter uma carteira de trabalho assinada e saboreiam o gostinho de faturar dezenas de milhões de reais. Instalados num dos pólos de tecnologia mais avançados do país, o Pólis, da Universidade de Campinas (Unicamp), no interior de São Paulo, os empresários César Gon, de 35 anos, Bruno Guiçardi, de 35, e Fernando Matt, de 33, são donos da Ci&T, especializada em desenvolvimento e terceirização de aplicações de softwares. A empresa é a única brasileira na lista das estrelas emergentes na área de tecnologia da informação, segundo a pesquisa The Global Outsourcing 100, publicada pela revista americana Fortune. A presença no ranking deverá ajudar a abrir mais portas no exterior. Quando deixaram a Unicamp, onde se formaram em Ciência da Computação, em meados dos anos 90, os jovens decidiram tocar o próprio negócio. O que não lhes faltava era autoconfiança. ""Éramos ousados e não escondíamos"", afirma Guiçardi. Os três acreditavam tanto na própria competência que disputaram um projeto para o gigante IBM logo após a formatura. Venceram a concorrência e foram obrigados a correr para abrir uma empresa. A sede funcionava em um quarto alugado, numa modesta casa num bairro afastado de Campinas, e tinha apenas três computadores usados. Hoje, a Ci&T funciona numa área de 1.200 metros quadrados, tem 500 funcionários e faturou R$ 40 milhões no ano passado, 25% dos quais com negócios no exterior. A empresa mantém seis escritórios no Brasil, uma filial nos Estados Unidos e uma representação na Inglaterra. Nos últimos dois anos, as exportações dos serviços cresceram a uma média de 200% ao ano, segundo os sócios. Com uma carteira de clientes como IBM, HP, Johnson & Johnson, Petrobras, Vale do Rio Doce, Embraer e Yahoo!, a Ci&T desenvolve uma média de 150 projetos por ano. É dela, por exemplo, o portal que integra os distribuidores da HP na América Latina e o site da Natura, pelo qual a empresa se relaciona com suas mais de 566 mil vendedoras autônomas. ""Trabalhamos para antecipar o futuro, apostando em processos e sistemas que só mais tarde se tornarão padrão no mercado"", diz Guiçardi. Para comemorar cada projeto entregue, a equipe cumpre sempre o mesmo ritual: abre um champanhe e guarda a rolha, em um grande pote de vidro, devidamente identificada com a data e o nome do cliente. ""Para nossa alegria, já renovamos o pote muitas vezes."" Antecipar tendência, no entanto, nem sempre termina em brindes. Há riscos nessa opção de trabalho mais arrojada. O maior deles é lidar com resultados imprevisíveis. ""Muitas vezes, investimos em tecnologias que não emplacam"", diz Guiçardi. Outro problema é conseguir monitorar o crescimento acelerado. ""Quando atingimos 300 funcionários, não havíamos feito nenhuma preparação ou treinamento de pessoal."" O resultado foi uma confusão generalizada que afetou o atendimento aos clientes. Houve atrasos na entrega dos projetos e queda na qualidade dos serviços. Os sócios levaram um ano para pôr ordem na casa. A Ci&T caminha lado a lado com a academia. Há três anos a empresa instalou dentro da Unicamp um Laboratório de Inovação. Lá mantém 12 pesquisadores dedicados a desenvolver novos processos de tecnologia da informação. A proposta chamou a atenção da Fundação de Amparo à Pesquisa do Estado de São Paulo (Fapesp), que está investindo R$ 1 milhão em novos estudos com a Ci&T. No início do ano, os sócios montaram uma empresa chamada Digital Assets, especializada em softwares de governança e reutilização de programas. O empreendimento nasceu dentro da Ci&T, e quando tinha uma carteira razoável de clientes mudou-se para a incubadora tecnológica da Unicamp. ""Incubada, ela teria maiores chances de atrair capital de investidores."" Foi exatamente o que aconteceu. A Digital Asset recebeu dinheiro do fundo mineiro Novarum de capital de risco e está quase deixando a incubadora. ""Nosso sonho era faturar R$ 1 milhão. Hoje estamos de olho nos R$ 100 milhões"", diz Guiçardi. Da revista Pequenas Empresas & Grandes Negócios Foto: Ricardo Correa/PEGN",pt,175
26,2442,1474982976,CONTENT SHARED,6031953227014493100,-35428957105270993,2954312914497351516,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36",MG,BR,HTML,http://epoca.globo.com/vida/noticia/2016/09/o-chefe-e-gay-e-dai.html,o chefe é gay. e daí?,"O técnico de laboratório de cinema Lucca Najar e o estagiário de comunicação Gael Benitez são transexuais masculinos. Ambos nasceram e foram registrados como pertencentes ao sexo feminino, mas se sentiam no corpo errado. Para evitar discriminação, comportaram-se como era esperado tradicionalmente de mulheres, até o início da vida adulta - Najar tem 25 anos, e Benitez 21. A história dos dois começou a mudar quando foram trabalhar e estudar no Centro Universitário Una, de Belo Horizonte, empresa premiada no ranking ÉPOCA GPTW neste ano. Os dois encontraram um ambiente favorável para assumir sua real identidade de gênero. Najar foi contratado ainda como mulher em 2012 e hoje sente orgulho ao ver seu nome social masculino no crachá, nas folhas de prova e na lista de chamada. ""Fui bem recebido por colegas, professores e funcionários"", afirma. O mesmo aconteceu com Benitez. ""Percebi que o Una era um lugar tranquilo para iniciar minha transição"", afirma. >> Elektro, Google e Sama lideram entre as melhores empresas para trabalhar em 2016 EXEMPLAR Lucca Najar, Roberto Reis e Gael Benitez (da esq. para a dir.) , do Una. O programa de combate ao preconceito na empresa já desperta interesse acadêmico (Foto: Alexandre mota/Nitro/ÉPOCA) Gente educada ajuda muito, mas o ambiente acolhedor do Una resulta não apenas da boa vontade de professores e funcionários. Trata-se de uma decisão da empresa e de efeitos do projeto Una-se Contra a LGBTfobia, concebido em 2011 e coordenado por Roberto Reis, professor do Instituto de Comunicação e Arte (LGBT é a sigla para lésbicas, gays, bissexuais, travestis e transexuais). Reis é gay, mas em empregos anteriores nunca havia falado disso, por receio. No Una, o medo desapareceu. ""É importante e libertador poder me posicionar"", afirma. O projeto inclui ações educativas para todos que circulam pelos 12 campus da instituição. Os professores participam de oficinas sobre diversidade de identidade e orientação sexual. Em 2015 e 2016, o Una recebeu o Prêmio Direitos Humanos e Cidadania LGBT, promovido pelo CellosMG, ONG responsável pela organização da Parada do Orgulho LGBT na capital mineira. Carolina Marra, reitora do Una, reconhece a dificuldade de combater o preconceito numa instituição com mais de 30 mil alunos. ""O primeiro passo é admitir que há preconceito, depois assumir que isso não é legal e reagir como instituição"", diz. Esse tipo de ambiente ainda é raro no mundo corporativo, mesmo entre as empresas dispostas a passar pelo crivo do GPTW. Mas há uma transformação visível em curso. Neste ano, no grupo das empresas premiadas, nove profissionais, oito do Google e um da farmacêutica AstraZeneca, optaram por não se identificar como homens nem como mulheres. Na ficha demográfica de gênero dos funcionários nessas duas organizações, pela primeira vez ÉPOCA GPTW usa a opção ""outros"". >> ""Quero diversidade social, de gênero, raça e idade"", diz a presidente da Microsoft Mesmo entre as empresas interessadas em lidar com o tema, há dúvidas sobre como proceder. ""Ainda há polêmica em questões simples - qual banheiro usar? Colocar ou não na mesa de trabalho um porta-retratos com a foto do companheiro ou companheira?"", afirma Reinaldo Bulgarelli, secretário executivo do Fórum de Empresas e Direitos LGBT e sócio-diretor da consultoria Txai, que atua em responsabilidade social. Bulgarelli criou o fórum em 2013 e lançou uma carta pública com dez compromissos para a promoção dos direitos humanos de LGBTs. Apenas 33 empresas a assinaram, até o momento. Organizações interessadas em enfrentar o tema podem buscar referências onde já se acumula alguma experiência. Ben Boyd, executivo-chefe para América Latina e Canadá da agência de comunicação e consultoria Edelman, sugere um roteiro. Primeiro, conseguir o apoio de líderes-chave da organização. Depois, definir um executivo que assuma a responsabilidade sobre o programa. ""Vale a pena conversar com outras empresas que tenham políticas de inclusão e grupos de funcionários LGBTs, para aprender as melhores práticas e evitar erros"", afirma. Boyd é gay e criou em sua empresa o programa Edelman Equal. PORTAS ABERTAS Camila Crispim, desenvolvedora de software na ThoughtWorks. Ela cansou de ouvir ""papinho preconceituoso"" - e achou uma organização que leva a questão a sério (Foto: Leo Caldas/ÉPOCA) Entre as premiadas no GPTW, há diferentes iniciativas e experiências. A consultora sênior Paula Miyuke Tomiyoshi, de 36 anos, ainda não tinha feito sua transformação física quando ingressou na desenvolvedora de software SAP, há sete anos. Vestia-se como homem. Soube da política da SAP para funcionários LGBTs quando conversou com seu gerente sobre a vontade de mudar de gênero. ""Ele me tranquilizou e ofereceu a opção de eu trabalhar em casa durante a transição"", diz. Ela aceitou. Por nove meses, dividiu-se entre as tarefas do escritório, o tratamento com hormônios e cirurgias de feminilização. No período em que Paula esteve fora, a SAP tomou providências para tornar seu retorno o mais natural possível. Houve conversas na equipe e o psicólogo que cuidava dela falou com o pessoal e tirou dúvidas sobre transexualidade. A empresa enviou uma carta a clientes enfatizando seu apoio à funcionária. Apesar dos cuidados, Paula se sentiu discriminada por alguns colegas na volta ao escritório, já como mulher. ""Mesmo que a empresa tenha uma política, o preconceito é uma questão individual"", afirma. O programa de diversidade da SAP, chamado de Pride@SAP, deverá ser ampliado, e a empresa faz uma pesquisa com os fornecedores. ""A SAP tem uma posição clara sobre o assunto. Queremos trabalhar com companhias comprometidas com a diversidade e a inclusão"", diz Marcelo Carvalho, diretor de RH. Outra empresa de tecnologia da lista ÉPOCA GPTW, o Google tem vários comitês de diversidade criados pelos funcionários. Um deles é o Gayglers, liderado por Zen Junior, de 30 anos, analista de suporte. O grupo se reúne quinzenalmente e promove palestras internas e externas, eventos e apresentação de vídeos sobre LGBTs. Zen escolheu o Google para trabalhar há quatro anos quando soube do Gayglers. ""Sou realizado no Google, faço o que gosto e ainda posso militar pela causa LGBT"", afirma. ""O Google defende que as pessoas sejam elas mesmas. Não aceitamos preconceito"", diz Monica Santos, diretora de RH. >> GPTW 2016: As empresas que mostram o jeito certo de enfrentar a crise Companhias que toleram discriminação no ambiente de trabalho arriscam-se, no mínimo, a perder bons profissionais. Gabriel Brasileiro, de 30 anos, supervisor de comércio exterior, é gay e começou a trabalhar aos 18 numa multinacional sem abertura para esse tipo de diálogo. ""É muito difícil representar o tempo todo, ouvir piadas e ficar calado"", afirma. Por isso, procurou emprego na Monsanto, pois sabia que a empresa tinha uma política de promoção da diversidade. Foi contratado em dezembro de 2013 e assumiu a orientação sexual com tranquilidade. Chefia uma equipe de 12 pessoas e se sente respeitado por todos. Carlos Brito, diretor de RH para a América do Sul da Monsanto, acha que as empresas têm papel crucial em ajudar a sociedade a se tornar mais tolerante. ""Elas têm de ouvir, entender os desafios e se comprometer com o tema"", diz. A falta de empenho da maioria das companhias em lidar com o tema prejudica as equipes e o indivíduo. A analista de recrutamento e seleção Nathalia Carneiro tem 26 anos e é bissexual. Em dois empregos anteriores, teve de esconder seu namoro com outra mulher. Diz que, quando descobriram, não conseguiu mais avançar na carreira. Decidiu trabalhar na empresa de tecnologia Movile porque soube da receptividade da empresa e da política de oferecer chances de desenvolvimento independentemente da orientação sexual de seus profissionais. ""Fui acolhida e sou respeitada"", afirma. Profissionais dos grupos LGBTs que trabalham em ambientes preconceituosos gastam energia preciosa tentando não ser julgados nem mal interpretados. Camila Crispim, de 28 anos, gay, é desenvolvedora de software na ThoughtWorks. Em empregos anteriores, evitava participar de happy hours com os colegas e oferecer carona para mulheres. Não queria que pensassem que estava assediando alguém. Uma das empresas parecia ter postura mais aberta, mas tolerava que funcionários manifestassem os preconceitos. ""Cansei de ouvir papinhos do tipo 'não sou preconceituoso, mas não quero um filho gay'"", diz. ""Vivia uma vida com a família e os amigos e outra no emprego."" No início de 2013, Camila foi contratada pela ThoughtWorks e se sentiu acolhida. ""Não aceitamos que ninguém seja chamado de 'veadinho'"", diz Taise Assis, diretora de Justiça Econômica e Social da empresa. ""Esse tipo de violência não é aceito aqui. Na primeira vez perdoamos, mas na segunda não."" Todo novo funcionário assina um contrato antiassédio e participa de uma palestra sobre diversidade. A empresa fez uma parceria com o projeto Transcidadania, da prefeitura de São Paulo, que visa à reintegração social de travestis e transexuais em situação de vulnerabilidade social. A ThoughtWorks recebeu de fevereiro a abril deste ano nove transexuais na empresa e ofereceu um curso gratuito de informática. A empresa também apoiou o desenvolvimento do site Transerviços por uma ex-funcionária transexual, que lista profissionais e empresas que não são transfóbicas. As corporações começam a descobrir as vantagens de olhar a diversidade com mais atenção. Monica Santos, do Google, acredita que a diversidade na empresa torne os funcionários mais satisfeitos, engajados e produtivos. É fato que organizações com maior diversidade na equipe tendem a lidar melhor com mecanismos de controladoria externa e chegam a soluções mais variadas para enfrentar problemas. Mesmo assim, a parcela de trabalhadores gays que não se sentem à vontade para assumir a homossexualidade diante de qualquer colega permanece grande - varia de 34% na Holanda a 92% na Índia, segundo pesquisas anuais feitas pela consultoria australiana Out Now. No Brasil, essa parcela é de 65%. A discriminação é uma violência contra direitos humanos e trabalhistas - além de provocar um monumental desperdício de energia, atenção e dinheiro.",pt,173
27,1737,1468337707,CONTENT SHARED,1356221992133852808,-1032019229384696495,1370975477017368957,,,,HTML,https://techcrunch.com/2016/07/11/the-brilliant-mechanics-of-pokemon-go/,the brilliant mechanics of pokémon go,"A court ruled that it could be a federal crime to share your Netflix password If you haven't seen it already, you will soon walking down the street. Every person you pass who is fervently looking at their phone is likely playing the number one game in the country right now: Pokémon Go. You might think it's popular because of the brand. Nintendo, which refused to make a Pokémon game for the longest time on a smartphone, has finally caved and brought its beloved franchise to the small screen. But what may be overlooked amid all that is that the game, on its own, is phenomenally well designed on its own, despite myriad bugs and endless server outages. If you look at all aspects of the game loop - the engagement, retention, virality and monetization - it nails pretty much everything on the head. Niantic managed to hit a very rare, exceptional home run on every textbook point of the game's development. That's not an easy feat. Only a few games in the history of the iPhone have managed similar success. The closest analogues are probably Minecraft and Candy Crush Saga, which also rocketed to not only the top of the App Store download charts, but also the top grossing charts. Pokémon - much like Minecraft before it - launched immediately at the top of the charts. So its immediate success, based on the App Store rankings, isn't necessarily unprecedented. So, what makes this game so engaging and, from what we've seen so far, potentially very addictive? Let's break it down into its core pieces. Engagement Some of the best popular games have bite-time playing sessions. But the session time in Pokémon go can essentially be as long as the player wants, because there is a constant way to increase the length of the session time by walking to more pit-stops. That's a really hard thing to do in a game. Most session times are restricted to levels or gated with lives or energy. For Pokémon Go, there's just enough friction to inspire players to potentially pay to extend the length of their play session with less work, but also offer them the ability to go out of their way to extend that session time without having to pay. When I think about the structure of the play sessions in Pokémon Go, I often think of a role-playing game called Persona 4 Golden for the PlayStation Vita. The overall unit of time in the game is a day in the life of your character. The sessions are built into bite-size chunks based on the time of the day - morning, afternoon, and evening - and save points are littered through most parts of the game. And the combat sections of the game are also segmented into levels, with the option of leaving a dungeon at any point to save your game and end the playing session. In the same way as Pokémon Go, the game's session time can basically be extended to as long as the player wants while still keeping the basic mechanics of the game intact. In the case of Persona 4 Golden, that friction isn't necessary because the player has already bought the game, but for Pokémon Go it's incredibly well executed. Pokémon Go, like other well-designed popular mobile games, offers a quick ramp up that teases a lot of front-loaded rewards to get the player to come through the door and shut it behind them. That's important to grab their attention, but there's also multiple layers of rewards that keep players wanting to stay in the game. You can collect items in order to power up your Pokémon and evolve them, but it's also important to level up your own character. There are different layers of currency built into the game that progress along different time curves, giving each layer of progression its own speed and flavor. In that way, players can hit rewards at different increments of the game without feeling trapped in a grind for everything to level up at the same time. Amid the entire play session, the game has to stay open. That keeps you from getting distracted and flipping out to other apps. I find myself walking with my phone in my pocket, but with the game open often enough while wearing headphones. Whenever there's a chime, I take the phone out of my pocket and start playing - whether that's collecting Pokéballs or trying to capture something new (or some crappy junk Pokémon for the sake of experience). The game world is vibrant and beautiful, making it something easy and fun to see. It's filled with flair and flashes that are visually stimulating and signal new elements of the game. All this makes the player want to keep their eyes - or ears - glued to their phone, ready to engage with it the moment something new happens. All of this is great design, and doesn't even mention the brand equity Pokémon has built up. Nintendo has sold nearly 60 million 3DS units . Pokémon X & Y alone have totaled nearly 15 million in sales . That's an incredible nearly 25% penetration rate for all Pokémon enabled devices. If Nintendo were to barely scratch that with the nearly 2.5 billion smartphones in the world ( according to Statista ) that alone represents a staggering install base. Pokémon already is a worldwide phenomenon, and that alone is probably enough to get the player in the door beyond simply encountering other players and hearing about it organically. Pokémon Go currently only supports the original 150 Pokémon as well, tapping into the an untapped nostalgia that players have been waiting nearly a decade for. Retention An array of user-generated gameplay experiences is critical to building strong retention, and all the pieces are already built into the Pokémon Go experience. Each capture session is unique - the angle of the Pokéball is different, the placement of the Pokémon is different, and there's also an opportunity to have a unique experience also tied with the real world. You have probably seen on your Facebook feed screenshots of Pokémon sitting on other peoples' heads or in their laps. Each capture moment offers a unique player session, and while many will be similar, there's the tantalizing opportunity to have something truly unique that's really exciting. There's also an incredibly sticky part of user-generated content that exists alongside the game: the actual walk. Each walk a player goes for, in theory, is unique. The environment in the real world is different. the people who you run into may be different, the weather may be different, or the time of day. The environment in the game is also different, with Gyms constantly in flux and new Pokémon appearing at different intervals and in different places. Each walk also actively engages the player physically - and exercise naturally triggers a positive feel for your body, adding an additional layer of delight to the gameplay experience. This is such a new mechanic for a popular game that's unprecedented. For most games, the user-generated gameplay is restricted to an imaginary universe. It's a level on Candy Crush Saga that you get that lucky explosive cookie. It's a player-versus-player round in World of Warcraft where you get that lucky critical hit. It's a round of Destiny where you are just on fire and keep getting headshots. But all of these take place inside a screen, interfaced by a controller - whether that's a real controller or a touchscreen. The mastery curve is also smooth - over time you build a strong array of Pokémon that help you advance further in the game. The end of the game, like the regular Pokémon games, is a moving target, and there's basically always someone out there that's slightly better than you. That gives players a constant incentive to continue progressing along the mastery curve. Virality What's also unprecedented is Niantic's spin on the game's viral loop. In Pokémon Go, there's no feature that allows you to extend the life of your playing session by inviting or reaching out to friends. In fact, the social graph is almost non-existent in Pokémon Go. Instead, your in-game social graph is an extension of a supplemented version of your real-world social graph. A smartphone owner sees someone playing the game, becomes curious, downloads the game, and plays it - both interacting with other players and inspiring curiosity in other potential new players. And the rest of the time you're looking at screenshots of what's happening in the game in your Facebook feed, or texting friends when you managed to catch that rare Pokémon. You can read stories in many places on the internet of people randomly interacting with each other related to Pokémon Go. I experienced this already when walking around San Francisco, only to have a car drive by with one of the passengers yelling that there was a rare-ish Pokémon down the street (it was an Ivysaur, for those keeping tabs). This is table stakes for the Pokémon Go experience, and it's what gets new players in the door. This kind of virality is especially powerful because it isn't limited to an existing social graph. The whole viral loop is augmented in such a way that a non-connected interaction in the real world can lead to a new player, a download, and then monetization of that player. That's why I think this interpretation of the viral loop mechanic is so fascinating and is going to be so successful. Never before has a game immediately achieved such popularity in such a way that it regularly intersects with the real world. A lot of people consider it to be an augmented-reality experience, and in many ways you could consider it to be that. But it's not just an experience that uses your camera to play - it's an experience that crosses the boundary between an imaginary universe and the real world. I think the proper term that should be applied to this would be mixed-reality. The phrase augmented reality just doesn't give the game enough credit for being able to break that fourth wall and constantly move the player between an imaginary world and the real world. And it also represents an enormous opportunity for the game if Niantic decides to implement other important aspects of Pokémon like trading. Without an embedded social graph the game has already grown to immense popularity. Just imagine if it began to add an additional layer of player interaction - even if, again, it only takes place in the real world. Monetization So it's no wonder that the game has already hit the top of the App Store's top grossing charts. There's a lot going on in the monetization component of Pokémon Go, but again, it nails nearly every angle of attack to get players to make a payment. You can extend the life of your play session with more Pokéballs. You can speed up your growth curve by getting Egg Incubators, further increasing your array of potential Pokémon to further progress in the game. You can increase the rate of the engaging capture sessions by buying Incense. The same can be said for Lure Modules, which not only represent accelerated progression in the game but yet another way to tap into Pokémon Go's viral loop. Players congregate around areas - whether for catching Pokémon or building up their Pokéball stock - and that increases the probability that new, curious players will come by and discover the game independent of the App Store or other methods like Facebook App Install ads. Users paying for this contribute to the entire community of players given the benefit it offers everyone else. The most important aspect of this is that the gameplay, unlike most of the most-popular mobile and social games, is not gated. Paying Niantic and Nintendo money simply allows players to progress more quickly, but it doesn't impede their progress overall. Players have an opportunity to progress through the game at their own rate. As the saying goes, you still need the slow boat to China if you're going to be successful. Niantic here does such a good job of creating just enough friction that, at the exact moment, it can capture an opportunity for monetization. Players don't feel compelled to spend money, and instead they're offered a delightful experience when they elect to spend money. Those eye-popping visuals continue, they keep throwing Pokéballs, and they don't have to wait to see some of the most powerful Pokémon game. Final thoughts All this together creates a very powerful, sticky, and accelerating game loop that is helping the game grow at such an incredible rate. But there's another underlying thread amid all this: It bodes very well for holdout franchises to expand into mobile devices amid fear of cannibalizing devices or other parts of the market. Even Final Fantasy, in a way, has found its way onto mobile devices with Final Fantasy Record Keeper. Nintendo, amid the runaway early success of the game, added $9 billion to its market cap . This is such a strong, powerful signal to holdout franchises that haven't quite entered the smartphone ecosystem. And there's a good reason to do so: if that 2.5 billion device number from Statista is accurate, it offers such an enormous opportunity that it may be well worth eating up some potential hardware sales. This is a transition that the advertising market faced in the not too distant past. Google is constantly hounded by the need to shift its advertising revenue to mobile devices, in the hope that less valuable ads can be traded for a larger volume of ads on mobile devices. Facebook has built a business worth hundreds of billions of dollars off its mobile advertising products. Video game stalwarts will face the same dilemma: do you trade hardware and console sales in favor of the incredible volume of smartphone users? Is it worth the risk to assume people will still buy your consoles when Mario is available on your phone? Can a company like Nintendo offer an array of experiences that span multiple devices? In this week's blowout success of Pokémon Go, the answer for now appears to be rounded up to a Yes. Alas, there's no way to mash down+A+B. Get on that, Niantic. Featured Image: Eduardo Woo / Flickr UNDER A CC BY-SA 2.0 LICENSE",en,171
28,949,1463146938,CONTENT SHARED,-5784991738549272379,2279740393166882579,1579934520221634711,,,,HTML,http://irving.com.br/esp8266/nodemcu-esp8266-o-modulo-que-desbanca-o-arduino-e-facilitara-a-internet-das-coisas/,nodemcu (esp8266) o módulo que desbanca o arduino e facilitará a internet das coisas...,"A partir de 2005 o Arduino se tornou a principal ferramenta de makers e hobbistas para a o prototipação de projetos eletrônicos. Pois bem, eis que em meados de 2014 surgiu um concorrente à altura (eu diria muito superior): O ESP8266, um módulo eletrônico muito menor, mais veloz, potente e muito mais barato do que o Arduino, além de já possuir conexão Wi-Fi. Com isso, esse módulo possibilita os mais diversos projetos, conectados à Internet e é possível programá-lo usando o mesmo software e linguagem do Arduino. O potencial do ESP é tão grande que ele venceu o prêmio de hardware do ano (2015/16) pelo IoT Awards . Algumas informações sobre ele: Confusão de nomes, quem é quem? ESP8266 (na verdade ESP8266EX) é um chip de arquitetura 32 bits com Wi-Fi integrado, medindo apenas 5mm x 5mm E produzido pela companhia chinesa Espressif . O seu tamanho tão pequeno dificulta a utilização, mesmo soldá-lo é uma tarefa complicada, então uma outra empresa chinesa, a AI-Thinker , passou a produzir módulos utilizando o chip e colocando alguns componentes extras (Memória adicional, memória EEPROM, antena...). Esses módulos sofrem rápidas atualizações e começaram a ser chamados por números, os mais conhecidos são: ESP-01 - que contava apenas com 8 conectores, e servia mais para ser utilizado como um módulo para o Arduino; ESP-07 - que foi bastante vendido, contava com 16 pinos, antena de cerâmica e conector para antena externa; E o ESP-12E - Uma atualização do ESP-12, que conta com 22 pinos, entre esses pinos extras estão os da interface SPI, que possibilita ligarmos vários módulos ao ESP (displays, cartões SD, e diversos 'shields'...) Agora vamos ao NodeMCU A gravação dos módulos ESP não é das mais simples, é necessário um conversor USB/Serial (FTDI), para que os dados possam ser passados do computador para o módulo. Outro ponto dos módulos ESP é que eles utilizam 3,3V (inclusive alguns conversores só possuem saída de 5V), o que não é uma tensão fácil de obter. O NodeMCU é constituído por um módulo ESP-12E, um conversor FTDI, e um regulador de tensão de 5 para 3,3V e possui o espaçamento entre os pinos padrão (2,54mm) o que nos permite conectá-los à placas de prototipação (protoboard), e sua programação é bem mais simples, sendo necessário apenas ligar um cabo USB que também serve como alimentação para o circuito. O projeto original foi pensado para se utilizar a linguagem de programação Lua (Brasileira inclusive) mas é possível fazer upload dos códigos em outra linguagem normalmente. Várias empresas lançaram diferentes versões, a ""original"" NodeMCU , as mais acessíveis WeMos , Amica e DOIT , e as mais confiáveis SparkFun Thing e Adafruit HUZZAH . Tamanho Uma imagem vale mais que mil palavras, nesta imagem podemos ver o Arduino Uno, um NodeMCU e um shield Wi-Fi. O NodeMCU faz o papel dos outros dois componentes juntos, ou seja, é capaz de receber programação e já tem uma interface Wi-Fi, possibilitando uma imensa gama de projetos. Vamos aos números: O Arduino Uno mede aproximadamente 5,3cm x 7,9cm e o shield Wi-Fi tem praticamente o mesmo tamanho (é usado em cima do Arduino Uno, não há aumento no espaço utilizado), já o NodeMCU mede 2,5m x 5,1cm, um terço o tamanho do Arduino, isso sem contar na possibilidade de usar apenas o chip principal, que mede apenas 2,4cm x 1,6cm (quase 11 vezes menor). Hardware Neste quesito a vitória do NodeMCU é mais expressiva, o Arduino UNO possui um microcontrolador (ATMega 328P) de 16MHz (velocidade de processamento, um celular atual é aproximadamente 100 vezes mais veloz), possui uma memória RAM de 2KB (Memória onde são armazenadas as variáveis, os valores que você salva no meio do código) e uma memória flash de 32KB (espaço para armazenamento do programa em si). Já o NodeMCU possui um processador (Tensilica LX106) que pode atingir até 160MHz (10 vezes mais rápido que o Arduino), uma memória RAM de 20KB e uma memória flash de 4MB! (Tão grande que é possível fazer download de uma atualização do próprio código!). Os shields Wi-Fi mais comuns possuem conexões 802.11 b e g (11MBps e 56MBps de limite de velocidade, respectivamente). Já o ESP possui as mesmas conexões além da 802.11n que possui um limite 300MBps de velocidade e tem um alcance até duas vezes maior. Preço Aqui a situação fica melhor ainda! Nos sites chineses é possível encontrar um ESP8266 por até US$1,70 e um NodeMCU por menos de US$2,30 (menos de 12 reais), um Arduino UNO original não sai por menos de 20 dólares. No mercado brasileiro, usando apenas um site como comparação um NodeMCU sai por aproximadamente R$70, e um ESP8266-12E por R$45. Um Arduino UNO cópia está saindo por R$60 e um shield Wi-Fi não sai por menos de R$250... Ou seja, o custo-benefício do ESP/NodeMCU é altíssimo e é possível encontrá-lo a preços mais acessíveis no MercadoLivre, por exemplo. Facilidade Como foi dito, a programação do ESP8266 pode ser feita de várias maneiras: Em C, através da SDK do fabricante (Espressif) Em Lua, que é a linguagem oficial do NodeMCU E em C++*, a mesma linguagem no Arduino A mais simples de ser utilizada é a linguagem do Arduino, as bibliotecas são muito completas, sendo necessário pouca programação para já conectar o ESP no seu roteador Wi-Fi e fazer as mais diferentes e úteis aplicações. O código de exemplo abaixo, procura por todas as redes nas redondezas e as imprime na tela juntamente com a potência do sinal e o tipo de encriptação. Contras Claro que nem tudo são flores, um dos pontos negativos do ESP é a presença de somente uma entrada analógica, o que dificulta a utilização de muitos sensores simples, além disso, o valor máximo aceito nessa entrada é de 1 Volt, sendo necessário algumas conversões para obter o valor desejado. Há também um número menor de portas digitais, mas nada que comprometa a maioria dos projetos. E aí? É fácil evidenciar a enorme vantagem que o ESP/NodeMCU leva sobre outros dispositivos, ele mescla o hardware potente e de baixo custo das manufaturas chinesas com a facilidade de programação do Arduino, que nasceu na Europa mas é difundido pelo mundo todo. Este pequeno chip, como a mídia especializada sempre diz, é claramente um IoT enabler , tem tudo para estar presente nos mais diversos gadgets daqui pra frente. E a Espressif já anunciou a pré-produção do ESP32 , um chip ainda melhor que o 8266 com processador dual core, 500KB de RAM e Bluetooh 4.0",pt,166
29,1409,1466013280,CONTENT SHARED,1854874463930846880,534764222466712491,7914515688450890484,,,,HTML,https://mulheresnacomputacao.com/2016/06/14/o-dia-em-que-tive-mais-medo-de-estar-errada/,o dia em que tive mais medo de estar errada.,"Passei meu sábado em Belém do Pará, mais especificamente na Ilha de Combú na Floresta Amazônica. E eu tive um dos dias mais incríveis da minha vida. Mas foi também o dia em que eu tive muito muito medo. Medo de estar errada. Muito errada. Vou contar a história inteira. E vou voltar lá pra minha primeira infância. Desde pequena eu sou bem curiosa e acho que não sabia, mas isso era sede de mundo. Sede de saber o que tem em cada canto desse planeta. Entender como tudo funciona e isso me fez estar onde eu estou. Arrumei uma aliada muito boa lá pela metade da minha jornada, que sem ela não seria possível: a tecnologia. Achei respostas. Decidi que faria isso da vida. Mostrar essa aliada pra todo mundo. Ser a representante da tecnologia que eu conhecia, que me libertou e não que ameaçava. Ela não precisava ser só minha e no último sábado, lá pelas seis da manhã quando abri os olhos eu estava bastante confiante nisso apesar do sono, estava vindo de uma avalanche de novas informações, passado por cinco capitais em seis dias....enfim estava tudo bem bem, até que... Entrei em um barco, e partimos pra uma ilha, a Ilha de Combu... Tinham várias pessoas no barco , voluntários, minha equipe, pessoal da Intel, time do Barco Hacker, enfim...só gente do bem! Tomo mundo tagarelando a viagem toda. As palafitas, a navegação, a falta de conexão estável, a falta de tomadas era esperado até certo ponto, mas quando vimos 17 crianças de 3 a 12 anos que conheciam a energia elétrica apenas à 4 anos todos nos calamos. Não tinha a menor ideia do que fazer! Tive um minuto de branco! Todas as minhas crenças passaram em alguns segundos na minha frente! Será que é tecnologia a resposta pra elas? Será que um dia elas iam conseguir aproveitar aquele conhecimento? Será que elas vão entender? Será que é isso mesmo? Será que é pra todo mundo mesmo? Será? Será? Questionei tudo e por pouco não desisti...quase que fiquei só ali conversando e disfarçando...coloquei todo mundo em roda pra ganhar tempo e pensar no que eu faria. Olhei na carinha de cada um e todos estavam vidrados em mim, no meu conhecimento, no que eu estava falando...tudo isso enquanto duas menininhas se empurravam pra ver quem sentava do meu lado. Crianças! Como todas as outras...ABERTAS! CURIOSAS! SEM MEDO! Aquela cena que era trivial, serviu de faísca e começou a transformação! Minha cabeça estava sempre um pensamento na frente tentando traduzir pra eles o que era um microcontrolador, uma protoboard, um resistor...de um jeito que a lição final fosse: EU SOU CAPAZ! FUI EU QUE FIZ ISSO! ISSO É TECNOLOGIA E EU NÃO PRECISO TER MEDO. Sentamos no chão, perdemos o medo da placa, montamos circuito, programamos, todos, juntos, iguais! Conforme iam conseguindo acender as luzinhas (LEDs) e escolher o intervalo que quisessem todos vibravam, riam, gritavam, comemoravam...fiquei maravilhada com a rapidez com que eles se empoderaram com aquele conhecimento! Eu já estava querendo chorar e gritar pro mundo que estava dando certo, que tecnologia empoderava qualquer um, independento do lugar do mundo que estivéssemos! Mas ainda tinha a prova final...eles tinham 15 minutos pra pensar e desenhar soluções usando aquilo que eles tinham visto para os problemas da comunidade, para quê eles usariam aquele aprendizado? Será que eles iam conseguir aplicar? Eu precisava me segurar mais um pouco! Me controlar. Eles ficaram maravilhados com as canetinhas e réguas, muitos não tinham uma. Deixamos várias de presente. Começaram as apresentações, eles estavam com vergonha, não sabiam se estavam certos, se eram bons...ai se eles soubessem! Não se envergonhariam nem por um segundo...muito pelo contrário! As ideias eram incríveis...resolviam problemas reais, problemas deles com uma simplicidade admirável. Vou contar a que mais me chamou atenção: um braço extensível com uma plaquinha que coleta informações como cor do açaí e informa se ele está maduro ou não, evitando que alguém tenha que subir no pé, num primeiro momento parece meio superficial, até que eles foram me mostrar como é o processo...uma pessoa com uma folha amarrando os pés tem que subir uns 15 metros, correndo o risco de cair ou a árvore quebrar só pra ver se já está na hora de colher....pra alguém conseguir fazer isso tem que ""calejar"" o pé e a mão na raça...um problema que não está sendo resolvido! Isso é problema de verdade! Isso é solução de impacto! Quando os pitches acabaram e eles foram comer o lanche, coloquei meu óculos escuros e chorei! Chorei! Chorei porque eles estão apartados, eles estão isolados, excluídos, eles não estão tendo os problemas resolvidos... Chorei porque não tem preço ver a expansão dos sonhos de crianças. Os horizontes mudam quando um LED acende! E basta um! Ensinar tecnologia para eles é muito mais que passar conhecimento técnico, é mostrar que eles podem! Depois dessa eu nunca mais vou sentir medo! E se sentir...vou com medo mesmo! Para quem quiser saber mais sobre tudo que a gente vem fazendo dá uma olhada no Mastertech , na Ponte21 e na Maratona Maker ❤ Beijos! PS.: Comentem aí como tecnologia mudou a vida de vocês!",pt,165
30,1492,1466620390,CONTENT SHARED,6997620589258672675,-48161796606086482,-4406729997686684554,,,,HTML,https://pagamento.me/abrimos-uma-conta-no-original/,abrimos uma conta no original.,"Você já deve ter percebido que esse portal fala bastante de bancos digitais, certo? Fizemos um post "" Nubank - a experiência"" ( ), em 31 de Janeiro de 2015, onde contamos como foi ter se cadastrado, validado os dados e ter digitalizado os documentos pelo app. Foi uma verdadeira aula de ""usabilidade"" e de praticidade. Recebemos o cartão em 7 dias após cadastro. Fizemos no mês passado, o processo completo de abertura de conta no Original. Acompanhe como foi. Banco digital: primeiras impressões A conta foi aberta em 19 dias exatos. Iniciamos o processo de cadastro via app dia 03/04 e recebemos a confirmação de abertura com o número da conta e agência, no dia 22/04. Já o cartão, foi enviado somente no dia 05 de maio. Mais de um mês pós cadastro. Bancos de varejo como Itau, Santander e Bradesco, com seu processo normal de abertura, teriam 5 dias para abrir a conta, mais 5 dias para enviar o cartão. O que totalizaria o processo completo em 10 dias. Para nós o grande atrito está na coleta da assinatura digital presencial do Original, que ainda está lenta entre cadastro e coleta. Do cadastro (03/04) até o dia da coleta e conferência presencial dos documentos (20 de abril), foram 17 dias. Apesar do desafio do prazo, foi uma boa experiência. Na prática foram 4 passos básicos: 03/04 - Cadastro no app; 20/04 - Coleta da assinatura digital; 22/04 - Confirmação da conta aberta; 05/05 - Recebimento do cartão Vamos às ilustrações. 1. Cadastro no app do Original O ponto de melhoria visto pela nossa equipe foi justamente a questão do cadastro dos dados no app. Por se tratar de uma abertura de conta, acreditamos que algumas questões de regulamentação e KYC (know your customer) devam ser seguidas à risca, ainda. Porém, o tempo para finalizar o cadastro total, digitalização de documentos e fotos, durou cerca de 20 minutos. O ideal era reduzir ainda mais isso. Mesmo assim, foi um belo trabalho do banco, criar um mecanismo para abrir contas digitais de uma forma realmente prática. Bem mais simples e mais rápido do que abrir contas tradicionais. Adeus fila! 2. Coleta da assinatura digital Recebemos um agente de coleta, para confirmar o documento e coletar novamente a digital dos dedos, só que dessa vez presencialmente. O double check , segundo o próprio representante, é norma. A burocracia trouxe um mimo: o banco enviou um café especial para dar as boas vindas. 3. Confirmação da conta aberta Com a confirmação da conta aberta, fizemos o acesso via web e já fizemos os testes funcionais: depositando dinheiro, simulando investimento e testando a plataforma. A agência Try mandou muito bem na concepção / design. Ficamos bem surpresos com o resultado das cores e da facilidade de navegação da estrutura. Bem diferente dos bancos tradicionais. 4. Recebimento do cartão Ufa! Tá na mão. E ainda veio com outro brinde �� O Original abriu uma porta bem importante em se posicionar como um banco digital de fato. Não ter agência física é de realmente uma grande mudança, mas também um possível realidade nos próximos anos. Sugestão: leia o texto "" 3 sinais que fazem das fintechs, uma bomba explosiva para os bancos brasileiros"" e entenda como isso de abrir contas em agências bancárias tradicionais, vai morrer em breve. Belo trabalho Original �� Fotos: reprodução Original.com.br",pt,161
31,1366,1465827586,CONTENT SHARED,-8518096793350810174,1895326251577378793,-1287126046112431871,,,,HTML,http://computerworld.com.br/microsoft-adquire-linkedin-por-us-262-bilhoes,"microsoft adquire linkedin por us$ 26,2 bilhões","O mercado de tecnologia começou a semana em ritmo intenso. A Microsoft anunciou no começo dessa segunda-feira (13/06) que desembolsará nada menos que US$ 26,2 bilhões, em dinheiro, pelo LinkedIn. Em comunicado ao mercado, a fabricante do Windows assegurou que a rede social corporativa manterá sua marca, cultura e independência. Jeff Weiner seguirá no comando do projeto, reportando-se a Satya Nadella. A compra representa o maior valor já desembolsado pela Microsoft em sua história. A quantia é superior ao preço pago pela Nokia e Skype, combinados, e corresponde a US$ 196 por ação. O LinkedIn é a maior rede social corporativa do mundo, contabilizando um total a 433 milhões de usuários ao redor do mundo e uma oferta de serviços que vai desde ferramentas de recrutamento até publicidade. Apesar de manter independência, a companhia passará a integrar o segmento de Produtividade e Processos de Negócio da gigante de software. ""Juntos, vamos acelerar o crescimento do LinkedIn, bem como do Office 365 e Dynamics na busca por dar mais poder a cada pessoa e organização"", pontou Nadella. Weiner afirma que a combinação das ferramentas da rede social com as ferramentas em nuvem da nova controladora criarão condições de ""mudar a forma como o mundo trabalha"". A expectativa é que a transação esteja completa ao final de 2016. A conclusão do negócio ainda depende de aprovações de órgãos reguladores de mercado. As empresas disponibilizaram um vídeo no YouTube , no qual Nadella e Weiner falam sobre os motivos e estratégias por trás da aquisição. Mais compras Também nessa segunda-feira, a Symantec revelou que desembolsaria US$ 4,65 bilhões pela Blue Coat . Combinadas as operações, a companhia deve faturar US$ 4,4 bilhões em 2016. Do total, 62% das receitas originam-se de contratos com clientes corporativos.",pt,156
32,785,1462381699,CONTENT SHARED,1738052593226421681,6013226412048763966,5796685774598185282,,,,HTML,http://www.pnl.com.br/programacao-neurolinguistica/publicacoes/como-resolver-conflitos-no-ambiente-corporativo-usando-a-pnl-,como resolver conflitos no ambiente corporativo us,"por Gilberto Craidy Cury em 25 de abril de 2016 Muitas pessoas confundem divergência com conflito, com briga, mas na verdade, o conflito, principalmente dentro do ambiente corporativo, nada mais é do que a diferença de opiniões somada à má comunicação. As diferentes opiniões no trabalho são comuns e quando bem gerenciadas levam ao fortalecimento das equipes e ao crescimento profissional. A maneira sadia de lidar com elas é também aquela que mais facilita o desenvolvimento de indivíduos e da organização: a boa comunicação. Segundo uma pesquisa realizada pela Universidade de Harvard, cerca de 70% dos problemas nas organizações tem relação direta ou indireta com problemas de comunicação. Sendo assim, ensinar as pessoas e equipes a se comunicar melhor é uma excelente maneira de gerenciar os problemas. No entanto, não basta colocar um de frente pro outro e pedir que falem, é preciso desenvolver nas pessoas a capacidade de entender umas às outras e respeitar opiniões divergentes. Nesse contexto, a Programação Neurolinguística - PNL é a ferramenta mais eficiente de ser aplicada aos treinamentos. A PNL é, entre outros, um conjunto de ferramentas e técnicas de comunicação que trabalha individualmente nos colaboradores sua sensibilidade, capacidade de percepção, intuição, flexibilidade, empatia e inteligência emocional. Ela permite que possamos aprender e modificar modelos de comunicação interpessoal e intrapessoal, em pouco tempo e de maneira muito eficaz. Promove, também, o autoconhecimento, alicerce do auto desenvolvimento. Outra qualidade trabalhada pela ferramenta é a capacidade de gerenciar pessoas, uma vez que a liderança qualificada exige a capacidade de entender e respeitar os indivíduos. A qualidade do sucesso na liderança depende da qualidade das habilidades pessoais de se comunicar e do bom relacionamento entre interlocutores. É importante expressar nossos posicionamentos e objetivos com clareza, gerando uma atmosfera de confiança, com habilidade para influenciar nosso interlocutor. Saber reconhecer sinais verbais e não-verbais, distinguir qualidades de voz e entonação, conhecer estratégias e modelos de negociação, utilizar a criatividade para a solução de problemas, são alguns dos caminhos que a PNL apresenta. Dentro do universo das empresas, trabalhar as pessoas é a melhor forma de desenvolver equipes. Afinal de contas, as equipes são um conjunto de indivíduos que trabalham juntos para um resultado ideal. Os gestores que decidem investir em PNL podem esperar uma mudança imediata no clima organizacional, diminuição de conflitos, melhor entrosamento, maior qualidade de vida, entre tantos outros benefícios. Existe um senso comum entre os gestores de RH que diz que as pessoas são admitidas pelas qualidades técnicas e demitidas pelas incompetências comportamentais. O técnico você aprende em muitos lugares. A prática, você conquista com o tempo. Já a habilidade comportamental, também reconhecida como atitude, a PNL é capaz desenvolver e transformar tanto no âmbito pessoal quanto no profissional. O grande diferencial de se trabalhar a comunicação entre as equipes com a PNL é que os treinamentos feitos são capazes de promover mudanças profundas, não precisando aplicar novos treinamentos à equipe. Isso significa muito mais eficácia e assertividade, já que as transformações são duradouras, perenes. Bom para o profissional, para o líder e principalmente para a empresa. Gilberto Cury é presidente da SBPNL - Sociedade Brasileira de Programação Neurolinguística.",pt,152
33,2505,1475597263,CONTENT SHARED,-5920475612630001479,2754566407772265068,8294541181816066509,"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36",MG,BR,HTML,https://hackernoon.com/how-it-feels-to-learn-javascript-in-2016-d3a717dd577f?gi=9a4d3c4ae4de,how it feels to learn javascript in 2016,"Edit: Thanks for pointing typos and mistakes, I'll update the article as noted. Discussion in The following is inspired by the article ""It's the future"" from Circle CI. You can read the original . This piece is just an opinion, and like any JavaScript framework, it shouldn't be taken too seriously. No JavaScript frameworks were created during the writing of this article. -The actual term is Front End engineer, but yeah, I'm the right guy. I do web in 2016. Visualisations, music players, flying drones that play football, you name it. I just came back from JsConf and ReactConf, so I know the latest technologies to create web apps. HackerNews -Oh my god no, no one uses jQuery anymore. You should try learning React, it's 2016. and -It's a super cool library made by some guys at Facebook, it really brings control and performance to your application, by allowing you to handle any view changes very easily. . -Yeah, but first you need to add React and React DOM as a library in your webpage. Hey, I got this new web project, but to be honest I haven't coded much web in a few years and I've heard the landscape changed a bit. You are the most up-to date web dev around here right? -So one is the actual library and the second one is for manipulating the DOM, which now you can describe in JSX. Cool. I need to create a page that displays the latest activity from the users, so I just need to get the data from the REST endpoint and display it in some sort of filterable table, and update it if anything changes in the server. I was thinking maybe using jQuery to fetch and display the data? -JSX is just a JavaScript syntax extension that looks pretty much like XML. It's kind of another way to describe the DOM, think of it as a better HTML. Oh, OK. What's React? -It's 2016. No one codes HTML directly anymore. That sounds neat. Can I use React to display data from the server? -Not quite. You need to add Babel, and then you are able to use React. Wait, why two libraries? -Oh, Babel is a transpiler that allows you to target specific versions of JavaScript, while you code in any version of JavaScript. You don't HAVE to include Babel to use ReactJS, but unless you do, you are stuck with using ES5, and let's be real, it's 2016, you should be coding in ES2016 like the rest of the cool kids do. JSX? What is JSX? -ES5 stands for ECMAScript 5. It's the edition that has most people target since it has been implemented by most browsers nowadays. What's wrong with HTML? -Yes, you know, the scripting standard JavaScript was based on in 1999 after its initial release in 1995, back then when JavaScript was named Livescript and only ran in the Netscape Navigator. That was very messy back then, but thankfully now things are very clear and we have, like, 7 editions of this implementation. Right. Anyway, if I add these two libraries then I can use React? -The fifth and seventh edition respectively. Another library? What's Babel? -You mean ES6? Yeah, I mean, each edition is a superset of the previous one, so if you are using ES2016, you are using all the features of the previous versions. ES5? ES2016? I'm getting lost over here. What's ES5 and ES2016? -Well, you COULD use ES6, but to use cool features like async and await, you need to use ES2016. Otherwise you are stuck with ES6 generators with coroutines to block asynchronous calls for proper control flow. ECMAScript? -It's 2016 man, no one uses jQuery anymore, it ends up in a bunch of spaghetti code. Everyone knows that. 7 editions. For real. And ES5 and ES2016 are? -Well, you include those three libraries but bundle them up with a module manager to load only one file. Wait, what happened with the sixth? -The definition depends on the environment, but in the web we usually mean anything that supports AMD or CommonJS modules. Right. And why use ES2016 over ES6 then? -Definitions. There are ways to describe how multiple JavaScript libraries and classes should interact. You know, exports and requires? You can write multiple JavaScript files defining the AMD or CommonJS API and you can use something like Browserify to bundle them up. I have no idea what you just said, and all these names are confusing. Look, I'm just loading a bunch of data from a server, I used to be able to just include jQuery from a CDN and just get the data with AJAX calls, why can't I just do that? -It's a tool that allows you to bundle CommonJS described dependencies to files that can be run in the browser. It was created because most people publish those dependencies in the npm registry. Right. So my alternative is to load three libraries to fetch data and display a HTML table. -It's a very big public repository where smart people put code and dependencies as modules. I see. And what's a module manager? -Not really. It's more like a centralised database where anyone can publish and download libraries, so you can use them locally for development and then upload them to a CDN if you want to. Riiight. And AMD and CommonJS are...? -Yes, but it's 2016 now, no one uses Bower anymore. OK, that makes sense... I think. What is Browserify? -Yes. So for instance, if you want to use React , you download the React module and import it in your code. You can do that for almost every popular JavaScript library. npm registry? -Angular is so 2015. But yes. Angular would be there, alongside VueJS or RxJS and other cool 2016 libraries. Want to learn about those? Like a CDN? Oh, like Bower! -It is, that's why you use a task manager like Grunt or Gulp or Broccoli to automate running Browserify. Heck, you can even use Mimosa. Oh, I see... so I need to download the libraries from npm then? -Task managers. But they are not cool anymore. We used them in like, 2015, then we used Makefiles, but now we wrap everything with Webpack. Oh, like Angular! -Yeah, but apparently in the web we love making things complicated and then going back to the basics. We do that every year or so, just wait for it, we are going to do assembly in the web in a year or two. Let's stick with React, I'm already learning too many things now. So, if I need to use React I fetch it from this npm and then use this Browserify thing? -It's another module manager for the browser while being kind of a task runner as well. It's like a better version of Browserify. That seems overly complicated to just grab a bunch of dependencies and tie them together. -Well, maybe not better, it's just more opinionated on how your dependencies should be tied. Webpack allows you to use target different module managers, and not only CommonJS ones, so for instance native ES6 supported modules. Grunt? Gulp? Broccoli? Mimosa? The heck are we talking about now? -Everyone is, but you shouldn't care anymore with SystemJS. Makefiles? I thought that was mostly used on C or C++ projects. -Well, unlike Browserify and Webpack 1.x, System is dynamic module loader that allows you to tie multiple modules in multiple files instead of bundling them in one big file. Sigh. You mentioned something called Webpack? -Yes, but because HTTP/2 is coming now multiple HTTP requests are actually better. Oh, Ok. Why is it better? -Not really. I mean, you could add them as external scripts from a CDN, but you would still need to include Babel then. I'm extremely confused by this whole CommonJS/ES6 thing. Jesus christ, another noun-js. Ok, and what is this SystemJS? Wait, but I thought we wanted to build our libraries in one big file and load that! -I would transpile it from Typescript using a Webpack + SystemJS + Babel combo. Wait, so can't we just add the three original libraries for React?? -Typescript IS JavaScript, or better put, a superset of JavaScript, more specifically JavaScript on version ES6. You know, that sixth version we talked about before? Sigh. And that is bad right? -Oh, because it allows us to use JavaScript as a typed language, and reduce run-time errors. It's 2016, you should be adding some types to your JavaScript code. -Yes, you would be including the entire babel-core, and it wouldn't be efficient for production. On production you need to perform a series of pre-tasks to get your project ready that make the ritual to summon Satan look like a boiled eggs recipe. You need to minify assets, uglify them, inline css above the fold, defer scripts, as well as- -Flow as well, although it only checks for typing while Typescript is a superset of JavaScript which needs to be compiled. I got it, I got it. So if you wouldn't include the libraries directly in a CDN, how would you do it? -It's a static type checker made by some guys at Facebook. They coded it in OCaml, because functional programming is awesome. Typescript? I thought we were coding in JavaScript! -It's what the cool kids use nowadays man, you know, 2016? Functional programming? High order functions? Currying? Pure functions? I thought ES2016 was already a superset of ES6! WHY we need now this thing called Typescript? -No one does at the beginning. Look, you just need to know that functional programming is better than OOP and that's what we should be using in 2016. And Typescript obviously does that. -So was Java before being bought by Oracle. I mean, OOP was good back in the days, and it still has its uses today, but now everyone is realising modifying states is equivalent to kicking babies, so now everyone is moving to immutable objects and functional programming. Haskell guys had been calling it for years, -and don't get me started with the Elm guys- but luckily in the web now we have libraries like Ramda that allow us to use functional programming in plain JavaScript. Sigh... and Flow is? -No. Ramda. Like Lambda. You know, that David Chambers' library? OCaml? Functional programming? -David Chambers. Cool guy. Plays a mean Coup game. One of the contributors for Ramda. You should also check Erik Meijer if you are serious about learning functional programming. I have no idea what you just said. -Functional programming guy as well. Awesome guy. He has a bunch of presentations where he trashes Agile while using this weird coloured shirt. You should also check some of the stuff from Tj, Jash Kenas, Sindre Sorhus, Paul Irish, Addy Osmani- Wait, I learned OOP in college, I thought that was good? -Well, you actually don't fetch the data with React, you just display the data with React. Are you just dropping names for the sake of it? What the hell is Ramnda? -You use Fetch to fetch the data from the server. David who? -I know right? Fetch it's the name of the native implementation for performing XMLHttpRequests against a server. And Erik Meijer is...? -AJAX is just the use of XMLHttpRequests. But sure. Fetch allows you to do AJAX based in promises, which then you can resolve to avoid the callback hell. Ok. I'm going to stop you there. All that is good and fine, but I think all that is just so complicated and unnecessary for just fetching data and displaying it. I'm pretty sure I don't need to know these people or learn all those things to create a table with dynamic data. Let's get back to React. How can I fetch the data from the server with React? -Yeah. Every time you perform an asynchronous request against the server, you need to wait for its response, which then makes you to add a function within a function, which is called the callback pyramid from hell. Oh, damn me. So what do you use to fetch the data? -Indeed. By manipulating your callbacks through promises, you can write easier to understand code, mock and test them, as well as perform simultaneous requests at once and wait until all of them are loaded. I'm sorry? You use Fetch to fetch the data? Whoever is naming those things needs a thesaurus. -Yes, but only if your user uses an evergreen browser, otherwise you need to include a Fetch polyfill or use Request, Bluebird or Axios. Oh, so AJAX. -It's JavaScript. There has to be thousands of libraries that all do the same thing. We know libraries, in fact, we have the best libraries. Our libraries are huuuge, and sometimes we include pictures of Guy Fieri in them. Callback hell? -They are libraries to perform XMLHttpRequests that return promises. Oh, Ok. And this promise thing solves it? -We don't use the ""J"" word in 2016 anymore. Just use Fetch, and polyfill it when it's not in a browser or use Bluebird, Request or Axios instead. Then manage the promise with await within an async function and boom, you have proper control flow. And that can be done with Fetch? -Await allows you to block an asynchronous call, allowing you to have better control on when the data is being fetch and overall increasing code readability. It's awesome, you just need to make sure you add the stage-3 preset in Babel, or use syntax-async-functions and transform-async-to-generator plugin. How many libraries do I need to know for god's sake? How many are of them? -No, insane is the fact you need to precompile Typescript code and then transpile it with Babel to use await. Did you just say Guy Fieri? Let's get this over with. What these Bluebird, Request, Axios libraries do? -It does in the next version, but as of version 1.7 it only targets ES6, so if you want to use await in the browser, first you need to compile your Typescript code targeting ES6 and then Babel that shit up to target ES5. Didn't jQuery's AJAX method started to return promises as well? -Look, it's easy. Code everything in Typescript. All modules that use Fetch compile them to target ES6, transpile them with Babel on a stage-3 preset, and load them with SystemJS. If you don't have Fetch, polyfill it, or use Bluebird, Request or Axios, and handle all your promises with await. It's the third time you mention await but I have no idea what it is. -Is your application going to handle any state changes? This is insane. -Oh, thank god. Otherwise I would had to explain you Flux, and implementations like Flummox, Alt, Fluxible. Although to be honest you should be using Redux. Wat? It's not included in Typescript? -Oh, if you are just displaying the data you didn't need React to begin with. You would had been fine with a templating engine. At this point I don't know what to say. -I was just explaining what you could use. We have very different definitions of easy. So, with that ritual I finally fetched the data and now I can display it with React right? -I mean, even if it's just using templating engine, I would still use a Typescript + SystemJS + Babel combo if I were you. Err, I don't think so. I just need to display the data. -There's a lot, which one you are familiar with? I'm going to just fly over those names. Again, I just need to display data. -jTemplates? jQote? PURE? Are you kidding me? Do you think this is funny? Is that how you treat your loved ones? -Transparency? JSRender? MarkupJS? KnockoutJS? That one had two-way binding. Stop. Just stop. -PlatesJS? jQuery-tmpl? Handlebars? Some people still use it. I need to display data on a page, not perform Sub Zero's original MK fatality. Just tell me what templating engine to use and I'll take it from there. -Mustache, underscore? I think now even lodash has one to be honest, but those are kind of 2014. Ugh, can't remember the name. It was a long time ago. -Jade? DustJS? Err, doesn't ring a bell. Another one? Another one? -Nunjucks? ECT? Maybe. Are there similar to that last one? -Mah, no one likes Coffeescript syntax anyway. Jade? Err.. maybe it was newer. -I meant Pug. I meant Jade. I mean, Jade is now Pug. No. -Probably just ES6 native template strings. No. No. No, you already said Jade. Sigh. No. Can't remember. Which one would you use? Let me guess. And that requires ES6. Which, depending on what browser I'm using needs Babel. Which, if I want to include without adding the entire core library, I need to load it as a module from npm. Which, requires Browserify, or Wepback, or most likely that other thing called SystemJS. -Just don't forget to polyfill Fetch if it's not supported, Safari still can't handle it. Which, unless it's Webpack, ideally should be managed by a task runner. -That's fine, in a few years we all are going to be coding in Elm or WebAssembly. But, since I should be using functional programming and typed languages I first need to pre-compile Typescript or add this Flow thingy. -I hear you. You should try the Python community then. And then send that to Babel if I want to use await. -Ever heard of Python 3? So I can then use Fetch, promises, and control flow and all that magic. You know what. I think we are done here. Actually, I think I'm done. I'm done with the web, I'm done with JavaScript altogether. I'm just going to move back to the backend. I just can't handle these many changes and versions and editions and compilers and transpilers. The JavaScript community is insane if it thinks anyone can keep up with this. Why?",en,150
34,1273,1465223368,CONTENT SHARED,8890720798209849691,1895326251577378793,-1847389231177111235,,,,HTML,https://www.nngroup.com/articles/top-intranet-trends/?utm_source=Alertbox&utm_campaign=2449df7535-Diary_Studies_Top_Intranet_Trends_06_06_2016&utm_medium=email&utm_term=0_7f29a2b335-2449df7535-40280153,top 10 intranet trends of 2016,"Summary: Hero images, carousels, fat footers, video, minimalist design, and responsive navigation, are among some of the top feature trends of the best intranets of 2016. We even see a revival of online help that's actually helpful to employees exploring new features or attempting complex tasks. Each winning intranet has its own style, feature set, and personality, unique and special in its own way. But, great minds do think alike, and some themes and features are common in multiple or all of the winning intranets in our Intranet Design Annual 2016 . The 10 best-designed intranets for 2016 may be leading-edge cases, but the trends in their design should spread to more mainstream intranets in the next few years. Although every intranet feature won't work well at every organization, feature trends from outstanding intranets can inspire your intranet redesign. Stay ahead of the curve and consider taking on some of the following intranet design trends now: Help and tutorials Simple, minimalist-looking design Better photos Search evolution Carousels and heroes Fat footers Left-side navigation Social features targeted at particular topics or groups Video Business communication Help and Tutorials After many years of being chastised for being unhelpful, online help went out of fashion. This was reinforced by the idea that an interface should stand on its own, and not need help to be usable. This year, however, we saw a resurgence of Help that is helpful. While the winning-intranets' mostly employ nice-and-easy user interfaces, occasional more-involved or new interface elements do benefit from well-designed help. Help features can aid in discoverability of functionality, and expedite employees' learning and understanding of the intranet and its capabilities. At Intermountain Healthcare, employees who need a little assistance can refer to the Help section in the right rail. Similarly, The Co-operators provides thorough, wide-ranging guidance about how to make the most of its intranet. Simple, Minimalist-Looking Design The "" flat and boxy"" designs prevalent in years past are much less pronounced this year. Most of the designs, however, continue to boast a simple, sometimes minimalist , aesthetic. Some designs, such as those from Enbridge; Repsol; and Cadwalader, Wickersham & Taft, use generous white space. Intermountain Healthcare, NAV CANADA, Swedish Parliament, and (to a lesser degree) American Cancer Society all use rectangles for a boxy, easy-to-scan design. Better Photos Whether it indicates a strong commitment to photography , more people sharing photos, or simply today's better phone cameras, the photographs on the winning intranets are quite engaging. Photos typically relate strongly to the material they accompany and often show employees doing their work. For example, the Enbridge site shows an employee evaluating a particular job site. DORMA shows two employees joking around. Search Evolution Intranet search is a lion that the best intranets tame . The evolution of search on this year's winning sites is impressive. To enhance its intranet search, Cadwalader, Wickersham & Taft consolidates data sources to produce a single point of entry to knowledge resources. Salini Impregilo's search, which appears on every page, allows employees to search the entire intranet for news, people , projects, and documents. Carousels and Heroes Although the carousel is still a prominent feature on winning intranets, the hero is making a comeback. Some organizations, such as NAV CANADA and Repsol, still opt for multiple images and statements in one area, while others, such as Salini Impregilo and Intermountain Healthcare prefer one hero image to make the desired statement. Fat Footers Large footers at the bottom of pages became popular a few years ago and remain common today. Employees often know that the information they're seeking is on the intranet, but they can't always find it. When using a public-facing website, people may have the option to leave the site, and will. But employees often know that the intranet is the main source or the only place to turn to to find particular content. Providing organized links at the bottom of pages provides employees with one more chance to locate what they need. The content in these footers can be arranged in many different ways: repeat the global navigation suggest related content present popular links On the NAV CANADA intranet, the wide footer navigation repeats the global navigation topics and offers a list of the megamenu links. The Enbridge footer includes links to the public sites of Enbridge companies and information for contacting the Enterprise Service Desk. It also lists Ethics and Conduct information, along with a reminder: "" Let's work together to maintain a respectful workplace ."" All of these visually use an obvious aesthetic element to indicate the footer; this element can be: colored background that is different from that of the rest of the page border (line) delineating the footer from the page content combination of the above Side Global Navigation Because mega menus don't work in a phone UI, designers often use a mega menu for desktop navigation , and an accordion or some other option for mobile navigation . Some teams opt for menu UIs that translate easily from desktop to mobile . One such pattern popular this year is the vertical navigation bar down the left side of the page. The Swedish Parliament and Cadwalader, Wickersham & Taft are among the winning organizations that follow this navigation pattern. Targeted Social A great trend from past years continues this year: presenting social tools in an understandable, targeted way. Gone are the days of displaying a wall feed on the homepage or in personal profiles with no additional description or context. Great intranets use social features to encourage further communication about important or trending topics. DORMA's CEO participates in the social features, leading employees by example and playing a big role in the social features' success. Two major company events occurred near the features' launch; these events provided a source of content, increased interest, and natural momentum for success. The Cadwalader, Wickersham & Taft intranet lists recent hires and employee anniversaries on the homepage. Such seemingly simple features can go a long way toward building a sense of community and inclusiveness in an organization. The Co-operators offers a variety of social features, including the weekly Five Minutes With ... (an employee-profiles feature), polls, achievements, Popular Links , and the ability for users to submit news. Also, in the site's executive blogs, senior leaders share their knowledge and opinions and ask employees to do the same. The Trending Now section on the American Cancer Society intranet summarizes the site resources, search terms, and pages that receive the most traffic. This section is a simple and automated way to keep users informed of the site's most popular items. Enbridge employees can easily access the company's public information on social sites - including Twitter, Facebook, LinkedIn, and YouTube - via links in the middle of the homepage. This section also includes a link to the @enbridge blog. Providing quick access to external sites raises employee awareness about information that Enbridge is sharing with the public and about the conversations occurring on social-media sites. Video Tools to create, edit, and post videos have made them accessible and easy to deal with. Prevalence in social channels has lowered the expectation for high-quality video production . In fact, many people welcome the simplicity and folksiness of more realistic, just-shot-myself type of videos. With these changes, individuals, teams, and even high-level managers are taking advantage of video and sharing information in this way. Videos are often stored in their own section of the intranet, which allows employees to sort, filter, and search by topic. But videos, like written content, are also presented on the homepage, in news sections, and cross linked from related contend. Business Communication Business people have learned that the intranet is the perfect place to communicate their goals and statuses to all employees. This information helps employees realize how the organization is doing and motivates them to work toward and achieve organizational goals. It also adds a level of respect, signaling to all employees that they are important enough to know where the organization has set it sights, and that that each person can play positive a role in those plans. Conclusion Whether your organization is large or small; formal or informal; public, private or government; consider which of the trends here can be implemented, deployed, and used successfully on your intranet. Choose a few to do over the next year to enhance the user experience, and increase the business value of your intranet. Full Report For more information about themes, intranet best practices, and full-color screenshots of the 10 winners, download the 2016 Intranet Design Annual . The report download comes with a folder containing each image as a .png to make it possible to zoom in and study the designs in detail.",en,150
35,1887,1469561171,CONTENT SHARED,4105873627900556540,-2525380383541287600,8115788345630016900,,,,HTML,http://agiletesters.com.br/topic/103/a-arte-de-desenvolver-testes-cucumber-capybara,sua comunidade de teste.,"Esse é o segundo artigo sobre desenvolvimento de testes e dessa vez resolvi falar um pouco de cucumber + capybara e vem sendo o framework para meus testes. Hoje existem diversos artigos que falam sobre cucumber e minha intenção é mostrar como funciona os steps de forma mais fácil. Bem, vamos primeiro partir para instalação e configuração do ambiente para começarmos a desenvolver os testes. Como eu venho utilizando MAC para desenvolvimento, vou focar a maior parte do tempo nele, porém fiquem a vontade para perguntar algo caso utilizem Linux ou Windows (o conceito será o mesmo). Antes de mais nada, baixar o Xcode pelo link: . Aprendi que sempre que instalar o SO, a primeira coisa será instalar o Xcode. Bem, depois do Xcode, vamos baixar o Homebrew via terminal com o comando: ruby -e ""$(curl -fsSL )"" . Para explicar o que é o homebrew basta dizer: ""O Homebrew instala as coisas que você precisa que a Apple não forneceu para você."" - fonte: . Ou seja, é um gerenciador de pacotes. Reinicie a máquina (isso mesmo, para ""completar as instalações""). Com o Homebrew instalado, será necessário instalar o ""Qt"" - Framework multiplataforma para desenvolvimento de interfaces gráficas - fonte: ( ) e serve basicamente para podermos utilizar o selenium como driver padrão de execução para os testes. Para instalar o Qt, é bem simples: ""brew install qt"". Por default o ruby já vem instalado, mas vou deixar registrado os comandos para instalar a versao 1.9.3-p545: curl -sSL | bash -s stable --ruby=1.9.3-p545 Ps: Caso o ""curl"" não estiver instalado, basta instalar com ""brew install curl"" Bem, agora vamos ao que de fato insteressa, as gems, então vamos a lista das básicas: Com isso tudo já da para trabalhar um pouquinho \o/. Bem, a estrutura básica para um projeto em cucumber é essa: Para explicar, dentro da pasta ""specifications"" se encontra aquela famosa estrutura que vemos em todos os artigos por ai: language: pt (colocar cerquilha antes) Funcionalidade: Aprendendo a trabalhar com capybara Cenario: Exemplo basico de cadastro Dado que eu acesse o facebook Quando eu preencher os campos de cadastro E clicar em Abrir uma conta Então primeiro cadastro completo Até ai beleza e depois??? Depois salve o arquivo como ""cadastro.feature"". Depois que escrevemos, vamos a parte das ""configurações"" no arquivo ""env.rb"", pois é lá que definimos as gems que eu vou utilizar e claro, definir o driver que eu vou utilizar como base para execução dos testes. O modelo básico vai ficar da seguinte forma: Depois de configurar (por hora são apenas essas), vamos deixar o teste pronto para ""desenvolver"", então, basta ir no terminal, navegar até a pasta features de seu projeto e executar o comando ""cucumber"". É só isso??? Não!! O que fizemos apenas foi pedir para o cucumber ""ajustar"" os steps para podermos dar vida ao desenvolvimento, e vai ficar algo mais ou menos assim: language: pt (colocar cerquilha antes) Funcionalidade: Aprendendo a trabalhar com capybara Agora a brincadeira vai começar a ficar engraçada, como vocês viram, não executou por estar apenas em português estruturado como algumas pessoas já me perguntaram, pois eu preciso agora dar o caminho das pedras para que cada step possa fazer sentido, pois é a partir daí que vamos começar a desenvolver. A primeira dica que eu dou é seguir passo a passo, ou seja, sempre que você executar e terminar um passo, começe outro. Alguns lugares falam pra copiar tudo e colocar já em um arquivo.rb, mas faça isso não, vai um por um que você tem melhores resultados . Então, vamos pegar o primeiro passo e colocar em um arquivo novo: Considere cada passo como um método a ser executado, e em ruby todo método finaliza com um end, com cucumber, os inícios Dado, Quando, Então, E são os inícios desses métodos. A base de execução do cucumber é em cima de Expressões Regulares (regex), o que quer dizer que vamos nos deparar sempre com os caracteres (/^ $/), etc. Mas não vou entrar no contexto das expressões regulares agora, mas explicando a regex (/^que eu acesse o facebook$/), sempre que em alguma feature eu escrever ""Dado que eu acesse o facebook"", ele será executado, sem que eu precise escrever o código todo de novo, ou seja, economizo muito tempo =). Bom, mãos na massa, e para isso, a frase ""pending # express the regexp above with the code you wish you had"" será substituída por comandos da gem capybara ( ). Como eu falei no primeiro post A arte de desenvolver testes, o coração da automação está em fazer as tarefas que são repetitivas se tornarem automáticas e para isso eu tenho que encontrar e trabalhar em cima dos elementos da minha página e para isso eu tenho ferramentas que me facilitam, a que eu mais gosto é o Firebug (complemento do firefox) e posso encontrar os elementros apenas selecionando, e posso escolher id, css ou xpath para poder indicá-los. Partindo deste princípio, o desenvolvimento fica muito mais fácil, pois geralmente o que eu faço nos elementos é clicar, informar um valor e validar. No capybara basicamente se utilizam os comandos: visit ' ' - Para visitar alguma url. page.find(:id, ""id do elemento"").click - Clica em um elemento definido por ID. page.find(:css, ""css do elemento"").click - Clica em um elemento definido por CSS. page.find(:xpath, ""xpath do elemento"").click - Clica em um elemento definido por XPATH. page.all(:id, ""id do elemento"")[0].click - Clica no primeiro elemento dentro de uma lista definido por ID. page.all(:css, ""css do elemento"")[0].click - Clica no primeiro elemento dentro de uma lista definido por CSS. page.all(:xpath, ""xpath do elemento"")[0].click - Clica no primeiro elemento dentro de uma lista definido por XPATH. PS: Quando nos depararmos com um checkbox, radiobutton, utilizar da seguinte forma: page.find(:radio_button, 'nome do radiobutton').set(true) - Nesse caso, ele vai selecionar aquele radiobutton. page.find(:checkbox, 'nome do checkbox').set(true) - Nesse caso, ele vai selecionar aquele checkbox. fill_in 'nome do elemento para inserir valor', :with => ""Aprendendo Capybara"" - Irá inserir no elemento a string Aprendendo Capybara. select 'Nome do item no Drop Down', from: 'nome do elemento drop down' - Seleciona um item de um drop down. ex: select 'Apto', from 'tipo_moradia' click_button 'Cadastrar' - Clic no botão cadastrar. click_link 'Home' - Clica no link Home caso haja algum na página. expect(page).to have_content 'Cadastro efetuado com sucesso' - Procura a mensagem e caso tenha, será sucesso. Basicamente é isso =). Vou colocar todos os steps abaixo para mostrar como ficaria =) para não ficar maior que já está essa mini aula rs: Salve o arquivo com o nome cenario1.rb. No terminal, navegue até a pasta do projeto e digite cucumber. O resultado será esse: Note que o mais legal de trabalhar com cucumber é a facilidade que ele tem de chamar um cenário ou outro através de frases, frases essas que são nada mais nada menos que os próprios critérios de aceite de uma estória, ou seja, desenvolver o critério de aceite se torna rápido e fácil, e para o review, mostrar tudo verdinho é segurança de deploy =). Desculpem-me por alongar esse assunto, mas acho necessário passar uma visão mais simplista de como desenvolver os testes, não fizemos mágica alguma para realizar a primeira etapa do cadastro do facebook em 14 segundos. Para quem está começando, esse é o caminho das pedras!!! Há formas de rodar em background (sem abrir navegador), o que vai mais rápido ainda o teste, mas isso vai ficar para a próxima. Dúvidas, sugestões, críticas são muito bem vindas, meu email é thiagomarquessp@gmail.com caso alguém queira saber de cara como rodar em background. Desenvolver testes é muito mais legal do que parece!!! =) Algumas fontes legais: Capybara doc: Regex doc (as que eu mais gosto): e",pt,148
36,2994,1485194633,CONTENT SHARED,-6728844082024523434,801895594717772308,7194441186926042361,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36",MG,BR,HTML,http://merowing.info/2017/01/seniority/,seniority,"People use different words to classify Engineer skill, some companies give you more senior role just based on a number of years you have been working there, but what does it mean to be Senior? In this article, I composed a complete and final list of API's that you need to know to classify yourself as Senior Engineer. Here's a list of all the APIs you must know: * API's don't matter If you thought that what makes people Senior is memorizing API's, then this article is most definitely for you. Sure, experience matters and helps you be a better developer, but knowing any particular API is not of great importance, anyone can learn a new API given enough time. How each company gives title is very arbitrary, sometimes it's based on years of experience, other times, unfortunately, its a matter of how friendly you are with management. What matter is not the title you hold, but what you represent. Technical skills will usually come to you naturally with years. I want to focus on something many people miss: soft skills. Let's split this into Personal Principles and Working with Others. Personal Principles Reliable You need to be reliable, never promise you do something and then drop the ball. Others people work often depends on being able to gauge when your piece of the puzzles are ready, for that reason it is better to be on the safe side with estimates and over-deliver than to promise too much and fail to deliver. There are always tasks you can pick up if you finish your sprint sooner, seniors are proactive and do not sit on their hands until they are assigned new tasks. Accountable The only person that does not make mistakes is the person that does nothing. Making mistakes, or hitting roadblocks is normal. More experienced people know that when that happens they let other people know, and they are not afraid to ask for help. Don't be afraid to admit your mistakes, it is only human, and it will build more trust when working with others. Flexible Don't get attached to particular framework or language. Only one thing is sure: you will change them many times in your career . Some languages that I used early in my career are obsolete now. I would go further and suggest that you should embrace change , force yourself to challenge your assumptions, it makes you less ignorant of other opinions/ways. There is more than one way to skin a cat. I usually have some side-project going, and I have a rule that every six months I try using a different approach to things. It allows me to distinguish the best solutions from those I am used to. Endowment bias states that we value things we own/know more than they are worth. One thing to remember is that change is hard and requires time. Trying something for a couple of days just won't cut it. Pragmatic Many programmers would like to just focus on the engineering side of things, always do the cleanest code they can and ignore the business side of things. That is not how our jobs work. People hire you because they need to solve some specific problems for their core business, be it a new app or anything else, not to write code for the sake of code. Delivering things matters . You need to invest time into understanding the business requirements, only knowing them you can make well-informed decisions and focus your work on things that matter. Working with others A single person rarely creates software products on their own. You have to work with other people. Invest time into learning how to be a team player. This will make a huge difference in how your career progresses. Lead by example People will look up to you and how you behave will heavily influence team culture. If you are: Proactively grabbing new tasks from the backlog when you are free Looking for ways to improve the architecture and tooling Teaching and volunteering to pair program or help with issues Giving everyone space to state their opinion and discuss with arguments Treat everyone as an equal conversation partner Other people will adopt those behaviors, and your team will prosper. Listen and learn from others In engineering, there are many different ways to achieve the same end results. It's way too easy to get set in our ways, we have to exercise our flexibility. Be open to other opinions, do not dismiss other people ideas because they are different to your default thought. Spend some time discussing others' opinions to better understand what they mean and how their ideas would work, all the pros and cons, etc. Only then you can evaluate whose approach makes more sense. It does not matter whether the idea comes from people that are more experienced than you or less. I have learned plenty useful things from Junior developers in my teams, they often have fresh ideas and challenge our assumptions. Which bring me to another important point: Do not leverage your 'power' or 'seniority' . Never treat people as inferior and never use arguments like 'Let's do X because I've higher position than you / I've been programming for N years.' When an Engineer uses that kind of argumentation, it makes him look like a child from kindergarten, not a professional. Don't be that person. Use factual arguments. If you cannot find reasons behind your way of doing things, maybe it is not that good? Avoid being reactive, discussions can get heated, but a professional understands that a critique of an idea is not the same as a critique of a person. Pause and think before replying, and don't interrupt other people. Let them finish and focus on understanding their message before discussing it. Learning how to manage your impulses and emotions is crucial in living with others. Learning time management will allow you to maintain sanity. Take a look at some recommended reading. Become a mentor to other devs Mentoring other developers is incredibly rewarding, and it also helps foster the right team culture. I want everyone in the team to feel comfortable asking me any questions. Be it to learn how to do something or questioning my decisions. I have seen few teams that people were afraid to ask questions because managers were awful and used that to evaluate people performance based on that. Encourage your teammates to ask for help, or advice how to do something. If you want to be the mythical 10x engineer, help other engineers in your team grow. Remember: There are no stupid questions, and if people are asking about your code it usually means it can be simplified or improved in some way. Actively looking for ways to improve the process No process is ideal. You should always be on the lookout for improvements. If you criticize something, always propose alternatives. You can have excuses or results, but not both. If you see pain points when working on your code, or when observing your teammates, think about how they how they could be resolved. You do not immediately jump on writing another library or tool if something happens twice. However, when a pattern keeps repeating itself many times for different people, it means it is time to look at ways of improving it. iOS community is one of the best communities I had the privilege to be a part of. I would encourage you to open source those tools and libraries and share your experienced with others. Writing about things often makes you realize how much you still have to learn. Subscribe to newsletters like iOS Dev Weekly or Swift News for ideas. Courage to fight for healthy team culture I cannot stress this point enough, I have seen many people complain about their team culture over the years, my question is always 'What are you doing about it?' You need to fight for a healthy team culture, if you see that someone is misbehaving do not be afraid to speak up . Propose things like team contract or code of conduct. The way I think about this is simple: If I see something I do not like, then I question it in the team environment. Otherwise, the team might become toxic, and if that happens, I quit. If you are going to quit otherwise, why are you afraid of fighting for your team? Many people might be afraid to speak up, especially minorities and introverts. Often they will reach out to you in private to say thanks. Things to look for: Everyone is equal, make sure everyone can express his or her opinion, even those people that are shy. When people are interrupted by someone, make sure to make it easy for them to speak their mind e.g. ""Hey X, you were saying?"" Conclusion Being a good developer and team member is about so much more than programming. With years you can gain much technical experience, but if you do not invest time and energy into improving your soft skills, you can create a bottleneck for your career, one that you might not even see. Read more about how I work with clients I'd like to thank Paul Yorke , Cezary Bielecki , Gabriel Peart and Adam Shott from The New York Times for helping me with initial draft of this article.",en,148
37,1293,1465324965,CONTENT SHARED,2555983212310147009,7774613525190730745,5630253306675486090,,,,HTML,https://www.infoq.com/articles/Database-Version-Control,the definitive guide to database version control,"In the brave new world of big data and business intelligence, the only technology constant is change, especially when it comes to the database. Almost daily changes in business needs due to demographics, increased service delivery demand and the regulatory environment drive database change. So when it comes to database change, agility through automation - the ability to do more with less more rapidly to accelerate delivery - is what differentiates highly competitive, world-class enterprises from the rest of the crowd. If your competitor can deliver relevant features faster, with better quality, you will lose market share. Agile development arose from the need to move more quickly to deal with ever-changing requirements, ensuring optimum quality despite resource constraints. The big release approach is obsolete - waiting six months until the next rollout or release is self-defeating. Agile development reduces release scope to complete each change faster, minimizing the impact of each change. Agility is expected from tech companies and IT divisions to support changing business needs. The next logical step is to link development with operations - to adopt DevOps. To master Agile sprint deployments effectively with DevOps, you need the ability to implement deployment and process automation internally within development and QA, or to production. Otherwise, deployments and releases will require manual steps and processes, which are prone to human error and cannot be frequently replicated. The automation required relies on a version control repository that manages all software assets ready to be built and deployed to the next environment. The build process starts by cleaning the working space and getting the relevant files from the version control repository. This critical phase prevents out-of-process changes. These changes can still happen though they can be avoided if developers save their changes directly in the build server working space, instead of checking-in the changes to the version control repository. This example may sound absurd because developers know if they do so, their changes will be lost, as the technology enforces the process. This phase also prevents the build phase from accepting work-in-progress changes by referring to only those changes that were submitted to the version control repository in a check-in process. The version control repository acts as the single source of truth . The Database is a Key Component Most IT applications today have many components using different technologies; mobile, ASP, PHP, application servers, Citrix, databases, etc. These must all be in sync for the application to work. If, for example, a new column was added to a table, or a new parameter was added to a stored procedure, all other application components must be synchronized to the structure change to function correctly. If this synchronization breaks, the application can fail by calling the wrong parameters to the stored procedure, or by trying to insert data without the new column. The unique properties of the database component differentiate it from other components: A database is more than just SQL scripts. It has a table structure, code written in the database language within stored procedures, content saved in reference tables or configuration tables, and dependencies between objects. A database is a central resource. Several developers can work on the same object, so their work must be synchronized to prevent code overrides. Deploying database changes is not as simple as copying and replacing old binaries. Database deployment transforms version A into version B while keeping business data and transferring it to the new structure. Database code exists in any database, and can be directly modified in any environment. This is in contrast to other components, where everything starts from a clean workspace in the build server. Must-Have Requirements Several challenges must be addressed when managing database changes. You must: Ensure all database code is covered (structure, code, reference content, grants) Ensure the version control repository acts as the single source of truth Ensure the deployment script being executed knows environment status when the script is executing Ensure the deployment script handles and merges conflicts Generate a deployment script for only relevant changes Ensure the deployment script knows the database's dependencies There are four common approaches to managing database changes in development and deploying them internally (Dev, QA) or to the production environment. Utilizing SQL alter scripts generated during development Utilizing a changelog activities tracking system Utilizing simple compare & sync Utilizing a database enforced change management solution Utilizing SQL Alter Scripts Generated During Development The most basic method for managing database changes is to save the alter command in a script or set of scripts, and manage them in the exiting file-based version control. This guarantees a single repository that stores all the application component assets. Developers have the same functionality when checking-in changes for the database as they do when they check-in changes for .NET or Java, such as linking the change to the reason (CR, defect#, user story, etc.). Almost any modern file-based version control solution has a merge notification when several developers change the same file. But let's see if this solution actually overcomes the challenges for the database, and avoids the potential pitfalls: Ensures all database code is covered - since the developer or DBA writes the script, they can ensure it will handle all database code. Ensures the version control repository acts as the single source of truth - not really, as the developer/DBA can login directly to the database (in any environment) and make changes directly in the database. Manually-Written SQL Scripts Changes made to the deployment scripts as part of scope changes, branch merges, or re-works are done manually and require additional testing. Two sets of scripts must be maintained - the create script and the alter script for the specific change for the release. Having two sets of scripts for the same change is a recipe for disaster. Ensures the deployment script being executed knows the environment status when the script is executing - this depends on the developer and how the script is written. If the script just contains the relevant alter command, then it is not aware of the environment status when it is executed. This means it may try to add the column although it already exists. Writing scripts that will be aware of the environment status at execution time significantly complicates script development. Ensures the deployment script handles conflicts and merges them - although the file-based version control provides the ability to merge conflicts, this is irrelevant to the database as the version control repository is not 100% accurate and cannot be the single source of truth. The script might override a hot fix performed by another team, leaving no evidence that something went wrong. Generates deployment scripts for only relevant changes - scripts are generated as part of development. Ensuring the scripts include only relevant and authorized changes - based on the tasks being approved - requires changing the script(s), which creates more risk to the deployment and wastes time. Ensures the deployment script knows the database dependencies - developers must be aware of database dependencies during the development of the script. If a single script is being used, then the change is usually being appended. This can result in many changes to the same objects. If many scripts are being used, then the order of the scripts is critical and is maintained manually. Bottom line: not only does this basic approach fail to solve the database challenges, it's also error-prone, time consuming, and requires an additional system to keep track of the scripts being executed. Utilizing a Changelog Activities Tracking System Another common approach is to use XML files, which use an abstract language for the change and keep track of the execution. The most common open source solution for this is Liquibase. With XML files, Liquibase separates the logical change from the physical change, and allows the developer to write the change without knowing the database-specific command. At execution time, it converts the XML to the specific RDBMS language to perform the change. Changes are grouped into a changelog, and they can be in a single XML file or many XML files referred by a major XML file which contains the order of the changes. The XML files can be saved using the existing file-based version control, which offers the same benefits as the basic approach. In addition, based on the Liquibase execution track, it knows which changelog(s) have already been deployed and shouldn't run again, and which were not yet deployed and should be deployed. Let's see if Liquibase answers the challenges: Ensures all database code is covered - managing changes to reference content is not supported in the XML files used by Liquibase, and must be handled as an external addition, which can result in changes being forgotten. Ensures the version control repository acts as the single source of truth - Liquibase doesn't have any version control functionality. It depends on third-party version control tools to manage XML files, so you have the same challenges in making sure the file-based version control repository reflects the version that was tested. The process that will ensure the version control repository can be the single source of truth requires developers to check-in changes in order to test them. This can result in work-in-progress changes being deployed to next environment. Ensures the deployment script being executed knows the environment status when the script is executing - Liquibase knows which changelogs have been deployed and will not execute them again. However, if the logical change is to add a date column that exists in varchar format, the deployment will fail. Also, overrides of out-of-process changes cannot be prevented. Ensures the deployment script handles conflicts and merges them - any change being made to the database outside of Liquibase can cause a conflict, which will not be handled by Liquibase. Out of process changes are not handled Generates deployment scripts for only relevant changes - changes can be skipped at the changelog level, but breaking a changelog into several changelogs requires writing a new XML file, thus requiring more tests. Ensures the deployment script knows the database dependencies - the order of the changes is maintained manually during the development of the changelog XML. Bottom line: using a system that tracks change execution does not address the challenges associated with database development and, as a result, does not meet the deployment requirements. Utilizing Simple Compare & Sync Another common approach is to generate the database change script automatically by comparing the source environment (development) to the target environment (test, UAT, Production, etc.). This saves the developers and DBAs time because they don't have to manually maintain the script if it is a create script or an alter script for the release. Scripts can be generated when needed, and refer to the current structure of the target environment. Let's review the challenges and see if this approach overcomes them: Ensures all database code is covered - most compare & sync tools know how to handle the different database objects, but only a few have the functionality to handle compare & sync of the reference data. Ensures the version control repository acts as the single source of truth - simple compare & sync does not utilize the version control repository when performing the compare and generating the merge script. Ensures the deployment script being executed knows the environment status when the script is executing - the best practice is to generate the script just before executing it so it will refer the current environment status. Ensures the deployment script handles conflicts and merges them - simple compare & sync tools compare A to B (source to target). Based on the simple table at the right, the tool then generates a script to ""upgrade"" the target to match the source. Without knowing the nature of the change, the wrong script can be generated. For example, there is an index in the target that was created from a different branch or critical fix. If this index does not exist in the source environment, what should the tool do? Drop the index? If there is an index in development, but not in production, was it added in development? Dropped in production? Using such a solution requires deep knowledge of each change to make sure they are handled properly. Generates deployment scripts for only relevant changes - the compare & sync tools compare the entire schema and show the differences. They are unaware of the reason behind the changes, as this information is stored in the ALM, CMS, or version control repository, which is external to the compare & sync tool. You might get a lot of background noise, making it difficult to determine what you actually need to deal with. Ensures the deployment script knows the database dependencies - compare & sync tools are aware of database dependencies and generate the relevant DDLs, DCLs, and DMLs in the correct order. Not all compare & sync tools support generating a script that contains objects from several schemas. Bottom line: compare & sync tools satisfy some of the must-have requirements, but fail to deal with others. Scripts must be manually reviewed, and cannot be trusted in an automated process. Utilizing a Database Enforced Change Management Solution Database enforced change management combines enforcement of version control processes on database objects with generation of the deployment script when required, based on the version control repository and the structure of the environment at that time. This approach uses ""build and deploy on-demand,"" meaning the deploy script is built (generated) when needed, not as part of development. This allows for efficient handling of conflicts, merges, and out-of-process changes. Build & Deploy On-Demand How does database enforced change management handle the challenges? Ensures all database code is covered - structure, business logic written in the database language, reference content, database permissions and more are managed. Ensures the version control repository acts as the single source of truth - the enforced change policy prevents anyone using any IDE (even command line) from modifying database objects that were not checked-out before and checked-in after the change. This guarantees the version control repository will always be in sync with the definition of the object at check-in time. Single Process Enforcing Version Control Ensures the deployment script being executed knows the environment status when the script is executing - building (generating) the deployment script when needed (just before executing) guarantees it knows the current environment status. Ensures the deployment script handles conflicts and merges them - by using baselines in the analysis, the nature of the change is known and the correct decision whether to promote the change, protect the target (ignore the change), or merge a conflict is easy. Baseline Aware Analysis Generates deployment scripts for only relevant changes - the integration with application lifecycle management (ALM) and change management systems (CMS) enables you to assign a reason to the change, as is done in the file-based version control or task management system. Ensures the deployment script knows the database dependencies - the sophisticated analysis and script generation algorithm ensures the DDLs, DCLs, and DMLs will be executed in the correct order based on the database dependencies, including inter-schema dependencies. In addition to the must-have requirements, there are other requirements, such as supporting parallel development, merging branches, integrating with the database IDE, and supporting changes originating from data modeling tools. You must verify that these requirements will be addressed by whichever method you choose. Bottom Line The database component has special requirements, and therefore creates a real challenge for automation processes. In the old days when there were only a few releases per year, it was common and understandable to invest time manually reviewing and maintaining the database deployment scripts. Today, with the growing need to be agile and provide releases faster, the database must be part of the automation process. Developing SQL or XML scripts, or using simple compare & sync are either inefficient and/or risky approaches when it comes to automation. The most effective method is to implement database enforce change management . About The Author Uri Margalit is the Director of Product Management at DBmaestro , an Enterprise Software Development Company focusing on database development and deployment technologies. Uri has over 15 years' experience in enterprise software and systems management and has held senior Product Management and R&D Management positions at a number of leading software companies.",en,146
38,1359,1465736425,CONTENT SHARED,1862503310075246782,3915038251784681624,6469413037900917152,,,,HTML,http://cio.com.br/carreira/2016/06/10/cinco-competencias-essenciais-ao-it-leaders/,cinco competências essenciais ao it leaders - cio,"Quanto mais a empresa considera a TI como uma área estratégica, menos valoriza competências técnicas para o CIO. Isso não significa, no entanto, que o líder de TI possa se dar ao luxo de deixar de lado os conhecimentos específicos da sua área. Assim como os super-heróis das histórias em quadrinho, o CIO precisa ter várias identidades. No momento em que está sentado em frente ao board, deve assumir uma postura e um discurso totalmente orientados aos negócios. Já quando encontra-se na mesa de negociação com fornecedores ou conversa sobre o escopo de um determinado projeto com sua equipe, tem de resgatar a bagagem de conhecimentos técnicos. Essa multiplicidade de visões também se aplica às competências exigidas dos CIOs. Isso porque, além da identidade técnica e de negócios, os profissionais são cobrados por sua capacidade de atender às demandas das diversas áreas da companhia e por gerenciar a equipe de TI e os fornecedores. Além disso, eles precisam encontrar tempo para idealizar produtos e serviços inovadores. Equilibrar essas diferentes tarefas representa um fator crucial para o sucesso dos líderes de TI. A seguir, seguem as competências essenciais para os CIOs, na visão de especialistas e de profissionais que atuam no setor: Conhecimento do negócio - Por mais interessantes que as tecnologias pareçam para a equipe de TI, os argumentos técnicos não podem ser utilizados para justificar um projeto para a diretoria e as demais áreas da organização. Assim, os CIOs devem conhecer a fundo o negócio da companhia para entender como as iniciativas da sua área estão alinhadas aos objetivos da organização e quais os resultados práticos esperados. Um projeto de TI é um investimento como qualquer outro da empresa e, em muitas ocasiões, pode inclusive concorrer com as demais áreas. Uma reestruturação de parque tecnológico, por exemplo, necessita estar alinhada à necessidade de crescimento da empresa. Não faz mais sentido trocar só por trocar. Capacidade de comunicação - No dia-a-dia das organizações, boa parte das atividades de TI passa despercebida pelos funcionários da companhia. Na realidade, o CIO e a sua equipe só são lembrados em situações negativas, como quando o sistema cai ou o computador para de funcionar. Com isso, a imagem do trabalho da área de tecnologia da informação fica prejudicada dentro das organizações. E o pior, essa percepção chega até o board da companhia, o que reflete diretamente no humor de investimentos em novos projetos. O CIO que pretende reverter essa situação precisa estar preparado a estruturar uma melhor comunicação de sua área com todos os stackeholders da organização. Para tanto, precisa investir em ferramentas que o ajudem a divulgar as iniciativas de TI a toda a companhia, bem como criar um canal para que os diversos usuários consigam expressar opiniões sobre produtos e serviços oferecidos pela equipe de tecnologia. Gestão de pessoas - Os resultados da área de TI também estão diretamente relacionados à capacidade que o CIO tem para recrutar, reter e desenvolver seus colaboradores. Essa capacidade de gestão e motivação das equipes é essencial a qualquer profissional em posição de liderança, mas tende a ser ainda mais crítica na TI, uma vez que trata-se de um setor no qual faltam pessoas capacitadas e, portanto, a retenção de talentos é essencial. Perfil inovador - Quando buscam um profissional para ocupar a posição de CIO, as empresas buscam pessoas com postura voltada à inovação. Na prática, isso seria representado, por exemplo, por um CIO que, antenado aos lançamentos do mercado no qual atua, percebe uma nova maneira de se relacionar com os clientes e leva essa sugestão à área de marketing. Conhecimento técnico - Desde que o líder da área de tecnologia passou a exercer uma função estratégica nas organizações, existe uma dúvida a respeito sobre, até que ponto, o conhecimento técnico representa algo essencial para quem ocupa a posição de CIO. Todo o conhecimento técnico que o profissional levou anos para adquirir começa a parecer inútil e um pouco enferrujado. Mas os especialistas aconselham que revisitar essas habilidades é extremamente importante para que o líder cultive um repertório necessário para o relacionamento com os técnicos da sua área. Um dos pecados que o CIO comete é distanciar-se do conhecimento técnico. Sem essa habilidade, o profissional não consegue saber como o departamento de TI pode contribuir com as demais áreas da organização e não consegue liderar sua própria equipe. ""Durante os últimos 20 anos, o mercado deu muita ênfase à capacitação voltada aos negócios"", diz a fundadora e presidente da empresa de recrutamento de executivos Valuedance, Susan Cramm. Ela afirma que essa tendência produziu CIOs que, hoje, perderam completamente o contato com a parte técnica da operação de TI. ""E o desempenho desses líderes é afetado negativamente por isso"", afirma a especialista. Susan aconselha os gestores de tecnologia a buscarem maneiras para colocar em prática o conhecimento teórico adquirido na universidade e no início da carreira. ""Entretanto, esse reencontro com o repertório não pode tornar-se uma mais um item na lista de obrigações diárias do CIO"", informa, ao apontar que isso deve ser um exercício prazeroso.",pt,145
39,1034,1463694744,CONTENT SHARED,-1453783314552286835,-1032019229384696495,-3222296078930623200,,,,HTML,https://flights.airberlin.com/en-DE/progressive-web-app,progressive web app - first introduced on google i/o | airberlin,"airberlin is delighted to be the first airline in the world to develop a progressive web app. Hannes Schwager, Head of Mobile & Innovations at airberlin: ""The intelligent networking of mobile services in the airline industry will continue to become increasingly important and we know that our passengers, no matter where they are in the world, want to be able to check in easily and conveniently from their mobile devices. That is why we have worked with our internal innovations team to create an app that combines the best features of an app and a web application."" Progressive web app technology makes it possible for airberlin passengers to access their personal boarding pass and further travel information about their destination at any time, wherever they are and even without an internet connection, after a one-time web check-in. This means airberlin can give its passengers even more flexible travel comfort - and makes the mobile future a little more tangible today. The innovation will be presented exclusively at the prestigious Google I/O developer conference from 18 th to 20 th May 2016 in San Francisco. Google will present the latest hardware and software solutions to a professional audience of more than 7,000 developers and countless live stream viewers during the internet giant's most important technology conference. Progressive web app: all about it All the technical details What is a Progressive Web App? A website that provides an app-like experience Offline first approach using caching Service worker (JavaScript) Available for Chrome 40, Firefox 40 and Opera The benefits of a progressive web app Available offline or with a slow connection Add-to-home icon can be added in the dialogue as needed Push alerts are possible Very fast loading times Use case: beta version PWA look & feel Step by step through the progressive web app Dashboard - Add flight One-step check-in Boarding passes: always on hand Journey details at a glance Name: Schwager PNR: IO2016 How we built the airberlin PWA by Marian Pöschmann and Axel Michel What we used Web Components : Following the simple idea, that everything except the app shell is a component. Using Polymer 1.0 ( ) as a Polyfill, we created components for the slider, which contains all available board-cards, the board-card itself (displaying the destination image and flight information), the boarding pass (QR-code), journey details, destination information, as well as the check-in and confirmation forms. Custom Events : To interact between the different components as well as the the basic application script which handles / centralizes async requests, history, app data and date handling. WebSQL : In addition to the offline service worker we tried to improve offline experience by using localForage ( ) as a wrapper around IndexedDB, WebSQL, and / or localStorage. The complete communication between server and client is JSON based to simplify this kind of data storage. HistoryAPI : The PWA is a one-page application. Since service worker / cache does not work with a hash in an url, we decided to use GET parameter to identify the different states and views of the app. A decision, with which we ran into minor offline availability problems. As a learning - do not use keys only, always send a valid pair, if you want the request to be handled properly. Or you recreate your own request depending on the relevant data of the url (see the code example of the service worker further down). Service worker : As mentioned, it's a single page app with only one use case, that means - compared with other projects - adding on service worker offline support had been rather simple. The app-shell itself is cached inside the install routine, the outline and the data on its first request. Removing the correct data at the correct time was a bit more challenging. We also integrated push notifications to enable the two-tap check-in process. VanillaJS : For the rest. Some basic DOM-selectors, a few async requests, there was no need for additional libraries. Except moment.js ( ), since we wanted some kind of 'date awareness' for the app. It has to handle different timezones, e.g. to display the correct journey step or to disable the boarding pass. Manifest and Meta data : Allows the user to add the application to the home screen. Even if there is currently no offline support on iOS, we decided to provide a proper app icon, title and color theme on android and iOS. What we did Lazy loading - Don't block the DOM : Besides our major goals: A quick check-in and the possibility to complete boarding without any additional application, we wanted the app to be fast. Therefore we load everything except the critical CSS and some basic placeholders asynchron without blocking the DOM. Basic HTML structure: Initial JavaScript: So basically all what is sent with the first request is a skeleton of the app, with a menubar and colored placeholders for the dashboard, menu options, footer (the web components) and a short intro text in the visible area. The main CSS, the core script and the vendor scripts (polymer, moment, local forage) are loaded deferred, finally the core loads the different polymer elements. Paul Lewis wrote a great article on that: Comparing the actual check-in page of m.airberlin with the new approach the initial loading time is almost the same (about ~1.5s and ~2.5s on 3G), but the starting point of rendering is significantly lower. (~0.5s vs. ~1.2s), this is still (and will always be) work in progress. However, using lazy loading and placeholder styling to prevent FOUC (Flash of unstyled content) reduces the time of starring at a blank screen. Minimize - Blurry to hurry : Serving images in different sizes, depending on screen size and resolution should be common sense, but a lot of modern mobile devices require rather large images for a high quality result. Based on the concept used by Facebook in its native apps and an HTML/SVG/CSS adaption described in this article ( ) we use very small images (60x40 px) and load the optimized variant async - until it comes from cache. Get the Service Worker working : Our very first version of the service worker had been a few lines of code. We loaded all of our static assets when initializing the worker, and once installed it fetched and cached simply everything. For our demo this is sufficient, since we only provide one flight, one destination, there won't be any flight history and nothing which can be out of date. In real, we needed more. We have flight data, e.g. an e-ticket which has to be disabled or removed after the flight. In addition, we have related data for each flight, e.g. some information and images for each destination. Spamming the cache by storing all related data for all flights over all time isn't a good idea. Therefore we decided to remove everything with a delay of 48 hours after the flight. Since we also use WebSQL / local storage, we needed to do this twice. The following code fragments are showing our current implementation draft: Code Fragment app.js: Code Fragment service-worker.js: Inside the app.js (triggered from different polymer elements) a method tests, if the stored data is still valid or not. In case it is outdated, the script uses service worker post messages (see: ) to trigger the cache delete inside the worker and removes the data from local storage. Finally it triggers a custom event to tell all listeners (polymer elements) that the data has been changed. The sample code also contains part of the fetch handler. We reduced the cache ‚url' identifier to the check-in number by creating an own Request. Which allows us to easily identify the data when deleting and brought up a second benefit: The requested URL's might contain various parameters in future which change the url, but not the data itself (e.g. data for google campaigns), reducing the url to its relevant part provides a solution for this. One more thing I would like to mention: In our first draft we put all web-components inside the install routine of the service-worker, and we returned all this, so we blocked the service-worker until every asset had been loaded. At the very same time we do some lazy loading for these elements, the CSS and scripts. While fetching we stored 'everything' in cache. Scanning the timeline of the page, and the amount of requests, it was more efficient to reduce the number of required files and let fetch do the rest. Add to home screen - if useful : Currently you can't trigger ‚add-to-home-screen' message and there is some unknown magic in it under which conditions the message is shown. In our case we also have an additional condition when this message should be displayed. Going offline is possible after you checked-in, not before. Since you can't trigger the message, you could go for your own dialog, explaining the user how to do ‚add-to-home-screen' via the menu. To implement your own conditions, you'll need to defer the event: In our case the message pops up when the client closes the massage window, that the ticket has been saved. Process integration : Due to it's atomic structure, a polymer element normally comes with inline CSS and JavaScript. Surely you are able to integrate external stylesheets and scripts in polymer elements, but the inline approach is fast. Only downside, like this it is complicated to maintain the code, especially the CSS part. Our solution - a Grunt task named 'grunt-inline' ( ) and SCSS as CSS-precompiler. Each element gets its own SCSS file which includes basic settings (like variables and functions), as well as normalizing styles. The grunt task takes the generated CSS and writes the inline style and script tags to the element. For sure, the very same is possible with gulp and LESS. What we learned (so far) Web components : Using arrays of objects as a database for polymer elements can be quite tricky when updating the data. Especially when you work with nested components, in our case the slider which contains all board-cards. Since the rendering of all data went from server to client, we only need to transfer a bit of JSON data and some binary files, great for response time, great for the capacity utilization of the server. Measuring the loading time over different devices it gets a bit more ambivalent. Pre-rendering complex data on the server and serve the complete html is faster on older / less powerful devices (although the start of first rendering takes longer). If the client cache is working, this is only true for the very first request - if not you should do some serious testing what you do on server side and what not. Service worker : On the first view easy to integrate and the benefits are simply great: Not only the increase of speed and rendering, the offline availability, it also removes a lot of requests from the server (which helps to speed up thinks even further). The complicated part is not the script itself, it is the classic question how to cache properly. Also, for the moment the implementation of service workers differs a lot, only a subset of android users get the full benefit of this technology. However this is no argument for 'not implementing', but you still need an eye on the word progressive inside Progressive Web Apps. On iOS for example, you could add our PWA to the home screen too, due to browser caching and local storage you could open it (with some luck) even if your network is slow or down but it won't take long and you would see the dinosaur. Continue Reading: about airberlin airberlin is one of the leading airlines in Europe and flies to 147 destinations worldwide each year. This includes german destinations such as Berlin and Dusseldorf , popular european travel destinations such as Majorca , Paris und Istanbul as well as destinations in Italy and Spain . Germany's second largest airline carried more than 30.2 million passengers in 2015. airberlin offers a global route network through its strategic partnership with Etihad Airways, which has a 29.21 percent share in airberlin, and through membership of the one world ® airline alliance. topbonus, the frequent flyer programme of airberlin is one of the leading programmes in Europe with more than 4 million members. The airline with the award-winning service operates codeshare flights worldwide with 22 airlines. The fleet is among the most modern and eco-efficient in Europe. Together with other airlines, airberlin belongs to Etihad Airways Partners, a new brand with which Etihad has been uniting shared activities since the end of 2014.",en,144
40,2944,1484126235,CONTENT SHARED,3660989387512978561,9210530975708218054,1216354488778990261,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36",SP,BR,HTML,https://www.thoughtworks.com/pt/insights/blog/discutindo-devops-na-pratica,discutindo devops na prática,"Práticas de DevOps e Entrega Contínua ajudam a aumentar a frequência de deploys na sua empresa, ao mesmo tempo aumentando a estabilidade e robustez do sistema em produção. Neste webinar, Danilo Sato, autor do livro ""DevOps na prática: entrega de software confiável e automatizada"" , discute princípios, práticas e ferramentas de DevOps, cobrindo: Como automatizar o build e deploy de uma aplicação web e o gerenciamento da infraestrutura Como monitorar o sistema em produção Como evoluir a arquitetura e migrá-la para a nuvem Quais ferramentas estão disponíveis para começar a prática DevOps na Prática webinar - 10-6-2014",pt,143
41,1079,1464055142,CONTENT SHARED,943818026930898372,-6946355789336786528,4492969654113742216,,,,HTML,http://exame.abril.com.br/pme/noticias/inovador-nubank-ganha-premio-no-vale-do-silicio,"inovador, nubank ganha prêmio no vale do silício | exame.com","São Paulo - O Nubank , startup brasileira que oferece um cartão de crédito sem anuidade gerido através de um aplicativo, recebeu nesta semana o prêmio Marketers That Matter, do Sage Group do Vale do Silício . A empresa tem tido enorme sucesso na divulgação de seu produto. Até agora, cerca de 3 milhões de pessoas já se cadastraram para receber o cartão Nubank. Detalhe: tanto sucesso aconteceu sem nenhum investimento em mídia tradicional, segundo Cristina Junqueira, diretora e co-fundadora do Nubank. ""Não tínhamos verba para marketing e, mesmo se tivéssemos, nossos concorrentes sempre teriam mais. Como concorrer com o Itaú?"", explica Cristina em entrevista a EXAME.com. Consciente da sua posição de desvantagem em termos financeiros, a startup decidiu apostar em uma estratégia mais moderna. A solução encontrada foi baseada principalmente nas redes sociais, no relacionamento com a imprensa e na propaganda boca a boca. ""Focamos em Facebook, Twitter, Youtube, Instagram"", exemplifica a co-fundadora. Foi justamente essa estratégia que chamou a atenção do Vale do Silício. Para Cristina, outro ponto fundamental para o sucesso da startup é a qualidade do serviço entregue ao cliente. ""Tem também o aspecto de entregar a promessa que estamos vendendo. Ter a expectativa do cliente atendida ou superada é muito importante"", afirma. A satisfação do cliente Nubank ajudou a criar o que Cristina chama de ""crescimento viral"", através da propaganda boca a boca. Atualmente a startup tem um índice de satisfação de 88% (medido através do Net Promoter Score - NPS) e recebe elogios dos clientes a todo momento. Alguns exemplos do tipo de atendimento oferecido são bem inusitados. Uma cliente que teve o cartão mordido por seu cachorro entrou em contato para pedir um novo e, ao recebê-lo, também ganhou um ossinho roxo para o seu cão ( conheça outros exemplos ). O prêmio Marketers That Matter é o segundo reconhecimento da startup só nesse mês. Ela também foi eleita a Melhor Empresa de B2C na América Latina pelo Latam Founders Network.",pt,142
42,1182,1464797377,CONTENT SHARED,2285214528595997209,-9009798162809551896,-1393306465829315553,,,,HTML,http://www.mundodocker.com.br/docker-e-dotnet/,docker e .net,"Oi pessoal! Rodar .Net em container? SIM!! é possível, e vamos mostrar hoje como você pode fazer esse ambiente funcionar e testar sua aplicação. Bem, antes de tudo você precisa entender todo o movimento que a Microsoft vem fazendo para que sua linguagem de programação torne-se cada vez mais utilizada pelos desenvolvedores, para isso eles repensaram tudo que já fizeram, e lançaram uma versão do .Net com features muito parecidas como as encontradas em node e ruby on rails, e a batizaram de ASP.NET Core. Conceitualmente o ASP.NET Core é uma plataforma open source de desenvolvimento leve e eficiente para aplicação escritas em .net, veja abaixo uma lista dos principais pontos do asp.net Core: Suporte integrado para criação e utilização de pacotes NuGet; Pipeline de request mais leve e modular; Ambiente configurado para cloud; Suporte a injeção de dependência; Novas ferramentas que facilitam o desenvolvimento web; Capacidade de hospedar o processo no IIS ou em um Self-Host; Suporte para desenvolvimento de aplicação asp.net em Windows, Linux e Mac; Open Source e com comunidade ativa; Todos os pacotes do .Net no NuGet; .Net Core construido com suporte a versionamento side-by-side; Uma unica Stack para Web UI e Web APIs; Por que a Microsoft vem tomando esse rumo? Simples, para atrair mais pessoas para o seu produto, tendo em vista que as principais linguagens, ou as que mais crescem tem nativas todas essas features, a melhor alternativa para a Microsoft foi mudar a abordagem e atrair pessoas para melhorar seu produto/serviço e claro, isso consolida ainda mais o Windows Azure, fazendo com o desenvolvimento e deploy de Apps em .net torne-se menos doloroso. Vamos ver isso rodando em um container? Simples, tendo o Docker instalado (em qualquer host, seja ele Windows, Linux ou Mac), execute: Dentro do container, você deve criar sua aplicação, de forma parecida com o que ocorre com o ruby on rails, para isso o .net core disponibiliza um utilitário chamado: dotnet, veja: O comando dotnet new criará toda a estrutura necessária para rodar uma aplicação hello world básica, ele criará um arquivo chamado project.json com o seguinte código: Esse arquivo project.json contém as informações de dependência que sua aplicação precisará para rodar, é fundamental que você informe nesse aquivo todos os pacotes que necessita. Nessa mesma estrutura será criado um arquivo chamado Program.cs com este código: Agora basta realizar a instalação das dependências: E pronto, sua aplicação estará pronta para rodar, basta executar o comando: Fácil certo? E é possível deixar ainda mais fácil, para isso, basta criar Dockerfile e gerar uma imagem com esse ambiente: Agora crie a imagem: docker build -t myapp . E crie um container baseado nessa nova imagem: docker run -d myapp , quer ver o que retornou nessa execução? docker logs iddocontainer , ele deverá retornar algo desse tipo: Hello World! . É claro que você pode aprimorar esse ambiente, principalmente se quiser rodar uma aplicação web, mas esse ambiente básico já serve como inicio para seus testes ;). Gostou? Então nos ajude divulgando o mundodocker.com.br , grande abraço!",pt,142
43,1834,1469097976,CONTENT SHARED,4259370161044254504,-2979881261169775358,6708890816588756654,,,,HTML,https://medium.com/unboxd/how-i-built-an-app-with-500-000-users-in-5-days-on-a-100-server-77deeb238e83,"how i built an app with 500,000 users in 5 days on a $100 server","How I built an app with 500,000 users in 5 days on a $100 server There seems to be a general consensus in the world of startups that you should build an MVP (minimum viable product) without caring too much about technical scalability. I've heard many times that the only priority is to just get the product out there. As long as your business model would work at scale, you're good. You shouldn't waste time and money on making a technically scalable product. All you worry about is testing your assumptions, validating the market and gaining traction. Scalability is a concern for later. Unfortunately, this somewhat blind belief has led to some terrible failures. And Pokémon GO reminded us of it. One person who won't make this mistake again is Jonathan Zarra, the creator of GoChat for Pokémon GO. The guy who reached 1 million users in 5 days by making a chat app for Pokémon GO fans. Last week, as you can read in the article, he was talking to VCs to see how he could grow and monetize his app. Now, GoChat is gone. At least 1 million users lost and a lot of money spent. A real shame for a genius move. The article states that Zarra had a hard time paying for the servers that were necessary to host 1M active users. He never thought to get this many users. He built this app as an MVP, caring about scalability later. He built it to fail. Zarra hired a contractor on Upwork to fix a lot of performance issues. The contractor stated that the server costs were around $4,000. Since my calendar says it's 2016, I assume he isn't talking about $4000 of hardware, but $4000 in monthly or yearly virtual server and traffic costs. I've been designing and building web platforms for hundreds of millions of active users for most of my career. I can say $4,000 is a totally unnecessary amount of money for 1M users in a chat app. Even for an MVP. It means the app's server tech was designed poorly. It's not easy to build a cost-efficient, scalable system for millions of monthly users. But it's also not terribly complicated to have some sort of setup that can handle at least a decent amount of users on some cheap servers in the cloud. You just have to take it into account when building the MVP, by making the right choices. GoSnaps: 500,000 users in 5 days on $100/month server Similarly to GoChat, I also launched a Pokémon GO fan app last week, called GoSnaps . GoSnaps is an app to share Pokémon GO screenshots and images on a map. The Instagram/Snapchat for Pokémon GO. GoSnaps grew to 60k users its first day, 160k users on its second day and 500k unique users after 5 days (which is now). It has 150-200k uploaded snaps now. It has around 1000 concurrent users at any given time. I built image recognition software to automatically check if an uploaded image is Pokémon GO-related, and resizing tools for uploaded images. We run this whole setup on one medium Google Cloud server of $100/month, plus (cheap) Google Cloud Storage for the storage of images. Yes, $100. And it performs well. Technical comparison of GoChat and GoSnaps Let's compare GoChat and GoSnaps. Both apps probably fire a lot of requests per second to fetch chats/images within a certain area of the map. This is a geospatial lookup in the database (or search engine), either by a polygon of latitude/longitude locations or by a specific point. We use a polygon and we fire this request every time someone moves the map. These types of queries are heavy operations on a database, especially in combination with sorting or filtering. We get this type of search request hundreds of times per second. GoChat probably did too. Unique to GoChat is that it had to fetch and post a lot of chat messages every second. The article about GoChat talks about 600 requests per second for the whole app. Those 600 requests are a combination of map requests and chat messages. These chat messages are small and could/should be done over a simple socket connection, but happen often and have to be distributed to other chatters. This is manageable with the right setup, but disastrous with a poor, MVP-like setup. GoSnaps, on the other hand, has a lot of images being fetched and 'liked' every second. The snaps pile up on the server, since old snaps stay relevant. Old chats do not. Since the actual image files are stored in the Google Cloud Storage, the amount of requested image files is not a concern for me as a developer. Google Cloud handles this and I trust Google. But the requested snaps on the map are my concern. GoSnaps has image recognition software that looks for patterns on all uploads to see if an image is Pokémon-related or not. It also resizes the images and sends them to Cloud Storage. These are all heavy operations in terms of CPU and bandwidth. Way heavier than distributing some small chat messages, but less frequent. My conclusion is that both apps are very similar in terms of scalability complexity. GoChat handles more small messages while GoSnaps handles larger images and heavier server operations. Designing an architecture for these two apps both require a slightly different approach, but are similarly complex. How I built a scalable MVP in 24h GoSnaps is built as an MVP, not as a professional business product. It was built entirely in 24 hours. I took a NodeJS boilerplate project for hackathons and used a MongoDB database without any form of caching. No Redis, no Varnish, no fancy Nginx settings, nothing. The actual iOS app was built in native Objective-C code, with some borrowed Apple Maps-related code from Unboxd , our main app. So how did I make it scalable? By not being lazy. Let's say I would consider an MVP as solely a race against the clock to build a functional app as quick as possible, regardless of technical backend quality. Where would I have put my images? In the database: MongoDB. It would require no configuration and almost no code. Easy. MVP. How would I have queried the snaps within a certain area that got the most likes? By just running a plain MongoDB query on the entire pile of uploaded snaps. Just one database query on one database collection. MVP. All of this would have destroyed my app and the app's feature. Look at the query I would have had to run to get these snaps: ""find all snaps within location polygon [A, B, C, D], excluding snaps marked as abuse, excluding snaps that are still being processed, ordered by number of likes, ordered by valid Pokémon GO snaps first and then ordered by newest first"". This works great on a small dataset, great, MVP. But this would have been totally disastrous under any type of serious load. Even if I would have simplified the above query to only include three conditions/sorting operations, it would have been disastrous. Why? Because this is not how a database is supposed to be used. A database should query only on one index at a time, which is impossible with these geospatial queries. You'll get away with it if you don't have a lot of users, but you'll go down once you get successful. Like GoChat. What did I do instead? After applying the CPU-expensive image recognition and doing resizing, the resized images are uploaded to Google Cloud Storage. This way the server and database don't get hit for requesting images. The database should worry about data, not images. This saves many servers by itself. On the database side, I separate the snaps into a few different collections: all snaps, most liked snaps, newest snaps, newest valid snaps and so forth. Whenever a snap gets added, liked or marked as abuse, the code checks if it (still) belongs to one of those collections and acts accordingly. This way the code can query from prepared collections instead of running complicated queries on one huge pile of mess. It's simply separating data logically into some simple buckets. Nothing complicated. But it allows me to query solely on the geospatial coordinates with one sorting operation, instead of a complex query as described above. In simple terms: it makes it straightforward to select data. How much extra time did I spent on all of this? Maybe 2 to 3 hours. Why I did this in the first place? Because that's just the way I set things up. I assume my apps will be successful. There's no point in building an app assuming it won't be successful. I would not be able to sleep if my app gains traction and then dies due to bad tech. I bake minimum viable scalability principles into my app. It's the difference between happiness and total panic. It's what I think should be part of an app MVP. Choose the right tools for your MVP If I would have built GoSnaps with a slower programming language or with a big framework, I would have required more servers. If I would have used something like PHP with Symfony, or Python with Django, or Ruby on Rails, I would have been spending my days on fixing slow parts of the app now, or adding servers. Trust me, I've done it many times before. These languages and frameworks are great in many scenarios, but not for an MVP with low server budget. This is primarily due to the many layers of code that are usually used for mapping database records to logic and unnecessary framework code. It just simply hits the CPU too hard. Let me give you an example on how much this actually matters. As said, GoSnaps uses NodeJS as the backend language/platform, which is generally fast and efficient. I use Mongoose as an ORM to make the MongoDB work straightforward as a programmer. I'm not a Mongoose expert by any means and I know the library by itself has a huge codebase. Therefore Mongoose was a red flag. But yeah, MVP. At one point last weekend, our server's 4 NodeJS processes were running at 90% CPU each, which is unacceptable to me for 800-1000 concurrent users. I realized that it had to be Mongoose doing things with my fetched data. Apparently I simply had to enable Mongoose's ""lean()"" function to get plain JSON objects instead of magical Mongoose objects. After that change, the NodeJS processes dropped to around 5-10% CPU usage. Just the simple logic of knowing what your code actually does is very important. It reduced the load by 90%. Imagine having a really heavy library, like Symfony with Doctrine. It would have required a couple of servers with many CPU cores to just execute the code alone, even though the database is supposed to be the bottleneck, not the code. Choosing a lean and fast language is important for scalability, unless you have a lot of money for servers. Choosing a language with a lot of useful available libraries is even more important, since you want to build your MVP quickly. NodeJS, Scala and Go are good languages that cover both of these requirements. They provide a lot of good tools with a lot of good performance. A language like PHP or Java by itself is not necessarily slow, but is usually used together with large frameworks and codebases that make the application heavy. These languages are great for clean object oriented development and well-tested code, but not for quick and cheap scalability. I don't want to start a big programming language argument, so let me just state that this is subjective and incomplete. I personally love Erlang and would never use it for an MVP, so all your arguments are invalid. My previous startup Cloud Games A few years ago, I co-founded Cloud Games , an HTML5 games publisher. When we started, we were a B2C gaming website focused on the MENA region. We spent a lot of effort on gaining users and reached 1M monthly active users (MAU) after a few months. At the time, I used PHP, Symfony2, Doctrine and MongoDB in a pretty simple and lean setup. I used to work for Spil Games with 200 million MAU, which used PHP at the time and then moved to Erlang. After Cloud Games reached approximately 100,000 MAU, we started to see real server pain with Doctrine and MongoDB due to the huge overhead of these PHP libraries. I did set up MongoDB, indexes and queries the right way, but the servers were having a hard time processing all the code. And yes, I used PHP's APC cache and so forth. Since cloudgames.com was still very static, I was able to migrate the MVP to NodeJS with Redis in a few days. Similar setup, different language. This led to an immediate decrease in load by about 95%. Granted, this had more to do with avoiding PHP libraries than with the actual language. But a minimalistic NodeJS setup makes more sense than a minimalistic PHP setup. Especially since MongoDB and frontend code are also 100% JavaScript, like NodeJS. PHP without its frameworks and libraries is just another language. We needed this cheap setup, since we were a self-funded, early-stage startup. Cloud Games is now doing well and still based on a cost-efficient NodeJS architecture. We might not have managed to be successful with a more costly tech setup, given the fact that we've been through some really tough times as a startup. Designing a low-cost, scalable architecture has been essential for success. MVP and scalability can coexist If there's an opportunity for your app to grow exponentially due to hype or possible media coverage, make sure to consider scalability as part of your MVP. The principles of minimum viable products and scalable tech can coexist. There's nothing sadder than building a successful app and seeing it fail because of technical issues. Pokémon GO itself has had a lot of issues, but is so unique and hyped that it didn't matter. Small startups don't have this luxury. Timing is everything . One million GoChat users and half a million GoSnaps users probably agree with me. Please like and follow! If you liked this article, please like it here below on Medium. This would mean a lot to me. Feel free to comment for advice on scalability. At Unboxd , we're always happy to see other apps grow! Thank you!",en,140
44,2797,1480504938,CONTENT SHARED,-7294716554902079523,4227773676394505435,6923518953276678853,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.100 Safari/537.36",MG,BR,HTML,http://blog.caelum.com.br/usando-o-git-add-interativo/,usando o git add interativo,"Muitas vezes, quando estamos desenvolvendo alguma funcionalidade nova, precisamos criar novas classes, modificar outras e até mesmo mexer em algumas partes que não necessariamente são relacionadas com a funcionalidade nova que estamos implementando. Quando terminamos tudo, ficamos com um cenário assim no git: Como você pode ver, no desenvolvimento da funcionalidade de solicitar cotações em meu sistema, eu precisei modificar views, DAOs, modelos, criar novos arquivos css, javascript e um controller. Além disso, durante o desenvolvimento, eu arrumei alguns testes que outro desenvolvedor quebrou e precisei criar novos testes para o código que acabei de criar. O caminho mais rápido pra commitar tudo isso seria fazer tudo em um único commit, com uma mensagem genérica: O problema dessa abordagem é que ela pode tornar mais difícil encontrar quando um bug começou a acontecer, pode atrapalhar outro desenvolvedor que está tentando entender o que aconteceu em cada arquivo e também pois perde boa parte do propósito do git, que é manter um histórico de todas as alterações do projeto. Será que existe alguma coisa que pode nos ajudar dessas situações? Usando o git add interativo O git add interativo é uma ferramenta que nos ajuda a lidar com cenários como esse, ele nos permite adicionar arquivos específicos de forma mais rápida e até mesmo selecionar partes específicas de um arquivo pra commitar. Para usar ela, basta digitar o comando: Logo de primeira ela nos mostra quais são as opções, para usar cada uma delas, basta digitar o número correspondente e dar enter. 1. Status Nos mostra essa mesma tela, listando os arquivos que foram modificados 2. Update Nos permite adicionar as mudanças que fizemos em arquivos já existentes, ou seja, passar arquivos de not staged para staged . Por exemplo: ao digitar o número 2 e dar um enter, ele me mostra uma lista dos arquivos que foram modificados e um número em frente a cada um deles. Para adicionar um arquivo, eu posso tanto digitar apenas o número dele, como 8 ou, se eu quero adicionar mais de um arquivo, posso digitar um intervalo: 1-5 Como você pode ver, os arquivos que eu adicionei ficam com um * do lado. Assim que eu terminar de adicionar todos os arquivos, é só dar mais um enter sem inserir nenhum número para voltar ao menu principal. Pronto, eu adicionei apenas 6 de meus 9 arquivos que foram modificados e agora eu posso escrever uma mensagem de commit bastante detalhada sobre quais modificações eu fiz. Mais opções do git add interativo 3. Revert Permite voltar arquivos que estão staged para not staged . (o mesmo que git reset HEAD caminho/para/o/arquivo.java ) 4. Add untracked Permite adicionar novos arquivos no projeto. (equivalente a git add caminho/para/o/arquivo.java ) 5. Patch Permite escolher um arquivo e adicionar pedaços específicos dele. 6. Diff Nos mostra um diff dos arquivos que estão staged mostrando exatamente as diferenças de cada um dos arquivos que foram modificados. (equivalente ao git diff --cached ) As opções 3 - revert , 4 - add untracked e 6 - diff funcionam de forma idêntica à opção 2 - update , já a opção 5 - patch que nos permite escolher pedaços específicos de nossos arquivos para commitar, será tema de um próximo post. Se você usa bastante o git e quer aprender mais dicas como essa, dê uma olhada em nosso curso online de git",pt,139
45,2535,1475854719,CONTENT SHARED,5854206600849997966,7645894863578715801,-2165791432286103639,Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:49.0) Gecko/20100101 Firefox/49.0,SP,BR,HTML,http://blog.kaczmarzyk.net/2016/10/03/the-hardest-thing-in-computer-science/,the hardest thing in computer science,"I firmly believe that the hardest thing in computer science is naming things . I saw it many times during code reviews or analyzing legacy code, over and over: Wrong (vague or meaningless) names are often the consequence of bad design. For instance, it is impossible to find a good, concise name for a class/module which holds too many responsibilities. When something (e.g. a class) is named badly, programmers tend to care less about it. You have probably seen generic ""Service"" classes, that grow very fast, as more and more methods are added to them. Wrong names erase very important parts of the domain language from the code. The consequence is, that it is very hard to truly understand the domain model, especially when you are not the author of the code. This leads to even more shallow understanding of the system and the business. And that is the shortest route to even more bad code. ""(...) when you program, you have to think about how someone will read your code, not just how a computer will interpret it."" - Kent Beck Just imagine what would happen if, in real life, most of the names were vague, meaningless, or simply wrong. Not a good prospect, right? Yet you can see it in the source code very often. I decided to describe some common naming anti-patterns that I observed during my career. I think that eliminating them can really improve readability and, in turn, the overall quality of any codebase. Parameters and arguments are not the same thing Let's consider the following method signature: It is very common to see an usage similar to the following: The thing is, that the actual argument passed to a method is always much more specific than the corresponding parameter. A parameter might be generic, while an argument is a concrete value. It has its context, it has its intention. Therefore it deserves a much more specific name than just a repetition of the parameter name. Just see how much more information we get, if the above code is rewritten as follows: The idea is very simple - an argument is more than a parameter which it fits. Surprisingly often, though, programmers seem to forgot about it. Do not let an interface name pollute the name of your implementation The difference between parameters and arguments might be taken to another level when you consider interfaces and their implementations. If you read Dependency Inversion Principle ( SOLID ) carefully, it is clear that an interface is what a class (or a system or whatsoever) exposes for another components to express what it expects from them. Implementing an interface is meeting such expectations (i.e. making a class or another system something that you can ""plug in"" to this interface). Sadly, many programmers see it other way around, as if an interface was something that an implementation exposes to tell other classes about its capabilities. ""Could you pass me the charger, please?"" ""Sure!"" - real life conversation ""Could you pass me an electrical outlet implementation?"" ""WAT!?"" - if our daily language was as sloppy as the code often is This misunderstanding is another source of very bad names that lead to bad design. Let me make an example. I once dealt with two systems that were integrated with each other. The first system was sending some messages to the second one. The name of the second system was very vague, it just reflected a generic concept in the first system. The first system was basically a router which was preprocessing some messages and sending them to departments ( Department was an actual name used in the domain model of the first system). Most of these departments were external organizations with some web-service APIs. But here comes the second system. It wasn't 3rd party, it was maintained by the same group of people who maintained the first one. From the perspective of the first system, it was dealt with as any Department . So it was named ""MessageDepartment"" (seriously). The name reflected just the fact that it accepted messages and was plugged into a system that dealt with Departments . It wasn't any external department, just another subsystem of a larger whole. Yes, it accepted messages, but it would be much more informative to tell what it actually did with them. This extremely generic and vague name caused more and more functionality being added to the system. The developers ended up with adding many (too many) responsibilities to a single module. Among others, it included document/message browsing and management, review/verification process, notification feeds and duplicate detection. All of these were quite big and deserved a separate module/subsystem to preserve cohesion. It was clearly visible that something was wrong when you tried to describe that thing: it was difficult to find a concise definition/name. What the system became was just a big bag with different, unrelated functions. I firmly believe that the design would have been much better, if the system had been named accurately from the beginning. I.e. if the emphasis was put on what it really was (e.g. a notice board) rather than what it was integrated with. It would not have been so easy to randomly put new things into it so, hopefully, the functionality would have been split into separate modules. Express expectations through parameter names Let's consider the following code snippets: The good thing here is that a private method has been nicely extracted. Its name is quite OK too. Actually it could have been a proper code. The problem was that the logic in the private method was valid only for Notes (a type of Documents). It has not been used for any other documents (and should not have been). Therefore it would be great if the parameter was named more precisely, for example as follows: Variable or field is much more than its type How of then do you see code like this: In some cases it might be OK, but in many others variable names may (and should) carry additional information. For example: Another example I once found: Can you tell me what this person field really is? You probably could if you took a look on its usage, but you would not have to if somebody cared enough to write just: Another, more complex example: It is an actual snippet that I once saw during a code review. At least three names (marked as (1) , (2) and (3) in the comments above) could be improved here. But let's focus on the last one ( interviewStatus2InterviewResult ). It tells that the variable is a mapping of statuses to full interview results. It is true (at least), but do we really need that? The type of the variable tells the same and is clear enough, I think ( Map<InterviewStatus, List<InterviewResult>> ). The really important fact is that the map contains interview results for a single candidate. It is a consequence of the grouping operation few lines above. The code is not clear, so a good name would be even more helpful here. I think that this code requires much more refactoring in general, but even just naming improvements make it noticeably less ugly: Context of a method invocation is more than just the method itself In another system I worked on, I once found a method called advancedSearch . There was nothing wrong with the method itself (let's not worry about what ""advanced"" actually meant). The problem was in other place. The system had an on/off flag for duplicate detection (i.e. whether or not it should throw an exception if a duplicated item was submitted). The flag was named enableAdvancedSearch . How would you interpret it without knowing the explanation I gave above? I bet it would not be obvious. This advancedSearch method was actually used in other parts of the system too. The badly named flag wasn't about disabling/enabling the method/feature itself, but rather one particular context of using it. The intention (i.e. checking for duplicates) was much more important than the means of executing it (i.e. by invoking advancedSearch ). Therefore the proper name would be enableDuplicatePrevention . Name something if you can Logical expressions, literals, lambdas, even code snippets - they are all anonymous. Sometimes (very often, I think) it is a good idea to just name them. The easiest way to do that is to extract a method. Let's consider the following snippet as an example: Why should a reader be forced to even look on these 7 lines of code when all he or she wants is to get a general idea about what is going on at the high level? All that happens in these lines is just selecting the highest value out of a collection of status codes. It is a simple logic, but it would be easier to follow if it was explicitly named. And doing that is as easy as extracting a private method (e.g. status = theHighestStatusOf(statuses) ). Another example: Which in my opinion would be much better if a private method was extracted: The idea is simple. Just remember to be careful, because no name at all is actually better than a wrong name. Summary It is, for sure, not a complete list of naming anti-patterns. I hope it is just a good starting point for improving names in the code and finding more rules about them. Improving names improves readability, which positively affects the overall code quality. Which, I hope, is the goal for the most of us. Thanks for reading and I hope you will find it useful! Photo credit: Geoffrey Fairchild",en,139
46,2422,1474815847,CONTENT SHARED,9175693555063886126,1374824663945909617,-7520177282913389815,,,,HTML,https://bittencode.wordpress.com/2015/07/08/15-minutos-sobre-docker/,15 minutos sobre docker,Neste post eu compartilho um vídeo e uma apresentação sobre o que é o Docker e como começar a usar. O objetivo aqui é mostrar como é fácil começar a usar ao compartilhar a referencia da documentação e mostrando a execução do Docker em linha de comando. Este post serve pra quem já ouviu falar sobre Docker e quer então começar a brincar com ele. É um guia simples para os iniciantes. Descrevo então sobre: 1. Conceitos básicos 2. Instalação 3. Repositórios de images 4. Criar suas próprias images 5. Executar containers baseados nas images. Espero que gostem! Segue os slides: Referência:,pt,139
47,1306,1465396491,CONTENT SHARED,310515487419366995,-5527145562136413747,3192880191028684629,,,,HTML,http://blog.runrun.it/erros-de-portugues-em-e-mails/,71 erros de português que precisam sumir dos seus e-mails,"Escrever um e-mail não deveria ser uma coisa tão penosa. Não deveria ser aquele momento em que você excomunga o idioma porque hesita entre uma e outra forma de grafar as palavras. Não deveria ser como assumir um risco. Sobretudo, não deveria ser um novo 7 a 1 todos os dias. Por isso, preparamos uma lista com os 71 erros de português e dúvidasortográficasmais comuns em e-mails! Mais do que esclarecer suas dúvidas, você vai se espantar com algumas expressões que usa, mas que estão fora da norma gramatical. Guarde esta lista, caso se esqueça de algo, e aproveite para a compartilhar com seus colegas! 1. Ao invés de / Em vez de ""Em vez de"" é usado como substituição. Ex: São Paulo em vez de BH. ""Ao invés de"" é usado como oposição. Ex: Grande ao invés de pequeno. 2. De encontro a / Ao encontro de ""Ao encontro de"" expressa harmonia. Ex: Obrigada! Sua ajuda veio ao encontro do que eu precisava. ""De encontro a"" expressa embate. Ex: Brigaram porque a opinião dele ia de encontro ao que ela acreditava. 3. Através de / Por meio de ""Por meio de"" é o mesmo que ""por intermédio"". Ex: Conseguimos por meio de muito trabalho. ""Através de"" expressa a ideia de atravessar. Ex: Olhava através da janela. 4. Em princípio / A princípio ""A princípio"" equivale a ""no início"". Ex: A princípio, achei que não seria capaz. ""Em princípio"" equivale a ""em tese"". Ex: Em princípio, todo homem é igual perante a lei. 5. Se não / Senão ""Senão"" significa ""caso contrário"" ou ""a não ser"". Ex: Me avise, senão vou esquecer. Não fez senão o prometido. ""Se não"" é usado para expressar uma condição. Ex: Se não puder, nos avise antes. 6. Retificar / Ratificar ""Ratificar"" é o mesmo que confirmar. Ex: Os dados ratificaram a previsão. ""Retificar"" é o mesmo que corrigir, emendar. Ex: Vou retificar os dados da empresa. 7. À medida que / Na medida em que ""Na medida em que"" equivale a ""porque"". Ex: Cancelamos a reunião na medida em que a negociação havia sido adiada. ""À medida que"" mostra relação de proporção. Ex: A produtividade aumenta à medida que a equipe usa a ferramenta. 8. Eminente / Iminente ""Eminente"" significa ""excelente"". Ex: É uma professora eminente. ""Iminente"" significa deverá acontecer em breve. Ex: O sucesso do projeto é iminente. 9. Bastante / Bastantes ""Bastante"" concorda com o substantivo. Só não concorda com adjetivos e advérbios. Por isso se diz ""Há bastantes e-mails"" e ""Andei bastante rápido"". 10. Sessão / Seção ""Sessão"" com ""ss"" quer dizer o tempo de um evento. Ex: Sessão de cinema, ou sessão de acupuntura. ""Seção"" com ""ç"" quer dizer ""departamento"" ou ""divisão"". Ex: A seção de arte moderna do museu, ou a seção de carnes do supermercado. 11. Tachar / Taxar ""Tachar"" com ""ch"" é ""censurar"", ""rotular"". Ex: Foi tachado de louco. ""Taxar"" com ""x"" é receber taxa, imposto. Ex: Grandes fortunas serão taxadas. ""Trás"" só existe na expressão ""Para trás"". Se você está se referindo ao verbo ""trazer"", lembre-se da letra z nele e use sempre ""traz"". 13. Descrição / Discrição ""Descrição"" é o detalhamento de algo. Ex: Não havia uma descrição clara do trabalho. ""Discrição"" é a qualidade do que é discreto, não chamativo. Ex: É bom ter discrição durante a negociação. 14. Afim / A fim de ""A fim de"" indica ideia de finalidade. Ex: Irei ao evento a fim de praticar o networking. ""Afim"" é um adjetivo, o mesmo que ""semelhante"". Ex: Temos ideias afins. 15. Desapercebido / Despercebido ""Despercebido"" significa ""sem atenção"". Ex: A mudança passou despercebida. ""Desapercebido"" significa desprovido, desprevenido. Ex: Estava desapercebido de dinheiro. ""Demais"" significa ""excessivamente"". Também pode significar ""os outros"", na expressão ""os demais"". ""De mais"" se opõe a ""de menos"". Ex: Uns têm privilégios de mais; outros de menos. 17. Tão pouco / Tampouco Tampouco corresponde a ""também não"", ""nem sequer"". Ex: Ele não fez o que pedi, tampouco o que você pediu. Tão pouco corresponde a ""muito pouco"". Ex: O fim de semana foi delicioso, mas durou tão pouco. Mal opõe-se a bem. Ex: Acordo mal-humorada. Estava malfeito. Mau opõe-se a bom. Ex: Hoje é um mau dia para conversarmos. Homens dizem ""obrigado"". Mulheres dizem ""obrigada"". 20. Descriminar / Discriminar ""Discriminar"" significa ""separar"" e também ""discernir"". Ex: Discriminar por orientação sexual é desprezível. As notas fiscais já foram discriminadas. 21. A cerca de / Acerca de ""Descriminar"" significa ""inocentar"" e também ""descriminalizar"". Ex: A juíza descriminou o réu. ""Acerca de"" é o mesmo que ""a respeito de"". Ex: Deveríamos discutir mais acerca de política. Já ""a cerca de"" indica aproximação. Ex: Moro a cerca de 3Km daqui. b) A forma correta é a segunda Lembre-se que o sentido é ""dar resposta a alguém"", portanto, sempre acompanha a preposição ""a"". Anexo é um adjetivo e concorda em gênero e número com o substantivo a que se refere. No caso, a palavra ""imagens"" é feminina e está no plural. Além disso, a expressão ""em anexo"" é recusada pelos principais linguistas. 24. Visar o objetivo / Visar ao objetivo O verbo visar, no sentido de almejar, pede a preposição ""a"". No entanto, quando ele está junto de outro verbo, dispensa-se a preposição. Ex: Visamos viajar para o exterior este ano. 25. Precisar de fazer / Precisar fazer Assim como o verbo anterior, ""precisar"" só vem junto da preposição ""de"" quando há um substantivo. Ex: Precisamos de mais foco. Precisavam tirar umas férias. 26. Media a reunião / Medeia a reunião Lembre-se dos outros verbos irregulares com final ""-iar"": ansiar, incendiar e odiar. Por maior que seja seu ódio, você não diz: ""Eu odio"". 27. Interviu, interviram / Interveio, intervieram O verbo ""intervir"", assim como ""convir"", se conjuga como o verbo ""vir"". 28. Quando dispor / Quando dispuser O verbo ""dispor"", assim como ""repor"", ""propor"" se conjuga como o verbo ""pôr"". 29. Preveu, preveram / Previu, previram O verbo ""prever"", assim como ""rever"", se conjuga como o verbo ""ver"". ""Chego"" e ""trago"" só existem na expressões ""Eu chego"" e ""Eu trago"". O verbo ser pede o particípio irregular, que não termina em -do. Por isso se diz ""foi impresso"" e não ""foi imprimido"". O verbo ter pede o particípio regular. Por isso se diz ""tinha acendido"" e não ""tinha aceso"". 32. A curto, médio, longo prazo / Em curto, médio, longo prazo A expressão exige a preposição ""em"". A palavra ""ora"" não só existe como significa ""agora"". 34. Quando ver / Quando vir ""Quando vir"" se refere ao verbo ver no futuro e na condicional. Ex: Quando eu te vir, vou te dar um abraço apertado! Além disso, ""quando ver"" não existe. A menos que você se refira a um quiz ( aqui estão vários! ), escreva ""quis"". ""A nível de"" é uma expressão coringa, que não tem sentido próprio. Procure substituir, por ex., ""a nível de Brasil"" por ""a nível nacional"", ou ainda melhor, por ""com relação ao Brasil"". Em outros casos, a expressão é inútil. Em vez de dizer ""problemas a nível de foco"", diga apenas ""problemas de foco"". Mesmo após a última reforma ortográfica, a palavra continua sendo grafada com hífen. O verbo ""esquecer"" só é usado com a preposição ""de"" quando vem acompanhado de um pronome oblíquo (me, te, se, nos...). O mesmo vale para o verbo ""lembrar"". No sentido de tempo decorrido, o verbo ""fazer"" só é usado no singular. Para indicar tempo passado, usa-se o verbo haver. O ""a"", como expressão de tempo, é usado para indicar apenas tempo futuro ou distância. Ex: Falarei com o diretor daqui a cinco dias. Ele mora a duas horas do escritório. Além disso, se desejar usar a expressão ""atrás"", o verbo ""haver"" deve ser removido. ""Há dois anos atrás"" . No sentido de existir, o verbo ""haver"" fica sempre no singular. Já nas locuções verbais, ele concorda com o sujeito. Ex: Elas haviam feito um ótimo trabalho. ""Quite"" deve concordar com o substantivo a que se refere. ""Onde"" se refere a um lugar em que alguém ou alguma coisa está. "" Aonde"" é formado pela preposição ""a"", porque indica movimento. Quem vai vai a algum lugar. Nessa mesma lógica, não existe a expressão ""daonde"", pois quem vem vem de algum lugar. Existe apenas ""de onde"". O verbo assistir, no sentido de ver, exige a preposição ""a"". Caso contrário, significa ""ajudar"". Ex: A enfermeira assistiu o paciente por horas. Quem admite admite ""algo"". Isso classifica o verbo ""admitir"" como transitivo direto. Assim como quem ama ama algo. Quando isso ocorre, o verbo concorda com o sujeito. Quem precisa precisa ""de algo"". Isso classifica o verbo ""precisar"" como transitivo indireto. Quando isso ocorre, o verbo fica no singular. O verbo ""implicar"" tem sentido de ""requerer"" e também de ""acarretar"" e, em ambos casos, não admite preposição. Não se usa a preposição ""em"" nessa expressão. ""Entre eu"" só pode ser usado antes de um verbo no infinitivo. Ex.: ""Passou-se um bom tempo entre eu começar o trabalho e você me ajudar."" Tem refere-se à 3ª pessoa do singular do verbo ""ter"" no Presente do Indicativo. Têm refere-se ao mesmo tempo verbal, porém na 3ª pessoa do plural. Vêm, Convêm e Retêm Verbos de movimento exigem a preposição ""a"". A regência do verbo preferir é com a preposição ""a"" e não ""do que"". O correto é meio-dia e meia, pois o numeral fracionário concorda em gênero com a palavra hora. A menos que você esteja dizendo que a meia do seu pé está ansiosa, o correto é no masculino, sempre que quiser dizer ""um pouco"". No sentido de ""metade"", concorde com o substantivo. Ex: Meia hora, meia xícara de chá. ""Menas"" não existe. ""Perca"" é verbo. Ex: Não quero que você perca sua fé em mim. ""Perda"" é substantivo. Foi uma perda incalculável. c) Abandonando pleonasmos 57. Na minha opinião pessoal = Na minha opinião 58. Repetir de novo = Repetir 59. Multidão de gente = Multidão 60. Encarar de frente = Encarar 61. Duas metades iguais = Metades 62. Preferir mais = Preferir 63. Há anos atrás = Há anos ou Anos atrás d) Descomplicando o uso da crase Motivo de erros de português há gerações, a crase é simplesmente quando duas letras se fundem numa só: a preposição ""a"" e o artigo feminino ""a"". Algumas pessoas inclusive preferem ler ""à"" como ""a a"". Tendo isso em mente, fica bem mais fácil entender quando a crase é necessária. Veja alguns erros: 64. De segunda à sexta / De segunda a sexta Você está dizendo ""De segunda até sexta"" e não ""De segundo até a sexta"". Portanto, não há duas vezes o ""a"". Logo, não faz sentido haver crase. 65. Das 17 até às 18h / Das 17 às 18h É o mesmo caso que acabamos de explicar. 66. À partir de / A partir de Nenhum verbo exige crase antes. Prazo é uma palavra masculina e, portanto, não acompanha o artigo feminino ""a"", necessário para haver crase. 68. Refiro-me aquilo / Refiro-me àquilo As palavras ""aquilo"" e ""aquele"", masculinas, levam acento quando provêm da fusão do ""a"" preposição com a letra ""a"" de ""aquilo"". Ex: Refiro-me àquilo que você disse na reunião ontem. 69. Disse à você / Disse a você Não ocorre crase antes de pronome pessoais (eu, você, ele, ela, nós, vocês, eles, elas), uma vez que nenhum deles vem acompanhado do artigo feminino ""a"". 70. A vista, a disposição, a beira, a espera, a base / À, à, à, à, à Sem o acento grave, todas essas expressões são apenas substantivos. 71. Vou à Curitiba, Vou a Bahia / Vou a Curitiba, Vou à Bahia Quando estiver se referindo a cidades e países, lembre-se: Vou a, volte de... Crase pra quê? Vou a, volta da... Crase há! No exemplo: Vou a Curitiba (porque volto dE Curitiba) vs. Vou à Bahia (porque volto dA Bahia) e) Descomplicando os porquês Por fim, um dos mais célebres erros de português é a confusão que se faz entre os porquês. Veja como é mais simples do que parece! Sempre que a palavra ""motivo"" estiver implícita na expressão, use ""por que"". Mesmo que não seja uma pergunta. Caso haja pontuação (seja vírgula, ponto final, de exclamação ou interrogação) após, acentue a palavra ""quê"", ficando ""por quê"". Se é possível substituir por ""pois"", use ""porque"". Se é possível trocar pela palavra ""motivo"", use o substantivo ""porquê"". Exemplo: Você não sabe por que [motivo] eu fiz aquilo, e agora me pede que eu explique por quê. Mas eu não irei dizer o porquê! Ainda estou triste porque você me ofendeu. Agora que você deu um importante passo para não cometer mais erros de português em e-mails, é hora de avançar ainda mais. O número de e-mails e a forma como a troca de demandas fica desordenada provavelmente são um pesadelo diário para você e sua equipe. O fim do pesadelo está na contratação de uma ferramenta de gestão de projetos, tarefas e fluxo de trabalho, como o Runrun.it. Experimente grátis e sinta a diferença:",pt,136
48,1485,1466605328,CONTENT SHARED,2719909253419802298,-5527145562136413747,101989957506145943,,,,HTML,http://blog.runrun.it/mulheres-millenials-geracao-y/,as expectativas das mulheres da geração y são muitas,"Elas configuram uma nova era do talento nas empresas: alta escolaridade, confiança, ambição e objetivos claros em relação à carreira. Nascidas entre 1980 e 1995, as mulheres da Geração Y , ou millenials, têm levado novas demandas ao mundo corporativo. Elas são comunicativas, gentis, democráticas e participativas. E estão promovendo mudanças de comportamento e criando novas expectativas dentro do mercado de trabalho. Por outro lado, apesar da grande atenção que a Geração Y recebe , poucos estudos que mapearam as atitudes e valores profissionais deste grupo diferenciaram seus integrantes por gênero. E, apesar de e mpresas de todo o mundo enfrentarem os desafios de remodelar e rejuvenescer sua força de trabalho, lhes faltam mulheres em posições de liderança. Este post vai mostrar um pouco mais sobre a profissional millennial, suas capacidades e fragilidades, para você tentar trazê-la para a sua equipe. Em muitas empresas, as posições de cunho gerencial e alto poder decisório estão sendo igualmente disputadas por ambos os sexos. Entretanto, a discriminação ainda existe e fica exposta quando o assunto é salário. Segundo pesquisa publicada pela PUC-RIO em 2014, a diferença entre a remuneração de homens e mulheres com o mesmo cargo é de até 30%. Este quadro levou o Brasil a ocupar o 82º lugar em um ranking sobre desigualdade de sexos , com 135 países, conforme pesquisa realizada pelo World Economic Forum (WEF). A PwC, consultoria global, tem 50% de profissionais mulheres e 80% de profissionais da geração Milênio. Em parceria com o Instituto Optimum Research, divulgou a pesquisa "" The female millennial - A new era of talent "" em 2015. O estudo mostrou que 53% das mulheres classificaram as oportunidades de progresso na carreira como a característica mais atraente da empresa. Infelizmente, 71% das entrevistadas disseram não sentir que as oportunidades são realmente iguais para homens e mulheres. A densa participação da mulher no mercado de trabalho abre novas perspectivas tanto na esfera pública quanto na privada. Os números só crescem: elas já atingem escolaridade superior a dos seus colegas homens e, cada vez mais, investem na formação técnica e intelectual com o objetivo de alcançar novos espaços em todas as camadas organizacionais. As millenials demonstram grandes expectativas profissionais, procuram tarefas que proporcionem desafios, reconhecimento, crescimento e desenvolvimento, e que permitam trabalhar com autonomia. É a autoconfiança que abre portas para isso. Ou, em uma expressão muito usada hoje em dia, o ""empoderamento feminino"". Segundo a pesquisa da PWC , as jovens brasileiras (76%), Índia (76%) e Portugal (68%) são as mais confiantes em relação à carreira, enquanto que suas congêneres no Japão (11%) e Alemanha (19%) são as menos confiantes. Uma pesquisa realizada por Sylvia Ann Hewlett e Ripa Rashid em empresas do setor privado do Brasil confirma a tese: 80% das mulheres têm ambição de chegar ao topo da hierarquia organizacional. O estudo também identificou que as profissionais brasileiras possuem uma maior ambição pelo crescimento profissional em relação às americanas, inclusive. Por que elas escolherão sua empresa Com uma visão muito clara sobre a importância da experiência internacional, 71% das entrevistadas pela PwC gostariam de trabalhar fora do país. Portanto, oferecer a possibilidade de viagem profissional pode ser essencial para a contratação de uma millenial. Os resultados da pesquisa levaram a mais uma conclusão: elas verbalizam claramente que querem um trabalho que tenha propósito, querem contribuir de alguma maneira para o mundo e ter orgulho de seu empregador. Reputação, portanto, influencia bastante em suas decisões. As empresas devem se comunicar melhor e ser mais transparentes para atrair esses talentos. Para Juliana Albanez , coach especialista em comportamento e liderança feminina, as millenials ""são mulheres criativas e com propósito de vida. A vida profissional tem que fazer sentido, o trabalho tem que oferecer prazer e elas fazem questão do reconhecimento. São mulheres que farão história na luta pelo equilíbrio entre vida profissional e pessoal. E isso é só uma questão de tempo"". Outros aspectos que essas profissionais almejam são horário flexível, autonomia, boa remuneração e oportunidade de crescimento. Para o psicanalista e especialista em carreiras e gestão de pessoas, Paulo Paiva ""o modelo com rigidez de horário, deslocamento e processos está ficando obsoleto para as mulheres da nossa e próximas gerações. A questão do horário flexível está se tornando uma necessidade, não é mais um diferencial das empresas"". Para corrigir o défict de liderança de gênero, é preciso conduzir esforços paralelos: inserir diversidade na liderança em conjunto com mudanças na cultura organizacional. Inclusive, se interessar, fizemos um guia da cultura corporativa para a igualdade de gênero . E, para alcançar um resultado positivo, as organizações devem compreender quais são as expectativas de carreira dessas jovens profissionais para não só atrair, como desenvolver, contratar e reter o talento dessa geração. E ainda que o caminho para a igualdade de gêneros seja longo, as mulheres da Geração Y não encontram mais o mesmo ambiente profissional do qual fizeram parte suas mães e avós. Mais qualificadas e confiantes, elas impulsionam e fortalecem a demanda por novas estratégias de mercado e mudanças culturais em ambientes considerados engessados. Para manter a competitividade num mercado de constantes e rápidas mudanças, a adaptação das empresas acaba sendo inevitável. E essa já pode ser considerada uma grande vitória das millennials. As mulheres da Geração Y são multifuncionais, se preocupam em gerenciar o tempo da melhor forma e alcançar resultados consistentes. Conheça o Runrun.it , gerenciador de projetos e tarefas, que irá ajudar nessa tarefa, além de tornar a relação de trabalho e a comunicação muito mais transparentes. Experimente Grátis:",pt,134
49,887,1462920714,CONTENT SHARED,2072448887839540892,-3390049372067052505,-5764325092046707218,,,,HTML,https://medium.com/enrique-dans/welcome-to-googlebank-facebook-bank-amazon-bank-and-apple-bank-c9c3955006d4,"welcome to googlebank, facebook bank, amazon bank, and apple bank - enrique dans","Welcome to GoogleBank, Facebook Bank, Amazon Bank, and Apple Bank How would you like to bank with Apple, Google, Amazon, or Facebook? This is the question Fujitsu has just asked a sample of some 7,000 people throughout Europe : around a fifth said they were up for it. As the fintech environment garners an increasing number of experiences that tend to take place outside banking or insurance, and while the big tech companies push into financial products through payment initiatives, the idea seems to be gaining traction with more and more people . What is a bank anyway? If most of the business these days is mainly about moving bits around data bases, it's pretty clear that the big tech companies are going to have more expertise than banks that don't see technology as part of their business and tend to subcontract it out to the big consultancies or to simply continue doing thing like they did in the 1950s. Most banks are way behind the tech companies when it comes to dealing with clients, when it comes to their internet systems, and most definitely when it comes to analytics, as well as the assets they hold. Banks are generally still relatively small setups, less developed, and rooted in obsolete working practices. This is reflected in key areas such as risk analysis, which lags behind the big tech companies' use of machine learning, which in the case of outfits like Google are considered the basis for future growth . Until now, the tech sector has thought that it wasn't necessary to own a bank to disrupt the financial sector . But the signs are that some tech companies are increasingly taking on the role of banks . We can easily imagine a future in which these well-capitalized outfits realize that a banking license isn't such a big deal when it comes to building a sustainable competitive advantage and that they could integrate these services in the future if people accept the idea. The pundits have been asking for some time which sector Apple will disrupt next . The company would have no problem finding the money to meet the legal requirements, and with its history of cross selling, its network of shops and its client base, it would have few problems in becoming a powerful and profitable bank. What's more, the smartphone and companies like Venmo , Square , PayPal , Snapcash and many others, have pretty much put paid to the cheque book and the credit card. What's more, their target audience is the millennial generation born between 1981 and 1997 , is already the majority, and have no problems in accepting disruption to the financial sector. Let's be honest: the traditional banks do not enjoy sufficient loyalty from their customers that they could stand up to competition from new entrants from the technology sector. Could internet end up replacing the traditional banks ? For the moment, the most proactive are buying startups and companies with interesting business models , or those who are able to fit into their traditional ways of doing things , but... will such initiatives be enough to hold onto their places in an industry in which major disruption is coming?",en,134
50,2985,1484911446,CONTENT SHARED,-7113155163062752691,1374824663945909617,7124692265469031695,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.100 Safari/537.36",SP,BR,HTML,http://sensedia.com/blog/apis/arquitetura-de-microservicos-habilitando-apis/,arquitetura de microserviços habilitando apis - sensedia,"Esse post foi realizado pelo nosso consultor Rafael Rocha, e adaptado para o nosso blog, caso queira acessar o original em inglês, basta acessar este link . Contexto Hoje em dia a arquitetura de Microserviços se tornou dentro do segmento de desenvolvimento de software. Esse modelo arquitetural é uma abordagem que prega pela decomposição de aplicações em uma gama de serviços. O time de desenvolvimento pode adotar as tecnologias mais apropriadas para resolver problemas específicos . A arquitetura de Microserviços também aumenta a escalabilidade por possibilitar o uso de abordagens como auto-scaling e micro-container . APIs e microserviços estão muito relacionados já que o padrão é utilizar uma interface de APIs que é totalmente voltada a RESTful. Porém, hoje em dia nós ainda sofremos alguns problemas quando precisamos integrar com sistemas legados para externalizar funcionalidades como serviços, uma vez que a maioria dos sistemas não possuem protocolos como WebServices ou interfaces RESTful, então vamos explorar esse tema. O Problema e a abordagem de Microserviços A maioria das companhias querem expor APIs de forma interna (dentro da corporação) e/ou externa (para parceiros ou clientes), no entanto, os seus sistemas ou aplicações não foram contruídas para este propósito. A maioria das aplicações são baseadas na seguinte arquitetura: Aplicações monolíticas para web usando um banco de dados único. Ex: Java (JSF) com banco de dados Oracle. Produto ou plataforma como por exemplo ERP's da SAP. Aplicações de alto nível em escritas em Cobol Aplicações Cliente-Servidor implementados em VB6 e SQL Server. Quando nos deparamos com esse tipo de cenário, a solução comum é construir um adaptador para a exposição de um protocolo padrão. Esse componente deve se parecer com o diagrama abaixo: O adaptador é o componente principal para a solução já que é isso que vai possibilitar a externalização do serviço. Para promover essa padronização, alguns padrões devem ser utilizados: Compatibilidade total com padrão RESTful Organização por domínios de negócio Ser facilmente escalável Pacotes leves e com inicialização rápida. Para ter uma aplicação com um modelo de integração simples e escalável é necessário possuir um estilo arquitetural que cumpra com todos esses requisitos, e o estilo arquitetural que mais se aproxima dessas característica é o estilo arquitetural de Microserviços. Lembre-se que essa recomendação se aplicada quando o seu backend não é exposto com o protocolo HTTP, porque a maioria das soluções de API Gateways irá conseguir rotear e transformar facilmente as mensagens para comunicação com o backend. Outros cenários que a arquitetura de Microserviços é muito recomendada são: Orquestração: em alguns casos é necessário rotear e/ou chamar outros serviços dependendo de alguma condição específica. Composição: algumas vezes é preciso chamar mais de um serviço para compor e devolver uma resposta. Transformação de Mensagens Complexas: quando é necessário utilizar de algoritmos mais complexos para conseguir devolver uma mensagem para o consumidor do serviço. Por fim, repare que os microserviços precisam ser organizados dentro dos domínios do seu negócio. Desta maneira, podemos ver que esse estilo arquitetural é uma oportunidade de quebrar os monolíticos de seu negócio e prover uma melhor arquitetura, como o desenho abaixo: Um artigo muito interessante de Microserviços é esse . Estratégia de Implementação de Microserviços Uma vez decidido que a implementação do adaptador será baseada em uma arquitetura de Microserviços, algumas características são necessárias: Pacotes leves e baixo consumo de memória A inicialização da aplicação deve ser rápida para criar rapidamente em novas instâncias de containers ou servidores. Desenvolvimento rápido e simples baseado na especificação Swagger Features de segurança como OAuth2 Alguns dos frameworks que indicamos são: JavaScript: Outra característica crucial quando quando implementamos APIs usando Microserviços é a capacidade de integração com sistemas legados. Esse tipo de feature requer um framework específico que implementa os padrões de integração empresarial (EAI patterns), a recomendação nesse caso é utilizar o framework Java Apache Camel . Estratégia de Implantação de Microserviços Uma vez que o pacote com Microserviços está implementado e pronto, ele precisa ser implantado para estar disponível para o uso. A estratégia mais recomendada é utilizar de serviços PaaS (Plataforma como serviço) para implantar os pacotes de Microserviços. Isto porque este tipo de serviço oferece algumas features muito interessantes como: Uso de containers Orquestração de containers Armazenamento (sistema de arquivos e banco de dados) Monitoramento em tempo real Logging e rastreamento Outras duas características importantes são: Ser capaz de escalar para suportar o tráfego Oferecer APIs para automatizar o processo de implantação As principais ofertas de PaaS do mercado devem ser avaliadas para uma estratégia de implantação, e são elas: Outras opções para serem consideradas são a Amazon Elastic Beanstalk e Google App Engine. Os dois são muito interessantes porque possuem uma integração nativa com os serviços de Cloud e Infraestrutura já que também oferecem os serviços de infraestrutura como serviço (IaaS). Porém na nossa visão, a melhor alternativa para implantação de microserviços são aquelas que provém uma integração completa com uma plataforma de gestão APIs, e nesse caso o Sensedia API Suite oferece uma funcionalidade chamada BaaS (Backend as a Service) que utiliza das mesmas características de serviços PaaS e realiza esta integração. Essa funcionalidade permite que você faça a implantação e rode os seus microserviços, já expondo diretamente as APIs dos seus sistemas legados. As tecnologias suportadas pelo BaaS são: Vale lembrar que se você pode utilizar desta plataforma para rodar seus microserviços que não seja somente estes que habilitam a integração com legados, poderá ser sua plataforma oficial de execução de microserviços. Microserviços e Plataformas de API Management Uma vez que os Microserviços estão rodando corretamente, as APIs que expõem os Microserviços devem ser bem gerenciadas e algumas das features essenciais para isso, que a maioria das plataformas deste tipo oferecem são: Segurança e Resiliências : necessário para proteger o seu backend de pessoas ou aplicações não habilitadas para consumir essas APIs. Quando uma API é aberta ou para parceiros, os seus Microserviços devem estar protegidos contra os picos de tráfego, para que não fique fora de serviço e para isso é necessário ter controle de limite de tráfego (Rate Limit e Spike Arrest) e limite de tamanho do corpo da mensagem (Payload Size). Controle de Acesso: os consumidores da API devem estar sobre o seu controle, para isso é necessário a utilização de padrões de mercado como OAuth 2.0 ou JSON Web Tokens. Monitoramento e Rastreamento: é necessário conseguir monitorar todos os tipos de chamada realizadas na sua plataforma, além disso é necessário ter um mecanismo de log poderoso para encontrar os erros que vêm acontecendo na sua API. Todas as capacidades listadas acima são comuns em uma solução de API Gateway, porém algumas outras features cruciais para o gerenciamento completo das suas APIs, são elas: Caching: deve ser capaz de evitar chamadas desnecessárias em sua API, provendo uma latência muito melhor para as suas chamadas e economizando até mesmo o custo de sua infraestrutura de backend. Analytics: o uso da API monitorado em tempo real é de extrema importância, tanto para acompanhar o consumo e até mesmo para ter insights de como vender, monetizar e utilizar da melhor forma a sua API. Como foi mencionado antes, algumas Plataformas de Gerenciamento de APIs oferecem uma integração total com a plataforma de execução de microserviços. Esse tipo de funcionalidade oferece um gerenciamento total de todas as partes da solução, não sendo necessário uma infra separada. Sendo assim, a sua arquitetura ficará como a imagem abaixo: Conclusão Utilizar arquitetura de Microserviços possibilita o desenvolvimento de interfaces RESTful que irão expor o seu legado que nativamente não possui uma interface HTTP, porém o primeiro desafio é escolher as ferramentas corretas essa implementação. Existem muitos frameworks e linguagens que podem ajudar na implementação de microserviços, a decisão depende muito do cenário que está se enfrentando, porém algumas das mais utilizadas são citadas neste artigo. Depois de escolher seu kit de desenvolvimento, a próxima decisão a ser tomada é estabelecer a plataforma de implantação e execução, mais uma vez, a decisão depende muito do cenário sendo encarado, porém neste caso, o principal objetivo é a exposição de RESTful APIs de forma consistente que atendam aos requisitos funcionais e não-funcionais. O Sensedia API Suite é uma ferramenta de gerenciamento de APIs que consegue prover a funcionalidade de Backend as Services (BaaS) que podem substituir as responsabilidades de um PaaS na implantação e execução dos Microserviços. Além disso, a ferramenta consegue prover todas as funcionalidades chave para o melhor gerenciamento de uma API, como por exemplo API Gateway com Caching e Analytics. Em resumo, a recomendação é usar uma ou mais plataformas integradas que consiga lhe entregar o gerenciamento total de seus Microserviços e bem como das APIs que eles irão expor. Referências The following two tabs change content below.",pt,131
51,2242,1472739274,CONTENT SHARED,-615912190028612956,6013226412048763966,1930561315994163469,,,,HTML,http://vocesa.uol.com.br/noticias/acervo/coragem-e-o-principal-requisito-para-ser-um-bom-lider-diz-o-autor-britanico-simon-sinek.phtml,"ser um bom líder depende de inúmeros fatores. mas um deles, segundo o inglês simon sinek, é importantíssimo: a coragem para proteger as equipes e para arriscar quando necessário","Crédito: Divulgação"" title=""""Nós seguimos os bons líderes não porque precisamos mas porque queremos"", diz o inglês Simon Sinek | Crédito: Divulgação""> ""Nós seguimos os bons líderes não porque precisamos mas porque queremos"", diz o inglês Simon Sinek | Crédito: Divulgação Qual o principal requisito de uma liderança inspiradora? Essa é uma pergunta que gera muitas respostas. E o inglês Simon Sinek, especialista em gestão, encontrou um adjetivo satisfatório: coragem. Para ele, o que faz com que os profissionais sejam vistos por suas equipes como excelentes líderes é a capacidade de proteger as equipes e a ousadia para arriscar. Simon está lançando no Brasil o livro Líderes se Servem Por Último (44,90 reais, HSM) - e a metáfora do título explica exatamente o que ele acredita ser o grande papel de um bom gestor de pessoas: desenvolver os times, fazendo com que os subordinados sejam os primeiros a ""se alimentar"". Só assim as equipes conseguem ter força para produzir e motivação para trabalhar. VOCÊ S/A - A sua teoria de liderança diz que grandes líderes colocam os interesses dos times em primeiro lugar e ""comem por último"". Por que essa atitude é importante? SIMON- A metáfora ""os líderes comem por último"" se relaciona com a dos pais, que sempre alimentam as crianças primeiro - não porque leram isso em um livro, não porque foram ensinadas, simplesmente porque usam o instinto paterno ou materno. Esse instinto de cuidar de pessoas que dependem de você pode ser aplicado à liderança. Liderar não é sobre estar no comando. É sobre cuidar das pessoas que estão sob o seu comando. Essa é a verdadeira liderança. Os líderes que comem por último até poderiam comer primeiro, pois o cargo permite. Eles poderiam, também, conseguir regalias e vantagens por conta da posição que ocupam. Mas não fazem isso. Os verdadeiros líderes preferem sacrificar seus interesses para cuidar bem das vidas das pessoas que fazem parte de sua equipe - e nunca sacrificam a vida do time para cuidar de seus próprios interesses. Eles escolhem comer por último por uma questão de honestidade e lealdade. Os líderes são aqueles que protegem. E quando as pessoas se sentem seguras, elas transmitem essa segurança para os colegas, clientes e consumidores. VOCÊ S/A - Seu livro mostra a evolução da liderança da era Paleolítica até os dias de hoje. Como eram os líderes do tempo das cavernas? SIMON- O que faz com que alguém seja um líder na era Paleolítica e hoje é exatamente a mesma coisa. O que evolui são as condições em que operamos. No paleolítico, vivíamos em cavernas em tribos que tinham entre 100 e 150 pessoas. Nossos líderes eram homens fortes e grandes - e havia os homens-alfa. Naquela época, a questão era sobreviver. Os homens mais fortes, os ""alfas"", eram capazes de suportar cargas pesadas, encontrar alimentos e proteger a tribo do perigo. Por conta disso, eles tinham o privilégio de possuir a primeira escolha de carne da tribo (para se sentirem mais fortes e continuar caçando) e a primeira escolha da companheira que gostariam de ter (para garantir que os melhores genes fossem perpetuados). VOCÊ S/A - Eles tinham esses privilégios por que cuidavam bem da tribo? SIMON - Havia um código antropológico do que significava liderar no Paleolítico, que é o mesmo até hoje: os liderados permitem um tratamento preferencial para o líder. Mas isso tem um custo, o líder tem que agir de acordo com o interesse coletivo. Se os liderados dão ao líder todos os benefícios e ele não os protege do perigo, não é um líder de verdade. A liderança é um serviço. Serviço vem com sacrifício. Se não há sacrifício, não há serviço, nem liderança. VOCÊ S/A - Quais características transformam alguém em um líder excepcional? SIMON - O requisito para ser um líder não é ter visão ou carisma. É ter coragem. Liderar significa que temos que dar o primeiro passo, que temos que colocar a corda no pescoço para defender aquilo que acreditamos. Todos os bons líderes são corajosos e coragem não é algo que, miraculosamente, surge dentro de você. Nossa coragem vem da coragem dos outros, daqueles que fizeram algo antes de nós e que nos olham nos olhos e dizem: ""Eu acredito em você. Você consegue fazer isso"". Características como egocentrismo e ganância acabam com o significado da palavra liderar. A liderança é sobre cuidar das pessoas pelas quais você é responsável. Aqueles que se esquecem disso não são líderes, são simplesmente autoridades que mandam usando ferramentas como medo ou a necessidade que o empregado tem de manter aquele trabalho. Mas nós não seguimos esses profissionais. Nós seguimos os bons líderes não porque precisamos, mas porque queremos. VOCÊ S/A - Quais as diferenças entre gerentes e líderes? SIMON- Gerentes se preocupam com melhorias. Líderes se preocupam com saltos à frente. Gerentes se concentram em sistemas, métricas, processos e resultados. Líderes mantém o foco na percepção de como as ações do time influenciam nos resultados. Gerentes olham para os números. Líderes olham para o ""nós"". Todos os gerentes de métricas têm a oportunidade de se tornar líderes de pessoas. VOCÊ S/A - O Brasil está enfrentando uma crise econômica e, ao mesmo tempo, uma crise de liderança institucional. Nas empresas, o que os líderes precisam fazer para combater a descrença de seus times? SIMON- Se tornar líderes melhores. Liderança não é uma campanha de marketing. Não se trata de convencer as pessoas, mas de cuidar das pessoas. Liderança não é sobre enviar uma mensagem. É sobre a mensagem em si. Pense no Dr. Martin Luther King: 250 000 pessoas foram ouvi-lo falar de perto. Não houve lembretes por e-mail, campanha no Facebook ou hashtags para atrair essa multidão. Todos eles se reuniram porque tinham algo em comum - acreditar em uma América em que as leis deveriam ser iguais para todos, independentemente de cor da pele, religião ou classe econômica. Dr. King foi capaz de criar um movimento forte em um momento de crise porque não ficou parado esperando por um futuro melhor. Ele não fez o discurso do ""Eu tenho um plano"". Ele fez o discurso do ""Eu tenho um sonho"" e transformou em palavras o que ansiava. Com isso, Dr. King nos deu um mundo do qual poderíamos fazer parte e no qual poderíamos contribuir. Ele não quis tentar provar como poderia resolver tudo sozinho. Ele criou as condições (e o desejo) para que nós fizéssemos isso juntos, ao lado dele.",pt,130
52,2556,1476360149,CONTENT SHARED,-3040610224044779845,801895594717772308,-6061043432083340716,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36",MG,BR,HTML,https://medium.freecodecamp.com/10-tips-to-maximize-your-javascript-debugging-experience-b69a75859329?gi=581529e210bf,things you probably didn't know you could do with chrome's developer console,"Chrome comes with built-in developer tools. This comes with a wide variety of features, such as Elements, Network, and Security. Today, we'll focus 100% on its JavaScript console. When I started coding, I only used the JavaScript console for logging values like responses from the server, or the value of variables. But over time, and with the help of tutorials, I discovered that the console can do way more than I ever imagined. Here are useful things you can do with it. If you're reading this in Chrome (Or any other Browser) on a desktop, you can even open up its Developer Tools and try these out immediately. 1. Select DOM Elements If you're familiar with jQuery, you know how important the $('.class') and $('#id') selectors are. They select the DOM elements depending upon the class or ID associated with them. But when you don't have access to jQuery in the DOM, you can still do the same in the developer console. $('tagName') $('.class') $('#id') and $('.class #id') are equivalent to the document.querySelector(' '). This returns the first element in the DOM that matches the selector. You can use $$('tagName') or $$('.class') - note the double dollar signs - to select all the elements of the DOM depending on a particular selector. This also puts them into an array. You can again go ahead and select a particular element among them by specifying the position of that element in the array. For example, $$('.className') will give you all the elements that have the class className, and $$('.className')[0]and $$('.className')[1] will give you the first and the second element respectively. 2. Convert Your Browser Into An Editor How many times have you wondered whether you could edit some text in the browser itself? The answer is yes, you can convert your browser into a text editor. You can add text to and remove text from anywhere in the DOM. You don't have to inspect the element and edit the HTML anymore. Instead, go into the developer console and type the following: This will make the content editable. You can now edit almost anything and everything in the DOM. 3. Find Events Associated with an Element in the DOM While debugging, you must be interested in finding the event listeners bound to an element in the DOM. The developer console makes it easier to find these. getEventListeners($('selector')) returns an array of objectsthat contains all the events bound to that element. You can expand the objectto view the events: To find the Listener for a particular event, you can do something like this: This will display the listenerassociated with a particular event. Here eventName[0] is an array that lists all the events of a particular event. For example: ...will display the listener associated with the click event of element with ID 'firstName'. 4. Monitor Events If you want to monitor the events bound to a particular element in the DOM while they are executed, you can this in the console as well. There are different commands you can use to monitor some or all of these events: monitorEvents($('selector')) will monitor all the events associated with the element with your selector, then log them in the console as soon as they're fired. For example, monitorEvents($('#firstName')) will log all the events bound to the element with the ID of 'firstName' . monitorEvents($('selector'),'eventName') will log a particular event bound with an element. You can pass the event name as an argument to the function. This will log only a particular event bound to a particular element. For example, monitorEvents($('#firstName'),'click') will log all the click events bound to the element with the ID 'firstName'. monitorEvents($('selector'),['eventName1','eventName3',....]) will log multiple events depending upon your own requirements. Instead of passing a single event name as an argument, pass an array of strings that contains all the events. For example monitorEvents($('#firstName'),['click','focus']) will log the click event and focus events bound to the element with the ID 'firstName' . unmonitorEvents($('selector')) : This will stop monitoring and logging the events in the console. 5. Find the Time Of Execution of a Code Block The JavaScript console has an essential function called console.time('labelName') which takes a label name as an argument, then starts the timer. There's another essential function called console.timeEnd('labelName') which also takes a label name and ends the timer associated with that particular label. For example: The above two lines of code give us the time taken from starting the timer to ending the timer. We can enhance this to calculate the time taken for executing a block of code. For example, let's say I want to find the time taken for the execution of a loop. I can do like this: 6. Arrange the Values of a Variable into a Table Let's say we have an array of objects that looks like the following: When we type the variable name into the console, it gives us the values in the form of an array of objects. This is very helpful. You can expand the objects and see the values. But this gets difficult to understand when the properties increase. Therefore, to get a clear representation of the variable, we can display them in a table. console.table(variableName) represents the variable and all its properties in a tabular structure. Here's what this looks like: 7. Inspect an Element in the DOM You can directly inspect an element from the console: inspect($('selector')) will inspect the element that matches the selector and take you to the Elements tab in the Chrome Developer Tools. For example inspect($('#firstName')) will inspect the element with the ID 'firstName' and inspect($('a')[3]) will inspect the 4th anchor element you have in your DOM. $0, $1, $2, etc. can help you grab the recently inspected elements. For example $0 gives you the last-inspected DOM element, whereas $1 gives you the second last inspected DOM Element. 8. List the Properties of an Element If you want to list all the properties of an element, you can do that directly from the Console. dir($('selector')) returns an object with all of the properties associated with its DOM element. You can expand these to view them in more detail. 9. Retrieve the Value of your Last Result You can use the console as a calculator. And when you do this, you may need follow up one calculation with a second one. Here's how to retrieve the result of a previous calculation from memory: Here's what this looks like: 10. Clear the Console and the Memory If you want to clear the console and its memory, just type: Then press enter. That's all there is to it.",en,130
53,1539,1467026915,CONTENT SHARED,4084131344684656470,5127372011815639401,5134361217529549945,,,,HTML,https://deis.com/blog/2016/kubernetes-illustrated-guide/,the children's illustrated guide to kubernetes,"Kubernetes, Book Introducing Phippy, an intrepid little PHP app, and her journey to Kubernetes. What is this? Well, I wrote a book that explains Kubernetes. We posted a video version to the Kubernetes community blog. If you find us at a conference, you stand a chance to pick up a physical copy. But for now, here's a blog post version! And after you've finished reading, tweet something at @opendeis for a chance to win a squishy little Phippy toy of your own. Not sure what to tweet? Why don't you tell us about yourself and how you use Kubernetes! The Other Day... The other day, my daughter sidled into my office, and asked me, ""Dearest Father, whose knowledge is incomparable, what is Kubernetes?"" Alright, that's a little bit of a paraphrase, but you get the idea. And I responded, ""Kubernetes is an open source orchestration system for Docker containers. It handles scheduling onto nodes in a compute cluster and actively manages workloads to ensure that their state matches the users' declared intentions. Using the concepts of ""labels"" and ""pods"", it groups the container which make up an application into logical units for easy management and discovery."" And my daughter said to me, ""Huh?"" And so I give you... The Children's Illustrated Guide to Kubernetes Once upon a time there was an app named Phippy. And she was a simple app. She was written in PHP and had just one page. She lived on a hosting provider and she shared her environment with scary other apps that she didn't know and didn't care to associate with. She wished she had her own environment: just her and a webserver she could call home. An app has an environment that it relies upon to run. For a PHP app, that environment might include a webserver, a readable file system, and the PHP engine itself. One day, a kindly whale came along. He suggested that little Phippy might be happier living in a container. And so the app moved. And the container was nice, but... It was a little bit like having a fancy living room floating in the middle of the ocean. A container provides an isolated environment in which an app, together with its environment, can run. But those isolated containers often need to be managed and connected to the external world. Shared file systems, networking, scheduling, load balancing, and distribution are all challenges. The whale shrugged his shoulders. ""Sorry, kid,"" he said, and disappeared beneath the ocean's surface. But before Phippy could even begin to despair, a captain appeared on the horizon, piloting a gigantic ship. The ship was made of dozens of rafts all lashed together, but from the outside, it looked like one giant ship. ""Hello there, friend PHP app. My name is Captain Kube"" said the wise old captain. ""Kubernetes"" is the Greek word for a ship's captain. We get the words Cybernetic and Gubernatorial from it. Led by Google, the Kubernetes project focuses on building a robust platform for running thousands of containers in production. ""I'm Phippy,"" said the little app. ""Nice to make your acquaintance,"" said the Captain as he slapped a name tag on her. Kubernetes uses labels as ""nametags"" to identify things. And it can query based on these labels. Labels are open-ended: You can use them to indicate roles, stability, or other important attributes. Captain Kube suggested that the app might like to move her container to a pod on board the ship. Phippy happily moved her container inside of the pod aboard Kube's giant ship. It felt like home. In Kubernetes, a Pod represents a runnable unit of work. Usually, you will run a single container inside of a Pod. But for cases where a few containers are tightly coupled, you may opt to run more than one container inside of the same Pod. Kubernetes takes on the work of connecting your pod to the network and the rest of the Kubernetes environment. Phippy had some unusual interests. She was really into genetics and sheep. And so she asked the captain, ""What if I want to clone myself... On demand... Any number of times?"" ""That's easy,"" said the captain. And he introduced her to the replication controllers. Replication controllers provide a method for managing an arbitrary number of pods. A replication controller contains a pod template, which can be replicated any number of times. Through the replication controller, Kubernetes will manage your pods' lifecycle, including scaling up and down, rolling deployments, and monitoring. For many days and nights the little app was happy with her pod and happy with her replicas. But only having yourself for company is not all it's cracked up to be.... even if it is N copies of yourself. Captain Kube smiled benevolently, ""I have just the thing."" No sooner had he spoken than a tunnel opened between Phippy's replication controller and the rest of the ship. With a hearty laugh, Captain Kube said, ""Even when your clones come and go, this tunnel will stay here so you can discover other pods, and they can discover you!"" A service tells the rest of the Kubernetes environment (including other pods and replication controllers) what services your application provides. While pods come and go, the service IP addresses and ports remain the same. And other applications can find your service through Kurbenetes service discovery. Thanks to the services, Phippy began to explore the rest of the ship. It wasn't long before Phippy met Goldie. And they became the best of friends. One day, Goldie did something extraordinary. She gave Phippy a present. Phippy took one look and the saddest of sad tears escaped her eye. ""Why are you so sad?"" asked Goldie. ""I love the present, but I have nowhere to put it!"" sniffled Phippy. But Goldie knew what to do. ""Why not put it in a volume?"" A volume represents a location where containers can access and store information. For the application, the volume appears as part of the local filesystem. But volumes may be backed by local storage, Ceph, Gluster, Elastic Block Storage, and a number of other storage backends. Phippy loved life aboard Captain Kube's ship and she enjoyed the company of her new friends (every replicated pod of Goldie was equally delightful). But as she thought back to her days on the scary hosted provider, she began to wonder if perhaps she could also have a little privacy. ""It sounds like what you need,"" said Captain Kube, ""is a namespace."" A namespace functions as a grouping mechanism inside of Kubernetes. Services, pods, replication controllers, and volumes can easily cooperate within a namespace, but the namespace provides a degree of isolation from the other parts of the cluster. Together with her new friends, Phippy sailed the seas on Captain Kube's great boat. She had many grand adventures, but most importantly, Phippy had found her home. And so Phippy lived happily ever after.",en,128
54,2316,1473686450,CONTENT SHARED,-3716447017462787559,-1578287561410088674,1736179400644080759,,,,HTML,http://hipsters.tech/squads-nao-suicidas-hipsters-08/,squads não-suicidas - hipsters #08,"Como as equipes são divididas em uma empresa? Funciona bem? Eles tem o mesmo objetivo ou a mesma função? Nosso host Paulo Silveira conversa sobre mais um assunto da moda: Squads . Os esquadrões são uma forma que o Spotify encontrou para escalar Agile em empresas maiores, fazendo com que os times foquem em um objetivo comum e tenham autonomia... hummm ou algo assim. Muitas empresas abraçaram a técnica, com maior e menor sucesso. Vamos conversar sobre essa cultura e as dificuldades no processo de adoção. Participantes: Paulo Silveira , host do hipsters e gerente de produtos na Alura Pedro Axelrud , cuspidor de fogo e gerente de produtos no Nubank Gabriela Rojas , gerente de produtos no Nubank Marcell Almeida , gerente de produtos no Vivareal Pois é, não faltava gerente de produtos e startupeiro na sala! Links que apareceram: Produção e conteúdo: Alura Cursos online de Tecnologia Caelum Ensino e Inovação Edição e sonorização: Radiofobia Podcast e Multimídia",pt,128
55,3060,1486469778,CONTENT SHARED,-14569272361926584,-8781306637602263252,9157733567505152711,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36",SP,BR,HTML,http://guirmendes.blogspot.com.br/2017/02/java-8-streams-deeper-approach-about.html,java 8 streams - a deeper approach about performance improvement,"Introduction Java 8 was released almost three years ago, but it still lacks articles with deeper approach through Stream API. There are some good articles about it, but not a single one showing a real world example and comparing its performance against Java 7 style of coding. This article assumes that the reader already has some knowledge about Stream API, so many simple code will not be explained in here. To start learning about Stream API, I suggest this article , from Benjamin Winterberg. This article shows a real world alike example, with several different approach of coding, always comparing performance against Java 7. It is result of a study over Stream API's performance when dealing with file processing. The main goal is to use as many Streams and Lambdas as possible into Java 8's code and not to use a single line of Java's new features into Java 7's code, in order to learn how to use Java 8's new features. This project was my first contact with Java 8 at all and I will show how the application evolved to current status. Be ready to see some bad code as well! The text is divided in sessions and pretty much follows time line from the project's development. All benchmark results are posted at the end and previous sessions talk about how I implemented and improved the project. Please note that the article is long, so please be patient and reserve some time to learn where I made mistakes and maybe to help me find any error I may have implemented in the code. Input Data As input data for processing, I used a Brazilian government data file, known as Sigtap (can be found here ). It is a zip archive, containing lots of text files, divided into two groups: layout files and data files. Data files contains positional based information, where each line represents a database register. Layout files contains information about each position in data files. Those files looks very alike a database export, which is the way we'll treat them in this article. It is important to note that there is a layout.txt file containing all other layout files' information. Let's call this file the general layout. It'll be used later in this article. There is an example data into Resources' folder inside the project. Please take a look into those files to better understanding about the implementation decisions to write the code. The Project The objective is to convert the input data files into SQL inserts. The inserts are not printed or saved anywhere, just generated in memory, because of volume of data. It uses only Java default libraries to process everything, except the benchmarks itself. For that, the JMH library was used. Because of JMH, the jar of the project excpects a JDK argument called path to run properly. This argument is the path to Sigtap file's extraction folder, for example: '/path/to/Stream_Study/src/main/resources/Sigtap/'. Development Phase - Java 8 In this session, I'll present how the code was written, using some subsessions to make reading easily. Although it's not strictly necessary, it's recommended to read the next subsessions with this source file for consulting. Reading files The project started with Java 8 implementation. The first surprise was newer Files API. One code line to read all file's lines is impressive! Check out the code inside Image 1. Please nothe that Paths.get may expect the file's encoding. It's important to inform in this case because the input data is encoded with ISO 8859-1. Also note that exceptions are ignored because of example purposes. In this case, an empty List will be returned to prevent NullPointerException to happen. Processing files With this very nice start, let's start file processing. The strategy is read the general layout file to detect all tables and its columns. So, let's use a Map, where the key is the table's name and the value is the table's columns, as a List of String. This strategy brings a easy way to work with layout's information and to access desired data from data files. First nightmare: how to perform this conversion using Stream API? None of default options could do the job. So, let's Google it a bit. After Googling, I came to this StackOverflow . Its accepted answer does the job, but hey! What an Alien code! At this point, I was not ready to understand such a complex code. No problem understanding that it was the implementation of a custom Collector, but the Collector code itself. Note that at this moment, I inserted lots of waste into code. The strategy is to split the original file's list into sublists (Image 2). The sep argument is String::isEmpty , because I want to split my list when I find an empty line. The custom Collector receives three arguments: a Supplier, an Accumulator and a Combiner. The Supplier object will be returned after a call to Stream's collect using the custom Collector. The Accumulator function (actually a BiConsumer) will be called for each element of the stream and creates an output element. The Collector function (a BinaryOperator) will only be called if this Collector is used into a ParallelStream and is used to combine multiple results from Accumulator into one only result. Note that for serial Stream this function will never be called. The above code is not ready to ParallelStream, but in this case there is no difference, because we need the list to be processed serially, otherwise we won't get the expected result. After splitting the list into a List<List<T>>, it is time to convert it into a Map<T, List<T>>, as desired. Let's take a look into Image 3's code. The list argument is the original file's List of lines and the sep argument is String::isEmpty , because I want to split my list when I find an empty line. After calling the collect using the above splitBySeparator Collector, a new Stream is started, filtering empty lists (there are empty lists at this point) and collecting to a Map using the default Collector implementation that generates a Map. Note that this code is generic enough to receive any List and any Predicate that it'll work. Just note that the first sublist's element will be used as the Map's key, so some adjust may be needed at this point. Save this comment for Java 7's implementation. Validating Although it is an example project, it follows real world applications, so it needs some kind of data validation. In this case I choosed to simply compare if general layout and table's specific layout has same content. To do so, the simplest way is to convert the List of columns into a single String taking care of using the same sorting for both files. The Map.Entry in above code is an entry from the Map of general layout. The key is the table's name and the value is the List of this table's columns (with extra column processing information). This snippet just starts a Stream from each List of columns, sorts it and collect to String, using default joining Collector. Then, it throws an Exception if both are not the same. Please note the call of readFile method. It guarantees the equals' argument is from another file. Generating SQL inserts The next step is generate the SQL inserts desired. This is the core of processing in this project. At this point the data files are processed using the positional information contained into layout files. The Map.Entry in Image 5's code is an entry from the Map of general layout. The key is the table name and the value is the List of this table's columns (with extra column processing information). The first thing to do is validate the entry using the Image 4's validate method. Once it's validated, it's important to remove some header information from file, to prevent misbehaviour of the application. Some extra information about the layout now became important: this file's columns can be splitted using a comma, here convenientely replaced by SEPARATOR constant. After splitting the layout, there are only three information positions interesting for this project: the index 0, containing the column name; the index 2, which informs the position to start reading data in data file; and the index 3, indicating the position to stop reading data in data file. Next, it's the moment to get a list of each column's name. Getting a Stream from layout's List, I mapped each splitted layout line to its 0 index, then sorted the result and then collected using default Collector's toList method. After extracting column information, it's time to iterate over data file's content. Note that this is the first time I used a parallelStream in entire code so far. I used it here because this is the first point where there is no harm using parallelism and benchmarks showed parallelStream is faster at this point. And it is a major tip : test your own case to see if parallelStream is the best option. I used a forEach loop here to populate a List of Map to represent each line of the file. The Map is a key value representation of data, where the key is the column's name and value is the content of this column in this line of the file. Note that for each file line there is lots of columns so I used a new forEach inside the first, this time into layout's content, to convert each line into the Map I wanted. At this point, the processedDataList variable is a List of each data file's lines converted into a map of columns to its data. Now I used another parallelStream over processedDataList to in fact generate the insert. After removing null elements, I map the line to the SQL insert text and then collect it to a List, using default toList Collector. To produce the SQL insert, I used code shown in Image 6. There is a StringBuffer to build my final String and two Stream operations inside the method. The first one I already explained before. The second one is the most interesting. Starting a Stream from the list of columns, each column is mapped to its data value. Then I use Java 8's Optional to treat null elements, returning SQL's keyword NULL String when there is no data for that column in the Map. Also, it is important to map each empty value to SQL's NULL String. Then I used the default Collector's joining method to create a String. Orchestrating everything Now that I showed how to process the entire file it's time to understand how to orchestrating all this methods calls. Image 7's code shows how to orchestrate everything. This is the entry point method to all processing and it is very simple. First I use readFile to read general layout file. Then I use listToMap to transform the List of the file's lines into a Map where the key is the table's name and value is the List of table's columns. The next step is to get the Map's entry set, which is an information where I can get a Stream, to start processing. After getting Stream from Map's entry set, it is important to remove null elements. Then, each Map.Entry is converted to process method's output result using flatMap . Flat mapping is an extremely important concept of Stream. It allows you to convert one input line into multiple output lines transparently. In this case, each entry of the Map, representing a table, is converted to a List of Strings, representing all SQL inserts I need to run on that table. Last, it's time to produce the algoritm's output: a List of String. The default Collector's toList method is used here. Comments over this implementation The code presented in this session is the result of my first contact with Java 8. It contains lots of problems I'll discuss in the session called Improvement Phase - Java 8 . At this point, a good exercise is to understand this code and try find any performance problems to compare with my results to be shown. This way you may produce an even better optimized implementation than mine's. Development Phase - Java 7 After first implementation using Java 8, it's time to talk about first implementation using Java 7. The first version of Java 7's code was basically a translation of Java 8's implementation, to test performance with a consistent base. Although it's not strictly necessary, it's recommended to read the next subsessions with this source file for consulting. Reading files In this version, I used most common implementation to read files: Java 6 style. Please note the amount of extra code, comparing to Java 8's version. Processing files The same strategy of Java 8's version was used at this point: create a Map where the key is the table's name and value is a List of table's columns. No problem to write the Java 7 version, so no Googling needed. Please note that this method is much more readable. Also, there is no need to implement a two method solution, only one can do the job. Reading the code, the first loop does the same thing the custom Collector does in Java 8's version and the second loop generates the output Map, just like Java 8's default Collector's toMap method. Note that this implementation also has problems with empty Lists. Pay attention that this code is not completely generic and only Strings can be processed without modification. I choosed not to use Java 8's functional interface Predicate here to stay strictly into Java 7's default code style, exactly to show where Java 8 is better or worse, speaking of readability and usability. Validating Following Java 8's strategy again, let's validate input's content comparing the String generated from columns' List from both general layout and each table's layout file. Comparing to Java 8's code in Image 4, there is plenty more code in here. It's easy to understand, but Stream implementation also is easy to read. Here I use two for loops to convert each List into a String and then I check if both are equal. An inconvenient here is the need to remove the last SEPARATOR from output String, made through substring calls. I choosed removing last SEPARATOR's character to keep the method's comparing objects exactly the same as generated by Stream solution and keep consistency. Generating SQL inserts As of Java 8's version, here is the core of Java 7's implementation. Read carefully the Image 11's code. This is the most similar code snippet between versions. Almost every line of this code does the same thing as Java 8's version. Main difference is that I don't use parallelism in here, just sequential processing. Generating the SQL insert with Java 7 is a lot more code to write, but it is easy to read. The same approach here: a StringBuilder to construct the output String, two loops, one for column list and other for data list. Same problem here to remove final SEPARATOR character. It is really annoying, but does the job. Orchestrating Everything Again let's talk about how to orchestrate everything shown until here on Java 7's version. Again, very similar code between versions. In fact, the only differences are the listToMap specialized implementation (without a Predicate argument) and the loop instead of Stream's flatMap. This method also is the entry point to all processing. Java 8's version uses less code to do the same processing, but this code is easier to understand. Comments over this implementation This session's code is just a translation of Java 8's version. In fact, any methods can be replaced from both implementations. The only real difference over methods' interfaces is the use of Predicate as separator function on Java 8's listToMap , something that Java 7 can't provide to you and that can be very usefull when reusing code. Also is clear that Java 8 can be used to produce a lot less code instead of Java 7. At this point, there is a lot of waste code on both implementations. As spoiler, this Java 7 implementation is faster than Java 8's. More about it into Benchmark Results' session, after discussing improvements to each version. Again I recommend understanding all code shown in this session and try to remove waste code and improve performance. Improvement Phase - Java 8 At this session, the main goal is to show how the code evolved, after studying more and rethinking about the problem. Here I'll talk just about Java 8's implementation improvements and next session will talk about Java 7's version. Again, the sequence of facts is how I really developed the project. I developed two major evolutions using Java 8 and I'll call them V2 and V3, to simplify writing and understanding. I'll refer first version as V1 when needed. Same subsessions from above sessions will be used to show how each part evolved. Skipped subsessions has not been changed since previous version. Java 8's V2 Processing Files Validating Generating SQL inserts Lots of improvements here. First, all synchronized stuff has been removed to simplify and increase readability. After, one less Stream processing step is needed, joining last two parallelStreams into only one parallelStream. It means one less iteration over results and more performance! One important change is the use of a Supplier variable. It is very usefull when there is need to reuse a Stream. Java's Streams are not reusable, once it is processed with a terminal operation, the Stream can't be used again. But we still can set a Stream to a variable in order to reuse code using the Supplier functional interface. Look into columnSupplier variable's accesses. It has one get method which returns a new instance of the desired Stream each time it's called. It helps the readability of the code. Also note that this method now returns a Stream instead of a List. Java's Streams are only processed when a terminal operation is called, such as forEach , collect and reduce . The less calls needed to terminal operations the better, because this way Java can optimize code's execution time. More about how this can optimize your code's execution time on next session. Please come back and take a look into Image 7's code. Flatmap needs that lambda's output to be a Stream . The V1's collect approach was very wrong, since the process method's output List was only used to turn into a Stream again. Generating the insert text also was improved. Compare it to Image 6's code. Now I used a joining Collector's different constructor to inform a prefix and a suffix to output String. Note that baseInsertText variable's value does not changes for each processed data line. Now it's generated once for each table and reused for each data file's lines. Less code to process, more performance. Orchestrating everything Only change is to not need to start a Stream inside flatMap because process method already returns a Stream. Comments over this implementation After this improvement round, the code became more concise and easy to understand. Lots of wasting operations were removed. Looking at the code, doesn't seems to be such a huge improvement, but believe me, it is. This version made me start to think about how to develop considering the use of Stream API and helped me understand how this new world about Java programming works. Java 8's V3 Improvement Phase - Java 7 Java 7's version of this project also has been improved, again taking base at Java 8's V2 implementation. But the second version of Java 7's code is not just a translation of Java 8's second version, some other improvements has been applied too. There is only one improvement version based on Java 7, called V2. First version will be reffered as V1. Java 7's V2 This version brings all shareable optimizations used in Java 8's V2 plus some exclusive changes. Although it's not strictly necessary, it's recommended to read the next subsessions with this source file for consulting. Reading files This time, I decide to use Java 7's default implementation for reading files, using try-with-resource. It shortened a lot the code, also making it more readable. Compared to Image 8's code, there's a lot of improvement here on readability. Please note that multiple try blocks are not strictly necessary, but to use only one block implicates constructing objects passing a constructor as argument, which reduces readability. The most advantage of try-with-resource is that it's not necessary closing resources initialized inside the parenthesis. Processing files This subsection contains a lot of improvement, both in performance and readability. A new strategy was used to convert a List into a Map. The new approach needs only one loop over input List to generate the Map. There is an extra attempt to improve this section that won't be discussed in this topic, only in Benchmark Results' session. The code can be found here . Note how is possible to construct efficiently the desired Map using a simple flag. This solution also has no problems with empty Lists. This code is much more readable than Java 7's V1 implementation and much much more readable than Java 8's versions. Validating No significantly changes has been made into validate method in terms of performance. But readability has been very improved with use of a helper method to convert a List into a String using a default separator. Comparing to Image 10's code, the main change is just the use of a helper method for grouping List into a String. It's very clear how the code became cleaner and now there is some reusability. The helper method has no changes comparing with Image 10's code that it replaces. Generating SQL inserts There are some improvements here already known from Java 8's V2, such as eliminating one loop over data and processing only once the SQL insert's base text. Some waste code was removed, but general structure of this method was not changed. Compare it with Image 11's code, there's not so much changes in here. Note that generateInsert method has evolved a lot. Reusing baseInsertText variable's content simplified very much this method. Compare it with Image 12's code. Both readability and performance became better at this point. Comments over this implementation The major improvement into this version was the listToMap method. Other changes were just about removing unnecessary code, while listToMap is a really new code. But even with few changes, all waste code was decreasing significantly how performant this implementation can be. I hasn't found any new improvements to apply into this code at the moment and certainly some improvement may still be possible. Anyway, I'm very happy with the performance results for this implementation. Benchmark Results Now that all code has been properly explained, let's see how each version performate in a consistent test. As already told, I used JMH library to benchmark all code. It makes test results consistent and provides serious metrics. All benchmark code is availiable in this file and will not be explained here. Image 30 contains all benchmark results implemented in this project. All them will be properly explained in next subsessions in details, properly splitted to better readability. About table's columns: Benchmark column displays benchmark's name; Mode column displays which test this results refers - avgt means Average Time; Cnt column displays how many tests was performed to compute the score; Score column is the test's result and the most important column to compare; Error column is the error margin of the test; Units column is the measurement unit of Score and Error columns. Please note all benchmarks here used the Average Time method. Also, note that Benchmark name explains which version of Java is used and which version explained here the test refers. Also note that benchmark results are machine dependant, so running in different machine will produce different results. Read file results Let's start talking about reading files. Both Java 7's implementations has exaclty the same performance. Java 8's first and second implementations shows pratically the same results, a little bit slower than Java 7's versions, and Java 8's V3 implementation is very faster than anything! Why? Because readFile 's implementation in Java 8's V3 just reads the file into a Stream, which is just closed here, with no processing realized. In fact, this test only shows how much time takes to prepare a Stream, not to read the file itself. Analyzing the results, the verdict is to use Java 8's versions, V3 if a Stream is required or V1 if a List is required, except if performance is a very critical issue to your application and file reading operations are very common. Otherwise, Java 8's facility to write and read and the little difference between results are really big convincing arguments. Conversions from List to Map Please note here that some benchmarks does not refers to one version explicitly. Those tests are described partially back in Improvements' sessions as partial improvement code. Note also that Java 8's V3 splitList method is not tested here. It happens because this method throws lots of exceptions of to many open files while running benchmark tests, even collecting Stream result. Looking into Java 7's results, the partial study method using edges approach, not shown in this article, looks faster, but its more complex to understand code and its greater error margin was decisive to choose V2's implementation. Because of error margin, in past tests the results were different, with V2 implementation faster. Now looking into Java 8's results, the faster implementation is definitely the one using ForEach approach. This version is basically Java 7's V2 replacing traditional for loop by Java 8's forEach call and lambda argument. This code works, but has some inconvenient global variables because lambda can't access local non final variables. That is why I discarted this solution. Talking about the edges' approach, the same argument from Java 7's edges approach not has been choosen is applicable. V2's implementation is as fast as edges approach, but shows more stable results. In this case, Java 7's code is faster than Java 8's implementation. Also, the second Java 7 version's code is more easy to understand, so it's better to use Java 7's style at this point. Although Java 8's version has a very close performance, it lacks in readability. Full processing Surprise! Java 8's V3 optimized implementation is faster than Java 7's V2 optimized implementation?! Yes! But please notice that Java 8's V3 has lots more error margin than Java 7's V2. It means at long term, Java 7's V2 can be faster, or even that is more likely that Java 7's single shot calling may be faster than Java 8's single shot calling. I prefer to say both results are in a tie. But hey! That's awesome! It means that Java 8's Stream can be as fast as Java 7's default concepts and this is a great result! Looking towards other results, it's impressive how much performance can be obtained by just removing waste code. It also shows that Java 8's Streams need to be correctly written to obtain satisfatory performance. Conclusion This project shows a bit about Java 8 Stream's optimization. It's clear that is very easy to made a mistake when working with Streams and it may slow down your application's performance in a concerning way. It just means that Stream processing must be very carefully analyzed before releasing the code. It is important to note that a real world application would use a hybrid approach, mixing Java 7's concepts with Java 8's new Stream processing, and not this completely heterogeneous solution as shown here. This article is the proof that Streams can be used alone, but Java's default style is still easier to use and usually has better performance. Please use comments box or Github's issues/pull requests to interact about the results and suggest improvements. This way, all of us learn together how to improve development with Java 8's new features. Thank you for your time!",en,127
56,2680,1477670394,CONTENT SHARED,7814856426770804213,-9016528795238256703,9100012288934953337,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36",MG,BR,HTML,https://blog.devteam.space/new-macbook-pro-is-not-a-laptop-for-developers-anymore-d0d4b1b8b7de?gi=6d5f52f5f44,new macbook pro is not a laptop for developers anymore,"Here's why: #1. No Escape and function keys Today's Apple Event confirmed many of the rumors surrounding the long-awaited refresh of the Macbook Pro line. The Escape and Function keys on the laptops have been abandoned in favor of a touch bar that changed depending on the application that is being used. The last the Macbook Pro got a major update was a shocking 4 years ago and many publications are celebrating the new design. However, the lack of physical Escape and Function keys is a disaster for one major set of Apple's customers - Developers. Let's take a look at numbers: There are ~ 19 million developers in the world. And Apple has managed to sell ~19 million Macs over the past 4 quarters. What a coincidence! Yes, developers are drawn towards Apple products primarily for software reasons: the Unix-like operating system and the proprietary development ecosystem. But developers need to have a functional keyboard to make use of that software and now they don't. Why Tim Cook , why? This isn't to say that the touch bar is an inherently bad idea. You could locate it on top of the Esc and function keys instead of eliminating them entirely! Something like this: #2 Power. Almost no improvement for RAM and a processor The 2016 MacBook Pro ships with RAM and processor specs that are nearly identical to the 2010 model. Deja vu? RAM: At least it feels like that, because the MacBook Pro has had options of up to 16 GB of RAM since 2010. The only difference now is that you pay for the update. Processors: The MacBook Pro had options with 2.4 gigahertz dual-core processors back in 2010. Anything new in 2016? Not really, well... nope. That feels strange too, especially if you compare the price and the hardware quality to Apple's competitors. These days it's easy to find a Windows or Linux machine comparable to the Macbook Pro for $1,000 - $1,500. You don't need to go far to find them, notable brands like Lenovo, HP, Asus, Samsung, and Dell all offer them. And for those who aren't fans of Windows, Linux is always an option. With more resources poured into Microsoft developers ecosystem and Linux distributions, developers may soon have a wide range of great operating systems that can run on pretty cheap laptops. And to nail it: #3. What people are saying about MacBook Pro 2016 If you liked the post, click the �� below so more people will see it! :)",en,127
57,1312,1465424159,CONTENT SHARED,-2948321821574578861,7527226129639571966,3989456265529686673,,,,HTML,http://arquiteturadeinformacao.com/usabilidade/quando-usar-paginacao-e-quando-user-scroll-infinito/,quando usar paginação e quando usar scroll infinito?,"Scroll infinito é uma técnica que permite que o usuário continue rolando por uma infinidade de conteúdos sem precisar em nenhum momento apertar nenhum botão para carregar mais. Depois exemplos famosos desse tipo de interação são o newsfeed do Facebook, e o seu feed pessoal do Pinterest: você continua rolando a página indefinidamente, sem nunca precisar trocar de página ou apertar um botão para carregar mais itens. Aliás, essa técnica está totalmente alinhada com a estratégia desses serviços: eles não querem que você se dê conta que já está rolando a página há um tempão e que você chegou ao final dela; quanto menos você perceber isso, melhor . Por isso o scroll infinito é um ótimo alinhado de serviços que medem seu sucesso pelo tempo de engajamento do usuário no site ou aplicativo. O lado negativo do scroll infinito Na verdade é plural: negativos. Falta de sensação de controle. Alcançar um ""ponto final"" dá ao usuário a sensação de controle. Quando você tem um número de resultados limitados, você consegue facilmente se o resultado que você procura está ali ou não. No scroll infinito isso fica impossível, porque você não faz ideia do que vem pela frente. Falta de localização. Porque o feed é infinito, o usuário não consegue determinar um ponto específico na página onde está a informação que ele está procurando. Mesmo que ele consiga na primeira visita; em sua segunda visita ele já não consegue mais localizar onde estava aquela informação, já que a localização geográfica do conteúdo varia muito no scroll infinito. Além dos problemas acima, o scroll infinito ainda pode trazer um problema de performance para sua página ou app, já que carregar muito conteúdo na mesma página requer muita memória do seu browser. Por fim, se o usuário está tentando chegar até o rodapé do site para encontrar um link (termos de serviço, contato, etc.), ele pode se sentir frustrado com o fato de a página carregar mais conteúdo toda vez que ele volta a scrollar. A alternativa para isso é simplesmente não ter um rodapé, ou colocar os links que iriam rodapé em outro lugar do site. Quando usar paginação e quando usar scroll infinito? Scroll infinito funciona melhor para sites com conteúdo gerado por usuários (exemplo: Twitter, Facebook), conteúdo visual (exemplo: Pinterest, Google Images) ou sites que pretendem equilibrar a quantidade de tráfego em seu conteúdo. Já a paginação é mais universal, e funciona melhor para plataformas onde o usuário tem uma tarefa bem específica em mente (exemplo: busca do Google). Concorda? Discorda?",pt,126
58,1063,1463933351,CONTENT SHARED,5658521282502533116,-1561982036714087726,-6957674286814133812,,,,HTML,http://engenhariae.com.br/mais/colunas/ita-esta-oferecendo-10-cursos-gratuitos-a-distancia/,ita está oferecendo 10 cursos gratuitos a distância - engenharia é:,"O Instituto Tecnológico de Aeronáutica (ITA) , fundado em 1950, está oferecendo 10 cursos gratuitos por meio da plataforma de ensino on-line Coursera. A plataforma conta conta com mais de 12 milhões de usuários e dispõe de mais de mil cursos de instituições renomadas do Brasil e do mundo. Os professores que ministram os cursos são todos do ITA, instituto de ensino superior do Comando da Aeronáutica (COMAER), localizado no Departamento de Ciência e Tecnologia Aeroespacial (DCTA). Confira cada um dos cursos e faça sua inscrição: 1. Introdução ao Controle de Sistemas 2. Controle Usando a Resposta em Frequência 3. Arquitetura de Software em Projetos Ágeis 4. Desenvolvimento Ágil com Padrões de Projeto 5. Desenvolvimento Ágil com Java Avançado 6. 7. Princípios de Desenvolvimento Ágil de Software 9. TDD - Desenvolvimento de Software Guiado por Testes Projeto Final: Aplicativo para Web com Componente Gamificado 8. Técnicas Avançadas para Projeto de Software 10. Orientação a Objetos com Java PS: O curso é oferecido inteiramente de graça! Logo, caso você queira o certificado, é cobrado uma pequena taxa. Achou útil essa informação? Compartilhe com seus amigos! xD Deixe-nos a sua opinião aqui nos comentários. Nascido no interior de Minas Gerais, foi seminarista em uma congregação francesa, mas viu que que sua vocação era a de ser engenheiro. Criou o Engenharia é: exatamente às 11:28, do dia 2 de agosto de 2011. Você pode falar comigo pelo email: ademilson@engenhariae.com.br ;)",pt,126
59,2967,1484651225,CONTENT SHARED,-3191013159715472435,-1393866732742189886,2008706978002473337,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36",MG,BR,HTML,http://www.otempo.com.br/capa/pol%C3%ADtica/rob%C3%B4-acha-de-almo%C3%A7o-de-12-kg-a-cervejas-pagas-pelo-cidad%C3%A3o-1.1424088,robô acha de almoço de 12 kg a cervejas pagas pelo cidadão,"Em apenas dois meses, Rosie descobriu 3.553 casos suspeitos envolvendo a cota parlamentar dos deputados federais de todo o país. Rosie não é nenhuma funcionária da Ouvidoria da Casa, mas um robô criado por oito amigos que atuam na área de ciência de dados para fiscalizar as despesas dos deputados. A ferramenta cruza informações das notas apresentadas para reembolso com outras, como as da Receita Federal, a presença em plenário, a localização e as característica do lugar onde a compra foi feita. Assim, já foi encontrada uma nota de R$ 170 de um restaurante onde o quilo da refeição custa R$ 14. Ou seja, teriam sido consumidos 12 kg em um dia. Ainda tem o reembolso de um almoço de R$ 41 feito em São Paulo, apenas 35 minutos depois de o deputado ter discursado em Brasília. Só na última semana, 849 casos foram auditados. Destes, 629 resultaram em denúncias envolvendo R$ 378,8 mil pagos com dinheiro público por 216 deputados. O projeto responsável pelos levantamentos recebeu o nome de Operação Serenata de Amor. O nome revela a intenção de desvendar fraudes com valores pequenos. ""Na Suíça, uma ministra perdeu o cargo por ter comprado com dinheiro público um chocolate Toblerone. A ideia é ter um detalhamento tão preciso que seja capaz de detectar um chocolate pago irregularmente"", explica um dos membros, o publicitário Pedro Vilanova. A base de dados considera todos os reembolsos dos deputados que passaram pela Câmara Federal desde 2011. ""Alguns dos denunciados, dessa forma, já não estão mais no cargo, mas terão que prestar contas igualmente"", diz Vilanova. Por orientação jurídica, eles só divulgam o nome dos parlamentares depois de terem tido um retorno da Câmara. Entre os episódios em que já conseguiram não só resposta da Casa, mas a devolução do dinheiro pago equivocadamente está o do então deputado Odelmo Leão (PP-MG), eleito prefeito de Uberlândia. O mineiro gastou R$ 190,05 da cota parlamentar com envio de correspondência da sua campanha. Em agosto passado, Marco Maia (PT-RS), por exemplo, pediu ressarcimento de R$ 154,50 por duas refeições. Ele devolveu, em dezembro, R$ 77,25, referentes a um almoço. De acordo com as regras da Cota de Atividade Parlamentar, as despesas são autorizadas apenas para os parlamentares. O robô ainda encontrou um reembolso de R$ 135,15 para Vitor Lippi (PSDB-SP) na compra de cinco cervejas durante uma viagem à Califórnia, nos Estados Unidos. O tucano devolveu o valor e pediu desculpas. ""Aproveito para assumir a responsabilidade pelo erro cometido, é de praxe dessa assessoria pedir a glosa de itens não autorizados, como bebidas alcoólicas, mas infelizmente dessa vez não identifiquei o produto, já que estava em outra língua"", alegou o parlamentar no mês passado. Já o deputado Rocha (PSDB-AC) apresentou três notas de alimentação do mesmo dia. Duas delas, de R$ 52 e R$ 43, foram emitidas em Rio Branco, no Acre. A outra, de R$ 148, a 4.000 km, em Caxias do Sul, no Rio Grande do Sul. Ele alegou que o segundo reembolso foi de uma despesa feita uma semana antes. Como estava com pressa no dia, pediu a um assessor que voltasse outro dia para emitir o cupom, o que, segundo ele, explicaria a coincidência de data. Sem convencer ninguém, devolveu R$ 148. Criadores buscam mais financiamento Os dados brutos levantados pelo robô Rosie na plataforma Jarbas, ambos desenvolvidos pelos oito jovens na operação Serenata de Amor depois são analisados pelos profissionais e denunciados à Câmara dos Deputados, no próprio site do Legislativo. Como o volume de dados é muito grande, a equipe aceita a colaboração de outras pessoas que tenham conhecimento técnico ou simplesmente que estejam dispostas a divulgar a ideia, além, claro, de doações em dinheiro. Foi graças a um financiamento coletivo que eles conseguiram arrecadar R$ 80 mil e custear a investigação sobre as despesas da Copta de Atividade Parlamentar. O valor angariado foi superior à meta inicial de R$ 60 mil. ""O projeto prevê trabalho até o fim de janeiro e o começo de fevereiro, mas nós já estamos buscando novas formas de viabilizá-lo para que consigamos não parar de trabalhar. Continuamos precisando da ajuda de pessoas"", diz o publicitário Pedro Vilanova, um dos integrantes da ação. Na última semana, eles conseguiram organizar um mutirão com vários colaboradores. Os dados da Operação Serenata de Amor são públicos. Com algumas orientações da equipe, qualquer pessoa pode acompanhar os gastos de seu deputado. A cada semana um relatório novo do trabalho é publicado na página deles no Facebook, onde também é possível contatá-los. Seleção. A ação é uma das 20 escolhidas para a Hack Brazil, que acontece na Brazil Conference at Harvard & MIT, e elege projetos resolutivos em tecnologia, criatividade e inovação.",pt,124
60,1358,1465677689,CONTENT SHARED,-330801551666885085,7774613525190730745,-5387740831049193382,,,,HTML,http://blog.tiingo.com/switched-away-aws-packet-net-benchmarking-networking-disk-processing-speeds/,"aws vs packet.net why we left aws benchmarking aws's network, disk, and cpu performance - tiingo thoughts","If this sounds like a glowing review of Packet.net - it is. I found myself re-reading this post over and over, trying to make it sound less shrilly - but I can't. It's just a ridiculously good product and value - EC2 containers just don't make sense anymore. A friend once told me, ""Rishi - sometimes if you don't advocate a product aggressively - you can be doing society a disservice in your attempt to be neutral. If the value is so good, you must tell everybody about it."" This is one of those times. EDIT: Feeling really grateful the HackerNews community decided to link to Tiingo a second time. In the first HackerNews posting many of you asked for an API, which is what led to me finding the AWS bottleneck. The API launched [quietly] this week at: where Tiingo is now the first company to bring IEX (anti-HFT exchange/darkpool) data to mainstream FinTech. Kind of went full-circle as this post wouldn't have existed without the original HN coverage . TL;DR: The performance of AWS on network speed, disk speed, and CPU performance are quantitatively just ""not good,"" for what we needed. When we introduced real-time market data, we were in search of our bottleneck and realized it was AWS. We made the decision to switch to Packet.net and the below reflects on our decision and explains why. The benchmarks continue to reaffirm our decision. Having said all of this, certain features of AWS remain incredibly convenient like S3, Cloudfront, and Route53 - but we can't justify using EC2. In Networking : Packet is significantly faster, more stable, and 15%-44% cheaper In Disk Usage : Packet is more performant and 92% cheaper In CPU: Packet is 30-40% more performant and 15% cheaper In machines: Packet's systems are all bare-metal/dedicated, whereas AWS charges extra for dedicated machines If you've noticed Tiingo being particularly snappy these days, it's because I couldn't stand it anymore. I had tried everything - buying more expensive instances on AWS, allocating more space, scaling horizontally, but it wasn't matching up to my local dev machines. And so I started searching for the bottleneck - only to realize it was AWS. I started researching AWS, I found I wasn't alone. Many people experienced what I had but I tried prolonging the switch. Trying to change cloud service providers is frustrating: scripts break, performance temporarily suffers, you experience downtime, and you know there will be unknown-unknowns. Recently we just got real-time market data and this exacerbated the issues. Our websockets were being overwhelmed in queues and throwing more nodes at the problem was becoming expensive. We were trying to put a bandaid over a burst pipe. I finally decided on Packet.net and I want to share the reasons why. I've included benchmarking results to help emphasize the point. Our search was motivated by two major reasons: The costs were getting out-of-hand After reading the below Reddit post on AWS's [lack of] network stability, we started asking around and realized the experts were right... AWS's network is slow. If we are going to give our users real-time data directly from the exchanges, that's a heck-of-a-lot of data and we need it to be as fast as possible. The Reddit/Blog Post was from an engineer at Stack Overflow. . More specifically, this Reddit comment on AWS's network stability that seemed echo'd by many: We explored options like DigitalOcean , but Tiingo, like all financial data/analytics companies, is very data heavy and their plans didn't allow for flexible data storage (EBS on Amazon for example). We looked into Rackspace and Azure, but the cost differentials didn't make it seem worth the transition. Admittedly, having used Rackspace in the past - I've always loved their customer support and was personally disappointed I couldn't justify the cost. Eventually I came across Packet and spoke to their engineers since I hadn't heard of them before. I took a chance. It paid off. I told them my concerns and what I was trying to solve (market data connectivity and high data transfer rates). One of the co-founders, who was a networking engineer, personally oversaw my connectivity project to the data exchanges. I'm pretty sure this was Paul Graham 101 on start-ups and customer service. Ultimately though - I'm a data nut and so I decided to benchmark AWS vs Packet and was really curious about the Reddit comments on AWS's network stability. The benchmarks closed the deal for us. It was a no-brainer. Part of the major reason being that Packet.net is bare metal (dedicated physical machines) whereas AWS tends to be focused on virtual machines. The hardware/pricepoint is actually even cheaper on Packet. We are paying 1/3rd of what it would cost to get a similar, less performant, system on AWS. SO here you have it! The tests below compare AWS vs Packet for disk, network, and CPU benchmarking - and also cost. I've outlined and commented the results below so you can reproduce the tests. Hardware Since we are testing Packet vs AWS, we started off with the Packet hardware and found the AWS price equivalent. We started with the Type 1 and worked backwards to find the equivalent in performance/price on AWS. Note: For the network test, we also test a smaller machine. The reason for the lighter hardware is for load balancing (HAProxy in this sense). If all of the back-end servers can have high network throughput, but we need to send it to the end-user, the load-balancer's networking performance will be the determining factor. This is especially important in cases like real-time data. Packet: Type 1 (Server) (4 core, 8-threaded) 3.4ghz Intel Xeon E3-1240 v3 32gb $0.40/hr $0.37/hr if reserved for 1 month $292.80/month Type 0 (Load Balancer) (4 core) 2.4ghz Intel Atom C2550 8gb $0.05/hr $0.0459/hr if reserved for 1 month $36.60/month What somebody may choose as their load balancer *Note:We assume 732 hours in a month; but if you reserve a Packet instance for a month, they will only charge you 672 hours per month. However, to make apples-to-apples comparisons, all calcs in Price/Month assume you choose hourly pricing (732 hours for 1 month) to keep things normalized. AWS: m4.2xlarge (Server) 8 VCPU (2.4ghz Intel Xeon E5-2676) 32gb $0.479/hr $350.63/month xlarge was chosen for it's optimized network performance t2.medium (Load Balancer) 2 VCPU (Xeon processors burstable to 3.3ghz) 4gb $0.052/hr $38.07/month What somebody may choose as their load balancer OS : Ubuntu 14.04 server Network: For this test, we used iperf3 as per the AWS documentation ( ) We wanted to simulate a very real-world network configuration for ourselves - basically what our site looks behind a load balancer. Load balancers tend to require very low processing power, and serve as a network bottleneck to the user. We are testing: Internet -> Load-balancer (Haproxy) Load-balancer (HAProxy) -> Server Server -> Server The ""Internet"" machine used was an Azure machine. Not perfect, but we figured it was a good 3rd party control. You can view the detailed methodology in the Appendix below. Results: Performance: AWS came out incredibly inconsistent - with a high std. deviation and low mean transfer rates. What AWS considered a ""High"" performance network tier, was the least expensive tier on Packet. Why didn't we use AWS Elastic-Load-Balancer (ELB)? For our use case with websockets, - we found ELB to be lacking what we needed. This will be a blog post for a later day. What was particularly interesting was the inconsistency of the lower tier machines. We ran our benchmarks over an hour, and here is what the rates looked like when making requests to-and-from the lower tier (t2.medium) EC2 Instance. This seems consistent with their ""burstable"" instance - which is great and all...except Packet's lowest tier outperforms it: Pricing: The above AWS configuration is $.081/hour more expensive than Packet and also less performant. Another consideration is bandwidth costs. AWS charges $0.09/GB (for the first 10TB) out to the internet. Packet.net charges $0.05/GB out to the internet. Within the same data centers (availability zones in AWS), both Packet and AWS are free. However, when transferring to a different availability zone, AWS charges $0.02/GB and Packet.net charges $0.05/GB. Conclusion: Packet is the clear winner in this. In both absolute speed and stability. In terms of price, Packet is cheaper by $.081/hour in the above configuration, or 15% cheaper - and for the majority of our bandwidth we go external to the internet. In outbound internet traffic, Packet is 44% cheaper. Disk: Packet offers two storage types: Basic (500 IOPS) and Performance (15,000 IOPS). We created a EBS volume on both Packet & AWS with provisioned IOPS of 500 and then 15,000. Then we used sysbench to run an I/O test (see Appendix below for methodology). Results: Performance: When getting to the 15k IOPS, we saw a more significant performance differential favoring Packet. At Tiingo we used the performance tier given the amount of data we store and calculate. Price: Provisioning 15,000 IOPS on AWS @ $0.065/IOPS = $975. But wait, that's not all! They also charge $0.125/hour per GB. So a 15k IOPS 500GB HDD on AWS would be $1037.50 On Packet it would be 500GB * $0.15 = $75. Doing a bit of algebra, the cost for 15k IOPS on AWS would be cost effective if you have >39TB of storage. That's right - Packet is cheaper until you hit 39TB of storage.... Conclusion: Packet is literally 92.3% cheaper than AWS for 15k IOPS performance, and Packet is even more performant. It's the victor in disk performance as well. CPU: CPUs cannot be benchmarked purely on the speed of the processor [clock] alone. For these reasons, we ran a sysbench test as well on different threads. Results: Performance: The results are damning for AWS. On an 8 processor machine, the benchmark ran slower on 8 cores than 4. I ran this multiple times, double checked to make sure this was an m4.2xlarge. Then I spun up another m4.2xlarge and the results were more in line with what I expected (still slower than Packet). However, I am going to keep the original instance's benchmark below to highlight the point of noisy neighbors . With AWS, you can get a shared machine with other neighbors who are processor intensive and reduce your performance. This is what virtualization is. With Packet, you get a dedicated system. What most likely happened was that our original machine had a noisy neighbor. Here are the results - you can see at 8 threads Packet performs 4x faster than AWS. OK OK - I will show the second instance's performance - even when there are no noisy neighbors. Even with a non-noisy neighbor machine, Packet is 30-40% faster in processor benchmarks. EDIT: A user asked me to run the benchmark using a compute-optimized EC2 instance. I decided on c4.2xlarge which has 8 threads, but half as much memory (16gb). It cost $0.419/hour ($0.019/hr more expensive than a Type1 Packet server). Here are the results (Packet wins again but less drastic of a margin) Price: On the above setup, Packet is $0.079/hour cheaper. Conclusion: There really is no way around it - the above benchmarks show the issues with virtualization. Even with those issues aside, AWS is slower and more expensive. Packet wins this one again. Conclusion Even giving AWS the benefit of the doubt, there is no way around it - Packet is faster and SIGNIFICANTLY cheaper. Let's take a very real-world example of our server set-up: Packet: *Note:We assume 732 hours in a month; but if you reserve a Packet instance for a month, they will only charge you 672 hours per month. However, to make apples-to-apples comparisons, all calcs in Price/Month assume you choose hourly pricing (732 hours for 1 month) to keep things normalized. AWS: Packet is literally less than 1/3rd the price and is more performant than AWS. It's allowed us to deploy resources we didn't think would be affordable before. Thank you to everyone @ Packet for making this product possible. Further Steps: If anybody wants to continue this study, I would love to hear your results. AWS does allow you dedicated machines for extra $, but we didn't bother testing them since Packet is already cheaper than their virtual machines. Methodology: Networking: Setting up AWS: We want to make sure we give AWS the best chance. First, we have to make sure enhanced networking is enabled. Running the command: Will give us the output, and look for ""version"". In our instance we have version 2.11.3-k. Amazon recommends we upgrade. For the ubuntu users out there, follow this gist and run the commands: ixgbevf 2.16.1 upgrade for AWS EC2 SR-IOV ""Enhanced Networking"" on Ubuntu 14.04 (Trusty) LTS After rebooting run: Again to make sure the version now reads: 2.16.1 Let's also check via command line to make sure enhanced networking is supported (Ubuntu 14.04): If you get the output, you're good: Next, we used iperf3 to run the diagnostic scripts and scrapy bench . iperf3 is a common network benchmarking tool and scrapy is the framework that powers Tiingo's scraper farm. We figured Scrapy would be another real-time test to see how things flow. the iperf3 command was: Meaning we ran the tests for one hour (3600 seconds), and with 10 processors in parallel. Also note to set the -B option on Packet machines as it takes advantage of the full bonding algo and increases thoroughput. Note: make sure to use the internal IP addresses to give the best benefit of doubt �� Disk: First install/update sysbench on your Ubuntu machine using the code: Then we used the command: The file size must be greater than the RAM size for this test to properly work. CPU: See the above ""Disk"" section to set up sysbench. We then ran the command below, replacing ""num-threads"" with 1, 4, and 8 respectively",en,122
61,2964,1484579813,CONTENT SHARED,569574447134368517,-4465926797008424436,6992931052309362824,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2982.0 Safari/537.36",SP,BR,HTML,https://www.blockloop.io/mastering-bash-and-terminal,mastering bash and terminal,"If there is one tool that every developer uses regardless of language, platform, or framework it's the terminal. If we are not compiling code, executing git commands, or scp-ing ssl certificates to some remote server, we are finding a new version of cowsay to entertain ourselves while we wait on one of the former. As much as we use the terminal it is important that we are efficient with it. Here are some ways I make my time in the terminal efficient and effective. Assumed Settings Some of these commands list alt as a prefix character. This is because I have manually set alt as a meta key. Without this setting enabled you have to use the esc key instead. I recommend enabling the alt key. In Mac Terminal.app this setting is Preferences > Profiles tab > Keyboard sub-tab > at the bottom ""Use option as meta key."" In iTerm2 the setting is at Preferences > Profiles tab > Keys sub-tab > at the bottom of the window set ""left/right option key acts as"" to ""+Esc"". In GNOME terminal Edit > Keyboard Shortcuts > uncheck ""Enable menu access keys."" I also assume you're using bash. I know there are some cool newcomers out there like zsh and fish, but after trying others out I always found that some of my utilities were missing or ill-replaced. If you are not using bash then YMMV. I also assume you are using at least bash version 4. If you're on a Mac then, unless you have manually installed bash with homebrew, you are using an old version. Install bash with homebrew and include it in /etc/shells . Repeat Commands I spend a lot of my time in terminal repeating commands that I have previously run. One thing I noticed a lot of people do is use the up and down arrows to navigate their history of commands. This is terribly inefficient. It requires repositioning your hands and often times removing your eyes from the computer screen. Also, your history (depending on your HISTSIZE) can be very long. Using the up and down arrows is almost like searching through a terrible version of the Oxford English Dictionary which has one word per page. Instead of searching line-by-line I use search history ( ctrl-r and ctrl-s ). In your terminal window, before you type any text press ctrl-r and you should see your prompt change to (reverse-i-search): . Now begin typing any part of any previous command you have executed and you will see the most recent command which matches your search. If this is not the one you want, press ctrl-r again to search incrementally. For example, if you are searching for kubectl delete pods -l=app=nginx you would type kubectl or kubectl del . You should land on that command. If, while incrementally searching backward, you pass the one you're looking for, press ctrl-s to go the other direction and you will see your prompt change to (i-search): . Once you find the command you want press enter to execute it or move the cursor left/right to modify the command first. NOTE ctrl-s probably won't work by default for most terminals. You will need to add stty -ixon to your ~/.bashrc ( ~/.bash_profile for Mac). Sometimes you know that the command that you want to repeat is only two or three places back in history. In these cases it is sometimes easier to move up to that command directly. But you still should not use the arrow keys. Bash has keyboard shortcuts for this too! Here is where we use ctrl-p for ""previous"" or ctrl-n for ""next."" Pressing ctrl-p moves to the previous command in history (replacing the up arrow), and ctrl-n moves to the next command (replacing the down arrow). In mose cases your history will probably be set to record duplicates. This gets pretty annoying for me so I use the following setting to make sure my history doesn't get flooded with duplicate entries. Add this to your ~/.bashrc or ~/.bashprofile and your history will only keep the newest versions of commands. If you typed git status seven times, it will only record the latest one and delete the previous entries. Movements Now that we know we don't need the up and down arrow keys, what about the left and right? Unfortunately, these keys are still needed for single character movements, but I find myself using them less often. Here are some key combinations to move your cursor a little more efficiently. ctrl-a - move the cursor to the beginning of the current line ctrl-e - move the cursor to the end of the current line alt-b - move the cursor backwards one word alt-f - move the cursor forward one word ctrl-k - delete from cursor to the end of the line ctrl-u - delete from cursor to the beginning of the line alt-d - delete the word in front of the cursor ctrl-w - delete the word behind of the cursor The last four aren't necessarily movements, but I use them in conjunction most of the time. Copy / Paste One of my favorite MacOS command line utilities is pbcopy / pbpaste . I like them so much that I created the aliases for my linux machines using xclip (shared below). These two commands use your system clipboard (also called the pasteboard, hence the names). You can pipe data to pbcopy to copy something to your system clipboard or you can pipe pbpaste to another command to paste from your clipboard. Here are some examples that I use: In linux, I put the following in my ~/.bashrc to create the same effect. Changing Directories cd is one of my most used commands according to my bash history. One thing I find myself doing a lot is changing between two directories or briefly changing from directory a to directory b and then back to a. Depending on the reason I'm changing directories I will use either cd - or a combination of pushd and popd . If you type cd - and press enter, you will change to your previous working directory. On the other hand, sometimes I know that I want to go to some directory in a different place, but I might cd a few times to get there, but I want to mark my place so that I can get back quicker. In this case, you would use pushd like this. You can pushd multiple times to build a stack. I don't find myself doing this much, but it's there if you need it. Background Processes One of my pet peves about working with other software developers is that they almost always have ten or more terminal windows open at all times. Usually, they will have one terminal per directory they are working with (this can be avoided by using pushd , popd , and cd tricks mentioned above). But often they will have a few windows open that are running processes which have locked the window. This is difficult to work with because it requires flipping back and forth and knowing where everything is. For executing processes I like to use a mixture of some commands. If you need to run a command indefinitely you can send it to the background by first running it and then pressing ctrl-z . This will suspend or pause the process. After it has been suspended, type bg and press enter. This will move it to a running state, but it will no longer have control of your terminal window. However, if you close the terminal that job will terminate. To avoid this, you disown the process by typing disown and pressing enter. At this point the process is no longer a child of your current terminal process. I often use this to run kubectl proxy or python -m SimpleHTTPServer . ctrl-z - move the current process to the background in a suspended state. jobs -l - list the current background processes for the current tty session. bg - tell the most recent background process to continue running in the background fg - bring the most recent background process back to the foreground disown -h - disown the most recent background job. This will remove it from your current tty session. It will not be able to be brought back to the foreground. You will have to control it either with kill or something else. bg , fg , and disown can be used with the job number found in jobs -l . If you run jobs -l you will see the job number at the beginning of the line. If you want to bring the 2nd job to the foreground you run fg %2 . If you want to disown the fourth job then you run disown -h %4 , and so on. The plus sign (or minus sign) at the bigging of the line has meaning as well. A plus sign indicates that the job is the most recently used, or the one that will be targeted if you type any of the commands without a job ID. The minus sign is the second most recently used. I use ctrl-z a lot because I use a single terminal window for vim and as my command line interface. When I'm writing code in vim and I need to get back to my shell prompt I use ctrl-z to suspend vim. NOTE this will still print stdout and stderr to your command window. If you want to change that then you can redirect to files I modified my PS1 to show my current background job count. Working With Files Several times throughout the day I want to view the contents of a file. Before, I would cat the file or open it in vim . cat was annoying because it flooded my terminal history. This is when I learned to use less to open files with pagination. When you open a file with less the contents of the file become paginated and you start at page one. What's great about less is that many of my favorite key combinations work. You can use ctrl-u to page up, ctrl-d to page down, ctrl-p to scroll up one line, ctrl-n to scroll down one line, g goes to the top of the file, G goes to the bottom of the file, and / searches the file. While less is great for opening files, I may not know where the file is in the first place. Say I have a file named ""auth.py"" but I don't remember exactly where I put it. I could cd back and forth until I find it, or do what some people do and run start . and browse for it in a UI window (terrible workflow). Instead, I use either find , ag (silver searcher), or tree . find is great for searching by file name. You can run find . -type f -name auth.py to search the current directory for a file named ""auth.py"". tree is great for listing a directory in a tree format (much like how you see it in a UI). ag is an applciation called ""the silver searcher."" It is essentially a modernized version of grep and I find myself using it quite often. It's better than grep in that it automatically ignores commonly ignored files such as the .git directory, virtualenv, and anything listed in your .gitignore. I like silver searcher because the command line arguments are very similar to grep so my flags are generally transferrable. Note It's best to combine these commands with less because they will likely flood your terminal history . Choose a Few to Start With I did not use all of these when I first started using bash, nor did I memorize them all at once. I picked up one or two here and there over the years. It's difficult to memorize key combinations, especially when there are so many of them. Pick one or two shortcuts and focus on using them. I find myself using these commands by muscle memory, not by memorizing each keyboard shortcut. In fact, once I started to write this I had to open up the terminal and work around to remember which shortcuts I use. I hope these help you work with bash and terminal more effectively. It is easy to learn one new trick and force yourself to use it for a few days until you get used to it. Once you are comfortable with that command, pick up another one.",en,120
62,1419,1466082624,CONTENT SHARED,319340024738595907,4670267857749552625,7965979047714660786,,,,HTML,http://www.webmotors.com.br/comprar/mitsubishi/asx/2-0-4x4-awd-16v-gasolina-4p-automatico/4-portas/2013-2014/17115683,mitsubishi asx 2.0 4x4 awd 16v gasolina 4p automático - webmotors - 17115683,"Opcionais Airbag, Alarme, Ar condicionado, Ar quente, Banco com regulagem de altura, Bancos dianteiros com aquecimento, Bancos em couro, CD e MP3 Player, CD Player, Computador de bordo, Controle automático de velocidade, Controle de tração, Desembaçador traseiro, Direção hidráulica, Encosto de cabeça traseiro, Freio ABS, Limpador traseiro, Rádio, Retrovisores elétricos, Rodas de liga leve, Sensor de estacionamento, Tração 4x4, Travas elétricas, Vidros elétricos, Volante com regulagem de altura Observações do vendedor IPVA pago, Licenciado, Não aceita troca, Todas as revisões feitas pela concessionária Carro super novo,sou o segundo dono,excelente procedência!!",pt,120
63,1869,1469469603,CONTENT SHARED,-2447632164766022033,881856221521045800,-1412399050403999340,,,,HTML,https://tecnoblog.net/198814/github-estereotipos-programadores/,o que o github tem a nos dizer sobre os estereótipos entre programadores | tecnoblog,"O ser humano possui uma característica, talvez de ordem evolutiva, que nos torna em verdadeiras máquinas de classificação e padronização. Sentimos constantemente o impulso de agrupar e rotular qualquer coisa que pareça semelhante entre si, seja para nos igualarmos, seja para nos distinguirmos. No mundo da tecnologia, e cultura pop, isso não seria diferente. Seja num nível mais abrangente, com frases sem sentido como ""quem gosta de não conhece nada de computador"", ""quem joga RPG é virgem"" ou ""engenharia não é área para mulheres"", como em camadas bem mais específicas, a exemplo de estereótipos que ditam que Swift é coisa de hipster, C++ é uma linguagem de programação para velhos ou que ninguém de fato gosta de codificar em Java. Foi pensando nisso que um engenheiro de computação chamado Jeff Allen, do Trestle Tech , resolveu tirar isso a limpo usando ferramentas de estatística e linguagem R para cruzar informações pessoais de um pequeno número de homens e mulheres, todos com certo renome em suas áreas e linguagens de programação, e o que existe em seus repositórios no GitHub. Aliado a isso, ele usou o API Face , do sistema cognitivo de reconhecimento de faces da , para avaliar e classificar as fotos dessas pessoas. Como o site R-bloggers deixa claro, não podemos dizer que esse é um processo lá muito científico. Além disso, por questões de performance, Jeff restringiu seu estudo às seguintes linguagens: E ele chegou a alguns resultados bem curiosos. Por exemplo, esta é a distribuição de linguagens de programação entre as mulheres, com maior número de programadoras em JavaScript e R, e bem menos em C++, Java e Python: Ainda assim, quando a gente compara ao número de homens que programam, ou ao menos ao que o Face API identificou como homem, essa distribuição se torna bem disforme: É importante citar aqui que o processo envolve um conceito de gênero simplificado por limitações computacionais, que por ora avalia somente feições estéticas e não de identidade. Fica aí uma dica de evolução para o futuro dessa API. Mas voltando, vamos falar de uma distribuição estatística mediana em relação à idade dessas pessoas e a linguagem de programação que elas trabalham: E quais são as linguagens de programação que deixam os profissionais mais felizes? Para saber isso ao certo, somente com um estudo muito mais completo, mas Jeff resolveu analisar de uma forma mais poética: sorrisos! E essa coisa de que hipsters barbudos preferem programar em Swift, seria verdade? Veja bem... E já que chegamos nesse nível de maluquice, por que não cruzar tipos de linguagem de programação por bigodes e costeletas? Conclusão Se você for participar de um projeto em C++, deixe seu bigode crescer. Mentira. Na verdade, o cara precisaria de uma amostragem muito maior para que esse estudo tivesse o mínimo de precisão com relação a comunidade de programadores, e a Face API precisaria ser melhor ajustada para avaliar essas nuances de gênero, personalidade e identidade. Mas foi uma abordagem extremamente interessante do ponto de vista de análise de dados. Para quem se interessa por esse tipo de assunto e quiser ver a fundo como as consultas foram realizadas, Jeff disponibilizou o código-fonte, claro, em seu perfil no GitHub - note que você precisará de uma chave do Azure para a API da Microsoft.",pt,120
64,1592,1467288669,CONTENT SHARED,-5014627593450767720,-5380862725077089346,-5535680675523205102,,,,HTML,http://buytaert.net/drupal-is-for-ambitious-digital-experiences,drupal and ambitious digital experiences,"What feelings does the name Drupal evoke? Perceptions vary from person to person; where one may describe it in positive terms as ""powerful"" and ""flexible"", another may describe it negatively as ""complex"". People describe Drupal differently not only as a result of their professional backgrounds, but also based on what they've heard and learned. If you ask different people what Drupal is for, you'll get many different answers. This isn't a surprise because over the years, the answers to this fundamental question have evolved. Drupal started as a tool for hobbyists building community websites, but over time it has evolved to support large and sophisticated use cases. Perception is everything Perception is everything; it sets expectations and guides actions and inactions. We need to better communicate Drupal's identity, demonstrate its true value, and manage its perceptions and misconceptions. Words do lead to actions. Spending the time to capture what Drupal is for could energize and empower people to make better decisions when adopting, building and marketing Drupal. Truth be told, I've been reluctant to define what Drupal is for, as it requires making trade-offs. I have feared that we would make the wrong choice or limit our growth. Over the years, it has become clear that not defining what Drupal is used for leaves more people confused even within our own community. For example, because Drupal evolved from a simple tool for hobbyists to a more powerful digital experience platform, many people believe that Drupal is now ""for the enterprise"". While I agree that Drupal is a great fit for the enterprise, I personally never loved that categorization. It's not just large organizations that use Drupal. Individuals, small startups, universities, museums and non-profits can be equally ambitious in what they'd like to accomplish and Drupal can be an incredibly solution for them. Defining what Drupal is for Rather than using ""for the enterprise"", I thought ""for ambitious digital experiences"" was a good phrase to describe what people can build using Drupal. I say ""digital experiences"" because I don't want to confine this definition to traditional browser-based websites. As I've stated in my Drupalcon New Orleans keynote , Drupal is used to power mobile applications, digital kiosks, conversational user experiences , and more. Today I really wanted to focus on the word ""ambitious"". ""Ambitious"" is a good word because it aligns with the flexibility, scalability, speed and creative freedom that Drupal provides. Drupal projects may be ambitious because of the sheer scale (e.g. The Weather Channel ), their security requirements (e.g. The White House ), the number of sites (e.g. Johnson & Johnson manages thousands of Drupal sites), or specialized requirements of the project (e.g. the New York MTA powering digital kiosks with Drupal). Organizations are turning to Drupal because it gives them greater flexibility, better usability, deeper integrations, and faster innovation. Not all Drupal projects need these features on day one -- or needs to know about them -- but it is good to have them in case you need them later on. ""Ambitious"" also aligns with our community's culture. Our industry is in constant change (responsive design, web services, social media, IoT), and we never look away. Drupal 8 was a very ambitious release; a reboot that took one-third of Drupal's lifespan to complete, but maneuvered Drupal to the right place for the future that is now coming. I have always believed that the Drupal community is ambitious, and believe that attitude remains strong in our community. Last but not least, our adopters are also ambitious. They are using Drupal to transform their organizations digitally, leaving established business models and old business processes in the dust. I like the position that Drupal is ambitious. Stating that Drupal is for ambitious digital experiences however is only a start. It only gives a taste of Drupal's objectives, scope, target audience and advantages. I think we'd benefit from being much more clear. I'm curious to know how you feel about the term ""for ambitious digital experiences"" versus ""for the enterprise"" versus not specifying anything. Let me know in the comments so we can figure out how to collectively change the perception of Drupal. PS: I'm borrowing the term ""ambitious"" from the Ember.js community . They use the term in their tagline and slogan on their main page.",en,120
65,3066,1486553183,CONTENT SHARED,4876769046116846438,9109075639526981934,1133978080643679329,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36",MG,BR,HTML,http://ciclovivo.com.br/noticia/shopping-em-bh-tera-fazenda-urbana-de-2-700-m%C2%B2/,shopping em bh terá fazenda urbana de 2.700 m²,"O projeto apoiado no conceito 'farm to table' tem previsão de inaugurar em março. 1 de fevereiro de 2017 * Atualizado às 15 : 08 Além da estufa, o espaço contemplará uma loja para a venda dos produtos cultivados e um restaurante. | Foto: Divulgação Em uma parceria inédita entre o Boulevard Shopping e a Startup BeGreen, a capital mineira receberá sua primeira Fazenda Urbana - um espaço no próprio Boulevard para a produção de hortaliças sem agrotóxicos (orgânica) com um sistema inovador de cultivo em consórcio com a criação de peixes. Só existem oito unidades deste tipo no mundo. O novo projeto, que tem previsão de inaugurar na segunda quinzena de março, funcionará na área externa do shopping e trará todo um conceito alicerçado na sustentabilidade. No espaço haverá utilização de composto proveniente do lixo orgânico da Praça de Alimentação do Boulevard como substrato para o crescimento das verduras; e redução de consumo de água com captação da chuva. Além disso, não haverá emissão de CO², pois além da autossuficiência elétrica do projeto, o consumidor final irá adquirir os produtos da fazenda in loco, sem serviços de logísticas e entrega. A Fazenda Urbana sediada no Boulevard irá ocupar uma área de 2.700 m², com uma estufa de 1.500 m² e capacidade para produzir até 50 mil pés de alfaces baby e ervas por mês. Além da estufa, o espaço contemplará a loja Casa Horta, para a venda dos produtos cultivados e de produtores locais; A Casa Amora, restaurante conceito 'farm to table' (da fazenda para a mesa) com pratos que utilizem as hortaliças e legumes orgânicos e da produção local; e um espaço de convivência com mesas, deck e palco para realizações de eventos relacionados à vida mais saudável e à conscientização da nova agricultura. A Fazenda Urbana BeGreen Boulevard O projeto ainda contempla várias ações integradas, como a realização de ações de conscientização de crianças e jovens de escolas públicas e privadas, e eventos e treinamentos de produção sustentável que pretendem atingir pelo menos 1 milhão de pessoas por ano. ""Os visitantes terão a oportunidade de fazer visitas guiadas, comprar os produtos cultivados na fazenda e até consumi-los na hora."", conta Paulo Ceratti, gerente de marketing do Boulevard. ""Nosso principal objetivo é demonstrar que é possível ser sustentável e produtivo gerando mais empregos, menos lixo e sem prejudicar o meio ambiente. É um projeto inovador, que segue um movimento global que aproxima a produção do consumidor final."", explica um dos idealizadores do projeto, Giuliano Bitencourt. Outros produtores locais poderão vender seus produtos agroecológicos na loja Casa Horta. ""A intenção é que aconteça um comércio justo para os produtores e clientes que procuram por alimentos saudáveis, frescos e locais."", acrescenta Paulo. A sustentabilidade também fez parte da construção do espaço. O projeto primou por utilizar o mínimo de novos produtos, tendo como norte a reutilização de materiais. Para isso, toda a obra será feita de containers que virariam sucata; as mesas e cadeiras do espaço terão como matéria prima a madeira plástica, que sofre um processo de transformação do plástico jogado fora; e todo o piso será feito com material de rejeito de mineração. Por que uma fazenda urbana? A motivação original deste projeto se deve ao fato que atualmente 80% de todo produto fresco cultivado no Brasil é desperdiçado e acaba tendo como destino o lixo. Estes são dados da FAO - Organização das Nações Unidas para Alimentação e Agricultura. Tamanho desperdício tem origem na estrutura da cadeia de suprimentos agrícolas brasileira, que faz com que os alimentos colhidos frescos só cheguem à mesa do consumidor com no mínimo dois dias de colhidos. Além disso, o excesso de transporte e baldeações entre os diversos elos da cadeia aumenta ainda mais o desperdício. Esta complexa cadeia de suprimentos além de prejudicar a qualidade do produto final, também impacta no valor total pago pelo consumidor. Um exemplo disso é a trajetória de uma alface até chegar ao prato do consumidor final. Cultivada em uma grande fazenda, a alface é colhida e armazenada de forma incorreta para então ser transportada por centenas de quilômetros em um caminhão, no qual outros pés ficam pelo caminho, até chegar a um centro de distribuição (CEASA). Só então ela será entregue a uma feira ou supermercado e ficar à disposição do cliente final. E se a alface tivesse sido cultivada bem ao lado da casa deste consumidor? Esta é a proposta da Begreen e do Boulevard: uma fazenda bem perto do consumidor final, sem intermediários, sem transporte, com um produto retirado direto do cultivo para a mesa. A ideia de se criar este projeto surgiu quando Giuliano Bitencourt, um dos empreendedores, voltou do MIT em 2014 apaixonado pela ideia de poder cultivar dentro das grandes cidades. Para criar a BeGreen, convidou seu sócio Pedro Graziano, com uma trajetória profissional no ramo tecnológico, para se juntar ao projeto. Por meio do uso de tecnologia inovadora, a parceria entre o Boulevard e a BeGreen torna capaz de produzir de forma automatizada produtos agrícolas frescos e sem agrotóxicos. Tudo isso dentro de centros urbanos, com produtividade superior por metro quadrado, consumo reduzido de água e sem intermediários na distribuição dos alimentos. As fazendas urbanas parecem ser tendência no mundo. Recentemente, um shopping em Israel também inaugurou uma em sua cobertura ( veja aqui ) e no Brasil, o Shopping Eldorado, em São Paulo, também foi pioneiro em suas ações e abriga uma horta e um sistema de compostagem em sua cobertura desde 2012. ( veja aqui ) (25932)",pt,119
66,1459,1466443111,CONTENT SHARED,4788854083489560153,5127372011815639401,-5274409486477114174,,,,HTML,https://blog.risingstack.com/node-js-examples-how-enterprises-use-node-in-2016/,node.js examples - how enterprises use node in 2016 | @risingstack,"Node.js had an extraordinary year so far: npm already hit 4 million users and processes a billion downloads a week, while major enterprises adopt the language as the main production framework day by day. The latest example of Node.js ruling the world is the fact that NASA uses it ""to build the present and future systems supporting spaceship operations and development."" - according to the recent tweets of Collin Estes - Director of Software Technologies of the Space Agency. Fortunately, the Node Foundation's ""Enterprise conversations"" project lets us peek into the life of the greatest enterprises and their use cases as well. This article summarizes how GoDaddy, Netflix, and Capital One uses Node.js in 2016. GoDaddy ditched .NET to work with Node.js Charlie Robbins is the Director of Engineering for the UX platform at GoDaddy. He is one of the longest-term users of the technology, since he started to use it shortly after watching Ryan Dahl's legendary Node.js presentation at JSConf in December 2009 and was one of the founders of Nodejitsu. His team at GoDaddy uses Node.js for both front-end and back-end projects, and they recently rolled out their global site rebrand in one hour thanks to the help of Node.js. Before that, the company primarily used .NET and was transitioning to Java. They figured out that despite the fact that Microsoft does a great job supporting .NET developers and they've made .NET open source, it doesn't have a vibrant community of module publishers and they had to rely too much on what Microsoft released. ""The typical .NET scenario is that you wait for Microsoft to come out with something that you can use to do a certain task. You become really good at using that, but the search process for what's good and what's bad, it's just not a skill that you develop."" Because of this, the company had to develop a new skill: to go out and find all the other parts of the stack. As opposed to other enterprise technologies like .NET where most of the functionality was included in the standard library, they had to become experts in evaluating modules. GoDaddy started to use Node for the front-end and then ended up using it more in the back-end as well. The same .NET engineers who were writing the back-end code were writing the JavaScript front-end code. The majority of engineers are full stack now. The most exciting things for Charlie about Node.js are being handled mainly by the working groups . ""I'm very excited about the tracing working group and the things that are going to come out of that to build an open source instrumentation system of eco-tooling."" Other exciting things for him are the diagnostics working group (previously: inclusivity) and the Node.js Live events - particularly Node.js communities in countries where English is not used. Places like China, for example, where most of the engineers are still primarily speaking Chinese, and there's a not a lot of crossovers. ""I'm excited to see those barriers start to come down and as those events get to run."" As of talking about GoDaddy and Node: they have just released the project that they've been working on pretty extensively with Cassandra. It was an eight-month long process, and you can read the full story of ""Taming Cassandra in Node.js"" at the GoDaddy engineering blog. Netflix scales horizontally thanks to its Node container layer The next participants in Node Foundations enterprise conversation series are Kim Trott , the director of UI Platform Engineering and Yunong Xiao , Platform Architect from Netflix. Kim's been at Netflix for nine years - she just arrived before the company launched its first streaming service. It was the era when you could only watch Netflix with Windows Media Player, and the full catalog consisted only 50 titles. ""I've seen the evolution of Netflix going from DVD and streaming to now being our own content producer."" Yunong Xiao, who's well known for being the maintainer of restify arrived two years ago, and just missed the party the company held for reaching 15 million users - but since they are fastly approaching their 100 millionth subscribers, he'll have a chance to celebrate soon. Yunong previously worked at Joyent on Node.js and distributed systems, and at AWS as well. His role at Netflix is to have Node up and running in scale and making sure it's performing well. Kim manages the UI platform team within the UI engineering part of the organization. Their role is to help all the teams building the Netflix application by making them more productive and efficient. This job can cover a wide range of tasks: it could be building libraries that are shared across all of the teams that make it easier to do data access or client side logging, and building things that make easier to run Node applications in production for UI focused teams. Kim provided us a brief update on how the containerization of the edge services have been going at Netflix - since she talked about it on Node Interactive in last years December. When any device or client tries to access Netflix, they have to use something what's called edge services, which is a set of endpoint scripts - a monolithic JVM based system, which lets them mutate and access data. It's been working really well, but since it's a monolith, Netflix met some vertical scaling concerns. It was a great opportunity to leverage Node and Docker to be able to scale horizontally all of this data access scripts out. ""Since I've spoken at Node Interactive we've made a lot of progress on the project, and we're actually about to run a full system test where we put real production traffic through the new Node container layer to prove out the whole stack and flush out any problems around scaling or memory, so that's really exciting."" How Node.js affected developer productivity at Netflix? The developer productivity comes from breaking down the monolith into smaller, much more manageable pieces - and from being able to run them on local machines and do the containerization. We can effectively guarantee that what you're running locally will very closely mirror what you run in production and that's really beneficial - told Kim. ""Because of the way Node works we can attach debuggers, and set breakpoint steps through the code. If you wanted to debug these groovy scripts in the past, you would make some code changes upload it to the edge layer, run it, see if it breaks, make some more changes, upload it again, and so on.."" It saves us tens of minutes to test, but the real testament to this project is: all of our engineers who are working on the clients are asking: when do we get to use this instead of the current stack? - told Yunong. The future of Node at Netflix Over the next few months, the engineering team will move past building out the previously mentioned stack and start working on tooling and performance related problems. Finding better tools for post-mortem debugging is something that they're absolutely passionate about. They are also planning to be involved in the working groups and help contribute back to the community and so that they can build a better tool that everyone can leverage. ""One of the reasons why Node is so popular is the fact that it's got a really solid suite of tools just to debug, so that's something that we're actually working contributing on."" Node.js brings joy for developers at Capital One Azat Mardan is a technology fellow at Capital One and an expert on Node.js and JavaScript. He's also the author of the Webapplog.com, and you've probably read one of his most popular book: Practical Node.js. ""Most people think of Capital One as a bank and not as a technology company, which it is. At Capital One, and especially this Technology Fellowship program, we bring innovation, so we have really interesting people on my team: Jim Jagielski and Mitch Pirtle. One founded Apache Software Foundation and the other, Joomla!, so I'm just honored to be on this team."" Azats goal is to bring Node.js to Capital One and to teach Node.js courses internally, as well as to write for the blog, and provide architectural advice. The company has over 5,000 engineers and several teams who started using Node.js at different times. Capital One uses Node.js for: Hygieia, which is an open-source dashboard for DevOps. It started in 2013 and was announced last year at OSCON, and it has about 900 GitHub stars right now. They're using Node.js for the frontend and for the build too. Building the orchestration layer. They have three versions of the Enterprise API, and it's mostly built with Java, but it's not convenient to use on the front end. Capital One uses Angular mostly, but they have a little bit of React as well. In this case, the front-facing single page applications need something to massage and format the data - basically to make multiple codes to the different APIs. Node.js works really great for them for building this orchestration layer. ""It's a brilliant technology for that piece of the stack because it allows us to use the same knowledge from the front end, to reuse some of the modules, to use the same developers. I think that's the most widespread use case at Capital One, in terms of Node.js."" The effect of Node.js on the company Node.js allows much more transferable skill-sets between the front end and some of the back-end team, and it allows them to be a little bit more integrated. ""When I'm working with the team, and whether it's Java or C# developers, they're doubling a little bit on front ends; so they're not experts but once they switch to the stack where Node.js is used in the back end, they're more productive because they don't have that switch of context. I see this pure joy that it brings to them during development because JavaScript it just a fun language that they can use."" From the business perspective: the teams can reuse some of the modules and templates for example, and some of the libraries as well. It's great from both the developers and from the managerial perspective. Also, Node has a noticeable effect on the positions and responsibilities of the engineers as well. Big companies like Capital One will definitely need pure back-end engineers for some of the projects in the future, but more and more teams employ ninjas who can do front-end, back-end, and a little bit of DevOps too - so the teams are becoming smaller. Instead of two teams, one is a pure back end, and one is a pure front end - consisting seven people overall - a ninja team of five can do both. ""That removes a lot of overhead in communication because now you have fewer people, so you need fewer meetings, and you actually can focus more on the work, instead of just wasting your time."" The future of Node.js Node.js has the potential to be the go-to-framework for both startups and big companies, which is a really unique phenomenon - according to Azat. ""I'm excited about this year, actually. I think this year is when Node.js has gone mainstream."" The Node.js Interactive in December has shown that major companies are supporting Node.js now. IBM said that Node.js and Java are the two languages for the APIs they would be focusing on, so the mainstream adoption of the language is coming, unlike what we've seen with Ruby - he told. ""I'm excited about Node.js in general, I see more demand for courses, for books, for different topics, and I think having this huge number of front-end JavaScript developers is just a tremendous advantage in Node.js."" Start learning Node! As you can see, adopting Node.js in an enterprise environment has tremendous benefits. It makes the developers happier and increases the productivity of the engineering teams. If you'd like to start learning it I suggest to check out our Node Hero tutorial series . Share your thoughts in the comments.",en,119
67,2256,1472833539,CONTENT SHARED,-8381230866408697127,1766257854965201953,9211894396612985323,,,,HTML,http://m.olhardigital.uol.com.br/pro/noticia/inteligencia-artificial-da-ibm-criou-trailer-perfeito-para-filme/61735,inteligência artificial da ibm criou 'trailer perfeito' para filme,"A produtora 20th Century Fox fez uma parceria com a IBM para criar o trailer do filme Morgan . O filme fala sobre um super ser humano com modificações cibernéticas; por isso, a equipe de pesquisa da IBM usou a plataforma de inteligência artificial Watson para editar o ""trailer perfeito"" para o filme. Ele pode ser visto no final desta nota. Para realizar essa tarefa, o Watson ""assistiu"" aos trailers de 100 filmes de suspense e terror, divididos em cenas. Em cada cena, o sistema avaliou separadamente os sons, a composição das imagens e as emoções geradas pelo filme. Para analisar esse último aspecto, a inteligência artificial examinou as expressões faciais dos atores, os objetos usados nas cenas e a gradação de cores do filme. Em seguida, o sistema assistiu ao filme Morgan e separou as 10 cenas que seriam as melhores candidatas para o trailer. Segundo a IBM, o Watson conseguiu escolher cenas que nenhum dos editores tinha pensado em incluir; por outro lado, uma das dez cenas selecionadas pela máquina acabou não sendo incluída no trailer. Colaboração O processo também contou com a participação de humanos no processo de edição, já que o Watson não era capaz de fazer isso. Um editor colocou os letreiros, ordenou os momentos e selecionou a música para o trailer. Com a ajuda da inteligência artificial, o processo, que segundo a IBM pode levar de 10 a 30 dias, foi completado em 24 horas. Esse tempo começou a ser contado no momento em que o editor viu as imagens selecionadas pelo Watson até a hora em que o trailer ficou pronto. Uma utilização artística ou estética de inteligências artificiais não é exatamente novidade. O Google, por exemplo, está usando sua plataforma open-source Tensorflow para ensinar computadores a fazer arte , e esse processo já deu resultados . A empresa também usa essa tecnologia para combater cegueira e até ajudar a tratar câncer . O própri Watson já foi usado para criar desde sistemas de combate ao cibercrime até um chapéu seletor do Harry Potter . Dirigido por Luke Scott (o filho de Ridley Scott, diretor de Alien ), Morgan conta a história de um ser humano artificial criado ""em laboratório"". Morgan, como ele é chamado, é mantido em cativeiro e estudado por cientistas; no entanto, ele acaba se desenvolvendo mais rapidamente e melhor do que o esperado, e então se volta contra seus criadores. O trailer do filme, marcado para estrear dia 2 de setembro nos EUA, pode ser visto abaixo:",pt,118
68,1175,1464785508,CONTENT SHARED,-7463305179076477879,2416280733544962613,5253606329297313273,,,,HTML,http://techcrunch.com/2016/06/01/salesforce-buys-demandware-for-2-8b-taking-a-big-step-into-e-commerce/,"salesforce buys demandware for $2.8b, taking a big step into e-commerce","Salesforce made its name originally with cloud-based software to help salespeople manage their leads and close deals; and today the company took a big step into the business of sales themselves. Today the company announced that it would spend $2.8 billion to acquire Demandware, a cloud-based provider of e-commerce services to businesses big and small, which will spearhead a newer business area: the Salesforce Commerce Cloud. Demandware is a publicly-traded company, and Salesforce says that it will commence a tender offer for all outstanding shares of Demandwarefor $75.00 per share, in cash. This is a big premium on the company's current valuation - which was $1.87 billion at close of trade yesterday . The transaction is expected to close in Salesforce's Q2 2017, which ends July 31, 2016. ""Demandware is an amazing company-the global cloud leader in the multi-billion dollar digital commerce market,"" said Marc Benioff, chairman and CEO, Salesforce, in a statement. ""With Demandware, Salesforce will be well positioned to deliver the future of commerce as part of our Customer Success Platform and create yet another billion dollar cloud."" ""Demandware and Salesforce share the same passionate focus on customer success,"" said Tom Ebling, CEO, Demandware, also in a statement. ""Becoming part of Salesforce will accelerate our vision to empower the world's leading brands with the most innovative digital commerce solutions that enable them to connect 1:1 with customers across any channel."" This is a huge deal for Salesforce, as it extends the types of transactions that it will be open to with existing customers, and also gives it a new group of customers to upsell for other services that it offers, from marketing and online analytics through to back-office software for sales and other IT functions. Demandware customers include Design Within Reach, Lands' End, L'Oreal and Marks & Spencer, the company said. It also opens up Salesforce to competition with the likes of Amazon, who not only provides third parties with commerce software, but would prefer to be the go-to platform for transactions. Gartner estimates that worldwide spending on digital commerce platforms is expected to grow at over 14 percent annually, reaching $8.544 billion by 2020, according to figures provided by salesforce. Salesforce says its new Commerce Cloud ""will be an integral part of Salesforce's Customer Success Platform, creating opportunities for companies to connect with their customers in entirely new ways. Salesforce customers will have access to the industry's leading enterprise cloud commerce platform, and Demandware's customers will be able to leverage Salesforce's leading sales, service, marketing, communities, analytics, IoT and platform solutions to deliver a more comprehensive, personalized consumer experience."" more to come.",en,118
69,1424,1466115486,CONTENT SHARED,7534917347133949300,3891637997717104548,1233433224046298458,,,,HTML,https://www.sitepoint.com/the-importance-of-code-reviews/,the importance of code reviews,"I recently read this on Twitter : Sadly, it seems code reviewing is a practice that's foreign to many students, freelancers and agencies. [Translated] Apparently, it's not obvious to everyone that code reviews are actually helpful. Call me naive, but I really thought it was a process used in all IT companies. Apparently I was wrong, and it scares me. In this article, I'd like to give my thoughts on code reviews, why I believe they're an important part of the code shipping process, and how to get started with them. If you don't do code reviews, or if you feel like you could do better, I hope this write-up will help! What Is a Code Review? We live in the era of Wikipedia, so allow me to begin by quoting the definition given on the Code review entry: Code review is systematic examination (sometimes referred to as peer review) of computer source code. It is intended to find mistakes overlooked in the initial development phase, improving the overall quality of software. Reviews are done in various forms such as pair programming, informal walkthroughs, and formal inspections. A code review, as the name states, is the process of reviewing some code in order to make sure it works, and in order to improve it where possible. Ways to Do a Code Review As the Wikipedia definition notes, there are various ways to perform code reviews. However, in a world where so much code lives on GitHub , code reviewing often goes hand-in-hand with what we call a ""pull request"". A pull request is a request to introduce changes to a code repository using a distributed version control system (Git, SVN, Mercurial etc.). It works by ""pulling"" the original code, applying changes, then submitting a request to merge the changes in. GitHub made this process particularly easy and efficient thanks to its friendly user interface, abstracting most of the Git knowledge requirements. Why Reviewing Code Matters So, why does code reviewing matter? After all, we're all competent here. Surely we can ship code without having someone metaphorically standing over our shoulder, watching everything we do. In theory, yes. But in practice, there are many reasons why having an established code reviewing process helps. Let's look at a few of them. It limits risks This is probably the most important reason of all. Having someone double-checking our work never hurts, and limits the risk of unnoticed mistakes. Even good developers get tunnel vision sometimes. It's always good to make sure not to forget anything. For instance, proper keyboard navigation, screen reader accessibility, flexibility for internationalization and friendly, non-JavaScript behavior are often forgotten topics in the front-end world, to name but four. It dramatically improves code quality Let's make something clear: this is not about standards and code linting (at least not exclusively). It's about making code more efficient. In a team where everybody has their own background and strong suits, asking for improvements (because that's what it's about) is always a good idea. Someone could suggest a smarter solution, a more appropriate design pattern, a way to reduce complexity or to improve performance. It makes everyone better By joining forces, everyone can learn and get better. The code submitter is likely to receive feedback on their work, making them aware of possible problems and areas for improvement. The reviewers could well learn new things by reading through the code, and figure out solutions applicable to their own work. It helps being familiar with the project When a team works on a project, it's highly unlikely that every developer is working on every part of the application. Sometimes a developer will heavily work on one large part for a while, while another one is working on something else entirely. Doing code reviews helps people familiarize themselves with code they haven't written but might be asked to maintain in the future. It promotes knowledge of the codebase across the team, and is likely to speed up future development. How To Do It Properly Again, having an established code reviewing process is both extremely useful and important. Every team producing code should have some code review, one way or the other. That being said, doing meaningful and helpful code reviews is not always as straightforward as it might seem. Worry not, it's not like it's going to bite you if it's done poorly. It simply won't be useful, and could feel like a waste of time. Recently at my workplace, we had a retrospective about our code reviewing process. We knew some things were wrong when we realized only 3 out of 12 developers were engaging in code reviews. To help us change this, one of our Scrum Masters organized a retrospective to determine where there was room for improvement, and how we could bring it about. Planning ahead The most recurrent argument to justify the lack of participation in code reviews was that it takes time - time that people can't or aren't willing to take. I must say I don't really understand this argument myself, because I picture it like this: if a colleague comes to me directly and asks me to help them with something, I'm not going to say - ""Don't have time, not interested."" I'm going to find time to help. Maybe not right now, maybe in an hour - but I will obviously take time for them. Why? Because this is what being part of a team means if they want my opinion, it's because they value it one way or another, and therefore it makes only sense to give them. ""Why don't you take part in the code reviewing process?"" ""I don't have time."" To me, a pull request is no different from a coworker asking for help. Saying you don't have time is perfectly fine from time to time, but by systematically refusing to help, you're actively pulling yourself out of the team. This behavior is neither friendly nor positive. Take the time to help. To make it possible for developers to find time, we started taking into account that every developer will spend a bit of time (maybe 30 minutes) reviewing code every day. No more surprise when we end up spending half an hour on a large code review: it's part of the day. We also tried to dramatically reduce the amount of code constituting a pull request. We used to have mammoth pull requests - of thousands of changes across dozens of files. We try not to do that anymore. By making smaller pull requests, we make them easier to review, the feedback more relevant, and developers more willing to engage in this process. ""Ship small and often."" Giving context The second biggest problem we found was that we usually lacked an understanding of the code's context, which is needed if you're going to provide helpful feedback. When missing the context, we usually did no more than a syntax review - which, though useful to some extent, is not enough. You simple become what we call a ""human linter"". Fortunately, the solution to this problem is relatively simple: add a description to the pull request to explain what the objective is, and how to get there. It doesn't have to be a wall of text; just a few lines are usually enough. It also helps to add a link to the issue and/or story. Liv Madsen , one of our developers, even adds screenshots - or screencasts when relevant - to illustrate what she's done, which is amazing. Actually asking The third problem we pointed out was that we sometimes simply didn't realize there was something to review. Let's face it, we're flooded with tons of emails and notifications every day - so much so that it can be hard to keep track. We're only human, after all. Here, again, the solution is pretty simple: actually ask someone for a review. There are many ways to do that, from honking a horn in the office to pinging someone on Slack; to each team their own. We created groups on GitHub based on our activity, and when submitting a pull request, we always ping a group. Members of this group will receive a notification and are free to tackle it as soon as they have time. Sometimes, we ping a developer (or several) directly when it's more specific to someone's work. That also works. From there, pinged people can review the code and leave comments. We try to leave a comment even when there's nothing specific to report - if only to indicate that the code is ready to be merged. Because we had some pull requests blindly merged regardless of given comments, we established a strict ""reply or fix everything"" policy. When receiving feedback, either you fix it or you reply to explain why you didn't. In any case, you never leave a comment pending, and you certainly don't merge your pull request with non-handled comments. Wrapping Things Up Having a regular and efficient code reviewing process is essential to maintain high-quality code standards, grow as a team and share knowledge between developers. Asking for a code review is not a mark of weakness. There's nothing embarrassing about asking for help, and certainly not in the form of a code review. Accept all feedback given to you, and offer constructive (ideally positive) comments to people submitting pull requests. Find what works for you. Reviewing code should be a large part of the code shipping process, so you should tailor it to your team. Make it the way you want so that it's helpful and positive for everybody. Happy reviewing!",en,117
70,2375,1474371643,CONTENT SHARED,-4145260601063545880,-1393866732742189886,5698474775920535228,,,,HTML,https://hackernoon.com/why-learning-angular-2-was-excruciating-d50dc28acc8a?gi=13bcfa27581a,why learning angular 2 was excruciating,"A few months ago I decided to try my hand at web development. Angular 2 was new and shiny at the moment, so I decided to download it and start building a website. Now, as someone who is primarily a Java developer, this was an altogether new and very interesting experience. I followed the alleged ""5 Minute Quick Start Guide"" and after an hour and a half of wrangling with Angular 2 and its plethora of dependencies, I was able to get something up and running. Next, I started to build a real app. I decided to build a personal blog platform from scratch, mostly for the learning, and partially because I've been meaning to start a blog for ages. The desire to have a blog would be the carrot keeping me motivated to learn new technologies and keep building my project. Over the months, I've been keeping up with the Angular 2 releases and, each weekend, I've been chugging along on the blog. Oh, did I say chugging? I meant banging my head vigorously against the wall over and over and over again trying to understand and deal with the freaking Javascript ecosystem. Maybe I'm just used to the slow paced, sturdy, battle tested libraries of the Java world. Maybe learning web development starting with Angular 2 is like trying to learn a new video game and starting on hard core mode. I don't know. All I know is that the Angular 2 release was, from my perspective as a Java engineer, a freaking disaster. Every single minor release made lots of breaking changes. Every single time I checked back on the accursed ""5 Minute Quick Start"", huge swaths of it had completely changed. Don't even get me started on the whole Angular 2 router business. That was truly a fiasco, and agonizing to deal with as a user, particular one who is less familiar with the Javascript world. I find myself writing this post in a moment of passion, fuming over the latest Angular release which was announced a few days ago. Angular 2, for rulz this time, has been released and you can upgrade to it and not have to deal with breaking changes for a whopping 6 months! Well then, I naively thought to myself, I should upgrade my blog from @angular 2.0.0-rc.4 to @angular 2.0.0. This was the journey that I just underwent: Upgrade to @angular 2.0.0 Remove all 'directives' fields from my Components. Apparently Angular2 has decided that Modules are the way to go. Get rid of all imports anywhere that end with _DIRECTIVES. Upgrade @angular/forms from 0.3.0 to 2.0.0. For some reason, @angular/forms was super behind the rest of the versioning for angular. Until this release. Upgrade angular-material to 2.0.0-alpha.8-2 (can we just pause for a second and appreciate the ridiculousness of a version called 2.0.0-alpha.8-2??). Upgrade to typescript 2.o, which, as I was unpleasantly surprised by, is currently in beta. Having finally reached a version of relative stability in Angular, it was dismaying to realize that angular-material, a key tool in my stack, has unstable dependencies that are ahead of Angular 2's dependencies. By this point, `npm start` was working. This is where the hard part began, because now I had to deal with tremendously cryptic error messages that have been plaguing me ever since I started learning Angular 2. Such as this one: After some troubleshooting (by now I've gotten okay at debugging System JS's useless error messages), the issue was caused by this errant line in my systemjs.config.js file: I guess @anguler/router has a umd now. Whatever a umd is......... The next issue I encountered after that was this: Great, a freaking syntax error from a random file in angular-material. There is no helpful error message, no line numbers. There is precious little to help me figure out what to do next. I don't know whether to upgrade dependencies, downgrade dependencies, install new dependencies, change the syntax of my tsconfig.js file (which undoubtedly changed when I upgraded to typescript 2.0). I'm lost in a sea of confusion and frustration. Now, those of you who are seasoned web developers can probably troubleshoot an issue like this fairly easily. That's not what I'm getting at. It's not about the particular issue I just faced. It's that the Javascript ecosystem is utter chaos. Every new version of every new library comes with a slew of breaking changes. New libraries will be released before their APIs are nailed down. Libraries in beta are now old news because only alpha libraries are new and interesting. Stackoverflow posts that are older than 6 weeks old are no longer relevant and probably deal with an issue from an old version that I'm no longer using. The Java engineer in me is screaming with the frustration of this ecosystem. What sort of madness has web development devolved into? What the f**cking f**ck is going on with Javascript these days??? Okay, I think it's time to take a step back and say that I actually love Angular 2. When I get to a point where all the dependencies have been upgraded, and everything is working, when Intellij is underlining the correct things and my typescript compiler is hooked up right, Angular 2 is awesome. To be fair, I have been using versions of the library that have thus far not been officially released. Maybe, you say, it's my fault for trying to download and use a version of the library that is still in alpha/beta/release candidate and expecting it to work and be relatively easy to use. Perhaps you are right. But, considering the fact that hundreds of thousands of developers are already using Angular 2, we should ask ourselves the question: is it responsible to release libraries that are still very much a work in progress? Does it make sense to announce a release candidate and then make tons and tons of breaking changes over the course of evolving from rc.1 to rc.6? How many hundreds of thousands of human hours were wasted dealing with the pain of upgrading a version of Angular 2 and all of its related dependencies? And, as most engineers will attest to, developer hours are a precious commodity in today's job market. How many developers have been utterly burned by the experience of trying to use Angular 2 and have sworn off Angular forevermore in favor of React? How many other Javascript libraries are out there that have also caused such frustration and undue anguish for their users? Perhaps the Javascript ecosystem is just experiencing intense growing pains. Maybe developers are quickly understanding the errors in their initial designs and brazenly correcting them as they blaze on in iterating through better and better versions of their libraries. Maybe the Javascript world is where the vast majority of software engineers will live soon, since the tools there will have evolved rapidly and effectively. Perhaps people will accomplish more in Javascript and the tools written in Javascript will be easier to use than the tools in any other software engineering landscape. Or, maybe, just maybe, the Javascript world will always be sort of a joke, a place where engineering hipsters go to waste time and feel like they're on the cutting edge of innovation when really, they are living in a world of madness and chaos, throwing hours of productivity down the drain so that they can use the latest and 'greatest' tools out there. But I am just a humble Java engineer. What I'm most interested in is: what do you think?",en,117
71,1398,1465992728,CONTENT SHARED,-4888170580455425266,-8424644554119645763,-1573551387441998743,,,,HTML,https://www.jetbrains.com/help/resharper/2016.1/Speeding_Up_ReSharper.html,speeding up resharper (and visual studio),"This document presents a list of performance optimizations that can be applied if you experience performance issues with Visual Studio and ReSharper. Some of the tricks presented are ReSharper-specific, whereas others will affect Visual Studio performance whether you have installed ReSharper or not. We constantly make sure that ReSharper works fine on modern hardware and with medium- and large-size solutions without any tweaking. We believe that Visual Studio developers are working towards the same things. By trying to speed up ReSharper on outdated hardware, you deprive yourself of great features that can speed up your development performance. In this topic: Speeding up ReSharper Disable code analysis for current file You can temporarily disable design time code inspection for the current file by pressing Ctrl+Shift+Alt+8 . Pressing the shortcut again will re-enable the inspection. You can spot the status of code analysis in the current file by the status indicator : If you want to bind a different shortcut for this operation, look for the ReSharper_EnableDaemon command. Disable code analysis for specific files You can tell ReSharper to skip analyzing certain files without opening them. For example, you can skip files that contain well tested algorithms and that do not change much. To do so, go to and scroll to the Element to skip section, where you can pick the files and folders to skip. You can also specify a file mask for skipping files. You will also notice that all files where you disabled code analysis with Ctrl+Shift+Alt+8 are already in the list of ignored files. Turn off Solution-Wide Analysis On very large projects, turning on Solution-Wide Analysis may cause performance degradation, particularly on less powerful hardware. If you find this analysis to be taking up too many resources, simply switch it off: right-click the SWA circle in the bottom right corner of Visual Studio window and choose Stop Solution-Wide Analysis or Pause Analysis. A dialog box will pop up asking whether you want to turn off SWA. Say Yes and you're done. Disable context actions In ReSharper options, go to and , and uncheck the that are less helpful to you. Speed up typing If you experience slowdown while typing, you can turn off member signatures, symbol types and summary in completion lists - go to and clear the corresponding check-boxes: If this doesn't help, switch back to built-in Visual Studio IntelliSense under : Disable auto-formatting To speed up typing, you can also disable auto-formatting options under to avoid reformatting code while typing: Speed up code templates To speed up expanding code templates , you can turn off the Reformat and Shorten qualified references options for templates that you use: Disable unit testing If you don't use the ReSharper unit test runner , you can save some processing time by turning it off. Go to and clear the Enable Unit Testing check-box. Disable the navigation bar If you use File Structure Window , then you probably don't use the navigation bar on top of the editor. If so, you can disable the navigation bar by clearing the corresponding check-box in Visual Studio options: . If nothing helps If you've tried out everything described above and the performance is still down, you can temporarily disable ReSharper and check whether it was the cause of the slowdown. To disable/enable ReSharper, go to and click Suspend Now/Resume Now. If suspending ReSharper helps improve the performance but you still want to use it occasionally for code analysis , code cleanup or reformatting code , you might want to have a shortcut that quickly switches ReSharper on and off. Here is how to do it: go to and find the ReSharper_ToggleSuspended command, then press a shortcut of your choice and click Assign. Known Performance Problems The following is a list of known performance problems and their solutions. VS2010 with ReSharper on Windows XP slowness This known issue can be resolved by installing the Windows Automation API 3.0. For further details, see this article . Please note that this fix applies to Windows XP only - later Windows operating systems already have this API installed. Performance degradation after ReSharper upgrade If you have recently updated ReSharper and observe performance degradation with solutions that were opened with previous versions, you can attempt to speed thing up by clearing the ReSharper caches and deleting the solution .suo file. To clear the caches, go to and click Clear Caches. You can also tweak the Store solution caches in selector on this page: performance can be improved if your selected caches storage is mapped to a faster storage medium, such as a high-performance SSD or a RAM disk. Known Compatibility Problems Other Visual Studio extensions Major compatibility issues have been observed with the following products: DevExpress CodeRush/Refactor Pro (incompatible) Telerik JustCode (incompatible) Whole Tomato Visual Assist Productivity Power Tools Performance degradation has been observed with the following products: Some versions of the StyleCop ReSharper plug-in PowerCommands for Visual Studio There are also reports on Web Essentials contributing to low performance while editing .cshtml files. If you're affected by this problem, consider going to and setting Auto-format HTML on Enter to False. Parallels Desktop for Mac If you're running Visual Studio in a Windows virtual machine on your Mac using Parallels Desktop, ReSharper IntelliSense lists might be very slow to render. If this occurs in your setup, consider switching from Coherence mode to Full Screen mode. For guidelines on switching between the two modes, please see this Parallels Knowledge Base entry . Improving Visual Studio Performance Before starting to tweak Visual Studio settings, check that the most recent service pack and hot fixes are installed. Speed up editor scrolling The problem with editor scrolling arises due to hardware-accelerated editor rendering. If you experience this, try turning off the following options under : Automatically adjust visual experience based on client performance Use hardware graphics acceleration if available Save time on startup Turning off the start page and the news channel might save some time on startup. To do so, go to and choose to show empty environment at startup. Clean web cache If you work with web projects, web cache might slow down Visual Studio. To clean it, delete everything under %LOCALAPPDATA%\Microsoft\WebSiteCache . Disable unused extensions Go to , go through the list and check if you really need each of them. You can uninstall or disable the unused ones. Unload unused projects If you are not working on some projects, you can unload them from Visual Studio and reload them back when needed. Right-click on the project or a solution folder in the Solution Explorer and choose Unload Project or Unload Projects in Solution Folder - this will speed up both Visual Studio and ReSharper. By the way, ReSharper navigation features will work even for unloaded projects. Disable visual XAML editor On large projects, editing XAML files can feel slow even on good hardware. If you don't use visual XAML editor, you can partly solve the problem by disabling it. To do so, right-click on a XAML file in the Solution Explorer and choose Open With. In the dialog box that appears, select Source Code (Text) Editor and click Set as default. Last modified: 24 May 2016",en,117
72,2372,1474307005,CONTENT SHARED,-4084394822880420062,-1836083230511905974,949170573697719844,,,,HTML,http://android-developers.blogspot.com.br/2016/09/android-studio-2-2.html,android studio 2.2,"By Jamal Eason , Product Manager, Android Android Studio 2.2 is available to download today. Previewed at Google I/O 2016, Android Studio 2.2 is the latest release of our IDE used by millions of Android developers around the world. Packed with enhancements, this release has three major themes: speed, smarts, and Android platform support. Develop faster with features such as the new Layout Editor, which makes creating an app user interface quick and intuitive. Develop smarter with our new APK analyzer, enhanced Layout Inspector, expanded code analysis, IntelliJ's 2016.1.3 features and much more. Lastly, as the official IDE for Android app development, Android Studio 2.2 includes support for all the latest developer features in Android 7.0 Nougat, like code completion to help you add Android platform features like Multi-Window support , Quick Settings API , or the redesigned Notifications , and of course, the built-in Android Emulator to test them all out. In this release, we evolved the Android Frameworks and the IDE together to create the Constraint Layout. This powerful new layout manager helps you design large and complex layouts in a flat and streamlined hierarchy. The ConstraintLayout integrates into your app like a standard Android support library, and was built in parallel with the new Layout Editor. Android Studio 2.2 includes 20+ new features across every major phase of the development process: design, develop, build, & test. From designing UIs with the new ConstraintLayout , to developing C++ code with the Android NDK, to building with the latest Jack compliers, to creating Espresso test cases for your app, Android Studio 2.2 is the update you do not want to miss. Here's more detail on some of the top highlights: Design Layout Editor: Creating Android app user interfaces is now easier with the new user interface designer. Quickly construct the structure of your app UI with the new blueprint mode and adjust the visual attributes of each widget with new properties panel. Learn more . Constraint Layout: This new layout is a flexible layout manager for your app that allows you to create dynamic user interfaces without nesting multiple layouts. It is backwards compatible all the way back to Android API level 9 (Gingerbread). ConstraintLayout works best with the new Layout Editor in Android Studio 2.2. Learn more . Develop Improved C++ Support: You can now use CMake or ndk-build to compile your C++ projects from Gradle. Migrating projects from CMake build systems to Android Studio is now seamless. You will also find C++ support in the new project wizard in Android Studio, plus a number of bug fixes to the C++ edit and debug experience. Learn more . C++ Code Editing & CMake Support Samples Browser: Referencing Android sample code is now even easier with Android Studio 2.2. Within the code editor window, find occurrences of your app code in Google Android sample code to help jump start your app development. Build Instant Run Improvements: Introduced in Android Studio 2.0, Instant Run is our major, long-term investment to make Android development as fast and lightweight. Since launch, it has significantly improved the edit, build, run iteration cycles for many developers. In this release, we have made many stability and reliability improvements to Instant Run. If you have previously disabled Instant Run, we encourage you to re-enable it and let us know if you come across further issues. (Settings → Build, Execution, Deployment → Instant Run [Windows/Linux] , Preferences → Build, Execution, Deployment → Instant Run [OS X]). For details on the fixes that we have made, see the Android Studio 2.2 release notes . APK Analyzer: Easily inspect the contents of your APKs to understand the size contribution of each component. This feature can be helpful when debugging multi-dex issues. Plus, with the APK Analyzer you can compare two versions of an APK. Learn more . Build cache (Experimental): We are continuing our investments to improve build speeds with the introduction of a new experimental build cache that will help reduce both full and incremental build times. Just add android.enableBuildCache=true to your gradle.properties file. Learn more . Test Virtual Sensors in the Android Emulator: The Android Emulator now includes a new set of virtual sensors controls. With the new UI controls, you can now test Android Sensors such as Accelerometer, Ambient Temperature, Magnetometer and more. Learn more . Android Emulator Virtual Sensors Espresso Test Recorder (Beta): The Espresso Test Recorder lets you easily create UI tests by recording interactions with your app; it then outputs the UI test code for you. You record your interactions with a device and add assertions to verify UI elements in particular snapshots of your app. Espresso Test Recorder then takes the saved recording and automatically generates a corresponding UI test. You can run the test locally, on your continuous integration server, or using Firebase Test Lab for Android . Learn more . Espresso Test Recorder GPU Debugger (Beta): The GPU Debugger is now in Beta. You can now capture a stream of OpenGL ES commands on your Android device and then replay it from inside Android Studio for analysis. You can also fully inspect the GPU state of any given OpenGL ES command to better understand and debug your graphical output. Lean more . To recap, Android Studio 2.2 includes these major features and more: Learn more about Android Studio 2.2 by reviewing the release notes and the preview blog post . Getting Started Download If you are using a previous version of Android Studio, you can check for updates on the Stable channel from the navigation menu (Help → Check for Update [Windows/Linux] , Android Studio → Check for Updates [OS X]). You can also download Android Studio 2.2 from the official download page . To take advantage of all the new features and improvements in Android Studio, you should also update to the Android Gradle plugin version to 2.2.0 in your current app project. Next Release We would like to thank all of you in the Android Developer community for your work on this release. We are grateful for your contributions, your ongoing feedback which inspired the new features in this release, and your highly active use on canary and beta builds filing bugs. We all wanted to make Android Studio 2.2 our best release yet, with many stability and performance fixes in addition to the many new features. For our next release, look for even more; we want to work hard to address feedback and keep driving up quality and stability on existing features to make you productive. We appreciate any feedback on things you like, issues or features you would like to see. Connect with us -- the Android Studio development team -- on our Google+ page or on Twitter . What's new in Android development tools - Google I/O 2016",en,117
73,2149,1471924347,CONTENT SHARED,-2479936301516183562,-1032019229384696495,-1085681130193775758,,,,HTML,http://venturebeat.com/2016/08/22/gitlab-issue-boards/,"gitlab launches issue boards, an open-source task management tool that resembles trello","Source code repository software startup GitLab today is introducing Issue Boards, an open-source task-management tool that will be integrated into the existing web service for all users, free of charge. The tool provides a visual interface where team members can track the status of their projects. Rather than require users to list out all the tasks that have been completed and have yet to be done, the tool can take in existing issues from teams' repositories and drop them in automatically using the labels that have previously been applied to issues. All boards have two lists enabled by default: Backlog and Done, GitLab said in a blog post . Users can add unlimited lists - each one representing a step in the development process - but just one board, a spokesperson told VentureBeat in an email. Enterprises customers could get unlimited boards in the future, the spokesperson said. Issues can be filtered by their corresponding label, milestone, author, or assignee. Of course, developers have always been able to check out issues for projects hosted on GitLab. Issue Boards offer a different representation of things that people need to do. GitLab says the tool can help teams with thousands and thousands of issues do a better job of figuring out what to focus on. The thing is, many other tools help people manage tasks - Trello, Wrike, Asana, the recently launched Microsoft Planner , and so on. There are also open-source task management tools, including Kanboard, Restyaboard, and Wekan. GitLab's Issue Boards bear a striking similarity specifically to Trello, which had more than 12 million users as of January. But GitLab, which competes with Atlassian's Bitbucket and GitHub, has been seeking to make its offering more well rounded. Not only is it core functionality open source unlike GitHub. It now packages up additional tools like Mattermost, an open-source alternative to team communication app Slack, and also it now lets users easily build and run code inside a Koding cloud-based integrated development environment (IDE) . Founded in 2011 by Dmitriy Zaporozhets and Sytse ""Sid"" Sijbrandij, GitLab is currently used by more than 100,000 organizations, including Alibaba, CERN, Expedia, IBM, NASA, and SpaceX. You can find the code for Issue Boards here . Get more stories like this on Twitter & Facebook",en,116
74,972,1463359748,CONTENT SHARED,-6255158415883847921,6013226412048763966,7846471679835395183,,,,HTML,http://m.folha.uol.com.br/ilustrada/2016/04/1766160-para-neurociencia-motivacao-nao-e-fator-principal-em-mudanca-de-habitos.shtml,"para neurociência, motivação não é fator principal em mudança de hábitos","Ao contrário do que pregam livros de autoajuda, querer não é poder. A neurociência mostra que ação é mais importante que motivação na hora de dar uma virada na vida. Quer dizer: não dá para esperar ter desejo de comer salada para fazer dieta. ""A vontade anda de mãos dadas com a preguiça, o cérebro sempre quer gastar menos energia. É mais efetivo agir primeiro, até que a repetição leve a um novo hábito"", diz Pedro Calabrez, pesquisador do Laboratório de Neurociências Clínicas da Unifesp. Mudar é agir contra o script. Como os hábitos já estão gravados na memória, preferimos o piloto-automático a criar novas rotinas. ""É como um rio que corre pelo mesmo caminho. O cérebro tem a tendência de reagir sempre do mesmo jeito"", diz Paulo Knapp, psiquiatra e terapeuta cognitivo-comportamental. Não é por mal. Hábitos só existem para facilitar o cotidiano, lembra o neurocientista Jorge Moll, diretor-presidente do Instituto D'Or. ""Comportamento automatizado faz o cérebro mais eficiente. Não conseguiríamos fazer tanta coisa se fosse preciso pensar em cada ação."" Segundo ele, mais de 99% dos hábitos são bons. O problema é o 1% que sobra. ""Da procrastinação ao uso de cocaína, o hábito é formado graças a uma recompensa ou punição. Todos são vícios em potencial"", diz Knapp. Quanto mais rápida for a resposta de uma atitude, mais facilmente ela vira hábito, e mais difícil é mudar. É outra pegadinha do cérebro, que prefere recompensas a curto prazo. Por isso a meta de ficar em forma é dura de ser cumprida. ""A recompensa está distante, e comer traz prêmio imediato"", diz Moll. PLANO DE GUERRA Para driblar o comodismo e instituir novos hábitos, existem técnicas, algumas controversas. A mais célebre envolve repetir o novo comportamento por 21 dias. Mas, segundo um estudo do University College de Londres são necessários, em média, 66 dias para mudar. ""Isso é baboseira, não há tempo certo. Depende de quão antigo o hábito é. Mas há técnicas de psicologia comportamental, como colocar um alarme para comer na hora certa"", diz Moll. Calabrez explica que o hábito tem três dimensões: a ação em si, a recompensa e um gatilho que leva àquele comportamento, tipo o café que convida ao cigarro. Para desconstruir um vício, então, é preciso mudar ao menos uma das três etapas, como descobrir o gatilho e eliminá-lo, dividir o objetivo em metas menores e criar prêmios imediatos para ações com efeitos a longo prazo. Foi o que fez a blogueira Luiza Ferro, 26. Toda vez que ia ao shopping, comprava ""uma besteirinha"". O hábito passou a incomodar quando ela percebeu que gastava demais com coisas inúteis. Luiza deixou de ir ao shopping -o gatilho das compras. Também cancelou newsletters de lojas. E instituiu uma recompensa se conseguisse poupar: um curso no exterior. ""Demorou, mas deu certo. Depois, me interessei por sustentabilidade na moda e pela onda 'slow fashion'. Hoje tenho 40 itens no armário."" Para a psicanálise, a mudança não passa por treinar novos comportamentos. ""O indivíduo não deve ser condicionado. Precisa compreender que não pode se furtar da escolha de seus hábitos e da responsabilidade por eles"", diz o psicanalista Jorge Forbes. A virada é difícil porque envolve surpresa, acrescenta. ""Mudar é angustiante. As pessoas preferem um sintoma conhecido do que um desconhecido."" Fronteiras do Pensamento Fale com a Redação - leitor@grupofolha.com.br Problemas no aplicativo? - novasplataformas@grupofolha.com.br",pt,116
75,2024,1470832126,CONTENT SHARED,-3678789633202302491,7774613525190730745,6403122829531428804,,,,HTML,https://dzone.com/articles/do-you-suffer-from-deployment-anxiety-1,do you suffer from deployment anxiety? - dzone devops,"Whether you suffer from a diagnosed anxiety disorder or not, many of us who are responsible for deployments become uneasy when deploying code to production. Did my tests catch everything? What if something happens during a migration and I can't rollback? Will that small code change create an unstoppable reaction destroying everything in my database? Okay, maybe that's a little aggressive, but you get my point. Deploying to production creates anxiety among developers all over the world. So, how do we combat this anxiety without a prescription for Xanax? We've gathered a couple tricks to help ease your anxiety and optimize your release pipeline all at the same time. Automate Configuration and Deployment By automating a process, you are turning it into a constant. This is the best way to make sure your deployments behave the same every time you deploy an application. Like we learned during our 4th grade science fair, the control variable is unchanging and will give you confidence to learn from future breaks and failures (dependent variables). Create a Testing Environment QA testing should have its own environment and it should be production-like. This is the first major step towards Continuous Delivery and an overall healthier state of mind when deploying to production. Hopefully, you can automate your code's transition from a dev environment to a QA environment where it can run suites of functional and performance tests. Each QA environment should be complete with the right configurations for each new code release. Work With Your Support Team It's all too easy to get involved with your next project and forget to check in on the apps you already have running. DevOps principles often talk about breaking down the walls between departments and, in this case, that means working with your support team to ensure the health of your existing apps. If you can automate the way existing apps are tested and monitored by your dev and support teams, you will begin deploying to production with a lot more confidence. Use Canary Releases Danilo Sato's definition of a Canary Release on MartinFowler.com is the best I've found: "" Canary release is a technique to reduce the risk of introducing a new software version in production by slowly rolling out the change to a small subset of users before rolling it out to the entire infrastructure and making it available to everybody."" By releasing to a small portion of users, you can build confidence in your deployment before pushing it into production completely. Run Fire Drills Crashes, outages, breaks, and failures are all terrifying what-ifs. One way to become slightly more comfortable with these uncertainties is to run controlled fire drills with your teams. Come up with some creative, albeit terrifying, ideas like database crashes or datacenter outages, and run through those scenarios and your related response plan with your team. Incremental Deployments You can read a full blog on Incremental vs. Full Deployments here , but the basics of it are, by running incremental deployments you are updating only the assets on the site that are being changed, instead of re-creating the configuration of the site as a whole. They are faster deploys and they don't touch the systems that are already OK. Dependable Fast Rollbacks Nothing is more comforting than knowing you can roll back your release. Rollbacks are another process that can be automated so that if/when your deployments break, you'll notice serious errors quickly. It should be noted that you'll want to make sure your old code still works on the new database layout, if you were to rollback. Don't be afraid to re-test your old code before rolling back. Deploy Often The most comfortable developers I've met deploy to production a dozen or more times a day. This is the result of a Continuous Delivery pipeline. By automating the right processes and deploying often, you will be able to tweak and refine your pipeline to run smoothly. Am I saying you should start pushing out 30 deploys a day? No. But increasing your deployment rates over time will help create a release process that flows with far fewer bottlenecks and failures. One of the best ways to keep anxiety at bay is by learning from others mistakes. Check out our on-demand webinar: Lessons Learned: Scaling DevOps and Continuous Delivery for the Enterprise . continuous delivery, qa, pipeline, environment, deployment, canary release, rollback",en,115
76,2077,1471352498,CONTENT SHARED,-2584174137395076448,534764222466712491,-2603418925334381846,,,,HTML,http://epoca.globo.com/vida/noticia/2016/08/elektro-google-e-sama-lideram-entre-melhores-empresas-para-trabalhar-em-2016.html,"elektro, google e sama lideram entre as melhores empresas para trabalhar em 2016","Sama, primeiro lugar entre as empresas médias nacionais da lista ÉPOCA GPTW 2016 (Foto: Luís Lima) Época e Great Place to Work realizaram há pouco a cerimônia de premiação das melhores empresas para trabalhar de 2016. A pesquisa está completando 20 anos e é publicada com exclusividade por Época no Brasil. Como vem acontecendo nos últimos anos, o número de inscrições foi recorde: 1563 organizações disputaram um lugar entre as 150 vencedoras, média de 10,4 empresas por vaga. Em 2015 foram 1454 participantes e em 2014, 1276. Elektro, primeiro lugar entre as grandes na lista de ÉPOCA GPTW Brasil 2016 (Foto: Luís Lima) As empresas estão divididas em três categorias: Grandes, com 80 empresas que têm mil funcionários ou mais; Médias Multinacionais, com 35 organizações que possuem entre 100 e 999 empregados, e Médias Nacionais, também um total de 35 empresas com número de funcionários entre 100 e 999. As campeãs nas três categorias do ano passado conseguiram repetir o mesmo feito em 2016, destacando-se novamente como as melhores em suas categorias. São elas: * Grandes - Elektro * Médias Multinacionais - Google * Médias Nacionais - Sama Google, primeiro lugar na categoria médias multinacionais da lista Época GPTW 2016 (Foto: Luís Lima) Leia a lista completa:",pt,115
77,596,1461689512,CONTENT SHARED,6989198691754522425,7890134385692540512,-2241007388742885017,,,,HTML,https://www.sympla.com.br/front-in-bh-2016__54441,front in bh 2016,"Descrição do evento O Front in BH nasceu da necessidade de fortalecer a comunidade Front-end local, através de um circuito de palestras focado em promover o relacionamento e a integração entre os profissionais de Front-end e das áreas a afins. Além de profissionalizar ainda mais o mercado com a contribuição das palestras para a formação de profissionais mais críticos e capacitados, o Front in BH é pioneiro na abordagem de temas atuais e de interesse comum às áreas de desenvolvimento, design, métricas e afins, através de palestrantes renomados no mercado de desenvolvimento Front-end no Brasil e no exterior. Este pioneirismo impulsionou outros grupos a se organizarem, levando o conceito do Front in BH para pelo menos 5 regiões do país. Hoje o Front in BH chega à sua quinta edição com um passado de grande sucesso e a reputação de ser o melhor evento de Front-end do Brasil novamente.",pt,114
78,1976,1470253169,CONTENT SHARED,3170775058142440102,-7606731662737258050,6878120212962380972,,,,HTML,http://revistatrip.uol.com.br/tpm/milly-lacombe-fala-sobre-patriarcado-e-casamento,criadas para casar,"Eu tinha 9 anos quando fui arrancada de um jogo de futebol no recreio porque a diretoria de um dos colégios mais tradicionais de São Paulo havia se reunido e decidido que eu, uma menina, deveria brincar de boneca e não bater bola. Lembro da sensação de ser conduzida por uma das professoras a uma sala escura dentro da qual as garotas da classe celebravam o aniversário de uma boneca pálida, loira e de plástico. Sempre tive medo de bonecas porque a falta de vida delas me soava petrificante. Como petrificante foi ter que passar muitos minutos escutando ""parabéns a você"" para um ser inanimado enquanto ainda era capaz de ouvir os meninos gritando de alegria do lado de fora, o que indicava que embora eu fosse um dos melhores jogadores em campo minha ausência não era sentida. Da mesma forma, minha presença na sala escura tampouco era reconhecida. É preciso que entendamos a violência para além da delinquência: o que a diretoria daquela escola fez comigo no outono de 1976 foi uma violência. Assim como é uma violência que o recado dado a meninas desde o berço seja: o importante é casar e ter filhos. E ele vem de todos os lados: das lojas de brinquedo, dos pais, dos professores, da comunidade, da propaganda. Ainda que muita coisa tenha mudado desde aquele recreio há 40 anos, a condição feminina segue lutando para se livrar dos resíduos de séculos de patriarcado. LEIA TAMBÉM: Todos os textos de Milly Lacombe na Tpm ""Fascismo é a derradeira expressão da hierarquia patriarcal"", escreveu Virginia Woolf há mais de meio século. Para ela, a escola da vida não ensinaria a arte da dominação, ou a arte do poder e da matança, ou como adquirir terras, propriedades, capital. Num mundo mais feminino, a escola imaginada por ela ensinaria artes como medicina, matemática, música, pintura, literatura, e também a arte do sexo e do entendimento a outros seres humanos. O objetivo não seria segregar ou especializar, mas misturar. O objetivo não seria casar, mas amar. Só que, como escreveu Nicholas Kristof, colunista do New York Times, a grande ameaça a extremistas não são drones jogando bombas, mas meninas lendo livros. Em democracias, onde extremistas são mais raros, a perpetuação do patriarcado se faz pela propaganda e pela doutrinação, e segue funcionando como sempre. Politicamente, somos moldadas desde pequenas: casar é transmitido como objetivo final. A mensagem vem das formas mais singelas e doces, como por exemplo dos filmes românticos que terminam sempre com um casamento. Não há conversas sobre sexo, relações humanas, convívio, e eles terminam onde deveriam começar porque todos os que já casaram sabem que é aí que a coisa exige atenção, dedicação, esforço. Não temos isso, temos o casamento como destino, os afazeres do lar como brincadeiras de criança e, depois, como realidade do adulto que ""deu certo socialmente"". Em 1910, Emma Goldman, misturou casamento e política quando escreveu: ""Casamento nesses termos é como aquele outro arranjo patriarcal, o capitalismo. Ele rouba homens e mulheres de dignidade, de brilho, poda seu crescimento, envenena seu corpo, os mantém na ignorância, na pobreza, na dependência, e então institui a caridade, que rouba os últimos vestígios de autorrespeito"". E o professor britânico Timothy Morton disse a respeito do aquecimento global: ""Colocar a natureza num pedestal e admirá-la de longe faz pelo meio ambiente o que o patriarcado faz pela mulher"". Oprimidas e domesticadas A crueldade do patriarcado se desdobra em muitas áreas do cotidiano e é assimilada sem que percebamos. Somos, por exemplo, encorajadas a acreditar que não emagrecemos porque não nos exercitamos o suficiente, mas não se fala do açúcar e de como as legislações aplicadas a ele nos matam lentamente. Quando as taxas de obesidade nos EUA ultrapassaram todos os limites aceitáveis teve início a febre do low fat, mas o que não contaram é que ao tirar a gordura eles acrescentaram açúcar. Claro que homens são vítimas também, mas a eles não está imposta a ditadura da magreza e da gostosura. E, depois disso tudo, o recado que chega é: você precisa malhar, manter-se decente para poder casar, ter filhos e seguir casada. O patriarcado é bom em nos culpar por suas mazelas. Ao nos recusarmos a encarar o debate político que envolve o patriarcado, seguimos vivendo dentro desta distopia: oprimidas, domesticadas, doutrinadas para sermos mães e mulheres dedicadas, magras e obedientes. Alguns dos direitos mais fundamentais de todo ser humano são a criatividade e a liberdade, mas o patriarcado nos priva disso enquanto tenta convencer que a verdadeira liberdade é poder comprar dois sapatos pelo preço de um. Liberdade é a possibilidade concreta de todos nós desenvolvermos as faculdades, capacidades e habilidades com as quais a natureza nos dotou e convertê-las em valor social. É o que desejamos àqueles que amamos, mas é o que deveríamos desejar a todo ser humano se percebêssemos que somos parte de uma mesma substância. Demorei muitos anos para me livrar da ferida deixada pela diretoria da escola em que estudei, e incontáveis invernos para entender que eu não precisaria casar, ou sequer casar com um homem, para cumprir uma suposta função social e me sentir bem-sucedida. Foi uma longa travessia essa que me ensinou que casar não é fundamental, mas que amar é. A menos que não nos acanhemos de encarar o debate político, arregacemos as mangas e lutemos contra a crueldade do patriarcado, não seremos capazes de sair dessa armadilha em que nos meteram. Nada menor do que uma revolução pode nos tirar daqui. Virginia Woolf fica com a palavra final: ""Como mulher, não tenho um país; como mulher, não quero um país; como mulher, meu país é o mundo inteiro"".",pt,113
79,1487,1466608308,CONTENT SHARED,-4278025512576376201,-5527145562136413747,101989957506145943,,,,HTML,http://cannes.meioemensagem.com.br/cobertura2016/diario-de-cannes/2016/06/22/sentaquelavemtextao,#sentaquelávemtextão,"""Me cansei de lero-lero! Rita Lee e Roberto de Carvalho Dá licença, mas eu vou sair do sério. Quero mais saúde! Me cansei de escutar opiniões De como ter um mundo melhor."" É aquela coisa, né? Você tem que escrever o seu ""Diário"" e, de repente, entra aqui e tem tanta gente falando tudo sobre tudo e mais um pouco. Você se pergunta: sobre o que vou escrever? Sobre verdades. Já foram quatro dias da minha parte do Festival. É muita informação. É informação demais. Hoje, quarta-feira, são quatro eventos num só: tem saúde, inovação, entretenimento e criatividade. Tudo ao mesmo tempo agora. No meio desse tanto, você espreme, joga fora o bagaço dos jabás e sobra alguma coisa para pensar. Isso se você entendeu tudo o que foi dito, tudo o que foi mostrado. Se você prestou atenção. Se se interessou de verdade. Difícil acompanhar. Mas vale a pena. De verdade. Poderia falar sobre as tecnicidades do momento. A realidade virtual. O vídeo. O vídeo 360º. A programática. O algoritmo. O engajamento. O storytelling. O social. A inteligência artificial. O universo Makers - que devia estar mais presente - ou a Internet das Coisas. Poderia falar sobre a aplicação dos conceitos do digital no dia a dia das agências. Coisas como o pensamento beta, o jeitão ""startup"" de ser, o design thinking e etc. Palestras mais interessantes e palestras menos interessantes na mesma medida sobre tudo isso. Esses discursos todos para mudar o jogo, mudar o jeito de fazer, são lindos, interessantíssimos, mas a gente precisa realmente estar muito a fim de botar na prática. Precisa muito ""combinar com os russos"", como diria Mané Garrincha. Precisa quebrar barreiras, mudar as cabeças, especialmente dos cabeças. E entender que o jogo é outro. Bom, a revolução já começou faz tempo e muita gente vai ficar pra trás. Verdade. Isso tudo é importante, sim. Mas acho que os recados sociais e humanos que o mundo está deixando aqui e ali - para o mundo - são mais tocantes, emocionantes, fortes. O que importa de verdade. Nos últimos anos, o canto que mais escutamos é o da diversidade. Do respeito à diversidade. Em Cannes, 2016 é muito o ano das mulheres. Mas, se você parar e pensar, vai ver que isso não se restringe apenas ao Festival Internacional de Criatividade de Cannes. As discussões em relação à igualdade de gêneros, empoderamento feminino e a violência extrapolante do universo machista são cada vez mais assunto constante em tudo o que é lugar. Está todos os dias no Twitter, no Facebook, no Oscar®, no Festival de Cinema de Cannes, no esporte, no jornalismo - procure saber sobre o movimento ""Jornalistas contra o assédio"" -, na política, no trabalho e nas ruas de todo o mundo. Se você não sacou isso ainda, amigo, amiga, tem algo muito errado. Tão errado quanto todas as desigualdades e todo tipo de violência. Olhe ao redor, a mulher é o assunto da vez no mundo. Mas, até aí, temos uma estrada longa para correr em relação ao #RespeitoAsMinas. Já rolaram diversas palestras e momentos importantes no Festival que trazem o debate do feminino dentro da indústria da comunicação. Seja mostrando as iniciativas, botando as mulheres na frente das discussões ou até apoiando as causas. O app do Festival tem um botão onde você pode escolher pra qual organização internacional da causa feminina você quer doar 25 dinheiros por dia. Mas ainda tem muito o que fazer. Algumas participações já entraram pra história, se não do Festival, na vida de algumas pessoas. Por exemplo: Cindy Gallop, logo após mandar a real sobre o sexo real, deu uma bela cutucada no Festival por distribuírem um livro sobre criatividade que só tem homens como criativos. O próprio autor James Hurman reconheceu o erro de não ter entrevistado mulheres para a obra. A presença de Claudia Gonzalez da ONU no painel com Sir John Hegarty e os produtores de cinema Lawrence Bender e Steve Golin, apresentando em primeira mão, o comercial #WhatIReallyReallyWant feito para a campanha do Global Causes para meninas e mulheres. Em julho, o filme vai pro ar. Enfim. Respira bem fundo e entenda: nada pode se comparar com a participação de Madonna Badger. Uma mulher extraordinária. Responsável pela belíssima campanha #WomenNotObjects, que luta contra a objetificação da mulher na propaganda. Sua história pessoal, duríssima, fez todo mundo se acabar em lágrimas. Vi muita gente derretendo. Eu já estava preparado pelo amigo Rodrigo Maroni que me cantou a bola no dia anterior. Mas não podia imaginar o que edificante seria aquele momento. O jeito como ela conduziu sua apresentação de forma brilhante, mostrando coragem, sabedoria e sua determinação incansável de deixar um legado inspirado em suas três filhas. São verdades. Ditas de verdade. Tudo o que Madonna Badger falou, acabou reverberando dentro de mim com a ótima apresentação de Kim Getty, que tratou muito bem sobre a influência da publicidade na cultura do gênero. Leia mais aqui . As duas apresentações mostravam como é a presença do feminino na publicidade. Mas,no caso de Madonna, o bicho pega mais forte através do apelo sexual. Foi uma catarse. E, na minha cabeça, automaticamente, me veio a lembrança de um fato muito antigo que aconteceu no Brasil: em 1996, Claudia Liz, uma das modelos mais lindas do País, viveu um triste episódio ao ficar em coma após complicações numa cirurgia de lipoaspiração. Um trauma terrível que quase lhe tirou a vida. Na época, Claudia era casada com Celso Loducca e eu nunca me esqueci de uma entrevista dada por ele no calor tenso daquele momento, onde comentava sobre como esse mercado das modelos e da beleza podia ser irresponsável e mal. E, naquele dia, me lembro de olhar para o Celso e pensar: ""Cara, na boa, você também é muito responsável por isso."" Isso é um fato. Todos somos. E, sim, existe a #culturadoestupro. E a gente tem que lutar muito contra isso. Todos os dias. Aqui mesmo no ""Diário de Cannes"" já vejo resultados de que as mulheres não podem se calar, tem que falar muito! A colega Daniela Schmitz, inspirada no que viu e sentiu, colocou para fora sua própria história recente do assédio absurdo vivido por sua filha, participante do programa MasterChef Jr, de forma igualmente tocante. Para quem não sabe, o fato que ela cita, acontecido nas redes sociais por conta do programa, foi o que inspirou o movimento do #primeiroassédio que moveu a internet no Brasil. Nunca me esquecerei daquele dia. Como vi o movimento crescendo no Twitter e no Facebook e como fiquei tão chocado com os absurdos ditos sobre a filha da Daniela e sobre outras crianças que acabei não conseguindo mais assistir ao programa na TV. Deu bode total pra mim. E, sim, Daniela, precisamos falar sobre isso. De verdade. Para terminar, queria falar sobre saúde. Sim. Queria mesmo era poder falar mais e mais sobre saúde aqui no Festival Internacional de Criatividade de Cannes. Mas meu ingresso não permitia acompanhar os conteúdos. Espero ter acesso ao site do Festival. Sabe por que? Minha última experiência num evento internacional desse tipo e tamanho foi num congresso de medicina. Em 2014, enquanto estava fora do mercado, participei em Toronto do SIOP, o maior congresso de oncologia pediátrica, por conta da ONG que faço parte, o Beaba . Como em Cannes, ali tive a oportunidade de estar ao lado de gente do mundo todo, dividindo experiências sobre assuntos importantes. E lá também, acabei me envolvendo menos nas questões técnicas - até porque de medicina entendo pouco - para mergulhar na verdade da empatia. O que aprendi e aprendo junto ao ""mundo paralelo do câncer"" - definição precisa da minha amiga Simone Mozzilli, outra mulher sensacional que vocês deviam conhecer! -, enfim, a experiência vivida na relação com pacientes, ex-pacientes, médicos e enfermeiros, transformaram a relação que tenho com meu trabalho de publicidade. Quando você é exposto à verdade da vida real, não dá pra ser mais do mesmo. Por isso é libertadora a fala de Sir John Hegarty: ""We need to get out of this bubble that is advertising because, to be honest with you, it's very boring"". O cara fala sobre trabalhar com propaganda, e não viver a propaganda. É abrir os olhos para um mundo muito maior que tudo isso. Um mundo de verdade. Que precisa nos inspirar mais e mais e fazer com que nós movimentemos para trazer novas perspectivas para ele. De verdade! P.s: No Tinder de Cannes, um young de algum país diferente me manda uma verdade: ""Isso aqui é muito fake, né? Parece que todo mundo pretende salvar o mundo, mas, no fundo só quer pegar uns prêmios."" Há esperança!",pt,113
80,1395,1465962769,CONTENT SHARED,6062146090334604102,-8020832670974472349,-2734800878000447382,,,,HTML,https://paul.kinlan.me/serverless-sync-in-web-apps/,serverless data sync in web apps with bit torrent,"Our team has built a lot of Progressive Web Apps recently to demonstrate how we think they can be built: Airhorner , Voice Memos , Guitar Tuner, SVG-OMG are a few that spring to mind. One thing that is common across all of these sites is that they have no sever component to store and synchronise data. We built these sites as examples and reference implementations of the types of experiences that we can deliver on the web, they were never intended to be full ""Apps"" that you would build as a business. I recently had an idea for another Web App that was inspired by Ben Thompson's Future of Podcasting PodCast. I wanted to make Pod Casting as simple as visiting your site and pressing a record button on the page (I did something similar years ago called FriendBoo but that had to use the PSTN telephone system). Technically we have all the parts of the web platform to help us: Service Worker to make it work offline, Blob and IndexedDB to store chunks of data locally and MediaStream Recorder to take Microphone input and record it to a file. Finally we have the power of the URL to allow us to access the web app from anywhere there is a browser and an internet connection. Once you have access to the web, you would expect that the data you recorded from one device would be stored on a server somewhere and then be available again later from any machine that you access the site. Traditionally you create a web service that will store the data on a server somewhere and you would manage all that infrastructure to support all your users. However, I'm not in the mood to create a service that requires storage and retrieval of data from a service (getting that through our legal teams will be a nightmare) so everything would have to be stored locally inside the web app. The big question is how do you get your data out and shared to another instance of your web app on another device? Voice Memos created by Paul Lewis is another example of this. All the data is local to the app The data is recored and stored locally which means if you want to sync it with your other devices or share it with a friend, you can't. There are solutions, for example we could dynamically encode the audio file as base64 and create a custom URL - but thats not scalable. It was a bit of a conumdrum and one that I wanted to solve. I set up some simple requirements for what I would like to see: The user should be able to share a simple url that would point to their local data, The user should not have to manually save the data outside of their browser or web app, There should be no ""backend"" that stores the data, Synchornisation should ideally happen peer to peer. Now. Step back 6 months. I was at ColdFront conference last year and I saw a talk by Feross Aboukhadijeh about Web RTC Data Channel, BitTorrent and how he started a project called WebTorrent . It was a great talk, but I didn't hook things up in my head until recently about the potential of what he was talking about. Now I think I have the start of a solution through the use of WebTorrent.io. Web Torrent I won't explain WebTorrent too much other than to say that it combines BitTorrent style distributed data delivery with WebRTC and it is rather spectacular. I do encourage you all to check it out though. The theory I had was that if the user's client could act as a peer in the Torrent network, it would be then able to seed some of the data that is local to the web site, the user could generate a torrent link that could be shared either with another one of their devices or with another user and then the ""remote"" instance of the web app will fetch the data from the ""client"" instance. User vists page User does some work User saves it to IndexedDB User clicks ""Share"" and it generates a ""magnet:"" URL and then starts to seed the Audio Blob. User shares or opens the URL in another browser. Site parses magnet URL and connects to tracker. Site finds peers from the tracker, connects to peer(s) and downloads data. Rough flow of data For 1:1 connections this might be a bit of an overkill, I could just create a WebRTC signalling service and deliver the messages to the other instance to get the data from one client to another. The interesting thing with this approach is that it scales nicely when sharing data to more than one person. Applying this to a real world sample I briefly mentioned Voice Memos earlier. It was a great reference application form me to try and integrate my theory in to a working app. It is close to my idea of a podcasting app and it is also in need of a way to synchronise data between clients because it has no server based backing store for the recordings. Voice memos I didn' make too many changes other than adding in a ""Share"" button. You can check out the demo on BitTorrent Voice Memos . Record a simple audio file, save it, then share it - the ""Share"" will generate a URL that you can send to another device or send to another person. I'm quite pleased with the output. In all this demo took only a couple of hours to get ready. Here is a quick run down of some of the major things that I had to do. Add in the webtorrent.min.js script used to power all the magic. Create a singleton instance of the WebTorrent API that can be used across all of my other classes (inspired by Paul Lewis). Seed all the memos - this is currently a little overkill, but it got the demos working. The this.memos is an Array Instance of VoiceMemos that were stored in our IndexedDB. When the user clicks the Share button, create a custom URL that will contain all the details needed to get access to the torrent and stream the data into another client. We need to run some custom logic to fetch the torrent when we detect that the user has entered a URL that contains the torrent information. The Voice Memos app has a custom router which looks for a url that starts with '/share'. Now we need to parse out the torrent information, and then using the WebTorrent API fetch the file from the BitTorrent network. Once it has been fetched we have to then get the file data out of the Blob using a rather hackey XMLHttpRequest. Wrapping up I will end this all by saying that this is a massive hack and it shouldn't be used for actual private data - right now if goes out on the network if people have the URL to the torrent then it will be accessible. I do think the concept is important, the ability to synchronise data and have no middle-man or server logic in our web apps is an important concept and we should actively consider how we build such experiences. I'm off to keep playing with this...",en,113
81,1228,1464892323,CONTENT SHARED,6783622248311192269,3609194402293569455,6147896666617046192,,,,HTML,http://www.infomoney.com.br/negocios/como-vender-mais/noticia/5062199/par-aliancas-impede-que-seu-parceiro-assista-netflix-sem-voce,par de alianças impede que seu parceiro assista netflix sem você,"SÃO PAULO - Não são poucos os casais que criaram o costume de assistir séries juntos, mesmo que nem sempre consigam manter o mesmo ritmo. Pensando em ""evitar as brigas de casais"" causadas pelo fato de o parceiro ter assistido a um episódio sozinho, a Cornetto criou um par de alianças que só permite que uma pessoa assista à Netflix se a outra estiver por perto. As alianças possuem a tecnologia NFC, mesma utilizada no Pay e meios de pagamentos móveis. Por conta dela, o aplicativo para smartphone que conecta as plataformas de streaming - Netflix, Hulu, Amazon, entre outras - permite que as séries sejam assistidas somente quando ambas as alianças estiverem próximas do celular. Caso contrário, o app é bloqueado. ""Cornetto, o sorvete que uniu casais por décadas, criou um dispositivo para prevenir a causa mais comum de brigas de casais atualmente: assistir episódios à frente de seu parceiro"", explica no site que anuncia o produto. Ainda não se sabe em quais países o produto será comercializado. No portal, os interessados podem registrar seu e-mail para ter acesso a uma espécie de pré-venda, também sem data de início definida. Confira o vídeo que mostra como as alianças devem funcionar:",pt,113
82,1523,1466862481,CONTENT SHARED,4419562057180692966,-1032019229384696495,6917925448322846329,,,,HTML,https://techcrunch.com/2016/06/25/latin-americas-chronic-inefficiency-could-drive-more-o2o-commerce-growth/,latin america's chronic inefficiency could drive more o2o commerce growth,"After 45 minutes on the phone with a travel agent, I've grown too frustrated with trying to figure out different alternatives for an international flight to the Maldives that stops in Paris on the way back. Within seconds, I'm compelled to pick up my mobile phone where Skyscanner and Kayak can help me much more efficiently. Tired and hungry, I find the refrigerator is empty. When I call in my order to my favorite Italian restaurant, the representative on the line doesn't recognize me and asks how she can help me. Yet, it's so obvious why I'm calling. Then she asks me for my phone number and address, and has no record of what I typically order when I call in for food delivery two to three times a week. The truth is that Latin America is extremely inefficient and its technology is behind North America. Yet, in Brazil, one of the world's largest economies despite the current crisis, e-commerce sales are expected to reach more than $22 billion this year (up more than 13 percent over 2015), and m-commerce in Brazil continues to rise at a fast pace, according to recent data from eMarketer . The mobile market opportunity is massive: more than 100 million people in Brazil are already online today, and there are nearly another 100 million more to go. There are numerous daily tasks that can be solved with ease in the U.S. In comparison, the lack of training, standardization and process in Brazil creates chronic inefficiencies in the service sector that push consumers to prefer and use mobile applications that provide everyday services in a standardized manner. In many cases, the experiences between online and offline service in the U.S. can be marginal. However, in Latin America it can be a significant and annoying difference. When you try to buy auto insurance with a broker by telephone, for example, the time you can expect to waste is much higher in Latin America. The lack of visibility into your options is huge. The bureaucracy in terms of the number of necessary documents and data required is exponentially higher. After headache-causing calls, the chances of getting only one quote without a comparison are big. The broker will likely just push their favorite insurance company. Brazil leads O2O commerce sector growth in Latin America Intense traffic in big metropolitan areas have helped create a new market for on-demand delivery by couriers on motor bikes. Crossing the city at high speeds while dodging cars and buses, they deliver documents and small parcels. \ There are now nearly one million ""motoboys"" in Brazil, and more than 200,000 of them work in the city of Sao Paulo alone. To make this fleet more efficient, motoboy hailing apps such as Rapiddo and Loggi are growing fast. On-demand pickup and delivery of laundry is another fast-growing O2O commerce service offering in big cities. Based on subscription fees, on-demand laundry operators are becoming an important service for the always-busy, always-late ""Paulistas"" (those born in Sao Paulo). ALavadeira is one example of a stand-out company in this new laundry service segment. With unemployment reaching a record high in Brazil, DogHero , which is similar to DogVacay or Rover, is having no problem finding new hosts for its popular pet-sitting platform. Whoever loves dogs with available time can apply to become a DogHero host for ""man's best friends."" In Brazil's larger cities, there is a huge shortage of hotels for dogs or kennels, making it difficult for dog owners to travel because they have no one to take care of their pets while they're away. In addition to new consumer offerings, the O2O commerce trend is helping reduce bureaucracy for small business owners. A new phenomenon is occurring in Brazil in response to the high unemployment rate: the number of individual micro-entrepreneurs (MEIs) are skyrocketing. More than 5.5 million of them now operate in Brazil. They offer a variety of services and goods from plumbing to manicures. In response, a government program was introduced at the end of 2009 to track and formalize them as business owners and to collect taxes. Prior to that, the MEIs never paid taxes or social security before. To help them navigate though it, Qipu is a new mobile app and service that helps MEIs manage taxes and finances. It also controls and automates access to social benefits. Qipu has grown fast with more than 200,000 active users. Mobile innovation will create Latin American service sector efficiencies The current experiential difference between the U.S. and Latin America creates two huge opportunities for O2O commerce growth. The first is the rapid adoption of new services that will balance it with the U.S. experience by creating more efficiencies where they're badly needed, led by fast mobile app adoption. T he second is we'll probably see new models in Brazil that didn't necessarily work as well in the U.S. Imagine a model like the instacart, in which you can use a mobile app to make grocery shopping that's delivered to your place in just a few hours. When you analyze the unit economics, Brazil could actually be an even better market than the U.S. for new on-demand businesses. Delivery person salaries, for example, are much lower in Brazil. There are fewer comparable services for the Brazilian to choose from, which provides an advantage for early-mover brands. Plus, the Brazilian consumer would likely have to travel more kilometers to find the products, most likely through heavy traffic or using poor public transport in large cities. For these reasons in particular, a Brazilian is even more likely to pick up a cell phone and request through a mobile app for a bicycle deliveryman do this work instead. It wouldn't be the first time that a model that did not work as well in the U.S. would work here. This happened in comparison e-commerce, for example, Buscapé for many years represented a large share of the domestic e-commerce. And the price-comparison model has never been more relevant in any other e-commerce market than Brazil. Thus, new eServices led my mobile innovation and rapid consumer adoption will fuel a growing on-demand economy in Brazil and Latin America through increased consumer convenience, the third most important vector driving consumer purchases after selection and price, by creating more efficiencies and less work. For this reason, I predict that innovative startups and larger companies that see the potential of solving inefficiencies will push growth of more than 50 percent year-over-year, even despite the current economic crisis in Brazil. This growth rate is already occurring today with iFood for food, MinutoSeguros for insurance, ViajaNet for airline tickets, Nubank with credit cards, to name a few examples of which I am very familiar with today. But be sure: the eServices revolution is coming soon for your plumber, your babysitter and your bank manager. Some more Brazilian startups that are disrupting inefficiencies with eServices include: Disclosure: Of the 22 Latin American companies mentioned in article, three of them (Magnetis, MinutoSeguros and ViajaNet) have received investments from Redpoint eventures, where Rodrigues is a partner and managing director. He's served as a board member, or is currently an advisor, shareholder or personal investor in six of the other companies (Buscape, Cheftime, iFood, Qipu, Rapiddo and Truckpad).",en,112
83,2243,1472744003,CONTENT SHARED,3495098006178009360,-1032019229384696495,4845766491468696400,,,,HTML,http://www.businessinsider.com/google-silicon-valley-tabs-spaces-debate-2016-8,a googler analyzed a billion files to settle the programming dispute made famous by hbo's 'silicon valley',"If you watch HBO's ""Silicon Valley,"" you may remember this now-classic scene from the most recent season, where our hero Richard Hendricks ends his relationship with a Facebook engineer over her programming style: The debate over tabs and spaces , as presented in HBO's Silicon Valley, is real: Developers have been arguing over using tabs versus spaces for formatting their code for almost as long as the concept of programming has existed. At stake is the aesthetics of the code itself - does putting a tab after each new line make it more readable? Or do you just push the space bar a few times? (A more detailed explanation of the tabs vs. spaces debate can be found here .) And so Google Developer Advocate Felipe Hoffa stepped in . By analyzing a billion files, taken from the 400,000 top programming projects on the GitHub social network for software developers, representing 14 terabytes of data all told, he was able to see who was using tabs, and who was using spaces, in most major programming languages. You can read his full results and methodology here , but the end result is bad news for the tabs-loving Hendricks. Check it out: Felipe Hoffa/Google As you can see, spaces far outpace tabs in every major programming language with the exceptions of C, one of the very oldest programming languages still widely used , and Google's Go, an upstart of a programming language that's finding fans among people writing software for the server. This doesn't exactly prove which one is better - but it certainly shows the way that programmers are working in real life. And in most ways that matter, it seems the debate is already over. SEE ALSO: 'Silicon Valley' built an entire episode around one of the most obscure fights in programming NOW READ: One of the oldest ways to write software is finally starting to fade NOW WATCH: Bumble founder: Here's what's seriously wrong with the growing trend in Silicon Valley called 'brogramming'",en,112
84,880,1462909635,CONTENT SHARED,3149164017776669829,-3390049372067052505,-5764325092046707218,,,,HTML,http://www.smartinsights.com/mobile-marketing/mobile-marketing-analytics/mobile-marketing-statistics/,mobile marketing statistics 2016,"Statistics on consumer mobile usage and adoption to inform your mobile marketing strategy mobile site design and app development "" Mobile to overtake fixed Internet access by 2014 "" was the huge headline summarising the bold prediction from 2008 by Mary Meeker, an analyst at Kleiner Perkins Caufield Byers who reviews technology trends annually in May. The mobile statistics that the team at Smart Insights curate in the regular updates to this article include: Ownership of smartphone vs Desktop Mobile vs desktop media and website use Mobile advertising response Smartphone vs Tablet vs Desktop conversion rates Well, we're now past the mobile Tipping Point as this report from comScore shows. So it's no longer a case of asking whether mobile marketing important, we know it is! It's now a question of using the statistics to understand how consumers behave when using different types of mobile devices and what their preferences are. To help you keep up-to-date with the rise in consumer and company adoption of mobile and its impact on mobile marketing, I will keep this post updated throughout 2016 as the new stats come through to support our 120 page Expert members Ebook explaining how to create a mobile marketing strategy . We also have this free summary mobile strategy briefing for Basic members. Our Mobile Marketing Strategy Briefing explains the key issues to plan for in mobile marketing. Download our Free Mobile Marketing Guide . We have grouped the latest mobile stats under these headings for key questions marketers need to answer about mobile to help them compete: Q1. Time spent using mobile meida Q2. Percentage of consumers using mobile devices Q3. How many website visits are on mobile vs desktop devices? Q4. Mobile device conversion rates and visit share for Ecommerce sites? Q5. Mobile - app vs mobile site usage? Q6. How important are mobile ads OK, let's go! Q1. How much time do consumers spend using mobile media? Mary Meeker's annual spring updates on mobile are a must-read if you follow consumer adoption of technology platforms, so we have used some of the key findings from the latest KPCB mobile technology trends by Mary Meeker. Her deck is nearly 200 slides, so we have selected the statistics which best summarise the importance of mobile devices today. The latest data shows that we are now well past the tipping point mentioned at the top of this post. Mobile digital media time in the US is now significantly higher at 51% compared to desktop (42%). The implications are clear - if you're not able to reach your audience through mobile search or display, or you're not providing a satisfactory mobile experience you will miss out compared to competitors who are. The trend in mobile device usage ('vertical screens') compared to all screen use again shows that we're well past the tipping point. Q2. Percentage of consumers using mobile devices? We've created a new summary showing the global popularity of using different digital devices using data from Global Web Index to include in for our State of Digital Marketing 2015 infographic. It clearly shows the popularity of smartphone ownership and emerging mobile devices like Smartwatches. Insight from comScore published in their February 2014 market review shows the picture that marketers need to build up. This panel data shows that the majority of consumers are ""multiscreening"", accessing retail sites on mobile or desktop, so consistent experiences across device need to be deployed. You need to answer this for your own site. As Rob Thurner explained in his post on KPIs to review mcommerce effectiveness , it's important to keep track of the split between users of mobile and desktop devices visiting your site(s). Using advanced segments in Google Analytics is the best way to do this. Q3. How many website visits are on mobile vs desktop devices? However, we need to be careful with interpreting data on hours spent, since we spend most of our time on smartphones checking email and using social media. This has led to the common mantra of 'mobile-first' design which I think is dangerous. Although mobile is growing in importance, this data from Adobe's latest Digital Index shows that in all industries the majority of visits are still on desktop. So with so many site visits still on desktop, it's important when designing using a responsive web design that the desktop experience isn't degraded and this has led to many larger businesses using an adaptive web design where layout and content are tailored for desktop, tablet and smartphone screen dimensions. Q4. Mobile device conversion rates and visit share for Ecommerce sites? We have a separate compilation of Ecommerce conversion stats if you're creating a business case for mobile optimised sites as explained in our mobile marketing strategy guide, this data is also valuable since it shows the variation in conversion rate by mobile type. This is the latest data from Monetate for their retail clients showing conversion rates. The data clearly shows that Smartphone add-to-cart and conversion rates are much lower than for desktop - important if you're making the business case for a mobile responsive site. This source is useful since it's a regular survey showing the growth in use of mobile site visitors. enables you to drill down to see usage by device type, for example iPad is still the dominant tablet, but Kindle Fire and Android tablets now account for over 10% of tablets. You can see that tablet and smartphone use nearly doubled in the year based on 500 million plus visits for these retail clients (see link above for methodology). Q5. Mobile media time - app vs mobile site usage? Consumer preference for mobile apps vs mobile sites should also be thought through as part of mobile strategy. This data from Nielsen on mobile media time shows the consumer preference for mobile apps which account for 89% of media time in mobile as might be expected from the use of the most popular social network, email and news apps. App usage (90% of time) dominates browsers in mobile usage We reported comScore data in May 2012 that showed that on smartphones 82% of mobile media time is via apps . Today, the latest data from Yahoo's Flurry analytics shows that 90 percent of consumer's mobile time is spent in apps . As they put it,put it: It's an App World. The Web Just Lives in It . This is a key insight as companies decide whether to develop mobile apps or create mobile device specific apps. This 90% figure is a key insight as companies decide whether to develop mobile apps or restrict themselves to mobile optimised sites. You do have to be careful about interpreting this since, as the chart below shows, Facebook, messaging, games and utility apps will naturally have the greatest time spent and browser use is still significant by volume if not proportion. But this has implications for advertising on mobile to reach consumers using apps like Facebook and Gmail. Q6. Mobile Ad Spend still lags behind Mobile Media Consumption So, how have advertisers responded to the change in mobile media time? The next chart shows that despite the growth in media time above, some advertisers are missing out since the right-most bar shows that there is a huge missing opportunity on mobile advertising This research sourced from a 2015 study by eMarketer into mobile ad budgets shows a different view. In 2015 mobile ad spending accounts for 49% of digital ad spending, which is only slightly behind the trends of how people are using their devices. These stats also show projections for future growth, which is important as it shows where the market is going. It is clear that mobile is the future, and within 3 years it will come to dominate digital ad spending. Q2. How consumers research products using mobile search and review sites Google's mobile path to purchase report surveyed 950 US consumers across 9 diﬀerent verticals (Restaurants, Food & Cooking, Finance, Travel, Home & Garden, Apparel & Beauty, Automotive, Electronics, Health & Nutrition) to assess how they researched purchases via mobile. A key finding is the starting point for mobile research. As might be expected search was the most common starting point, but it's lower than desktop showing the importance of branded apps and mobile sites. The 5 best sources for mobile marketing statistics? This update to this post features some of the latest updates on mobile statistics from 2014 and highlight some of the best sources to make the business case for investment in mobile marketing in your presentations and business cases to colleagues or clients. 1. Google Mobile Planet. A regular survey for different countries starting in 2011, this enabled you to prepare your own reports. Now this has been replaced by Google's Consumer barometer which enables you to create similar reports. In addition to downloads for each country, you can also create your own charts focusing on KPIs of interest. For example, if you're based in Australia you can look at usage by demographic. The weakness of the current data is that it focuses on Smartphones, not tablets. It may be useful for pushing back against over-enthusiastic colleagues or understanding consumer barriers. For example, less than a third of Australians have ever bought on a smartphone and you can see there are barriers of security and preference for desktop purchases. 2. ITU . The International Telecoms Union data reports mobile usage including mobile broadband subscriptions to show growth in use of mobile. This reported at country, continent and overall levels, so is the best overall source for mobile penetration worldwide. Much of the information is free - see their free mobile statistics section . 3. Flurry Mobile Analytics . This is a great source for showing the overall level of app usage across the four major mobile app platforms by country and drilling down into the popularity of individual apps for different sectors like retail, banking and travel. For example, the latest mobile app growth figures from Flurr y show growth of category use by more than 50% in many categories. Comscore is one of the key worldwide sources useful for marketers to help us find out about the changes in use of mobile media by consumers. This graph shows the pattern across Europe - follow the link above for US and other country breakdowns. The report shows much lower levels of adoption in other European countries though - not even a fifth in most. So extrapolating UK behaviour to other countries would seem to be a mistake with the mobile figure still key. The report also has useful summary of dayparts of different device behaviour, similar to others published. Retail mobile use Mobile was again the focus of the section on retail statistics. Audience growth rate is 80% + on mobile in these UK sites, but lower on grocer sites for obvious reasons. 5. Ofcom Internet usage report . Ofcom's Eighth International Communications Market Report was published in December 2014, this examines take-up, availability, price and use of broadband, landlines, mobiles, TV, radio and post across 17 major countries. As example, here's the picture of desktop vs mobile device in the UK showing that when you look at most important device, desktop and laptop remain important. We hope this compilation of statistics about mobile usage and effectiveness continues to be useful - please share if it is and we'll continue to update it in 2015. If you want a single source of the latest stats across the whole of digital marketing, for Expert members, we compile a regularly updated set of usage statistics to use in presentations - it's updated each quarter so all the latest stats are categorised in a single place for including in presentations. Our ""one-stop"" download includes the latest stats to include in presentations to make the case for digital investment. Download our Online marketing statistics compilation .",en,110
85,3026,1485525147,CONTENT SHARED,991271693336573226,3106760029136205156,-5854718852758691534,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36",NJ,US,HTML,http://www.brasilpost.com.br/2017/01/23/meme-luiza-esta-atenta-machismo-area-de-ti_n_14336764.html,este é o melhor jeito de entender como as mulheres sofrem machismo na área de ti,"Publicado: A conversa de WhatsApp entre duas amigas explicando o poder da família Kardashian , ""Luiza, você está atenta"", é o meme perfeito para explicar qualquer situação, inclusive as mais delicadas, como o preconceito. E foi com esta ideia que o PyLadies Teresina reformulou o meme para debater o machismo no mercado de TI (Tecnologia da Informação). O projeto foi criado para estimular mulheres a seguirem carreira no mercado de computação, uma área predominantemente masculina. Segundo dados do estudo Stack Overflow Developer de 2015, 92,1% dos profissionais de TI são homens . O baixo índice de mulheres no mercado não é justificado pela falta de interesse. De acordo com o PrograMaria, na escola, 74% das meninas demonstram interesse nas áreas de Ciência, Tecnologia Engenharia e Matemática, mas na hora de escolher a graduação, apenas 0,4% delas realmente rseguem essas áreas, o que significa que há uma grande barreira social nestas profissões. Pensando nisso, o projeto utilizou o meme ""Luiza, você está atenta?"" como ferramenta para propagar como a área de TI pode ser hostil com as mulheres. ""Você tá achando que as mulheres vão deixar o machismo na área de tecnologia barato? BANG, Luiza! A gente cria várias redes de mulheres pra ocupar esses espaços!"", escreveu o projeto no post do Facebook, que acabou viralizando : LEIA MAIS: - País com menor desigualdade de gênero, a Islândia está chocada com a morte desta jovem - Esta é a melhor explicação para quem não entende nada da família Kardashian",pt,110
86,1674,1467822323,CONTENT SHARED,5338677278233757627,-4627026983118548639,-3005922752286260244,,,,HTML,http://www.nytimes.com/2014/02/23/opinion/sunday/friedman-how-to-get-a-job-at-google.html,how to get a job at google,"MOUNTAIN VIEW, Calif. - LAST June, in an interview with Adam Bryant of The Times, Laszlo Bock, the senior vice president of people operations for Google - i.e., the guy in charge of hiring for one of the world's most successful companies - noted that Google had determined that ""G.P.A.'s are worthless as a criteria for hiring, and test scores are worthless. ... We found that they don't predict anything."" He also noted that the ""proportion of people without any college education at Google has increased over time"" - now as high as 14 percent on some teams. At a time when many people are asking, ""How's my kid gonna get a job?"" I thought it would be useful to visit Google and hear how Bock would answer. Don't get him wrong, Bock begins, ""Good grades certainly don't hurt."" Many jobs at Google require math, computing and coding skills, so if your good grades truly reflect skills in those areas that you can apply, it would be an advantage. But Google has its eyes on much more. ""There are five hiring attributes we have across the company,"" explained Bock. ""If it's a technical role, we assess your coding ability, and half the roles in the company are technical roles. For every job, though, the No. 1 thing we look for is general cognitive ability, and it's not I.Q. It's learning ability. It's the ability to process on the fly. It's the ability to pull together disparate bits of information. We assess that using structured behavioral interviews that we validate to make sure they're predictive."" The second, he added, ""is leadership - in particular emergent leadership as opposed to traditional leadership. Traditional leadership is, were you president of the chess club? Were you vice president of sales? How quickly did you get there? We don't care. What we care about is, when faced with a problem and you're a member of a team, do you, at the appropriate time, step in and lead. And just as critically, do you step back and stop leading, do you let someone else? Because what's critical to be an effective leader in this environment is you have to be willing to relinquish power."" What else? Humility and ownership. ""It's feeling the sense of responsibility, the sense of ownership, to step in,"" he said, to try to solve any problem - and the humility to step back and embrace the better ideas of others. ""Your end goal,"" explained Bock, ""is what can we do together to problem-solve. I've contributed my piece, and then I step back."" And it is not just humility in creating space for others to contribute, says Bock, it's ""intellectual humility. Without humility, you are unable to learn."" It is why research shows that many graduates from hotshot business schools plateau. ""Successful bright people rarely experience failure, and so they don't learn how to learn from that failure,"" said Bock. ""They, instead, commit the fundamental attribution error, which is if something good happens, it's because I'm a genius. If something bad happens, it's because someone's an idiot or I didn't get the resources or the market moved. ... What we've seen is that the people who are the most successful here, who we want to hire, will have a fierce position. They'll argue like hell. They'll be zealots about their point of view. But then you say, 'here's a new fact,' and they'll go, 'Oh, well, that changes things; you're right.' "" You need a big ego and small ego in the same person at the same time. The least important attribute they look for is ""expertise."" Said Bock: ""If you take somebody who has high cognitive ability, is innately curious, willing to learn and has emergent leadership skills, and you hire them as an H.R. person or finance person, and they have no content knowledge, and you compare them with someone who's been doing just one thing and is a world expert, the expert will go: 'I've seen this 100 times before; here's what you do.' "" Most of the time the nonexpert will come up with the same answer, added Bock, ""because most of the time it's not that hard."" Sure, once in a while they will mess it up, he said, but once in a while they'll also come up with an answer that is totally new. And there is huge value in that. To sum up Bock's approach to hiring: Talent can come in so many different forms and be built in so many nontraditional ways today, hiring officers have to be alive to every one - besides brand-name colleges. Because ""when you look at people who don't go to school and make their way in the world, those are exceptional human beings. And we should do everything we can to find those people."" Too many colleges, he added, ""don't deliver on what they promise. You generate a ton of debt, you don't learn the most useful things for your life. It's [just] an extended adolescence."" Google attracts so much talent it can afford to look beyond traditional metrics, like G.P.A. For most young people, though, going to college and doing well is still the best way to master the tools needed for many careers. But Bock is saying something important to them, too: Beware. Your degree is not a proxy for your ability to do any job. The world only cares about - and pays off on - what you can do with what you know (and it doesn't care how you learned it). And in an age when innovation is increasingly a group endeavor, it also cares about a lot of soft skills - leadership, humility, collaboration, adaptability and loving to learn and re-learn. This will be true no matter where you go to work.",en,108
87,2624,1477304783,CONTENT SHARED,-8954346068661072425,-6627505417926774253,7151797190085189711,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.59 Safari/537.36",MG,BR,HTML,https://code.facebook.com/posts/991252547593574/the-technology-behind-preview-photos/,the technology behind preview photos,"First impressions matter, whether you're on a first date, in a job interview, or just choosing new decorations for your house. Some of the first things you see when you visit someone's profile or page on Facebook are the pictures. These pictures are an integral part of the Facebook experience, but sometimes they can be slow to download and display. This is especially true on low-connectivity or mobile networks, which often leave you staring at an empty gray box as you wait for images to download. This is a problem in developing markets such as India, where many people new to Facebook are primarily using 2G networks. Our engineering team took this on as a challenge: What could we design and build that would leave a much better first impression? We initially focused on the cover photo, the beautiful, high-resolution picture at the top of profiles and pages. The cover photo is one of the most visible parts of these surfaces, yet it's also one of the slowest to load. There were two big reasons for this. First, cover photos often reach 100 KB, even after JPEG compression. That's a lot of data when you realize that 2G connections might be transferring data as slowly as 32 KB/second. The second reason is subtler. Before downloading a picture, the application makes a network request for the picture's URL from the GraphQL server. Then, to actually get the image, it uses that URL to make a second network request to the CDN to get the image bytes. The latency of this second network request can be quite long, typically much longer than the first network request. We needed to attack both of these problems simultaneously. 200 bytes To address these issues, we asked ourselves if we could create a visual impression of the image using only 200 bytes. Why 200 bytes? In order to remove that second network request, we needed to include some facsimile of the image itself in the initial network request. This in turn meant that the image had to be part of the GraphQL response, but GraphQL isn't designed to handle full-size image data. It was determined that if we could shrink a cover photo down to 200 bytes, it could efficiently be delivered via the GraphQL response. The cool thing about this solution, if successful, was that it addressed both the small byte requirement and the need for a second network request in one fell swoop. We estimated that this would allow us to display the preview photo in our very first drawing pass, reducing the total latency to display profiles and page headers significantly. Eventually, we still want to download and display the full-size image from the CDN, but this can be done in the background while ensuring that the user experience still feels snappy and enjoyable. The challenge now became how to squeeze a cover photo into 200 bytes! Displaying the right image We felt that a frosted-glass ""impression"" of an image would provide something both visually interesting and consistent with the original image. Having decided on the desired user experience, we needed to figure out the technical details to make it happen. What was the lowest resolution we could use? How would we compress the image? How would we display that image on the client? This is where things got interesting. Displaying the image was the most straightforward part: The frosted-glass look is relatively easy to achieve with a Gaussian blur filter. The nice thing about this blurring filter, besides looking good, is that it ""band-limits"" the signal. Band-limiting essentially means throwing away detail and quickly changing information in the original source image. The more we are willing to blur the displayed image, the smaller our source image can be. Image resolution and compression Clearly, the more we blur our images, the lower resolution we need and the more we can compress. On one extreme, if we send down just the average color of all the pixels in an image (aka the DC components of an image), that would require only a single ""pixel"" of 3 bytes - one byte each for RGB! We knew we needed higher resolution than just one pixel, but how few pixels could we get away with? Given the final frosted-glass effect we want to achieve on the client, we can determine the required blur radius for our Gaussian filter. From that blur radius, we were then able to compute the lowest-resolution image that would still give us the desired final image. For the display size of our cover photos, we found that this resolution was about 42 pixels. Above a 42x42-pixel image, we would get no additional fidelity in the displayed image; essentially, we would be wasting data. But assuming 3 bytes per pixel (for RGB components), that would still be 42x42x3, or 5,292 bytes - much higher than the desired 200-byte target. We started evaluating standard compression techniques to find the best way to compress this data to 200 bytes. Unfortunately, simply entropy encoding the image, with, say, zlib, gets you only a factor of 2. Still too big. We then evaluated a bunch of nonstandard techniques, but we decided it was better to leverage other code/libraries that we had. So, we looked at JPEG image encoding, which is a very popular image codec. Especially since our image is going to be blurred heavily on the client, and thus band-limiting our image data, JPEG should compress this image quite efficiently for our purposes. Unfortunately, the standard JPEG header is hundreds of bytes in size. In fact, the JPEG header alone is several times bigger than our entire 200-byte budget. However, excluding the JPEG header, the encoded data payload itself was approaching our 200 bytes. We just needed to figure out what to do about that pesky header! JPEG to the rescue (mostly) There are a few tables within the JPEG header, which accounts for its size. The question then became: Would it be possible to generate a fixed header that could be stored on client and therefore not need to be transmitted? In that scenario, only the payload would need to be sent, which would make this the winning format. The investigation began. For a given Q value, the quantization table is fixed; through experimentation and measurement, Q20 produced an image that would meet our visual needs. So that was a good start to a fixed header. Our images were not a fixed size but capped at 42x42 (we retain the aspect ratio in the reduced format). This amounted to 2 bytes that we could prepend to the payload and that could be placed by the client in the correct spot to make the header valid. As we looked through the rest of the standard JPEG header, the only other table that could change with different images and options was the Huffman table. This required a bit more work, because there is a trade-off between changes to Q, image data, and image size, which meant different frequency values within the Huffman table, which would lead to different levels of compression and different final payload byte count. Compressing quite a few images while trading off each of those took time, but in the end we had a Huffman table that we could use as a standard that would get us the byte count we wanted across the test images. Since we deal with a large number of images, it was always possible that the solution wouldn't scale, there might be extreme edge cases, we didn't have a real representative sample set, etc. To that end, a version number was added to the beginning so that the format would be future-proof. If we find any extreme cases or better tables in the future, we can update the version number for those images and ship new tables on the clients. So the final format became one byte for version number, one byte each for width and height, and finally the approximately 200 byte payload. The server would just send this format as part of the GraphQL response, and then the client could simply append the JPEG body to the predefined JPEG header, patch the width and height, and treat it as a regular JPEG image. After the standard JPEG decoding, the client could run the predetermined Gaussian blur and scale it to fit the window size. With this, we finally had a format that met our requirements - a highly effective solution that allowed us to reuse the relatively sophisticated JPEG image encoding scheme while transmitting only the data unique to each cover photo. In our data, we saw big improvements. For people on a slow connection, this helped speed up profile and page loads by 30 percent. Even on the fastest connections, this ensured that people would always see a cover photo preview immediately, making their overall experience more seamless. It took a lot of creativity to make it happen, but thanks to everyone's hard work, it paid off!",en,108
88,826,1462537265,CONTENT SHARED,-6542996094878850014,7890134385692540512,2046518916076451829,,,,HTML,https://medium.com/javascript-scene/how-one-jira-ticket-made-my-employer-1mm-month-7-metrics-that-actually-matter-ffb5b2376a6b,how one jira ticket made my employer $1mm/month: 7 metrics that actually matter - javascript scene,"The essence of agile is the continual process of improving efficiency. This is the only thing you really need to know about agile in order to put it to (productive) work in your organization. You should value: Individuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a plan If that doesn't look familiar, you really need to read this . I'll add some more of my favorite dev team values: Skills over titles Continuous delivery over deadlines Support over blame Collaboration over competition Why is your team's agile process so dysfunctional? Chances are you're losing track of those values, and you're trying to cram the same old chaotic waterfall into agile trappings like scrum meetings and retrospectives. Face to face scrum meetings can be counter-productive because they encourage competition over collaboration (employees compare themselves to each other), blame over support, deadlines over continuous delivery (""we have to close x tickets before Friday or we'll have to work the weekend!""), and a pecking order that infects a team like cancer. Where in the agile manifesto does it say, ""stand up for 15 minutes every day and feel slow-shamed because the coder across from you finished 6 tickets yesterday and you only finished one""? There's always the obnoxious over-achiever who comes in early and goes home late every day, and closes 2x as many tickets as the rest of the team. Note: almost always a relatively inexperienced, but super eager and super impressive. Too bad they're slowing down the rest of the team . Likewise, there's always the slow-poke who closes one or two tickets. Odd. They're a great mentor, and they're always chiming in on other people's tickets, helping them get unstuck, teaching, and giving great advice. They should be able to close 10x as many tickets as the rest of us, right? They must simply be terrible at time management. (Hint: They're not. You just suck at evaluating employees ). Ticket counting and ""velocity tracking"" are the worst ideas in software development management since waterfall chaos. Forget Counting Tickets Forget points. Forget estimates and commitments. Estimates are all worthless lies. Try weekly demos, instead. Get the marketing team to hype the features you finished last month, not the features you think you might be able to finish next month. Build yourself a good feature toggle and marketing release management system and you can still release features according to a marketing hype schedule, but you'll be hyping finished features, and your team will never have to burn out on night & weekend crunch times again. Tip: Your marketing and sales teams should never be allowed to discuss features ""in the pipeline"", and your sales & biz dev teams should never be able to commit to a deadline without a really flexible MVP (minimum viable product) plan. When I say flexible, I mean flexible: e.g., Elon Musk is taking us to Mars. Initial sales MVP: get a balloon into the clouds. Engineering estimates are usually wrong by orders of magnitude , but nobody wants to face up to that fact and deal in reality. What Should We Measure? The only thing that matters in software development is that the users love your software. Everything else in this list serves that purpose. Remember that. Now bring on the real metrics! The first 5 of these metrics are all essential business key performance indicators (KPIs) for nearly every app developer. You're going to wonder why I'm sharing these with a bunch of developers and telling you that these are the metrics you need to focus on , but bear with me. The remaining metrics will clear that up for you. 1. Revenue None of the other metrics mean a damn thing if you go broke. If you run out of fuel, it's game over. You're done. Pack up your office and go home. Core Tactics: Conversion rate optimization Crawlable content Sharable content Page load & perf Optimize to keep the lights on. 2. Monthly Active Users (MAU) Do you have any users? Do you have more than last month? If you're a venture funded startup, you'd better pray you're growing fast enough. In the beginning you can double month over month if you work hard. Of course, all hockey-stick curves are really S curves in disguise, but chances are good you have plenty of room left to grow. Core Tactics: Page load & perf Sharable content TDD & code review Optimize for growth. 3. Net Promoter Score (NPS) Remember when I said if you run out of fuel, it's game over? I tricked you, didn't I? You thought I was talking about money. Money isn't your fuel. Fans are your fuel. Fans are the key to unlocking more money. More sharing. More growth. More joy. More magic. Core Tactics: TDD & code review Page load & perf Collaboration with support & QA staff Optimize to turn users into evangelists. 4. Viral Factor Also known as k-factor or viral quotient. If you're not measuring this, start right now: i = number of invites (e.g., shares) per user c = conversion rate per share k = 1 is steady. No growth. No decline. k > 1 means exponential growth. k < 1 means exponential decline. You should have a giant screen in the middle of the office with a k-factor gauge in bright red for <1 , green for >1 , overlaid on your 3-month MAU growth chart. Core Tactics: Sharable content Integrate sharing into core product Conversion rate optimization Page load & perf Optimize for sharing and new visitor conversion. 5. Support tickets Nothing says ""this garbage is broken"" like an email to customer support. When was the last time somebody contacted support just to tell you how cool you are? Support tickets are your canary in the coal mine. When somebody says something is broken, don't think, ""it works for me!"". Even if it's user error, it's not user error. Chances are there's a flaw in the design and 1,000 other people are bothered by it too. 1,000 other people for every one person who cares enough to write you and complain about. 1,000 people who'd rather hit the back button than waste their time on your app for one more second. Ideally, you should aim for zero support tickets. You'll never reach that metric (if you're lucky), but you should consider every support ticket to be a bug report. Start categorizing the common ones. Count them and use them to prioritize fixes. I'm not saying the customer is always right. Sometimes customers don't know what they want until you give it to them. I am saying that if it's in your inbox, you're doing something wrong. Core Tactics: TDD & code review CI/CD Feature toggle & rollout Periodic bug burndown hackathons Collaboration with support & QA staff Optimize for problem-free customer experience. Key Engineering Focus Metrics As promised. The keys to unlocking the mysteries of the business KPIs. As it turns out, you can move all of the above needles a lot with two levers: 6. Bug Count Here's a shock. Some ticket counts are good for something. Be careful to categorize all the bug tickets as bugs, and then you can see a bug count. All software has bugs, but not all bugs need fixing. If a bug appears only on an ancient phone, and that phone is only used by one user of your software, and that user isn't even a paying customer, do you need to fix that bug? Probably not. Close it and move on. Prioritize the bugs that are hurting your users the most. Get busy and squash them. Core Tactics: TDD & code review Periodic bug burndown hackathons Collaboration with support & QA staff Optimize for a bug-free experience. 7. Performance I'm cheating a little this time. This one is going to contain 3 more critical metrics: Load time: The time it takes for your app to be usable after the user clicks the icon or hits your URL. Aim for <1 second. Beyond that, you lose users. Every ms you can trim off load time comes with measurable benefits to every metric above. Response time: The time from user action (like a click) to a visible response in the application. Aim for <100ms . Any more than that feels like lag. Animation time: The maximum time it takes to draw one animation frame. Aim for 10ms. Any more than 16 ms will cause noticeable jank, and may even make the user feel a bit queasy. You'll need a little breathing room for this one. Keep it under 10ms. Load time is by far the most important mini metric in this list. It will move the business KPI needles like nothing else I've mentioned. But response time and smooth animations cause a magical side-effect. Users are happier after using your app. You see, every little janky glitch, every little delayed response feels jarring to users on an almost subconscious level. Give the same user the same app with performance issues fixed, and they report much higher satisfaction ratings, even if they can't put their finger on why. Our little secret. Core Tactics: Periodic performance hackathons In-depth performance audits 10ms, 100ms, 1000ms, repeat Optimize for a jank-free experience. There's A Lot More To It Of course, this little rant can't go into great depth on how developers can directly manipulate viral factor and MAU numbers, but I'll leave you with a hint: You can. And when those numbers are staring you in the face every day, and you know that it's your job to move them - not management's job, not marketing's job -  your job, I'm sure you'll come up with some creative ideas to make it happen. If your manager thinks you have better things to do, send them this link. Now go out there and move the needle on some metrics that actually matter.",en,108
89,1762,1468438220,CONTENT SHARED,882422233694040097,-4092545774372727680,-3323769179501075221,,,,HTML,https://www.infoq.com/br/news/2016/07/infografico-machine-learning,infográfico: algoritmos para aprendizado de máquina,"Quais são os mais importantes algoritmos a serem utilizados em projetos com aprendizado de máquina? Qualquer pessoa inserida em projetos que façam uso desta tecnologia normalmente se questiona em algum momento, o que utilizar em decorrência de um projeto que utilize aprendizado de máquina. Anubhav Srivastava é um cientista de dados que trabalha e escreve sobre modelos de decisão utilizando Big Data e em seu campo de atuação busca aplicar estes modelos em diversas áreas de negócio da indústria, auxiliando empresas em suas tomadas de decisão. Srivastava, que também atua como escritor para o site thinkbigdata.in , publicou recentemente um infográfico com os principais algoritmos utilizados em aprendizado de máquina. O infográfico destaca alguns dos algoritmos mais utilizados, resumindo cada tipo de aplicabilidade com os disponíveis para uso. Trata-se de um material estruturado que possibilita uma consulta rápida para sanar dúvidas. Segundo o autor, é importante destacar que não há um algoritmo vencedor e que seja melhor em relação aos demais, o infográfico tem a intenção de ser apenas uma referência de uso. Srivastava destaca em sua publicação que: Para situações diferentes, e com diferentes algoritmos, mesmo que sejam concebidos para apresentar resultados semelhantes, o resultado normalmente será apresentado de forma diferente. Dependendo do que se deseja alcançar com a análise dos dados, um algoritmo pode lhe atender de uma forma melhor quando comparado a um outro que faz exatamente a mesma coisa. Outro fator importante é que o tamanho do conjunto de dados o qual estamos trabalhando faz uma diferença determinante no modelo que se quer aplicar. Além disso, a possibilidade de interações com os algoritmos existentes permite o aumento da relevância em uma variedade de aplicações. É importante lembrar que os algoritmos mais simples não são ruins ou obsoletos. Assim, a sugestão que dou é ao invés de buscar os melhores algoritmos, devemos nos concentrar em ganhar a consciência sobre os fundamentos destes diferentes algoritmos e suas aplicações. As técnicas cobertas no infográfico incluem entre outros: O infográfico completo com todos os algoritmos divididos por tipo de técnica de aprendizado de máquina pode ser visualizado na imagem abaixo: A intenção deste infográfico não é a de detalhar os algoritmos e suas técnicas de aprendizado de máquina e sim disponibilizar a informação de uma forma estruturada, fácil e acessível como um quadro que possa ser colado na parede e sempre estar a mão para uma rápida consulta. Para uma referência detalhada com alguns algoritmos, o autor cita ainda a lista disponibilizada na conferência de mineração de dados da IEEE de 2006.",pt,107
90,3097,1487257346,CONTENT SHARED,-8627051188605351707,-4465926797008424436,-5996702544087841012,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.37 Safari/537.36",SP,BR,HTML,https://dev.to/gonedark/when-to-make-a-git-commit,when to make a git commit,"You don't have to look through too many commit histories on GitHub to see people are pretty terrible about making commits. Now I'm not going to talk about writing commit messages. Here's the post on that. I want to talk about the equally important topic of when to make commits. I get asked this a lot at conferences. Enough to where I made two rules I've continually put to the test. I make a commit when: Anytime I satisfy one of these rules, I commit that set of changes. To be clear, I commit only that set of changes. For example, if I had changes I may want to undo and changes that completed a unit of work, I'd make two commits - one containing the changes I may want to undo and one containing the changes that complete the work. I think the second rule is pretty straightforward. So let's tackle it first. Over the course of time, you'll make some changes you know will be undone. Be it a promotional feature, patch, or other temporary change someday soon you'll want to undo that work. If that's the case, I'll make these changes in their own commit. This way it's easy to find the changes and use git revert . This practice has proven itself time and again, I'll even commit changes I'm simply uncertain about in their own commit. So back to the first rule. I think generally most of us follow this rule. However, you don't have to scroll through too many repositories on GitHub to see that we're pretty bad with commits. The discrepancies come from how we define a unit of work . Let's start with how not to define a unit of work . A unit of work is absolutely not based on time. Making commits every X number of minutes, hours, or days is ridiculous and would never result in a version history that provides any value outside of a chronicling system. Yes, WIP commits are fine. But if they appear in the history of your master branch I'm coming for you! A unit of work is not based on the type of change. Making commits for new files separate from modified files rarely makes sense. Neither does any other type abstraction: code (e.g. JavaScript vs HTML), layer (e.g. Client vs API), or location (e.g. file system). So if a unit of work is not based on time or type , then what? I think it's based by feature . A feature provides more context. Therein making it a far better measurement for a unit of work . Often, implicit in this context are things like time and type , as well as the nature of the change. Said another way, by basing a unit of work by feature will guide you to make commits that tell a story. So, why not just make the first rule: I make a commit when I complete a feature ? Well, I think this a case where the journey matters. A feature can mean different things, even within the context of the same repository. A feature can also vary in size. With unit of work , you keep the flexibility to control the size of the unit . You just need to know how to measure . I've found by feature gives you the best commit. Enjoy this post? Check out my comprehensive video series Getting Git .",en,107
91,1481,1466563387,CONTENT SHARED,-3285397592982852407,-2626634673110551643,3199267762118537905,,,,HTML,http://www.valor.com.br/empresas/4609273/bradesco-vai-testar-tecnologia-por-tras-do-bitcoin-em-sao-paulo,bradesco vai testar tecnologia por trás do bitcoin em são paulo,"SÃO PAULO - O Bradesco vai fazer, até o fim do ano, em São Paulo, um teste de uso do ""blockchain"", a tecnologia por trás da moeda virtual bitcoin. Se o cronograma for mantido, será o primeiro teste prático da tecnologia feito por um banco brasileiro. O ""blockchain"" funciona como um livro-caixa de transações feitas no mundo digital. A tecnologia tem atraído a atenção dos bancos ao redor do mundo por duas razões principais. As transações registradas não podem ser apagadas nem alteradas, o que garante segurança ao sistema. O funcionamento também independe de grandes estruturas de tecnologia, o que pode representar uma redução de custos em relação às formas tradicionais de processamento de transações. Segundo Marcelo Frontini, diretor de pesquisa e inovação do Bradesco, o teste com o ""blockchain"" será feito em Paraisópolis, Zona Sul da capital paulista, em parceria com a startup eWally. A empresa foi uma das selecionadas da mais recente turma do InovaBRA, programa de apoio a startups do Bradesco. A empresa desenvolveu uma carteira digital que pode ser usada com um smartphone para pagamentos e transferência de dinheiro. O ""blockchain"" é a tecnologia por trás do sistema. De acordo com o executivo, o sistema será integrado à rede de correspondentes bancários Bradesco Expresso. Assim, quem quiser fazer um depósito, retirar dinheiro ou fazer um pagamento usando a eWally, poderá ir a um desses pontos. ""Usando o Bradesco como entrada e saída, conseguimos monitorar o uso, o que dá segurança ao teste"", disse. O Bradesco está levando o ""blockchain"" bastante a sério. O banco acabou de se integrar ao consórcio R3, que reúne 43 bancos de todo o mundo para estudar e criar padrões para o uso do sistema. É o segundo banco nacional a entrar no consórcio. O Itaú foi o primeiro, há dois meses. Além da carteira digital da eWally, o Bradesco também vai fazer testes com o sistema de transferência de dinheiro por meio de remessas internacionais da startup BitOne, outra recém-aprovada no InovaBRA. Nesse caso, no entanto, as avaliações ainda são iniciais, para verificar a viabilidade do modelo. A BitOne desenvolveu um sistema de proteção cambial para as transferências (""hedge""), que tem o objetivo de reduzir o impacto de conversão do bitcoin para a moeda do destinatário da transferência. De acordo com Frontini, é preciso avaliar se o custo desse hedge não será alto demais, inviabilizando o sistema. O banco também está em conversas com a startup Ripple Labs, a principal referencia em remessas internacionais usando ""blockchain"". O ""blockchain"" é uma das vertentes do grupo de trabalho criado pelo Banco Central (BC) para discutir as ""fintechs"", como vêm sendo chamadas as tecnologias voltadas ao setor financeiro.",pt,105
92,1999,1470416057,CONTENT SHARED,7526364197140419661,6013226412048763966,-1503381279074578444,,,,HTML,http://www.jjhws.com/solutions/digital-health-coaching,digital health coaching,"Skills for Life: A Scalable Approach to Behavior Change We believe that inside each individual lies the power to change-to be healthier and happier. Behavior change can be incredibly challenging. That's why we're proud to have been one of the first to offer Digital Health Coaching, which combines advanced technology and behavioral science to emulate a live health coach. Our programs offer an individually tailored and scalable approach to coaching, with a demonstrated ability to deliver results across large populations. We bring this approach to health plans to support their pursuit of the Triple Aim goals-achieving better health outcomes, at lower cost, all while enhancing the member experience. Drawing upon a wide array of validated behavior science models, our Digital Health Coaching solution can help empower your members by helping them build the skills they need most. You are striving for lower costs and satisfied members who are engaged, motivated, and confident in their ability to make healthy behavior change-our solution can help you achieve that. Being engaged with your health means being engaged with your life. Digital Health Coaching can help your members engage with their health-every day. Scientifically Based Created by behavioral scientists, health professionals, and content tailoring experts, our programs are built upon a foundation of tailored guidance and actionable steps to develop skills. This skills-based approach can help people dealing with multiple challenges (diabetes, depression, sleep problems, and more). Individually Tailored We recommend concrete, tailored action steps based on user-identified health and wellness goals. The personalized experience is designed to help increase success by tapping into a user's 'core values'. Skills and action steps also easily map to real-world activities to help foster behavior change. A Compelling Experience For Participants Our programs are designed to provide an engaging and interactive user experience-holistically connecting users' health and behavior with their core values and life goals. Tailored coaching content is delivered in small packages to help create a quick yet effective interaction and help create healthy habits. Designed to be responsive for desktop, tablet, and phone. Click here to schedule a product demonstration.",en,105
93,1494,1466631321,CONTENT SHARED,-820343972901090172,3302556033962996625,3775313930401213418,,,,HTML,https://medium.com/@shanselman/stop-saying-learning-to-code-is-easy-659c5f4c0d7,stop saying learning to code is easy.,"Stop saying learning to code is easy. I saw this tweet after the Apple WWDC keynote and had thought the same thing. Hang on, programming is hard. Rewarding, sure. Interesting, totally. But ""easy"" sets folks up for failure and a lifetime of self-doubt. When we tell folks - kids or otherwise - that programming is easy , what will they think when it gets difficult? And it will get difficult. That's where people find themselves saying ""well, I guess I'm not wired for coding. It's just not for me."" Now, to be clear, that may be the case. I'm arguing that if we as an industry go around telling everyone that ""coding is easy"" we are just prepping folks for self-exclusion, rather than enabling a growing and inclusive community. That's the goal right? Let's get more folks into computers, but let's set their expectations. Here, I'll try to level set. Hey you! People learning to code! Programming is hard. It's complicated. It's exhausting. It's exasperating. Some things will totally make sense to you and some won't. I'm looking at you, RegEx. The documentation usually sucks. Sometimes computers are stupid and crash. But. You'll meet amazing people who will mentor you. You'll feel powerful and create things you never thought possible. You'll better understand the tech world around you. You'll try new tools and build your own personal toolkit. Sometimes you'll just wake up with the answer. You'll start to ""see"" how systems fit together. Over the years you'll learn about the history of computers and how we are all standing on the shoulders of giants. It's rewarding. It's empowering. It's worthwhile. And you can do it. Stick with it. Join positive communities. Read code. Watch videos about code. Try new languages! Maybe the language you learned first isn't the ""programming language of your soul."" Learning to program is NOT easy but it's totally possible. You can do it. More Reading Sponsor: Big thanks to Redgate for sponsoring the feed this week. How do you find & fix your slowest .NET code? Boost the performance of your .NET application with ANTS Performance Profiler . Find your bottleneck fast with performance data for code & queries. Try it free !",en,104
94,2702,1478176600,CONTENT SHARED,-6642751159620064055,3891637997717104548,-5518716332871437149,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36",SP,BR,HTML,https://capgemini.github.io/drupal/what-to-look-for-in-code-review/,what to look for in a code review ,"In a previous article on this blog, I talked about why code review is a good idea , and some aspects of how to conduct them. This time I want to dig deeper into the practicalities of reviewing code, and mention a few things to watch out for. Code review is the first line of defence against hackers and bugs. When you approve a pull request, you're putting your name to it - taking a share of responsibility for the change. Once bad code has got into a system, it can be difficult to remove. Trying to find problems in an existing codebase is like looking for an unknown number of needles in a haystack , but when you're reviewing a pull request it's more like looking in a handful of hay. The difficult part is recognising a needle when you see one. Hopefully this article will help you with that. Code review shouldn't be a box-ticking exercise, but it can be helpful to have a list of common issues to watch out for. As well as the important question of whether the change will actually work, the main areas to consider are: I'll touch on these areas in more detail - I'll be talking about Drupal and PHP in particular, but a lot of the points I'll make are relevant to other languages and frameworks. Security I don't claim to be an expert on security, and often count myself lucky that I work in what my colleague Andrew Harmel-Law calls ""a creative-inventive market, not a safety-critical one"" . Having said that, there are a few common things to keep an eye out for, and developers should be aware of the OWASP top ten list of vulnerabilities . When working with Drupal, you should bear in mind the Drupal security team's advice for writing secure code . For me, the most important points to consider are: Does the code accept user input without proper sanitisation? In short - don't trust user input. The big attack vectors like XSS and SQL injection are based on malicious text strings. Drupal provides several types of text filtering - the appropriate filter depends on what you're going to do with the data, but you should always run user input through some kind of sanitisation . Are we storing sensitive data anywhere we shouldn't be? Security isn't just about stopping bad guys getting in where they shouldn't. Think about what kind of data you have, and what you're doing with it. Make sure that you're not logging people's private data inappropriately, or passing it across network in a way you shouldn't. Even if the site you're working on doesn't have anything as sensitive as the Panama papers , you have a legal, professional, and personal responsibility to make sure that you're handling data properly. Performance When we're considering code changes, we should always think about what impact they will have on the end user, not least in terms of how quickly a site will load. As Google recently reminded us , page load speed is vital for user engagement. Slow, bloated websites cost money, both in terms of mobile data charges and lost revenue . Does the change break caching? Most Drupal performance strategies will talk about the value of caching. The aim of the game is to reduce the amount of work that your web server does. Ideally, the web server won't do any work for a page request from an anonymous user - the whole thing will be handled by a reverse proxy cache, such as Varnish. If the request needs to go to the web server, we want as much of the page as possible to be served from an object cache such as Redis or Memcached, to minimise the number of database queries needed to render the page. Are there any unnecessary uses of $_SESSION ? Typically, reverse proxy servers like Varnish will not cache pages for authenticated users. If the browser has a session, the request won't be served by Varnish, but by the web server. Here's an illustration of why this is so important. This graph shows the difference in response time on a load test environment following a deployment that included some code to create sessions. There were some other changes that impacted performance, but this was the big one. As you can see, overall response time increased six-fold, with the biggest increase in the time spent by the web server processing PHP (the blue sections on the graphs), mainly because a few lines of code creating sessions had slipped through the net. Are there any inefficient loops? The developers' maxims ""Don't Repeat Yourself"" and ""Keep It Simple Stupid"" apply to servers as well. If the server is doing work to render a page, we don't want that work to be repeated or overly complex. What's the front end performance impact? There's no substitute for actually testing, but there are a few things that you can keep an eye out for when reviewing change. Does the change introduce any additional HTTP requests? Perhaps they could be avoided by using sprites or icon fonts. Have any images been optimised? Are you making any repeated DOM queries ? Accessibility Even if you're not an expert on accessibility, and don't know ARIA roles , you can at least bear in mind a few general pointers. When it comes to testing, there's a good checklist from the Accessibility Project , but here are some things I always try to think about when reviewing a pull request. Will it work on a keyboard / screen reader / other input or output device ? Doing proper accessibility testing is difficult, and you may not have access to assistive technology, but a good rule of thumb is that if you can navigate using only a keyboard, it will probably work for someone using one of the myriad input devices . Testing is the only way to be certain, but here are a couple of simple things to remember when reviewing CSS changes: hover and focus should usually go together, and you should almost never use outline: none; . Are you hiding content appropriately? One piece of low-hanging fruit is to make sure that text is available to screen readers and other assistive technology. Any time I see display: none; in a pull request, alarm bells start ringing. It's usually not the right way to hide content . Maintainability Hopefully the system you're working on will last for a long time. People will have to work on it in the future. You should try to make life easier for those people, not least because you'll probably be one of them. Reinventing the wheel Are you writing more code than you need to? It may well be that the problem you're looking at has already been solved, and one of the great things about open source is that you're able to recruit an army of developers and testers you may never meet . Is there already a module for that? On the other hand, even if there is an existing module, it might not always make sense to use it. Perhaps the contributed module provides more flexibility than our project will ever need, at a performance cost. Maybe it gives us 90% of what we want, but would force us to do things in a certain way that would make it difficult to get the final 10%. Perhaps it isn't in a very healthy state - if so, perhaps you could fix it up and contribute your fixes back to the community, as I did on a recent project . If you're writing a custom module to solve a very specific problem, could it be made more generic and contributed to the community? A couple of examples of this from the Capgemini team are Stomp and Route . One of the jobs of the code reviewer is to help draw the appropriate line between the generic and the specific. If you're reviewing custom code, think about whether there's prior art. If the pull request includes community-contributed code, you should still review it. Don't assume that it's perfect, just because someone's given it away for nothing. Appropriate API usage Is your team using your chosen frameworks as they were intended? If you see someone writing a custom function to solve a problem that's already been solved, maybe you need to share a link to the API docs for the existing solution. Introducing notices and errors If your logs are littered with notices about undefined variables or array indexes, not only are you likely to be suffering a performance hit from the logging, but it's much harder to separate the signal from the noise when you're trying to investigate something. Browser support Remember that sometimes, it's good to be boring . As a reviewer, one of your jobs is to stop your colleagues from getting carried away with shiny new features like ES6, or CSS variables. Tools like Can I Use are really useful in being able to check what's going to work in the browsers that you care about. Code smells Sometimes, code seems wrong. As I learned from Larry Garfield's excellent presentation on code smells at the first Drupalcon I went to, code smells are indications of things that might be a deeper problem. Rather than re-hash the points Larry made, I'd recommend reading his slides , but it is worth highlighting some of the anti-patterns he discusses. Functions or objects that do more than one thing A function should have a function. Not two functions, or three. If an appropriate comment or function name includes ""and"", it's a sign you should be splitting the function up. Functions that sometimes do different things Another bad sign is the word ""or"" in the comment. Functions should always do the same thing. Excessive complexity Long functions are usually a sign that you might want to think about refactoring. They tend to be an indicator that the code is more complex than it needs to be. The level of complexity can be measured , but you don't need a tool to tell you that if a function doesn't fit on a screen, it'll be difficult to debug. Not being testable Even if functions are simple enough to write tests for, do they depend on a whole system? In other words, can they be genuinely unit tested? Lack of documentation There's more to be said on the subject of code comments than I can go into here, but suffice to say code should have useful, meaningful comments to help future maintainers understand it. Tight coupling Modules should be modular. If two parts of a system need to interact, they should have a clearly defined and documented interface. Impurity Side effects and global variables should generally be avoided. Sensible naming Is the purpose of a function or variable obvious from the name? I don't want to rehash old jokes , but naming things is difficult, and it is important. Why would you comment out lines of code? If you don't need it, delete it. The beauty of version control is that you can go back in time to see what code used to be there. As long as you write a good commit message, it'll be easy enough to find. If you think that you might need it later, put it behind a feature toggle so that the functionality can be enabled without a code release. Specificity In CSS, IDs and !important are the big code smells for me. They're a bad sign that a specificity arms race has begun. Even if you aren't going to go all the way with a system like BEM or SMACSS , it's a good idea to keep specificity as low as possible. The excellent articles on CSS specificity by Harry Roberts and Chris Coyier are good starting points for learning more. Standards It's important to follow coding standards . The point of this isn't to get some imaginary Scout badge - code that follows standards is easier to read, which makes it easier to understand, and by extension easier to maintain. In addition, if you have your IDE set up right, it can warn you of possible problems, but those warnings will only be manageable if you keep your code clean . Deployability Will your changes be available in environments built by Continuous Integration? Do you need to set default values of variables which may need overriding for different environments? Just as your functions should be testable, so should your configuration changes. As far as possible, aim to make everything repeatable and automatable - if a release needs any manual changes it's a sign that your team may need to be thinking with more of a DevOps mindset . Keep Your Eyes On The Prize With all this talk of coding style and standards, don't get distracted by trivialities - it is worth caring about things like whitespace and variable naming, but remember that it's much more important to think about whether the code actually does what it is supposed to . The trouble is that our eyes tend to fixate on those sort of things, and they cause unnecessary cognitive load . Pre-commit hooks can help to catch coding standards violations so that reviewers don't need to waste their time commenting on them. If you're on a big project, it will almost certainly be worth investing some time in integrating your CI server and your code review tool, and automating checks for issues like code style , unit tests, mess detection - in short, all the things that a computer is better at spotting than humans are. Does the code actually solve the problem you want it to? Rather than just looking at the code, spend a couple of minutes reading the ticket that it is associated with - has the developer understood the requirements properly? Have they approached the issue appropriately? If you're not sure about the change, check out the branch locally and test it in your development environment. Even if there's nothing wrong with the suggested change, maybe there's a better way of doing it. The whole point of code review is to share the benefit of the team's various experiences, get extra eyes on the problem, and hopefully make the end product better. I hope that this has been useful for you, and if there's anything you think I've missed, please let me know via the comments.",en,103
95,2350,1473970449,CONTENT SHARED,-6603970730135147059,-4028919343899978105,5740617098389918193,,,,HTML,http://canaltech.com.br/noticia/geek/pesquisadores-do-mit-desenvolvem-tecnologia-que-digitaliza-livros-fechados-79769/,pesquisadores do mit desenvolvem tecnologia que digitaliza livros fechados - geek,"Por Redação | em Uma nova tecnologia desenvolvida por pesquisadores do MIT promete revolucionar a maneira de digitalizar documentos e livros para o formato digital. Publicado na revista Nature Communications, a tecnologia propõe que livros possam ser digitalizados sem precisar escanear página por página, bastando inserir qualquer livro fechado para que o processo seja feito. Caso a tecnologia seja aperfeiçoada, ela poderá agilizar o trabalho de bibliotecas, por exemplo. A maneira de conseguir uma versão digital de documentos e livros se tornará muito mais fácil e rápida. No vídeo abaixo você pode verificar com mais detalhes como a tecnologia desenvolvida no MIT funciona: O processo utiliza radiação tetrahertz, que é absorvida pelo papel e tinta de uma maneira diferente. A câmera é tão precisamente ajustada que os pesquisadores conseguem detectar a diferença entre uma página e outra, apesar de haver apenas 20 micrômetros de ar entra cada uma. As páginas digitalizadas são lidas através de softwares ajustados para o processamento da imagem gerada. Ainda há um longo caminho a ser percorrido. Neste momento, a tecnologia permite a leitura de apenas nove páginas agrupadas. Mas, novas pesquisas poderiam maximizar este número, tornando os resultados surpreendentes, principalmente para arquivistas e bibliotecários. Com a nova tecnologia, será possível obter acesso a livros restritos e antigos através de uma cópia digital e também agilizar consideravelmente o tempo de digitalização de documentos. Assine nosso canal e saiba mais sobre tecnologia!",pt,101
96,1211,1464875897,CONTENT SHARED,3306277069425849869,9109075639526981934,3973684612244026225,,,,HTML,http://computerworld.com.br/google-segue-microsoft-e-lanca-ferramenta-analitica-gratis,google segue microsoft e lança ferramenta analítica grátis,"O Google está intensificando seu compromisso com o mercado de ferramentas analíticas corporativas. A companhia acaba de lançar uma nova ferramenta gratuita para visualização de dados. O Data Studio, versão grátis de um Sistema de visualização de dados, chega como parte da suíte de tecnologias de analytics apresentadas pela empresa há alguns meses. A solução inclui uma variedade de conectores que permite que usuários conectem outras ferramentas como o AdWords e Sheets, por exemplo. O sistema também se integra ao BigQuery. A empresa planeja, ainda, disponibilizar conectores com bancos de dados SQL ainda esse ano. O produto concorre contra o Microsoft Power BI, um dos principais lançamentos da fabricante do Windows sob o comando de Satya Nadella. O Google afirma que o Data Studio garante a empresas coletarem dados de uma variedade de fontes e, a partir disso, gerar relatórios para serem compartilhados interna e externamente, fornecendo melhor compreensão de grandes volumes de dados. Os relatórios incluem gráficos, planilhas, mapas de calor ou outros modelos visualmente úteis aos usuários. Empresas que quiserem recursos mais amplos poderão comprar licenças do serviço Data Studio 360. A camada gratuita, porém, dá aos clientes uma forma de testar a tecnologia e ver se é aderente a suas demandas. A grande diferença entre os dois produtos, de fato, é o número de reports que cada usuário pode criar - clientes da versão grátis podem gerar cinco relatórios.",pt,101
97,2234,1472606340,CONTENT SHARED,-5488842573681626972,-4465926797008424436,4923575442991716043,,,,HTML,http://www.testingexcellence.com/bdd-guidelines-best-practices/,bdd best practices and guidelines - testing excellence,"BDD Best Practices BDD Introduction BDD (Behaviour Driven Development) is a methodology for developing software through continuous example-based communication between developers, QAs and BAs. In this article we discuss some BDD Best Practices to get the most benefit. More than anything else, the primary purpose of BDD methodology is to encourage communication amongst the stakeholders of the project so that the context of each feature is correctly understood by all members of the team (i.e. shared understanding), before development work starts. This helps in identifying key scenarios for each story and also eradicate ambiguities from requirements. In BDD, Examples are called Scenarios. Scenarios are structured around the Context-Action-Outcome pattern and are written in a special format called Gherkin . The scenarios are a way of explaining (in plain english) how a given feature should behave in different situations or with different input parameters. Because Gherkin is structural, it serves both as a specification and input into automated tests, hence the name ""Executable Specifications"". What is a feature file and what does it contain Feature files are text files with .feature extension, which can be opened by any text editor as well as readable by any BDD-aware tool, such as Cucumber, JBehave or Behat. Feature files should start with the context of the feature (which is essentially the story), followed by at least one scenario in the following format Feature: Some terse yet descriptive text of what is desired In order to realize a named business value As an explicit system actor I want to gain some beneficial outcome which furthers the goal Scenario: Some determinable business situation Given some precondition And some other precondition When some action by the actor And some other action And yet another action Then some testable outcome is achieved And something else we can check happens too Scenarios in feature files should focus on the ""what"" rather than the ""how"". The scenarios should be concise and to the point, so that the reader can quickly grasp the intent of the test without having to read a lot of irrelevant steps. Why should we write feature files As mentioned before, the primary aim of the BDD methodology is to encourage communication amongst the delivery team. The aim of the feature files is to document the scenarios talked through in order to give an indication of how much work is involved in delivering the feature. The feature files are also the drivers for the automated tests. Feature files also serve as a definition of done (DoD), meaning that when all the scenarios have been implemented and tested successfully, we can mark the story as done. Who should write feature files It doesn't really matter who actually writes/types the feature files, it could be any member of the delivery team, however, the contents (scenarios) which are discussed by a trio of Dev-QA-BA are the essential part of feature files. Getting the shared common understanding of the feature is the key element. When should feature files be written Feature files should be written during the story grooming sessions where the details of each story is discussed. Feature files containing scenarios should be written before development starts so that developers as well as QA have a clear understanding of the intent of the story. There should be a shared understanding of the story. The scenarios serve as requirements to development. Where should feature files be kept There should be one source of truth serving both as specification and automated execution, therefore should be kept somewhere where every member of the team has easy access. Having said that, because the feature files are the drivers of the automated tests, they should ideally be kept in source control system (GitHub) so that any updates to the feature files is immediately reflected on to the tests. For non-technical members who have no experience with Git, we can always execute a dry-run of the feature files which will then output the list of all existing scenarios without actually exercising the feature files. How should we write feature files There are generally two ways of writing feature files - Imperative and Declarative Imperative style of writing a feature file, is very verbose, contains low level details and too much information. Pros: person reading the feature file can follow the step-by-step Cons: Because of too much detail, the reader can lose the point of the story and the tests. The feature file becomes too big, difficult to maintain and likely to fail due to UI updates. Declarative style of writing a feature file is concise and to the point, contains only relevant information about the story. Pros: The declarative style is more readable as it contains less steps in the scenario. The reader can easily understand the scope of the test and quickly identify if any key elements are missing.",en,100
98,1065,1463934631,CONTENT SHARED,5206308811707978799,-1032019229384696495,7846322948118114767,,,,HTML,http://techcrunch.com/2016/05/21/the-rise-of-apis/,the rise of apis,"It's been almost five years since we heard that "" software is eating the world ."" The number of SaaS applications has exploded and there is a rising wave of software innovation in the area of APIs that provide critical connective tissue and increasingly important functionality. There has been a proliferation of third-party API companies, which is fundamentally changing the dynamics of how software is created and brought to market. The application programming interface (API) has been a key part of software development for decades as a way to develop for a specific platform, such as Microsoft Windows. More recently, newer platform providers, from Salesforce to Facebook and Google, have offered APIs that help the developer and have, in effect, created a developer dependency on these platforms. Now, a new breed of third-party APIs are offering capabilities that free developers from lock-in to any particular platform and allow them to more efficiently bring their applications to market. The monolithic infrastructure and applications that have powered businesses over the past few decades are giving way to distributed and modular alternatives. These rely on small, independent and reusable microservices that can be assembled relatively easily into more complex applications. As a result, developers can focus on their own unique functionality and surround it with fully functional, distributed processes developed by other specialists, which they access through APIs. Faster, cheaper, smarter Developers realize that much of the functionality they need to build into an app is redundant to what many other companies are toiling over. They've learned not to expend precious resources on reinventing the wheel but instead to rely on APIs from the larger platforms, such as Salesforce, Amazon and, more recently, specialized developers. We're still in the early innings of this shift to third-party APIs, but a number of promising examples illustrate how developers can turn to companies such as Stripe and Plaid for payment connectivity, Twilio for telephony, Factual for location-based data and Algolia for site search. Indeed, the area is booming. On last check, ProgrammableWeb was providing searchable access to almost 15,000 APIs, with more being added on a daily basis. Developers can incorporate these APIs into their software projects and get to market much more quickly than going it alone. While getting to market more quickly at a lower cost is a huge advantage, there is an even more important advantage: Companies that focus on their core capabilities develop differentiated functionality, their ""secret sauce,"" at higher velocity. The benefits for the rest of the software development ecosystem are profound. Another advantage is third-party APIs are often flat-out better. They work better and provide more flexibility than APIs that are built internally. Companies often underestimate the amount of work that goes into building and maintaining the functionality that they can now get as a third-party API. Finally, third-party API developers have more volume and access to a larger data set that creates network effects. These network effects can manifest themselves in everything from better pricing to superior SLA's to using AI to mine best practices and patterns across the data. For example, Menlo's portfolio company Signifyd offers fraud analysis as an API. They aggregate retail transactions across hundreds of companies, which allows them to understand a breadth of fraud markers better than any individual customer could. A new breed of software companies Releasing software as an API allows those companies to pursue a number of different adoption routes. Rather than trying to sell specific industry verticals or use cases, often the customer is a developer, leading to an extremely low-friction sales process. The revenue model is almost always recurring, which leads to an inherently scalable business model as the end customers' usage increases. While the ecosystem of API-based companies is early in its evolution, we believe the attributes of these companies will combine to create ultimately more capital-efficient and profitable business models. This opportunity is not limited to new upstarts. Existing developers may have the opportunity to expose their own unique functionality as an API, morphing their product from application to platform. Some outstanding companies have built API businesses that match or exceed their original focus: Salesforce reportedly generates 50 percent of its revenues through APIs, eBay nearly 60 percent and Expedia a whopping 90 percent. The model is attractive to entrepreneurs and investors. Rather than trying to create the next hot app and having to invest heavily in marketing and distribution before validating scalable demand, it may make more sense to build a bounded set of functionality and become an arms merchant for other developers. The API model creates a compelling route to market that if successful can scale capital efficiently and gain a network effect over time. Currently, there are 9 million developers working on private APIs; as that talent sees the opportunity to create companies versus functionalities, we may see a significant shift to public API development (where there are currently only 1.2 million developers). Rethinking the value chain In the past, the biggest companies were those closest to the data (e.g. a system of record), able to impose a tax, or lock-in to their platform. In the API economy, the biggest companies may be the ones that aggregate the most data smartly and open it up to others. This enables new types of competitive barriers, as in Twilio's ability to negotiate volume discounts from carriers that no individual developer could obtain, or the volume pricing that Stripe enjoys by pooling payments across many developers. Companies like Usermind (a Menlo Ventures portfolio company) show great promise in allowing enterprises to move beyond their single-application silos by creating workflows and simplifying the API connections between their existing SaaS applications. While the ecosystem for API startups is attractive today, we believe it will only become stronger. Over the last five years there's been a broadening of interest in enterprise-oriented technologies like SaaS, big data, microservices and AI. APIs are the nexus of all four of those areas. As the world of enterprise software development further embraces third-party APIs, we expect to see a number of large companies emerge. The low-touch sales model, recurring revenue and lack of customer concentration lead to a very attractive business model. In addition, the benefits for the rest of the software development ecosystem are profound, as app developers can focus on delivering the unique functionality of their app and more quickly and less expensively deliver that ever-important initial product. Featured Image: Bloomua / Shutterstock",en,99
99,1702,1467992229,CONTENT SHARED,-5879360586463363298,3891637997717104548,-3159742872959650153,,,,HTML,https://medium.com/@hichaelmart/lambci-4c3e29d6599b,introducing lambci - a serverless build system,"Introducing LambCI - a serverless build system I'm excited to announce the first release of LambCI , an open-source continuous integration tool built on AWS Lambda �� LambCI is a tool I began building over a year ago to run tests on our pull requests and branches at Uniqlo Mobile . Inspired at the inaugural ServerlessConf a few weeks ago, I recently put some work into hammering it into shape for public consumption. It was borne of a dissatisfaction with the two current choices for automated testing on private projects. You can either pay for it as a service (Travis, CircleCI, etc) - where 3 developers needing their own build containers might set you back a few hundred dollars a month. Or you can setup a system like Jenkins, Strider, etc and configure and manage a database, a web server and a cluster of build servers . In both cases you'll be under- or overutilized, waiting for servers to free up or paying for server power you're not using. And this, for me, is where the advantage of a serverless architecture really comes to light: 100% utilization, coupled with instant invocations. Systems built on solutions like AWS Lambda and Google Cloud Functions essentially have per-build pricing. You'd pay the same for 100 concurrent 30 second builds as you would for 10 separate 5 minute builds. The LambCI Advantage From an ops perspective, all of the systems and capacity are managed by Amazon (SNS, Lambda, DynamoDB and S3), so LambCI is far simpler to setup and manage than Jenkins - especially given that you get 100 concurrent builds out of the box. From a cost perspective, it's typically far cheaper for private builds than the various SaaS offerings because you only pay for the time you use (and the first 4,444 mins/mth are free): So if you had 2 developers, each simultaneously running sixty 4-min builds per day (ie, 4 hrs each), LambCI would be more than 8 times cheaper per month than Travis ($15 vs $129). It's only if you need to be running builds 24/7 that SaaS options become more competitive - and of course if you're wanting to run builds for your open source projects, then Travis and CircleCI and others all have great (free) options for that. Performance-wise, Lambda reports as a dual Xeon E5-2680 @2.80GHz. If you have checked-in dependencies and fast unit tests, builds can finish in single-digit seconds - but a larger project like dynalite , with 941 HTTP-to-localhost integration tests, builds in about 70 seconds. 43 secs of that is actually running the tests with the remainder being mostly npm installation. On my 1.7GHz i7 MacBook Air the npm install and tests complete about 20% faster, so there's definitely an element of ""cloud"" speed to keep in mind. The public Travis option takes only a few seconds longer than LambCI to run dynalite's npm install and tests, but the overall build time is larger due to worker startup time (22 secs) and waiting in the queue (up to several mins - I assume this only happens if you don't have enough concurrency). What does it look like? Here's what it looks like in action - this is building a project with only a handful of tests and checked-in dependencies, so this is definitely faster than it is when building our typical projects, but I promise this is real and all running remotely on AWS Lambda:",en,99
100,998,1463503800,CONTENT SHARED,-1038011342017850,6735372008307093370,841012281439613986,,,,HTML,https://universidadedocotidiano.catracalivre.com.br/para-entender/para-entender-o-dia-internacional-contra-homofobia/,para entender o dia internacional contra a homofobia,"O Dia Internacional contra a Homofobia é comemorado em 17 de maio para lembrar a data em que a OMS (Organização Mundial da Saúde) retirou o homossexualismo da Classificação Estatística Internacional de Doenças e Problemas Relacionados à Saúde, deixando de considerar essa tendência um desvio, além de abolir o termo. Na área de saúde, o sufixo ""ismo"" caracteriza uma condição patológica. Entre 1948 e 1990, a OMS classificou a tendência como um transtorno mental. Em 17 de maio de 1990, uma assembleia geral aprovou a retirada da condição da Classificação Internacional de Doenças, com a alegação que ""homossexualidade não constitui doença, nem distúrbio e nem perversão"". A nova classificação entrou em vigor nos países-membros da ONU em 1993. A exclusão marcou o fim de um ciclo de 2.000 anos em que a cultura judaico-cristã encarou a homossexualidade como um pecado, depois como um crime e, por último, como uma doença. Dizer que a condição é um vício, uma tara ou uma doença a ser curada passou oficialmente à categoria de preconceito e ignorância. E foi por isso que 17 de maio foi declarado o Dia Internacional de Combate à Homofobia, quando pessoas do mundo inteiro se mobilizam para discutir a diversidade e a tolerância.",pt,98
101,713,1462202338,CONTENT SHARED,-4754223659064624252,5127372011815639401,1894192985308020631,,,,HTML,https://www.box.com/blog/effective-learning-through-code-workshops/,effective learning through code workshops | box blog,"At Box, we're very interested in the quality of our code, which is why we're constantly evaluating our processes to figure out how we can do better. We even have a team of Code Reliability Engineers (CREs) that help others write better code and provide training both internally and externally . Recently, we've turned a critical eye towards code reviews and have been implementing a new process called code workshops. The trouble with code reviews Code reviews are a tough topic to approach as many have had bad experiences in the past. The first code review I ever personally participated in was, in fact, horrible. It was organized by the senior members of a team I was on and the process was strange. We all sat in a room, spending the first 20 minutes staring at printouts of someone's code and scribbling notes on them. After that, we went around the room and everyone shared what they thought of the code while the author of the code listened intently. Then, the author was allowed to defend the code. I hated that experience. In my mind, we have just wasted time sitting together and making someone incredibly uncomfortable for no good reason. We didn't actually learn anything and it was time spent away from being productive. I felt like there was some value to doing code reviews, but this process felt wrong in every way: There was an overall accusatory tone that encouraged people to find things that were wrong and trivialize things that were right. People seeing code for the first time in the review meeting was a complete waste of time. Why weren't we prepared? The negative comments put the code author on the defensive, which led to less-than-productive conversations. Over the course of my career, I've heard a lot of feedback around these points whenever engineers say they hate code reviews. The kicker is that engineers actually love talking about code. We talk about it on Twitter. We talk about it over email and IM. We talk about it over lunch. We think some people write code better than others. We like semicolons. We hate semicolons. We love talking about this stuff, so why is it that we hate code reviews so much? The problem is typically the code review process. Too many times they are set up as adversarial (as in my first experience), which leads to defensiveness and bad feelings. When processes are too loose, people don't feel like there's much value in participating. When processes are too strict, people feel judged. Unfortunately, the best way to learn about code is to discuss it with other people, so how can you create a system that people actually enjoy and learn from? Building a better code review Over the years, I've worked with a process that I've come to call code workshops (since code reviews have a negative connotation, a different phrase eliminates some bias). Code workshops are a group session that are intended to socialize team expectations and best practices, as well as uncover issues and points that need to be addressed by the team as a whole. Code workshops take place on a weekly basis for one hour and with a maximum of 20 people in the room. The session is split in half, with two reviewers each reviewing a file (or set of files, if small enough) for 30 minutes. The key pieces to the meeting are: A reviewer is assigned (or selects) a piece of code to review ahead of time. The reviewer must choose a file that he or she didn't write. It's the reviewer's job to present the work of someone else. The reviewer reviews the entire file, not just diffs. Diffs are okay for bug fixes but to get a deep understanding of the code, you need the extra context. The goal of the reviewer is to learn as much about the code as possible. The reviewer reviews the code before the workshop. I typically recommend people set aside an hour for this. During that time, the reviewer adds comments directly into the file marked with REVIEW . The meeting begins with everyone closing their laptops except for the reviewer and one other person to take notes. The point is to have everyone pay attention to what's being discussed so we don't waste anyone's time. In the meeting, each reviewer goes over the file with their comments, focusing on the interesting pieces of code. The code is projected so that everyone can follow along. The reviewer cannot mention people's names during the review. The point is to review code, not people. Everyone is considered an owner of the code so there is no need to get defensive. If there are problems in the code, then the group needs to get better as a whole. It's the reviewer's job to encourage discussion through findings in the code. Look for interesting or problematic patterns, opportunities to establish new conventions, or pieces of the code that need more explanation. After the meeting, notes are sent out to all of the participants and relevant documentation is updated (if, for example, new conventions emerge). Code workshops improve code quality I've seen the benefits of this approach firsthand. The most obvious benefit is having a group align their expectations together. Code quality ramps up very quickly when everyone understands what is expected of them and can hold each other responsible for decisions that were made by the group. This is much more effective than sending out a document or wiki page that no one stops to read. Another benefit of this approach is that certain types of bugs become immediately apparent, whereas the same issues can easily be missed by simply reviewing diffs. Looking at the file as a whole means getting a better understanding of what the code is doing (or should be doing). Strange patterns are more easily perceived this way. Perhaps the most important benefit is giving people a forum in which to communicate effectively and safely with one another. It's my firm belief that most code quality problems can be traced back, in one way or another, to poor communication. When communication improves, and expectations are explicit instead of implicit, that's when real change happens. We've been doing code workshops at Box for almost two months and have seen some excellent improvement in how code is being written. Along with the identification of problems that had been missed for a long time, we also have started to formalize conventions that were previously implied, increased understanding of older parts of the code base, and cleaned up a lot of bugs. As a result, we've also added tools to help identify common issues we were seeing (such as adding JSHint for JavaScript). Getting a consensus understanding of what is good and what is bad has made new code less error-prone and more maintainable in the long-run. What's even more important is that engineers don't hate the process. It's an opportunity to do what we love to do, and that's talk about code with each other. Now that we have a way to express ourselves that is non-confrontational and structured with positive outcomes in mind, we're free to continue evolving our discipline and making improvements week over week.",en,98
102,2035,1470916417,CONTENT SHARED,-8992803137960175254,6013226412048763966,716425472356502779,,,,HTML,https://hbr.org/2015/05/4-reasons-managers-should-spend-more-time-on-coaching,4 reasons managers should spend more time on coaching,"There are managers who coach and managers who don't. Leaders in the latter category are not necessarily bad managers, but they are neglecting an effective tool to develop talent. We've been researching managers who coach and what distinguishes them. What has stood out in our interviews with hundreds of managers who do coach their direct reports is their mindset: They believe in the value of coaching, and they think about their role as a manager in a way that makes coaching a natural part of their managerial toolkit. These are not professional coaches. They are line and staff leaders who manage a group of individuals, and they are busy, hard-working people. So why do they so readily give coaching an important place in their schedule? Here are four reasons: They see coaching as an essential tool for achieving business goals . They are not coaching their people because they are nice - they see personal involvement in the development of talent as an essential activity for business success. Most managers will tell you that they don't have the time to coach. However, time isn't a problem if you think coaching is a ""must have"" rather than a ""nice to have."" Whether it's because they are competing for talent, operating in a highly turbulent market place, trying to retain their budding leaders, or aiming to grow their solid players, they believe that they simply have to take the time to coach. There are two assumptions behind this belief. First, that extremely talented people are hard to find and recruit. If you are known as a manager who will help those people thrive, they will gravitate to you. Second, that an organization cannot be successful on the backs of the extremely talented alone. You need solid players just as you need stars, and they will need a manager's help to build skills and deal with the changing realities of their marketplace. They enjoy helping people develop. These managers are not unlike artists who look at material and imagine that something better, more interesting, and more valuable could emerge. They assume that the people who work for them don't necessarily show up ready to do the job, but that they will need to learn and grow to fulfill their role and adapt to changing circumstances. Coaching managers see this as an essential part of their job. They believe that those with the highest potential, who can often contribute the most to a business, will need their help to realize their often-lofty ambitions. As one manager told us recently, ""Isn't helping others to be more successful one of the key roles of a manager?"" The manager must adapt his or her style to the needs and style of each particular individual. This of course takes a good deal of work on the part of the manager, but again, this is perceived as being part of the job, not a special favor. They are curious. Coaching managers ask a lot of questions. They are genuinely interested in finding out more about how things are going, what kinds of problems people are running into, where the gaps and opportunities are, and what needs to be done better. Typically, they don't need to be taught how to ask questions because it's a natural strength. This curiosity facilitates the coaching dialogue, the give-and-take between coach and learner in which the learner freely shares his or her perceptions, doubts, mistakes, and successes so that they together reflect on what's happening. They are interested in establishing connections. As one coaching manager stated, ""That is why someone would listen to me, because they believe that for that time, I really am trying to put myself in their shoes."" This empathy allows the coaching manager to build an understanding of what each employee needs and appropriately adjust his or her style. Some employees might come to coaching with a ""Give it to me straight, I can take it"" attitude. Others need time to think and come to their own conclusions. A trusting, connected relationship helps managers better gauge which approach to take. And coaching managers don't put too much stock in the hierarchy. As a coaching manager recently told us, ""We all have a job to do, we're all important, and we can all be replaced. Ultimately, no one is above anyone else. We just need to work together to see what we can accomplish."" Achieving this mindset is doable. It comes down to whether the business case is sufficiently compelling to motivate a manager to develop a coaching mindset. Managers need to ask themselves a few questions: Does your organization (or group or team) have the talent it needs to compete? If not, why not? Have you done a poor job hiring, or are people not performing up to their potential? It's really either one or the other. If the latter is true, it's your job to help get them to where they need to be. For managers who want to start coaching, one of the first steps is to find someone who is a good coach in your organization and ask her or him to tell you about it. What do they do? Ask why they coach. Listen and learn. Second, understand that before you start coaching, you need to develop a culture of trust and a solid relationship with the people you will be coaching. In spite of your good intentions, all the techniques in the world will make little difference if those you are trying to coach don't feel connected to you in some way. The relationship you develop is more important than the all of the best coaching methods that are available. Third, learn some of the basic principles of managerial coaching that will help you develop your own expertise as a coach. One of the core lessons for managers is that coaching isn't always about telling people the answer. Rather, it is more about having a conversation and asking good, open-ended questions that allow the people you are coaching to reflect on what they are doing and how they can do things differently in the future to improve performance. Finally, the mindset should be focused on the people you are coaching. Always remember the main principle: coaching is about them, not about you.",en,97
103,1014,1463602049,CONTENT SHARED,-4509487968959834430,6013226412048763966,5768100664019329427,,,,HTML,http://hbrbr.com.br/o-valor-de-seu-trabalho-de-coaching-vai-depender-de-suas-habilidades-de-acompanhamento-2/,o valor de seu trabalho de coaching vai depender de suas habilidades de acompanhamento - harvard business review brasil,"Independente do quanto uma sessão de coaching possa parecer bem-sucedida, enquanto em andamento, se ela não levar a uma mudança após ter sido concluída, ele não foi eficaz. Infelizmente é grande o número de gestores que não dão continuidade, desperdiçando o tempo importante que investiram nessa atividade . É possível tornar o processo mais produtivo ao se adotar as seguintes práticas após cada sessão. Utilize essa lista de dicas e perguntas para ajudá-lo a monitorar o progresso de todos de sua equipe, para quem você está atuando como coach. Ela lhe dará subsídios significativos para as reuniões de acompanhamento assim como nos intervalos entre as sessões. Logo após a reunião: Faça anotações. Você não se lembrará de tudo que vê, ouve e pensa sobre o progresso de seus coachees , então anote tudo em um local próprio. Considere também utilizar um template padrão para armazenar essa informação. Boas anotações lhe permitirão fornecer feedback relevante à medida que vai progredindo. Após cada sessão, pergunte-se: O que posso fazer para dar assistência ao desenvolvimento desse coachee no intervalo entre esta sessão de coaching e a próxima? O que aprendi desta sessão que não sabia antes de começar? O que a pessoa para quem estou atuando como coach aprendeu? Quais mensagens-chave foram reforçadas na sessão? De modo contínuo: Se ainda não são, essas tarefas devem se tornar parte da rotina de sua prática de gestão. Reserve um momento em sua agenda para trabalhar nas tarefas que tem maior dificuldade em lembrar ou concluir. Observe sinais de crescimento . A fim de fornecer um feedback relevante, é preciso saber o que está acontecendo. Faça um esforço deliberado para observar os tipos de interações ou tarefas que seus coachees priorizaram em suas sessões de coaching . Se você atuar na empresa, esteja em contato diretamente. Institua uma política explícita de portas abertas que encoraje seus coahees a procurá-lo se tiverem dúvidas. Saber que podem buscar ajuda pode motivá-los a perseverar quando se sentem emperrados. Comunique o impacto . À medida que observa a mudança das pessoas comunique o impacto de seu crescimento de forma explícita. Ouvir diretamente de você aumentará sua motivação (e as deixará satisfeitas). Fique atento a mudanças no relacionamento . Preste atenção à dimensão emocional de suas interações. Se perceber uma mudança que suscite inquietação, intervenha logo. Ainda que não possa fazer nada para ajudar, sua preocupação provavelmente será apreciada. Faça uma auto avaliação . Avalie periodicamente seu próprio desempenho como coach ao se fazer essas duas perguntas: Estou atendendo às necessidades de cada pessoa em minha equipe? Trate o seu papel com uma mentalidade de tentativa e erro e mostre flexibilidade para fazer ajustes no decorrer do percurso. Periodicamente entre em contato direto com todos para quem está atuando como coach para saber o que está funcionando e o que não está - mas confie no seu taco. Estou cumprindo minha obrigação? Coaching é uma via de mão dupla, então seja honesto consigo mesmo em relação a estar ou não atrapalhando o progresso das pessoas ou passando mensagens confusas quanto às suas expectativas. Faça o que puder para tornar todos em sua equipe mais bem-sucedidos. É claro, a tarefa de acompanhamento nunca acaba: oferecer apoio e dar a conhecer e justificar o que se passa são as tarefas perpétuas de um gestor. À medida que se sentir mais à vontade com o coaching , pode ser que não precise mais de uma lista como essa. Mas tenha a lista por perto caso precise se relembrar. Esse artigo foi adaptado a partir da série HBR Guide to Coach da Harvard Business Review.",pt,97
104,2235,1472612022,CONTENT SHARED,9042192299854648021,-1032019229384696495,-7722399387279263781,,,,HTML,https://www.thestreet.com/story/13689830/1/google-s-cloud-platform-is-now-a-force-to-be-reckoned-with.html,google's cloud platform is now a force to be reckoned with,"Though the Google Cloud Platform (GCP) isn't as large and doesn't get as much publicity as Amazon Web Services (AWS) and Microsoft Azure, it's quickly putting its larger rivals on notice with a series of impressive customer wins. And the big investments Alphabet ( GOOGL ) is making in GCP should yield additional big deals in the coming months. CNBC reported this morning Google is ""the front-runner"" to land PayPal ( PYPL ) as a cloud infrastructure client. The online payments giants currently relies heavily on its own infrastructure, but has tapped AWS to provide services for its Braintree and Venmo units. Alphabet and PayPal are holdings in Jim Cramer's Action Alerts PLUS Charitable Trust Portfolio . Want to be alerted before Cramer buys or sells GOOGL or PYPL? Learn more now . The report comes five months after it was learned Apple has begun using GCP to help power iCloud services -- Apple's cloud services also rely on AWS and Azure, as well as the company's own infrastructure. Not long before that, Spotify announced it's migrating its infrastructure to GCP. Other big Google cloud clients include Snapchat, Best Buy ( BBY ) and (thanks to the fact the company is a Google spinoff) Pokémon Go developer Niantic. PayPal's size alone would make it one of Google's most impressive cloud wins -- the company handled $81.5 billion worth of transactions in 2015 (up 23% annually), and claimed 188 million active customer accounts as of the second quarter.",en,96
105,585,1461620499,CONTENT SHARED,8586403905004879205,6013226412048763966,-2657475163512388247,,,,HTML,http://www.cio.com/article/3047154/leadership-management/how-to-augment-your-career-with-leadership-coaching.html,how to augment your career with leadership coaching,"Burnout . Pipeline problems . Harassment . The laundry list of issues for women's leadership in tech is long and the list of solutions seems much shorter. One of the popular pieces of advice for women in tech looking to advance their careers is to find a mentor. Sheryl Sandberg, in her book Lean In , talks about her mentors who encouraged her, challenged her, and advocated for her throughout her career. ARA Mentors , founded by IT recruitment specialist Megan McCann in 2013, has the admirable goal of connecting women in technology to one another through events and one-on-one mentoring sessions. Mentoring is one way to build and maintain community, as well as for women in tech leadership to receive meaningful advice as they advance their careers. However, finding the right fit in a mentor - someone who has the right time, experience, and willingness can be a challenge, even with great programs like ARA in the mix. [ Related: Top 10 U.S. cities for women in technology ] To overcome obstacles for women in tech, women can augment their professional lives and mentoring relationships with one of the most powerful tools for clarifying ambitions and reaching goals: executive coaching. Normally, the context of coaching is organizational; an organization hires a coach for either and at-risk or high-potential employee for development. However, executive coaching may be sought by an individual to increase her leadership skills and gain valuable tools to reach her professional ambitions, and research is starting to show that coaching is highly effective . For more details about executive coaching and its potential impact on women in tech, I interviewed Kelly Ross, who runs a niche coaching and consulting firm, Ross Associates , that focuses on leadership development and talent management. In addition to her professional work, Ross is a certified Hudson Institute Coach , holds an International Coach Federation Professional Certified Coach credential, and teaches at Northwestern University in their graduate coaching program . Corpolongo : Why would a woman in tech want to hire an executive coach? Ross : Hire a coach if you are in a new, bigger role and trying to figure out how you want to show up as a leader; if you are leading a team or organization going through change, especially if you personally find the change a challenge but have to lead others through that same change; if you have received feedback about something that could be professionally limiting and need help sorting out how to take action and change your behavior. Those leaders who are not executives yet, and know they need to build presence and own their stories to make the leap to senior leadership, will find a coach helpful. Corpolongo : Why does executive coaching work? What is your coaching philosophy? Ross : Coaching is about the client and the coach partnering to help the client make the change they desire. The client brings the agenda, [and] the coach manages the process through great questions. That sounds very simple but in my experience great coaching is magical, it helps the client discover answers, make a change that sticks, and understand what has been getting in their way. The client has to want to invest time, energy and money into coaching, it is not going to have an impact if the client isn't clear about what they want to get from the coaching or is not willing to do the work. Many times the breakthrough happens between sessions when something the client and coach discussed ""clicks"" for the client. I customize coaching for each client - some need more process than others. I'll ask you how you want to receive feedback - ""tough love"" or more gently. Many leaders are lonely and appreciate coaching for the thought partnership and the safe space to not have to know every answer immediately. Many of us are our own biggest obstacle and coaching can help you articulate your ""lines in the sand"" and sticking to the boundaries you establish. Coaching is not therapy, which often looks back into the past; coaching is present and future focused. Coaching focuses on understanding patterns and taking action while therapy is about an analysis of the past. Coaching is solution focused. Corpolongo : What do you think are any special considerations for women in technology in their leadership development? Ross : As women, working in technology or elsewhere, we are often hard on ourselves. We don't always invest in ourselves as we might for others. Coaching is personal, it is focused on you being the best version of yourself, and it is one of the best investments in yourself I know of. My own coach is really important to my success as she challenges me, and she supports me. For example, one of my clients, a sales leader at a leading technology organization, wanted to be promoted to senior leadership and needed to increase her presence and own her story. We worked together for a year, meeting every two to three weeks, to articulate her values, build and tell her story, and increase her presence and influence. Alternately, one of my clients is a leader in a tech organization working in product marketing. She used coaching to support her leaving her organization and launching her own business. Initially, we worked together every other week for six months as she envisioned the business she would launch, the skills she had or needed to build, and her plan for making the change from employee to business owner. We are in the midst of our second engagement focusing on the legacy and leadership she wants to leave in her current organization and the ideal life she will create when she launches her business. This article is published as part of the IDG Contributor Network. Want to Join?",en,96
106,2108,1471550601,CONTENT SHARED,-8190931845319543363,-1032019229384696495,-478224908960553328,,,,HTML,https://arc.applause.com/2016/08/17/gartner-hype-cycle-2016-machine-learning/,machine learning is at the very peak of its hype cycle - arc,"Machine learning has earned a prestigious honor in its march to technological relevance ... The peak of its hype cycle. According to the 2016 Gartner Hype Cycle for Emerging Technologies , machine learning is at the very ""peak of inflated expectations,"" the highest point in the S-curve that Gartner awards technologies in its Hype Cycle reports. 2016 has been full of machine learning and artificial intelligence news as companies like Google, Facebook, Microsoft, Intel and IBM announce and release new tools to take advantage of neural networks that can train computers. Many machine learning advocates may feel betrayed to think that machine learning is at the peak of its hype cycle, thinking that analysts like Gartner believe machine learning to be all fluff and no substance. But that is not the case. Many emerging technologies never actually make it to the peak of the hype cycle, fizzling out long before they can make a true impact. In reality, technologies that make it to the peak of the hype cycle are almost ready for universal deployment. Much of the research and development of the technology has already been done and the real implementation (and all the headaches that go with it) is about to begin. That's why technologies fall from the peak of inflated expectations to ""the trough of disillusionment"" as developers go through the painful process of tightening the nuts and bolts to make the technology easier to use for the average person or enterprise. Machine learning will be extremely valuable to enterprises over the next decade thanks to radical computer power and endless data, which will allow companies to adapt quickly to new situations, Gartner said. Immersive experiences such as virtual and augmented reality have already begun a move towards the mainstream, with both consumers and enterprises adopting them within the next five to 10 years. The Hype Cycle Offers A Glimpse Of The Future Companies that invest in immersive experiences, smart machines and ecosystem-enabling platforms are likely to have a significant advantage over their rivals in the very near future. According to Gartner's Hype Cycle, these are the three key technology trends that enterprises and developers need to succeed (or at least be aware of) in the digital economy. The report is the longest running of Gartner's annual hype cycles and is considered to be an accurate barometer as to where technology is heading over the next decade or so. At the same time, the so-called platform revolution will shift the emphasis from a technical infrastructure to a dynamic ecosystem that relies on both internal and external algorithms to drive value, said the report. Companies need to redefine their business strategies to become more dynamic, especially as emerging technologies are redefining how established platforms are used. ""These trends illustrate that the more organizations are able to make technology an integral part of their employees', partners' and customers' experience, the more they will be able to connect their ecosystems to platforms in new and dynamic ways,"" said Gartner research director Mike J. Walker, in a press release. ""Also, as smart machine technologies continue to evolve, they will become part of the human experience and the digital business ecosystem."" All three of the identified trends have been part of the hype cycle for quite some time, so it is not really a surprise to see them featured so prominently in the latest emerging technologies report. Gartner's reports concentrate on five phases of development for a given technology, all of which match expectation with time to mainstream adoption. The phases are a must-know reference point for companies that are engaged in strategic planning. Some technologies are obviously further developed than others, but all are-in theory-capable of giving companies a competitive advantage, said Gartner. The phases are as follows: The Innovation Trigger , where the technology moves from the realms of science fiction to a potential reality. Peak of Inflated Expectations , the period when the technology will be the best thing since sliced bread ... one day. Trough of Disillusionment , which turns excitement into the realization that the innovation is still a long way off. Slope of Enlightenment , or in other words when the innovation is mature enough to provide real-world business uses. Plateau of Productivity , when the technology is actually mature and being used by companies or industries on a regular basis. For example, machine learning currently sits at the top of the Peak of Inflated Expectations, with mainstream adoption expected within two to five years. Advances in neural networks and the ubiquitous availability of big data have made machine learning one of the major concepts in IT, so much so that it has the capacity to transform companies, the report said. See also: Smart Dust May Be The Pinnacle Innovation Of The Internet Of Things And then there is virtual and augmented reality. The technology has been around for decades but has often veered between the Peak of Inflated Expectations and the Trough of Disillusionment. Over the last year, the landscape has changed dramatically, thanks mainly to the fact that virtual reality experiences and hardware have started to flow into the consumer world. The latest hype cycle shows that virtual reality is now firmly ensconced in the Slope of Enlightenment and should reach mainstream adoption in five to 10 years. Augmented reality-which many people consider to be a more viable option-is slowly traversing the trough, with the expectation that this will happen over a similar timeframe. Why Technology Needs To Be Constantly Tracked Other sci-fi technologies from the 2015 version of the hype cycle that were on the upward slope-smart dust, human augmentation, brain-computer interface-are at least ten years from maturity, the report said. Autonomous vehicles , which occupied machine learning's position at the Peak of Inflated Expectations a year ago, have started the slow decline into disillusionment ... irrespective of the current bullish attitude of the auto industry. ""To thrive in the digital economy, enterprise architects must continue to work with their CIOs and business leaders to proactively discover emerging technologies that will enable transformational business models for competitive advantage, maximize value through reduction of operating costs, and overcome legal and regulatory hurdles,"" said Walker. ""This Hype Cycle provides a high-level view of important emerging trends that organizations must track, as well as the specific technologies that must be monitored."" Lead image: "" Machine Learning "" by Flickr user Thomas Hawk , Creative Commons . Stay in the know! Subscribe to ARC and keep up-to-date with a daily or weekly subscription.",en,96
107,2852,1481543955,CONTENT SHARED,-2097075598039554565,3609194402293569455,4506332254516554808,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36",SP,BR,HTML,http://tutorialzine.com/2016/12/the-languages-frameworks-tools-you-should-learn-in-2017/,"the languages, frameworks and tools you should learn in 2017","The Languages, Frameworks and Tools You Should Learn in 2017 Martin Angelov The software development industry continues its relentless march forward. In 2016 we saw new releases of popular languages, frameworks and tools that give us more power and change the way we work. It is difficult to keep track of everything that is new, so at the end of every year we give you our take on what is important and what you should learn during the next twelve months. The Trends Progressive Web Apps In 2016 we saw the rise of the Progressive Web App concept. It represents web applications that work offline and offer a native, app-like experience. They can be added to your smart device's homescreen and can even send you push notifications, bridging the gap with native mobile apps. We think that in 2017 PWA are going to become even more important and are well worth investigating. See our overview here . The Bot Hype Everybody is talking about bots right now. From platforms for running them, to frameworks for building them, the community is buzzing with activity ( read our intro here ). Bots are the new mobile apps, and if you hurry up you can catch the wave while everyone is excited. Once the novelty wears off, bots will probably be relegated to some boring role such as automated customer support. But hey, we can dream! Consolidation of Frontend Frameworks In the JavaScript community we have an incredible churn of frameworks and tools, with new ones being born almost every week. Until recently, the expectation was that the old tools would just be replaced by the new, but this is not what we saw in 2016. Instead, we saw the popular frameworks exchanging ideas and incorporating the innovations put forth by newcomers. So in 2017 it won't matter much which of the major JS frameworks you choose, their features are mostly comparable. The Cloud Companies and developers everywhere are embracing ""the cloud"". This is virtualized computer infrastructure that is available on demand and fully configurable from a control panel. The big three cloud providers are AWS, Google Cloud and Azure. Thanks to their ongoing competition prices have been falling, bringing it within the budgets of smaller companies and individual developers. Familiarizing yourself with the cloud workflow would be a good investment for 2017. Machine Learning Machine Learning (ML) has exploded in popularity during the last twelve months. And with the historic AlphaGo vs Lee Sedol match in March, it entered the mainstream. Smart computer systems that learn from raw data are revolutionizing the way we interact with our mobile devices. By the looks of it, ML will be an even bigger factor in 2017. Languages JavaScript continues its incredible pace of innovation. Catalyzed by the quick release schedules of web browsers, the JS standard is updated every year. The next edition, ES2017 , is expected to be finalized in mid 2017. It will bring the dream feature of many JS developers - аsync/аwait for working with asynchronous functions. And thanks to Babel , you can write ES2017 in every browser even today. TypeScript 2.1 was released in late 2016, bringing async/await for old browsers and improved type inference. TypeScript is a statically typed language which compiles to JavaScript. It adds powerful features like a classic OOP model and optional static typing to make large codebases easier to maintain. It is the preferred language for writing Angular 2 apps, and we recommend giving it a try. Here is our quick start guide about it. C# 7.0 is expected in 2017 and will enhance an already excellent language. Microsoft surprised everyone when they introduced the open source Visual Studio Code editor and .Net Core. Both of these run on Linux, Windows and macOS and allow you to write fast and performant applications in C# (read more here ). A vibrant community is forming around both of these tools, and we are confident there is an exciting year ahead of them. Ruby 2.3 was released earlier this year with a number of performance improvements. Ruby is also a good choice as a general purpose scripting language, but it shines when paired with Rails. The Ruby 3×3 initiative was announced, which will attempt to make the upcoming Ruby 3 release 3 times faster that the current version, opening the doors to using Ruby in more contexts. If you are looking for something more exciting, you can try out Crystal and Elixir , which both combine a friendly ruby-like syntax with superior performance. Or you can look into a functional language like Haskell or Clojure . Two other fast languages are Rust and Go which we recommend. Learn one or more of these: JS (ES2017), TypeScript, C#, Python, Ruby, PHP7, Java/Kotlin/Scala. Frontend The web platform made two major advancements recently - Web Assembly and Service Workers . They open the gates for fast and performant web applications that bridge the gap with native compiled applications. Service Workers in particular are the enabling technology for Progressive Web Apps and bring support for Notifications to the web platform, with more APIs to follow in the future. Angular.js 2 was released this year. The framework is backed by Google and is very popular with enterprises and large companies. It has a vast number of features that make writing everything from web to desktop and mobile apps possible. The framework is written in TypeScript, which is also the recommended language to write applications in. There is a lot to read about, but we think learning Angular 2 in 2017 would be a good investment. Ember is another solid choice for a JavaScript framework. It supports data bindings, auto-updating templates, components and server-side rendering. One benefit that it has over its competitors, is that it is more mature and stable. Breaking changes are much less frequent and the community values backwards compatibility. This makes the framework a good choice for long-lived applications. Two other frameworks that are worth a look are Aurelia and React . The ecosystem around React has grown considerably more complicated in the last year, making it difficult to recommend for beginners. But experienced devs can combine the library with GraphQL , Relay , Flux and Immutable.js into a comprehensive full stack solution. No frontend compilation would be complete without mentioning Bootstrap . Version 4 is currently in Alpha and a release is expected in 2017. Notable changes are the new versatile card component and the flexbox grid (see our comparison with the regular grid here ), which modernize the framework and make it a joy to work with. SASS and LESS remain the two most popular CSS preprocessors today. Although vanilla CSS is finally getting support for variables, SASS and LESS are still superior with their support for mixins, functions and code organization. If you haven't already, take a look at our SASS and LESS quick start guides. Learn one or more of these: Angular 2, Vue.js, Ember, Bootstrap, LESS/SASS. Backend There is plenty of choice for the backend, all coming down to your preference of a programming language or specific performance needs. An ongoing trend in web development is business logic to move away from the backend, turning that layer into an API which is consumed by the frontend and mobile apps. But a full stack framework is often simpler and faster to develop in, and is still a valid choice for a lot of web apps. Node.js is the primary way for running JS outside the browser. It saw many new releases this year which increased performance and added coverage for the entire ES6 standard. Node has frameworks for building fast APIs, servers, desktop apps and even robots, and a vast community creating every kind of module imaginable. Some frameworks that you may like to look into: Express , Koa , Next , Nodal . PHP is a web language first and foremost, and has a large number of web frameworks to choose from. Thanks to its excellent documentation and futures, Laravel has formed an active community around it. Zend Framework released version 3 which marks a great upgrade for this business oriented framework. Symfony also saw a lot of new releases this year, making it an even better choice as a full stack solution. Python has its own full stack/minimal framework combo in the form of Django and Flask . Django 1.10 was released in August introducing full text search for Postgres and an overhauled middleware layer. For the enthusiasts there is also Phoenix , which is written in Elixir and attempts to be a feature complete alternative to Rails with superior performance. If Elixir is one of the languages you would like to learn in 2017, give Phoenix a try. Learn one of these: A full stack backend framework, a micro framework. Databases MySQL 8.0 is going to be the next major release of the database. It is expected sometime in 2017 and it will bring a lot of improvements to the system. MySQL is still the most popular database management system and the entire industry benefits from these new releases. PostgreSQL 9.6 was released in September. It brought better full text search and sped up the database system with parallel queries and more efficient replication, aggregation, indexing and sorting. Postgres is used for massive, terabyte scale datasets, as well as for busy web apps, and these optimizations are welcome. For NoSQL fans, we can recommend CouchDB . It is a fast and scalable JSON storage system which exposes a REST-ful HTTP API. The database is easy to use and offers great performance. PouchDB is a spiritual counterpart to CouchDB that works entirely in the browser and can sync with Couch. This allows you to use Pouch in an offline ready web app, and get automatic syncing once internet connectivity is available. Redis is our favorite key value store. It is small, fast and versatile. You can use it as a smart memcache alternative, as a NoSQL data store or a process messaging and synchronization channel. It offers a large number of data structures to choose from, and the upcoming 4.0 release will have a module system and improved replication. Learn one of these: MySQL, Postgres, CouchDB, Redis. Tools Yarn is an alternative package manager for Node.js which is developed by Facebook. It is an upgrade over the npm command line tool and provides faster installs, better security and deterministic builds. It still uses the npm package registry as its backend, so you have access to the same incredible ecosystem of JavaScript modules. Yarn is compatible with the package.json format that npm uses, and is just a quick install away. The two most popular open source code editors - Visual Studio Code and Atom have seen an incredible amount of innovation in the past 12 months. Both of these projects are built using web technologies and have attracted huge communities of fans. The editors have plugins available which bring syntax checking, linting and refactoring tools for a large number of languages. Git is the most popular source code version control system out there. It is serverless and you can turn any folder on your computer into a repository. If you wish to share code, you have many options like GitLab , Bitbucket and Github , to name a few. For 2017 we suggest that you familiarize yourself with the git command line , as it will come in handy more times than you think. Desktop applications are not dead yet. Even though web apps are becoming more and more capable, sometimes you need powerful capabilities and APIs that are simply not available to the web platform. With tools like Electron and NW.js you can write desktop applications by using web technologies. You get full access to the operating system and the breadth of modules available to npm. To learn more about these tools, read our tutorials about Electron and NW.js . A recent trend in software team organization is to have developers who are in charge of their own software deployment. Also called DevOps, this leads to quicker releases and faster fixes of issues in production. Developers with operations experience are highly valued by companies, so familiarity with the technologies that enable it is going to be a huge plus from now on. Some of the tools that we recommend are Ansible and Docker . Experience with the Linux command line and basic system administration skills will also serve you well. Try out one or more of these: Yarn, Git, Visual Studio Code, Electron, Ansible, Docker. Tech The cloud has won over the entire software industry, with large companies closing down their datacenters and moving their entire infrastructure there. The three main platforms are AWS , Google Cloud and Azure . All three have powerful, ever expanding feature sets, including virtual machines, hosted databases, machine learning services and more. Prices are going down rapidly, and the cloud is within reach of small companies and individual developers. For 2017, it would be a good learning experience to deploy a side project to one of these providers. Artificial Intelligence was the buzzword of 2016. Speech recognition and image classification are only two of the user facing applications of the technology, with machines reaching and even surpassing human level performance. There are a lot of startups that apply AI and Machine Learning to new domains. And a lot of open source projects were released like Google's Tensor Flow and Microsoft's Cognitive Toolkit . Machine Learning is a very math-heavy topic, and for those just starting out there are comprehensive online courses available. Virtual Reality (VR) and Augmented Reality (AR) have been around for a while, but finally the technology is mature enough to offer a compelling experience. Facebook ( Oculus Rift ), Google ( Daydream ) and Microsoft ( Windows Holographic ) all have virtual reality platforms that welcome third party developers. VR headsets still face challenges like eliminating nausea and offering compelling use cases outside of gaming, but they are getting there. Learn one of these: Cloud deployment, a Machine Learning library, VR Development. by Martin Angelov Martin is a web developer with an eye for design from Bulgaria. He founded Tutorialzine in 2009 and it still is his favorite side project.",en,95
108,2007,1470659231,CONTENT SHARED,-78066964941874046,7645894863578715801,55662953596428738,,,,HTML,http://testdetective.com/microservices-testing/,microservices testing,"Modern software engineering is all about scalability, product delivery time and cross-platform abilities. These are not just fancy terms - since internet boom, smartphones and all this post-pc era in general, software development turns from monolith, centralised systems into multi-platform applications, with necessity of adjust to constantly changing market requirements. Microservices quickly becames new architecture standard, providing implementation independence and short delivery cycles. New architecture style creates also new challenges in testing and quality assurance. In this article I would like to outline strategies for testing microservices architecture. Most important thing is to understand difference between specific testing layers and their complementarity. Why microservices? What's the architecture of typical, corporate IT system in sectors like financial services, insurances or banking? Typically it's monolith-like architecture, based on single, relational database and SOAP API. One system is responsible for many domains and contexts. Those kind of systems have been working on production for years, and big companies are not willing to risk and change their proven-in-battle architecture. Although, the market requirements are constantly changing, so are technical requirements. Monolith architecture falls short when it comes to scalability or continuous deployment, not to mention technology stack commitment or single point of failure. Here comes microservices ! Are they a silver bullet for all your architectural problems? Well, no - they also have their own issues. What sets them apart though, is ability to quickly adjust your architecture to constantly changing requirements through independent development, deployment and scaling. In simple words, microservices are small, independent applications with single-domain responsibility. Those services communicate between each other through HTTP, usually with use of REST protocol. Microservice architecture is not just a new buzz word. World leading tech companies, like Amazon, Netflix or Twitter base their key system architecture on microservices. Even more significant is that not only new systems are build with use of microservices, but companies also rewrite their old, proven solutions into new architecture. Testing strategies New approach to architecture requires new approach to testing and quality assurance. Focusing mainly on automated testing, we can divide specific layers of tests. Important thing to notice here is that working with single layer is insufficient, and due to their complementarity, you should provide all of them to your project. unit tests - those are nothing new, and nowadays almost everyone understands advantages of TDD approach. In general, unit tests are automatic checks that provide us a feedback whether our implementation complies with the requirements. Unit tests check only system components in the smallest pieces, like single methods, with other dependencies mocked. Unit tests are great when it comes to fast, nearly continuous feedback of our implementation correctness in smallest granulation - methods or single system components. integration tests - as we said, unit tests check single components in isolation. When method A is dependent on method B , we would mock method B in unit test of method A , since we want our test to be deterministic and single-responsible. Although it's desirable at this level, we can have a situation where two system components, working correct in isolation, don't meet functional requirements when integrated together. There we introduce integration tests . Those are ones that check a number of system components or methods working together. For example, if we want to test an endpoint at integration level, we'd send a request, see if service layer done its job and validate a response. Remember that we are in microservices world and our system functionalities depends on few applications communication. So, if the endpoint that we're testing triggers client call to external service, should we let our tests be dependent of application owned by someone else? No - at the level of integration tests, we want to test only one application's components, with external services mocked out. contract testing - microservices architecture depends on communication between services. Although internal implementation of services is independent, interface and API must remain consistent. When we design our service and expose it to the world, we define a contract for our API. Let's imagine hypothetical situation: we're building a service for users registration. Before we store new user in our database, we check if user's ID number does't appear on fraud list. We integrate for this with external service, that we do not owned. Now, if someone - intentionally or not - breaks the contract in validation service, our registration functionality stops working correctly, although we haven't done any changes. Our integration tests where green obviously, since they work on external validation service mocks. Contract tests are automatic checks to assure, that the contract we've agreed on is still preserved. Idea is simple, but how to do that in practice? We depend here largely on the tool that we're using. Worth to mention are tools like Pact or Spring-Cloud Contract - they provide DSL for mocking service on the client side, and interaction playback and verification on server side. end-to-end tests - also known as functional tests, e2e tests are known and used probably even longer than unit tests. In world of microservices, they are much more complex subject though, since our functionalities are based on integration between many application and services. We do not want to mock anything at this level, so our tests are dependent on the state of specific applications. Furthermore, in distributed architecture we change system calls, known from monolithic applications, to network calls. This comes with all the network issues, like timeouts or packets loss. Last but not least, debugging tests with lots of dependencies to external services can be challenging. Nevertheless, end-to-end tests are crucial for our projects quality. To minimize our e2e testing efforts in microservices architecture, we should follow well know, yet rarely kept rules: First of all, functional tests are ones that gives you feedback of key functionalities from your user perspective. That kind of tests tend to be difficult to maintain and have long execution time. Therefore, the number of them should be limited. If some functionality can be tested on lower level - in unit or integration tests - it should be moved there. Keeping user perspective doesn't mean you should build your e2e framework with WebDriver-based libraries. Selenium has many advantages, but it falls short in terms of maintenance cost or execution time. Since you do microservices, majority of user actions can be simulated with service requests, which are not only faster but also more stable. Your test environments should be defined as a code. We don't want to be in situation where test results are dependent to test data or environment state. If you run your functional test suite once a day, good practice would be to build environment from scratch before test execution starts. Obviously, that should be done automatically, with use of tools like , or . Going beyond Test layers I've mentioned, are just core aspects of automated testing. Having vast of our test scope automated, leaves us with time to perform complex manual testing. You can do either exploration or context-driven testing. Another key aspect of quality assurance in microservices are performance tests. Your system architecture depends on distributed network calls, so you should put a great attention to performance of your application. There is a common question, whether you should load-test single endpoints in isolation, or whole functionalities, simulating chain of network calls. In my experience, both of approaches are important. Testing single endpoints gives you knowledge of some technical details of your implementation and lets you to profile service in isolation. Testing chain of service calls draws your attention to performance-bottlenecks of your system and lets you know where scalability effort should be put. Summary Testing in microservices architecture can be more challenging than in traditional, monolithic architecture. In combination with continuous integration and deployment, it's even more complex. It's important to understand layers of tests and how they differ from each other. Putting effort on automation aspect of your tests, doesn't mean you shouldn't drop on manual testing. Only combination of various test approaches gives you confidence in product quality.",en,95
109,1945,1470051678,CONTENT SHARED,2708089973817733462,7645894863578715801,8873526859008375851,,,,HTML,https://blog.frankel.ch/dont-talk-about-refactoring-club/,you don't talk about refactoring club,"The first rule of Fight Club is: You do not talk about Fight Club. The second rule of Fight Club is: You do not talk about Fight Club. I guess the same could be said about refactoring. That would first requires to define what I mean by refactoring in the context of this post : Refactoring is any action on the codebase that improves quality. Which in turn requires to define what is quality. Everyone I've talked with agrees on this: it's quite hard to do. Let's settle for now for the following tentative explanation: Quality is a feature of the codebase (including and not limited to architecture, design, etc.) which the lack of stalls further meaningful changes in the codebase. At the limits: 100% quality means changing the codebase to develop a new feature would require the smallest possible time; 0% quality means the time to do it would be infinite. Given this definition, refactoring includes: Now back to the subject of this post. Should we ask the customer/manager if a refactoring is necessary? Should we put a refactoring sprint in the backlog? I've witnessed first hand many cases where it was asked. As expected, in nearly all cases, the decision was not to perform the refactoring. Taking ages to implement some feature? No design change. Not enough test harness? No tests added. Why? Because the customer/manager has no clue what refactoring and quality means. Let's use a simple analogy: when I take my car to the mechanic, do I get to choose whether he'll check if the repairs have been correctly executed? Not at all. Checks are part of the overall package I get when I choose a professional mechanic. If choice was made possible, some people who probably opt not to do the checks - to pay less. So far, so good. But then if trouble happened, and probability is in favor of that, the mechanic would be in deep trouble. Because he's the professional and didn't do his job well. Developers would also get into trouble if they delivered applications with no tests or with a messy codebase; not their customer nor their manager - especially not their managing (depending on their kind of manager if you catch my drift). So I wonder why developers have to let people that don't know about code taking such important decisions. As a professional developer, you and no one else are responsible for the quality of the application you deliver. Your name is in the source code and the commit history, not your manager's. Stop searching for excuses not to refactor: don't ask, do it. Refactoring is part of the software development package, period. Please enable JavaScript to view the",en,95
110,672,1461893850,CONTENT SHARED,2280365999288629014,3891637997717104548,-7920053441420858215,,,,HTML,https://www.linux.com/news/best-linux-distros-2016,the best linux distros of 2016,"2015 was a very important year for Linux, both in the enterprise as well as in the consumer space. As a Linux user since 2005, I can see that the operating system has come a long way in the past 10 years. And, 2016 is going to be even more exciting . In this article, I have picked some of the best distros that will shine in 2016. Best Comeback Distro: openSUSE SUSE, the company behind openSUSE, is the oldest Linux company; it was formed just a year after Linus Torvalds announced Linux. The company actually predates Linux king Red Hat. SUSE is also the sponsor of the community-based distro openSUSE . In 2015, openSUSE teams decided to come closer to SUSE Linux Enterprise (SLE) so that users could have a distribution that shares its DNA with the enterprise server -- similar to CentOS and Ubuntu. Thus, openSUSE became openSUSE Leap , a distribution that's directly based on SLE SP (service pack) 1. The two distros will share the code base to benefit each other -- SUSE will take what's good in openSUSE and vice versa. With this move, openSUSE is also ditching the regular release cycle, and a new version will be released in sync with SLE. That means each version will have a much longer life cycle. As a result of this move, openSUSE has become a very important distribution because potential SLE users can now use openSUSE Leap. That's not all, however; openSUSE also announced the release of Tumbleweed, a pure rolling-release version. So, now, users can use either the super-stable openSUSE Leap or the always up-to-date openSUSE Tumbleweed. No other distro has made such an impressive comeback in my memory. Most Customizable Distro: Arch Linux Arch Linux is the best rolling-release distribution out there. Period. Ok, I could be biased because I am an Arch Linux user. However, the reason behind my claim is that Arch excels in many other areas, too, and that's why I use it as my main operating system. Arch Linux is a great distro for those who want to learn everything about Linux. Because you have to install everything manually, you learn all the bits and pieces of a Linux-based operating system. Arch is the most customizable distribution. There is no ""Arch"" flavor of any DE. All you get is a foundation and you can build whatever distro want, on top of it. For good or for worse, unlike openSUSE or Ubuntu there is no extra patching or integration. You get what upstream developers created. Period. Arch Linux is also one of the best rolling releases. It's always updated. Users always run the latest packages, and they can also run pre-released software through unstable repositories. Arch is also known for having excellent documentation. Arch Wiki is my to-go resource for everything Linux related. What I like the most about Arch is that is offers almost every package and software that's available for ""any"" Linux distribution, thanks to the Arch User Repository, aka AUR. Best-Looking Distro: elementary OS Different Linux distributions have different focus areas -- in most cases, these are technical differences. In many Linux distributions. the look and feel is an afterthought -- a side project at the mercy of the specific desktop environment. elementary OS is trying to change all that. Here, design is at the forefront, and the reason is quite obvious. The distro is being developed by designers who have made their name in the Linux world by creating beautiful icons. elementary OS is quite strict about the holistic look and feel. The developers have created their own components, including the desktop environment. Additionally, they choose only those applications that fit into the design paradigm. One can find heavy influence of Mac OS X on elementary OS. Best Newcomer: Solus Solus operating system has garnered quite a lot of attention lately. It's a decent-looking operating system that has been created from scratch. It's not a derivative of Debian or Ubuntu. It comes with the Budgie desktop environment, which was built from scratch but aims to integrate with Gnome. Solus has the same minimalistic approach as Google's Chrome OS. I have not played with Solus much, but it does look promising. Solus is actually not a ""new"" OS. It has been around for a while in different forms and names. But the entire project was revived back in 2015 under this new name. Best Cloud OS: Chrome OS Chrome OS may not be your typical Linux-based distribution because it's a browser-based operating system for online activities. However, because it's based on Linux and its source code is available for anyone to compile, it's an attractive OS. I use Chrome OS on a daily basis. It's an excellent, maintenance-free, always updated OS for anyone using a computer purely for web-related activities. Chrome OS, along with Android, deserves all the credit for making Linux popular in the PC and mobile space. Best Laptop OS: Ubuntu MATE Most laptops don't have very high-end hardware, and if you are running a really resource-intensive desktop environment then you won't have much system resources or battery life at your disposal -- they will be used by the OS itself. That's where I found Ubuntu MATE to be an excellent operating system. It's lightweight, yet has all the bells and whistles needed for a pleasant experience. Thanks to its lightweight design, the majority of system resources are free for applications so you can still do some heavy work on it. I also found it to be a great distro on really low-end systems. Best Distro for Old Hardware: Lubuntu If you have an old laptop or PC sitting around, breathe new life into it with Lubuntu . Lubuntu uses LXDE, but the project has merged with Razor Qt to create LXQt. Although the latest release 15.04 is still using LXDE, the future versions will be using LXQt. Lubuntu is a decent operating system for old hardware. Best Distro for IoT: Snappy Ubuntu Core Snappy Ubuntu Core is the best Linux-based operating system out there for Internet of Things (IoT) and other such devices. The operating system holds great potential to turn almost everything around us into smart devices -- such as routers, coffeemakers, drones, etc. What makes it even more interesting is the way the software manages updates and offers containerization for added security. Best Distro for Desktops: Linux Mint Cinnamon Linux Mint Cinnamon is the best operating system for desktops and powerful laptops. I will go as far as calling it the Mac OS X of the Linux world. Honestly, I had not been a huge fan of Linux Mint for a long time because of unstable Cinnamon. But, as soon as the developers chose to use LTS as the base, the distro has become incredibly stable. Because the developers don't have to spend much time worrying about keeping up with Ubuntu, they are now investing all of their time in making Cinnamon better. Best Distro for Games: Steam OS Gaming has been a weakness of desktop Linux. Many users dual-boot with Windows just to be able to play games. Valve Software is trying to change that. Valve is a game distributor that offers a client to run games on different platforms. And, Valve has now created their open operating system -- Steam OS -- to create a Linux-based gaming platform. By the end of 2015, partners started shipping Steam machines to the market. Best Distro for Privacy: Tails In this age of mass surveillance and tracking by marketers (anonymous tracking for targeted content is acceptable), privacy has become a major issue. If you are someone who needs to keep the government and marketing agencies out of your business, you need an operating system that's created -- from the ground up -- with privacy in mind. And, nothing beats Tails for this purpose. It's a Debian-based distribution that offers privacy and anonymity by design. Tails is so good that, according to reports, the NSA considers it a major threat to their mission. Best Distro for Multimedia Production: Ubuntu Studio Multimedia production is one of the major weaknesses of Linux-based operating systems. All the professional-grade applications are available for either Windows or Mac OS X. There is no dearth of decent audio/video production software for Linux, but a multimedia production system needs more than just decent applications. It should use a lightweight desktop environment so that precious system resources -- such as CPU and RAM -- are used sparingly by the system itself, leaving them for the multimedia applications. And, the best Linux distribution for multimedia production is Ubuntu Studio . It uses Xfce and comes with a broad range of audio, video, and image editing applications. Best Enterprise Distro: SLE/RHEL Enterprise customers don't look for articles like these to choose a distribution to run on their servers. They already know where to go: It's either Red Hat Enterprise Linux or SUSE Linux Enterprise . These two names have become synonymous with enterprise servers. These companies are also pushing boundaries by innovating in this changing landscape where everything is containerized and becoming software defined. Best Server OS: Debian/CentOS If you are looking at running a server, but you can't afford or don't want to pay a subscription fee for RHEL or SLE, then there is nothing better than Debian or CentOS . These distributions are the gold standard when it comes to community-based servers. And, they are supported for a very long time, so you won't have to worry about upgrading your system so often. Best Mobile OS: Plasma Mobile Although the Linux-based distribution Android is ruling the roost, many in the open source community, including me, still desire a distribution that offers traditional Linux desktop apps on mobile devices. At the same time, it's better if the distro is run by a community instead of a company so that a user remains in the focus and not the company's financial goals. And that's where KDE's Plasma Mobile brings some hope. This Kubuntu-based distribution was launched in 2015. Because the KDE community is known for their adherence to standards and developing stuff in public, I am quite excited about the future of Plasma Mobile. Best Distro for ARM Devices: Arch Linux ARM With the success of Android, we are now surrounded by ARM-powered devices -- from Raspberry Pi to Chromebook and Nvidia Shield. The traditional distros written for Intel/AMD processors won't run on these systems. Some distributions are aimed at ARM, but they are mostly for specific hardware only, such as Raspbian for Raspberry Pi. That's where Arch Linux ARM (ALARM) shines. It's a purely community-based distribution that's based on Arch Linux. You can run it on Raspberry Pi, Chromebooks, Android devices, Nvidia Shield, and what not. What makes this distribution even more interesting is that, thanks to the Arch User Repository (AUR), you can install many applications than you may not get on other distributions. Conclusion I was astonished and amazed when I worked on this story. It's very exciting to see that there is something for everyone in the Linux world. It doesn't matter if the year of the desktop Linux never arrives. We are happy with our Linux moments!",en,94
111,1435,1466187150,CONTENT SHARED,-447851796385928420,1895326251577378793,-4339659529018274221,,,,HTML,http://www.mobiletime.com.br/17/06/2016/flyhelo-app-permite-compartilhamento-de-jatos-e-helicopteros-em-sao-paulo/442262/news.aspx,mobile time - flyhelo: app permite compartilhamento de jatos e helicópteros em são paulo,"FlyHelo: app permite compartilhamento de jatos e helicópteros em São Paulo O conceito de economia compartilhada chegou à classe alta. Nasceu este ano em São Paulo a FlyHelo, empresa que propõe o compartilhamento de voos de helicóptero e de jato, conceito conhecido como ""coflying"", e que já faz sucesso na Europa e nos EUA. Através de um app ( Android , iOS ), o passageiro pode criar um voo e disponibilizar assentos para que outras pessoas dividam o custo da viagem com ele, ou comprar um assento em um voo já planejado. O pagamento é feito in-app, via cartão de crédito. ""Nosso público-alvo são executivos de 25 a 55 anos que trabalham muito e para quem tempo é dinheiro. São pessoas que não querem gastar cinco ou seis horas para chegar em Ilhabela para passar o fim de semana. De helicóptero demora apenas 30 minutos. E com a venda de assentos fica muito mais atrativo. Queremos conquistar um público desse segmento que nunca voou de helicóptero ou jato"", explica o CEO da FlyHelo, Hadrien Royal. O executivo relata que a maior parte da demanda se concentra nos finais de semana. Os passageiros costumam viajar na sexta no fim do dia ou sábado de manhã, e retornam no domingo à tarde ou segunda de manhã. Os destinos mais comuns partindo de São Paulo são Juqueí, Ilhabela, Angra dos Reis e Campos do Jordão. A criação de um voo pode ser feita pelo app com até duas horas de antecedência. ""Para trechos de curta distância ou para onde não há aeroporto, usamos helicópteros. Quando for acima de 400 Km, recomendamos jatos"", diz Royal. A FlyHelo não tem uma frota própria, mas parcerias com empresas de táxi aéreo com as quais provê opções de voos a partir de Congonhas e do Campo de Marte, no caso dos jatos, ou de vários pontos da capital paulista, no caso de helicópteros. O serviço começa agora a ser testado também no Rio de Janeiro, onde vai operar a partir do Santos Dumont e do aeroporto de Jacarepaguá. A FlyHelo trabalha apenas com empresas homologadas pela Anac, informa o executivo. A criação de um voo São Paulo-Juqueí custa R$ 4.990. E um assento para esse mesmo trecho, R$ 1.490. Dependendo do modelo, o helicóptero pode ter de três a seis lugares. Os jatos, de quatro a sete assentos. O serviço está disponível há um mês. Mais de 50 voos já foram realizados. A meta é chegar a mais de 500 voos mensais após 12 meses de operação.",pt,94
112,1231,1464897148,CONTENT SHARED,7414483722019578252,7645894863578715801,5662638437112457499,,,,HTML,http://greglturnquist.com/2016/05/good-developers-take-breaks.html,good developers take breaks * greetings programs,"Something that has become crystal clear since I joined the Spring team is how important it is to take a break. Good code happens when developers take breaks. Of course there are times when I find myself working solid until 8:00 pm or later. But I try to make that the exception rather than the norm. Most of the time, I'm pulled away by my kids coming home from school. In fact, I'm often annoyed at stopping a minute early. And yet I'm astonished at how many perplexing problems I've solved by simply stopping, doing something else , and **BAM**, the answer pops into my head. Sometimes within minutes, sometimes the next morning while putting shoes on one of my kids. Those are the moments when I remind myself that developers take breaks. I consider that payoff for having stopped early. This phenomena is well know. In fact, you've probably heard of it. I also so it addressed keenly at a writer's conference last year . Your subconscious is still working on the problem whether you are or not. Sometimes, when you aren't actively trying to fix it, your noodle is freed up to look back at what you did, what you've read, and other things. While chatting with a teammate of mine at DevNexus , he expressed that if he hadn't taken various breaks, gone for walks and thought about the architecture he was designing, the project he was striving to build would never have happened. Reflection is a critical component. My martial instructor often taught ""visualize, visualize, visualize."" It's a mechanism to put your mind on the subject, even when you're not actively pursuing it. The key thing is telling yourself to take that break. To pause. To stop and not burn the candle at both ends. If you work that hard, your subconscious will be on the ropes. Those extra ideas that can often succeed when you may have failed all day, may not come at all. It's a big leap I know, but give it a try.",en,94
113,1423,1466112806,CONTENT SHARED,-9019582414165805420,-1799631734242668035,-3130580864866225113,,,,HTML,http://exame.abril.com.br/tecnologia/noticias/bradesco-e-visa-anunciam-pulseira-que-substitui-cartao,bradesco e visa anunciam pulseira que substitui cartão | exame.com,"São Paulo - Você não vai mais precisar do seu cartão bancário para realizar pagamentos no débito no futuro. Ao menos, é isso que a Visa e o Bradesco querem e, por isso, anunciam hoje uma pulseira de pagamentos que funciona em diversos terminais comuns em pontos de vendas. O aparelho é chamado Pulseira Bradesco Visa e realiza a transferência monetária via NFC, uma tecnologia de comunicação por proximidade que troca dados criptografados com a maquininha de débito. O novo gadget de pagamentos será testado no Brasil durante as Olimpíadas do Rio. No país, mais de um milhão e meio de pontos de vendas têm suporte ao NFC, incluindo todos os 4 mil terminais nos Parques Olímpicos. ""O cartão vai migrar para uma série de outros fatores, para o e-commerce, o mobile commerce, o contactless e para os wearables"", afirmou Percival Jatobá, vice-presidente de produtos da Visa. O processo de transferência com a pulseira em nada se difere de um realizado com um cartão de plástico com chip, segundo as empresas. Na hora de pagar, o consumidor encosta o gadget na maquininha e digita sua senha, caso sua compra exceda o valor de 50 reais. Em compras de menos de 50 reais, não é preciso informar a senha do seu cartão. A pulseira é uma solução útil para quem quer sair para correr e não quer levar o smartphone, cartões ou dinheiro. Maleável e com encaixe de dois pontos, o acessório tem design confortável para o uso diário, apesar de apenas uma cor, a azul, estar disponível nessa fase inicial. A recarga será feita por meio de um aplicativo para smartphones Android e iOS, que é chamado Pulseira Bradesco Visa. É possível transferir o dinheiro via boleto ou qualquer cartão bancário. O consumidor tem um número de ID e cartão viculados à sua conta na pulseira, mas não é preciso ter conta no banco Bradesco para usar o acessório. Neste primeiro momento, cerca de 3 mil pessoas selecionadas irão testar a Pulseira Bradesco Visa. O intuito é que elas ofereçam informações de usabilidade que serão analisadas para um lançamento mais amplo do produto. A pulseira de pagamentos ainda não tem previsão de ser oferecida a todos. Confira abaixo uma breve demosntração de um pagamento realizado com a Pulseira Bradesco Visa.",pt,92
114,2310,1473628002,CONTENT SHARED,-7518373517401484139,-4465926797008424436,-4331427112707643376,,,,HTML,https://m.signalvnoise.com/my-favorite-people-and-resources-to-learn-android-programming-from-293f249e2b4e?gi=58d339366f0d,my favorite people and resources to learn android programming from,"���� Twitter I've really enjoyed following these Android community members on Twitter. These folks aren't just knowledgeable teachers and key open-source contributors. They're also positive-minded, hopeful, and friendly. Those qualities are just as important to me as being an expert in the area. Chiu-Ki Chan  - A devoted learner and teacher, Chiu-Ki does it all. She interviews folks , runs 360|AnDev , teaches on Caster , speaks , draws , writes , and probably does 100 other things I don't know about. �� Donn Felker   - Not only an Android GDE, Donn's got a great blog full of helpful posts. He's also half of the Fragmented Podcast along with Kaushik Gopal (who's pretty sharp in his own right). And if that weren't enough, Donn's also the head honcho at Caster.io , a fantastic site for video tutorials. Jake Wharton   - Honestly, if you don't know who Jake is, you might be in the wrong place. Just go here now. �� Kristin Marsicano   -   An instructor at Big Nerd Ranch, Kristin has a wonderful down-to-earth vibe and is clearly a great teacher. Her recent talk at 360|AnDev on the activity lifecycle is a great refresher for something you probably don't think about enough. Ryan Harter   - Ryan's a GDE who's been teaching a lot lately about how to reduce boilerplate code . He also helps run GDG Chicago West and is an instructor at Caster . The Practical Dev -OK, this isn't technically Android specific. But it's such an informative and entertaining commentary on programming, I had to include it. Sometimes reading general programming posts can be really enlightening (and hilarious). (Note: It'd be impossible to write about every single person who's a great Android teacher, but you can find more on this extended Twitter list that I'll keep adding to.)",en,92
115,1532,1466955705,CONTENT SHARED,-5625593730080264433,-1032019229384696495,-2318276786697086259,,,,HTML,https://techcrunch.com/2016/06/19/the-next-wave-in-software-is-open-adoption-software/,the next wave in software is open adoption software,"There's a big shift happening in how enterprises buy and deploy software. In the last few years, open technology - software that is open to change and free to adopt - has gone from the exception to the rule for most enterprises. We've seen the IT stack redrawn atop powerful open-source projects, with developers opting for an ""open-first"" approach to building solutions. More than 78 percent of enterprises run on open source and fewer than 3 percent indicate they don't rely on open software in any way, according to a recent market survey by Black Duck Software. Openness is a near truism in the Valley, but today projects like Hadoop, Cassandra, Docker and Mule are infiltrating even the most conservative and dogmatic organizations. As such, startups like Cloudera , DataStax and MuleSoft are generating hundreds of millions of dollars in revenue each year from real enterprise customers by selling proprietary, value-added products around their open projects. This is a new wave in software - one that's not only displacing incumbent markets, but creating entirely new ones. We call these Open Adoption Software (OAS) companies, and we believe they're primed to build meaningful businesses - and drive large economic outcomes. We're witnessing a big shift in how software is consumed. OAS companies are constructed differently. They go through three phases of company building: Project, Product and Profit . Companies built atop this ""3Ps"" model need to look largely the same and be held to similar financial standards as traditional enterprise software businesses by the time they make it into the ""Profit"" phase. We discussed this a few weeks back in a panel conversation with startups and financial analysts - a timely conversation, as some of these companies reach the scale to potentially IPO. This feels an awful lot like 2003, just before the first SaaS companies started going public. OAS is a customer-driven phenomenon Open software has already rooted itself deep within today's Fortune 500, with many contributing back to the projects they adopt. We're not just talking stalwarts like Google and Facebook; big companies like Walmart, GE, Merck, Goldman Sachs - even the federal government - are fleeing the safety of established tech vendors for the promises of greater control and capability with open software. These are real customers with real budgets demanding a new model of software. And the drumbeat is only getting louder. Each year we host 15 Fortune 500 CIOs as part of Accel's Tech Council , and we continue to hear criticism about proprietary software (""expensive, slow to change""). Here are a few trends we identified that are driving customers toward this new model: The Need for Speed and Control: The demand for innovation and rapid delivery means enterprises need agility from the software they adopt. Nothing is worse than waiting for a vendor to update a library when you're trying to stick to your release schedule. Open platforms allow companies to move faster and integrate at a deeper level without fear of lock-in by removing the dependency on proprietary vendors. Enterprises are no longer beholden to a vendor's product roadmap - they can innovate to their own requirements at any time. Everything is Web Scale: Enterprises are delivering solutions to a global, ever-connected base of users. Consider banks that support tens of millions of end users logging into their banking apps and hundreds of thousands of employees worldwide. Traditional, proprietary vendors are unable to deal with this onslaught of data and user scale. Fortunate for them, many early web 2.0 leaders (Google, Facebook, Linkedin, Yahoo) dealt with these problems and more, contributing much of their learnings to the open community. Developer Power and Network Effects: CIOs are empowering frontline developers to download and adopt the projects they need to drive innovation. Developers are looking to community-led technologies where they adopt, deploy and meaningfully participate . OAS extends beyond Moore's Law by also benefitting from something akin to Metcalfe's Law: its energy and rate of innovation grows exponentially with the developer networks around it. Open software can absorb learnings and requirements far faster than a proprietary vendor, while simultaneously hardening security and stability. Open software is in many respects, much safer. Hadoop and Docker are constantly stretched, pushed, molded and smoothed by their developer communities - they're far more mature than their age would suggest. All of this is to say: We're witnessing a big shift in how software is consumed. OAS is openly adopted and openly developed, and is quickly becoming a dominant model for how enterprises build and deliver IT. While most OAS companies have at least some amount of freely available or open-source components, open source and OAS should not be conflated. Open source describes a software development methodology, whereas OAS pertains more to a go-to-market and company-building philosophy. OAS is not about cheaper alternatives to proprietary on-premise software. It about creating new markets more so than displacing incumbents. It's innovative, it's developer-driven and it's the next wave of software adoption. The next wave of software With each successive wave of technology - from mainframe to client-server to 'X'aaS (IaaS, SaaS, etc.) to OAS - software has gotten progressively easier to adopt. Therefore, adoption has happened faster and has reached a broader audience than the wave before it. Each wave is driven by the democratization of some facet of technology. In the shift from mainframe to client-server, computers became accessible. In the shift to 'X'aaS, hosting and WAN connectivity became accessible. Now, with the shift to OAS, developer community innovation has become accessible. OAS not only represents a new way to provide innovative functionality, but is a delivery model innovation for developers. Through it all, the customer desire for bigger, faster and cheaper offerings remains constant. The technological innovations that each wave brings facilitate change in how software is packaged and delivered so that customers can gain some form of efficiency or cost savings. Being openly adopted is not a panacea. With all of these shifts, industry pundits predicted that the new wave will commoditize existing categories. While some layers of the stack do get cheaper, consumption on the layer above consequently expands dramatically as more applications are developed and new use cases emerge. This new usage outpaces any commoditization. Thus, the value of the market opportunity expands rather than contracts. Salesforce and Amazon Web Services (AWS) exemplify this. Literally thousands of new businesses exist as a result of these platforms than ever could have in the past. OAS is not an answer to all problems While OAS companies drive adoption much faster than their fully proprietary counterparts, being openly adopted is not a panacea. Particularly as public cloud vendors begin hosting open-source projects as a service, it's tremendously important that these companies thoughtfully decide which parts of the product will be open and which parts won't. There is definitely a unique failure mode in which OAS companies go too open and fail to monetize sufficiently. While we certainly believe in OAS, not all open projects are the basis for OAS companies, and not all of these companies are going to be publicly traded - some will be niches, some will struggle, some will be M&A opportunities. It's hard to predict the winners out of the gate. While OAS companies will likely have the same success rate as traditional software companies, there is reason to believe that the winners will be bigger than their predecessors. Featured Image: 31moonlight31/Getty Images",en,91
116,1318,1465488259,CONTENT SHARED,-5380376324140474506,881856221521045800,6178486427970494602,,,,HTML,http://exame.abril.com.br/tecnologia/noticias/mudancas-na-app-store-podem-nao-conquistar-desenvolvedores,mudanças na app store podem não conquistar desenvolvedores | exame.com,"San Francisco - A Apple anunciou uma série de aguardados melhoramentos na App Store, mas os novos recursos podem não reduzir preocupações de desenvolvedores e analistas de mercado que afirmam que o modelo da loja de aplicativos pode estar ultrapassado. A loja reformulada vai permitir que os desenvolvedores divulguem seus aplicativos em resultados de busca e dará a eles uma parcela maior das receitas com assinaturas. A Apple ainda afirmou que acelerou muito a velocidade de aprovação dos aplicativos vendidos na loja. O objetivo é sustentar um ciclo virtuoso no centro do lucrativo negócio criado pelos iPhones. Produtores de software desenvolvem aplicativos para o celular da Apple porque seus clientes têm interesse em pagar por eles e estes consumidores, por sua vez, pagam um prêmio pelo aparelho porque consideram que tem os melhores aplicativos. A loja é mais importante estrategicamente que nunca para a Apple, em um momento em que as vendas do iPhone começam a dar sinais de desaceleração e a companhia busca softwares e serviços para preencher esse espaço. O presidente-executivo da Apple, Tim Cook, afirmou em conferência recente que as receitas da App Store subiram 35% no ano passado. Mas a loja também é vítima de seu próprio sucesso. Oito anos depois de seu lançamento, a App Store tem mais de 1,9 milhão de aplicativos, segundo uma análise da empresa App Annie, o que torna praticamente impossível para desenvolvedores conseguirem um espaço de usuários. Além disso, fica cada vez mais difícil para os usuários encontrarem o que precisam, já que cerca de 14 mil novos aplicativos chegam à loja toda semana. ""O espaço para aplicativos ficou fora de controle"", disse Vint Cerf, um dos inventores da Internet e atualmente vice-presidente do Google durante uma conferência em San Francisco sobre o futuro da web. ""Precisamos deixar esta ideia de ter um aplicativo individual para qualquer coisa individual que você queira fazer."" Alguns usuários estão abandonando aplicativos para adotarem serviços de mensagens como o Slack ou o Messenger, do Facebook, que estão avançando sobre áreas como shopping e armazenagem de documentos. Enquanto isso, rápidos avanços em inteligência artificial podem levar a um mundo em que as pessoas utilizam assistentes digitais controlados por voz como o Siri, da Apple, em vez de abrirem aplicativos individuais. Se estas tecnologias decolarem, podem eliminar a vantagem desfrutada pela Apple por meio de seu forte ecossistema de aplicativos e dar mais força ao Google, que é amplamente considerado como líder na inteligência artificial. ""A atual dinâmica é muito favorável à Apple e isto é uma indicação de que podemos ter uma mudança para uma dinâmica diferente em que o Google terá uma forte vantagem"", disse Benedict Evans, sócio da empresa de investimentos de risco Andreessen Horowitz. ""Não importa o que você faça a uma loja de aplicativos, você sempre vai ter o mesmo problema: É uma lista de milhões de coisas"", acrescentou.",pt,91
117,2707,1478181100,CONTENT SHARED,-790959521412948853,-1578287561410088674,-8549049404053740422,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) SmartCanvas/0.1.0 Chrome/49.0.2623.75 Electron/0.37.2 Safari/537.36",SP,BR,HTML,https://macmagazine.com.br/2016/11/03/microsoft-lanca-teams-nova-plataforma-de-chat-concorrente-do-slack/,"microsoft lança teams, nova plataforma de chat concorrente do slack | macmagazine.com.br","A Microsoft tem investido cada vez mais em ferramentas para o ambiente corporativo. A novidade desta vez é o mais novo serviço de chat para interação em grupos, o Microsoft Teams . Sendo um competidor direto de serviços como o Slack e o HipChat , o Microsoft Teams é mais uma maneira de manter o trabalho de equipes em um só lugar, para não haver preocupação de perderem-se em uma montanha de aplicativos e serviços diversos. Com certeza, o trunfo de ferramentas como essas é a integração com serviços de terceiros ou da própria empresa, e o Teams parece fazer isso muito bem. Além de, é claro, funcionar com o pacote Office e Skype, sua API aberta possibilita que outros serviços de terceiros possam ser integrados a ele, como o Zendesk, Asana, Twitter, GitHub, etc. Outros recursos também funcionam normalmente com o app, como bots , emojis, stickers , GIFs, extensões e mais. O Microsoft Teams é multiplataforma e está disponível para Mac, Windows, iOS, Android e também para a web. Entretanto, ele faz parte do pacote de assinatura do Office 365, ou seja, é uma ferramenta paga. Por este motivo, talvez seja difícil mensurar o quanto as pessoas vão aderir ao aplicativo; mesmo assim, o potencial dele é grande e pode até significar uma ameaça aos serviços competidores. O que não quer dizer que eles vão se render facilmente; muito pelo contrário: o Slack inclusive já publicou uma carta aberta no New York Times para dar as boas-vindas à Microsoft, bem no estilo Apple em 1981: That feeling when you think ""we should buy a full page in the Times and publish an open letter,"" and then you do. �� pic.twitter.com/BQiEawRA6d - Stewart Butterfield (@stewart) 2 de novembro de 2016 Aquele momento em que você pensa ""nós deveríamos comprar uma página inteira no Times e publicar uma carta aberta"", e aí você faz isso. �� Com ""alguns conselhos de amigo"", o Slack lembra à Microsoft que ""o que mais importa não são os recursos"", que ""uma plataforma aberta é essencial"", que ""é preciso fazer isso com amor"" e, por fim, que ""o Slack está aqui para ficar"". Por último: Slack está aqui para ficar. Nós vamos aonde o trabalho acontece por milhões de pessoas ao redor do mundo. Então, bem-vinda, Microsoft, à revolução. Estamos felizes por você estar nos ajudando a definir essa nova categoria de produtos. Nós admiramos muitas de suas realizações e sabemos que você será um concorrente digno. Temos certeza de que você vai apresentar algumas novas idéias por conta própria também. E nós estaremos bem ali, prontos. A carta - que pode ser lida por completo em inglês aqui - muito se assemelha à que a Apple publicou no Wall Street Journal direcionada à IBM quando - na década de 1980 - esta decidiu entrar no ramo de computadores pessoais. As palavras não são exatamente as mesmas, mas a intenção com certeza foi. O Microsoft Teams está disponível para testes em 170 países e 18 idiomas, para os clientes do pacote Office 365 dos planos Enterprise ou Business . Seu lançamento oficial será no primeiro trimestre de 2017. Microsoft Teams de Microsoft Corporation [via MacRumors ] Se houver algum erro no post acima, selecione-o e pressione Shift + Enter ou clique aqui para nos notificar. Obrigado! chat colaboração conversa equipes HipChat Integração Microsoft Microsoft Teams Office 365 produtividade skype Slack times Sobre o Autor Aviso: nossos editores/colunistas estão expressando suas opiniões sobre o tema proposto e esperamos que as conversas nos comentários sejam respeitosas e construtivas. O espaço acima é destinado a discussões, debates sobre o tema e críticas de ideias, não às pessoas por trás delas. Ataques pessoais não serão tolerados de maneira nenhuma e nos damos ao direito de ocultar/excluir qualquer comentário ofensivo, difamatório, preconceituoso, calunioso ou de alguma forma prejudicial a terceiros, assim como textos de caráter promocional e comentários anônimos (sem nome completo e/ou email válido). Em caso de insistência, o usuário poderá ser banido.",pt,91
118,1370,1465842144,CONTENT SHARED,-4228415104574264137,3302556033962996625,-4935617751218062693,,,,HTML,https://medium.com/google-developers/up-your-app-s-sharing-game-with-direct-share-2a2bc0a9ad36,up your app's sharing game with directshare - google developers,"Up your app's sharing game with Direct Share As you send pictures, messages, goofy drawings, and humble-brags about your sweet life to the people you love, sharing suddenly rises in importance. You may even find yourself daydreaming about how sharing should work in your app. (Because we all know you didn't spend too long on it the first time around.) Thankfully, Marshmallow's Direct Share feature makes it easy to create a custom Share experience without too much hassle. Your app can specify direct share targets that will be displayed in the Share dialog presented to the user. To be clear, this means that instead of launching your app, you can now launch a specific conversation in response to a Share intent. Context can be critical, and your app has a chance to shine here. So, to spruce up your app and amp up your Share game, you'll need a ChooserTargetService . This service needs to implement onGetChooserTargets() to provide the direct share targets to the system. And this method is where you can get fancy . Maybe it makes the most sense to offer up the ten most recent conversations. Or perhaps your app offers a way for users to track who they care the most about, and you can provide a list of those BFFs instead. Or maybe you know which conversations (like group messages, for example) are most prone to using attachments, and those are your likely share targets. This magic is up to you. We just made the framework, so that you could shine. Then, in your manifest, you'll obvi declare the service. But when you do, don't forget to specify the BIND_CHOOSER_TARGET_SERVICE permission. Then, for each activity that you'll be using with the new service, you'll need to attach some new metadata to your intent filter . This is what will launch your wonderful service to provide the extra Share targets. Otherwise, you're stuck with the old-school approach. If you'd like to see this in action, we have a sample for you. Check it out and then update your own app to #BuildBetterApps. Join the discussion on the Google+ post and follow the Android Development Patterns Collection for more!",en,89
119,335,1460407450,CONTENT SHARED,-7681408188643141872,-3390049372067052505,2596549390664327450,,,,HTML,http://blog.intercom.stfi.re/the-end-of-apps-as-we-know-them/?sf=xzypkn,the end of apps as we know them - inside intercom,"The experience of our primary mobile screen being a bank of app icons that lead to independent destinations is dying. And that changes what we need to design and build. How we experience content via connected devices - laptops, phones, tablets, wearables - is undergoing a dramatic change. The idea of an app as an independent destination is becoming less important, and the idea of an app as a publishing tool, with related notifications that contain content and actions, is becoming more important. This will change what we design, and change our product strategy. No more screens full of app icons This is such a paradigm shift it requires plenty of explaining. Whilst it may not transpire exactly as I'm about to describe, there is no doubt what we have today - screens of apps - is going to dramatically change. Bear with me as I run through the context. The idea of having a screen full of icons, representing independent apps, that need to be opened to experience them, is making less and less sense. The idea that these apps sit in the background, pushing content into a central experience, is making more and more sense. That central experience may be something that looks like a notification centre today, or something similar to Google Now, or something entirely new. The primary design pattern here is cards. Critically it's not cards as a simple interaction design pattern for an apps content, but as containers for content that can come from any app. This distinction may appear subtle at first glance, but it's far from it. To understand it, and chart the trajectory, we need to quickly run through two things. Designing systems not destinations I covered this topic in detail in a previous post , so I'll quickly summarise here. Most of us building software are no longer designing destinations to drive people to. That was the dominant pattern for a version of the Internet that is disappearing fast. In a world of many different screens and devices, content needs to be broken down into atomic units so that it can work agnostic of the screen size or technology platform. For example, Facebook is not a website or an app. It is an eco-system of objects (people, photos, videos, comments, businesses, brands, etc.) that are aggregated in many different ways through people's newsfeeds, timelines and pages, and delivered to a range of devices, some of which haven't even been invented yet. So Facebook is not a set of webpages, or screens in an app. It's a system of objects, and relationships between them. Recent changes to iOS and Android notifications Things changed with iOS 8 and Android KitKat. Notifications used to be signposts to go to other places. A notification to tell you to open an app. To open a destination. But that is changing fast. For a while now, you can take action directly in Android notifications. Sometimes that takes you to that action in the app itself, but sometimes you can do the action directly, meaning that you don't need to open the app at all. iOS is following suit here and raising the bar. Interactive notifications. No need to open the app. The notification is the full experience. The next version of Android takes this even further, breaking notifications into independent cards. You can see that cards stack below each other. We've moved pretty quickly from notifications as signposts, to containers (cards) that include content , and actions on that content. Next up: cards housing full product experiences The next iteration is obvious. Lots and lots of notification cards that enable full product experiences and independent workflows right inside the card. Comment on the Facebook post. Retweet the tweet. Buy the item on Amazon. Check in for the flight. Share the news story. Add the reminder to your to-do list. Book the restaurant. Swap the fantasy football player. Annotate the run you just finished. Pay the bill. And on and on. Towards apps as services Breaking things right down into the individual atomic unit, including the content and actions. The atomic unit separate from the container of the app itself, so that it can show up anywhere, on any device. The atomic units are then reassembled based on context. Aggregated in a centralised stream. Or pushed to you on your watch. The content may be reformatted to enable more natural user input, optimized for your situation. Des sent me a text based message, but I'm driving so my watch reads it out to me. I speak my reply to Siri/Google and Des receives it as a text based message, because he's in work at his desk. The actions available change. All this and more is just about to happen. It may be very likely that the primary interface for interacting with apps will not be the app itself. The app is primarily a publishing tool. The number one way people use your app is through this notification layer, or aggregated card stream. Not by opening the app itself. In a world where notifications are full experiences in and of themselves, the screen of app icons makes less and less sense. Apps as destinations makes less and less sense. Why open the Facebook app when you can get the content as a notification and take action - like something, comment on something - right there at the notification or OS level. I really believe screens of apps won't exist in a few years, other than buried deep in the device UI as a secondary navigation. A concept design to make this concrete This is such a fundamental shift that to highlight where it may go, I'll start with a rough system design for how one might interact with a connected device in this world. - Imagine a vertical stream of cards, individually personalised and ranked based on who and what you care about, your current context (location, availability, etc.) and your likelihood to care about things based on historical data when you were in a similar context. - The cards can come from any source that you care about or have given permission to. - This looks a lot like Google Now, but on steroids. You will have almost as many unique sources in your stream as you have apps on your phone. - This also looks a lot like your notifications centre on your phone, but rather than merely signposts to open apps, these cards are notifying you, presenting you with the content to decide what to do next, and with the ability to interact with the content right there and then. So a card from Facebook has all the actions you would have for that content if you viewed it in the Facebook app. Like, comment, share, save, etc. all inline, with no need to open the Facebook app. Cards from travel apps allow you to book, cards from commerce apps allow you to buy, the list is endless. This is the beginning of the end for apps as destinations. Why open the app when you don't need to? Let's take this a step further. Imagine that you can scroll horizontally, and that shows you more content from the same source. So on a Facebook post, that is effectively your newsfeed presented horizontally rather than vertically. This would be the same for all sources, Twitter, Instagram, WhatsApp, news apps, etc. And of course on all devices. OK now let's go a step further again. Imagine that a parent card can support a child card, so for example a Facebook card can support (embed) a card from the BBC. Indeed something similar already exists with Twitter. This is also a little similar to the recently launched Apple Extensions, and is already happening in app development in China with Baidu and WeChat, where smaller apps are being bundled within bigger apps, only surfacing when some interaction in the UI invokes the smaller app. For example, in Baidu Maps you can find a hotel, check room availability, and make a booking, all inside the app. But again the apparent subtlety masks something much more profound. Embedded cards (child cards) within cards (parent cards) also mean you don't need to install the app to experience the content from the child card. You just need the parent card app on your device. Again, this is already happening, Twitter cards currently support Stripe payments inside the card. You don't need the New York Times app to see their content on Twitter. But imagine this pattern was widespread. Suddenly app developers have a powerful discovery channel. And some businesses may be comfortable always appearing as a child card, without ever having an app at all. One final step further. What if the cards came from other ? Like vending machines that you walk up to and pay through the card? Hotels you walk into and order your breakfast or pay for the wifi? The ramifications for websites might also be huge. If a publishing company, for example the New York Times, can push content to cards, and those cards can be seen in many different third party places (with revenue sharing agreements) why bother having a website at all? It's just a huge overhead. We will still open apps. Sometimes In this world, it feels dumb to open apps just to see what lies behind the red counter, or to have to switch between apps. Opening apps is still necessary and great for many contexts, especially composition of new content and dedicated deep workflows, and maybe changing preferences. But not for seeing what's new and interesting. A bank of app icons as a dominant design pattern feels old and inefficient now, and I think it'll disappear within a couple of years, correctly relegated behind a ""show me my apps"" action. The system will learn, creating new competitors As people interact or don't interact with cards presented to them, the system will learn when to show more or less from a specific source (app). As content from different apps will be presented side by side, this changes who you might think you are competing with. Competition is between products that do the same job, not products that are in the same category. This is already the case today; when faced with multiple notifications on a phone screen, they all compete with each other for your attention. Here at Intercom, we're big proponents of the Jobs To Be Done (JTBD) framework, which asks what Job people need to get done that your product fulfills. If you focus purely on the job, and not the industry, you realise airlines selling business class seats are competing with Skype for customers, as they address the same job: the need to have clear communication with colleagues. Similarly, apps will realise they are competing on Jobs they may not have realised their product addresses. Twitter for example, may be competing much more with apps addressing the Job of 'entertain me while I have a short amount of free time' e.g. Games and News apps, than with other social products. This intense competition means businesses will have to spend time designing great notifications/cards, because they will potentially be competing with cards from Facebook, or Amazon, or Google. The days of sending lots and lots of notifications to bring people back to an app are going away, with a much better focus on designing notifications that people engage with there and then, independent of opening the app. Three critical questions There are many signs pointing towards a near future that looks something like this. But many questions remain - these are three that I have no answers for: Will this happen at the app, notification, or OS level? One of the biggest challenges will be whether these experiences will occur: at an app level (like an evolution of Google Now), at a notification level (an evolution of the Android or iOS notification centre), or at the root OS level (a redesigned iOS for example that removes the sea of app icons). Will this be one consolidated stream, or multiple streams? Maybe we will have a friends stream, a news stream, a work stream. Will this be owned at a company level? Maybe there will be a Google version, an Apple version, etc. Or more open systems that are interoperable across platforms (like the web itself), the first of which (like Wildcard and Citia ) are being built now, could come to dominate. Towards better businesses and products This is just a sketch but at a conceptual level I think it's largely where we are headed. Large parts of this are built already; things like Google Now, Android notifications, iOS8 interactive notifications, iOS8 extensions, Twitter cards. Emerging platforms like Android Wear and Apple Watch are confirming these trends towards cards that work as notifications, content and actions. There are also a multitude of user benefits: This new paradigm matches much more closely with how real life works. We don't live our lives in silos, like the app silos that exist today. People start to forget about ""apps"" and just think about businesses and products and services. This is a great thing, the container for content should be invisible to users. This new paradigm also solves a critical problem around volume of incoming content. Navigating to lots of apps is so inefficient. A new problem emerging is an overwhelming volume of notifications. Things will need to be ranked, which will make them more manageable. It's also a better experience, apps maximising their usefulness in a quick lightweight fashion rather than dominating your attention in a slow heavyweight app-oriented experience. The constraint of an individual card also makes you think about the most important thing you could show, and only the most important actions relating to that. That constraint is very powerful. For businesses, it also starts to solve the app discoverability problem. Rather than relying on App Store promotion, advertising, or new deep in app linking to get discovered, an apps content can appear as a card in our stream, particularly when embedded in a parent card. Indeed there may not be a child app, the content and actions in that child card may come from the web. This paradigm shift also starts to ask questions of the bundle or unbundle dilemma (btw both are happening now, it's not just unbundling). Maybe in this world you can have your cake (unbundled simple focused one task experiences) and eat it too (bundled into a coherent individual stream). A deliberately designed eco system of cards where cards are simple, but can carry information and context from other cards you build. 5 key take aways These patterns reinforce two things we wrote about here on Inside Intercom early in their development. That cards are the future of the web , and designers need to design systems not destinations . Cards are happening. Systems are happening. Get fully up to speed on both of these things. Responsive design is a nice thing, but we're heading way beyond that. We're talking about designing content that may appear on an incomprehensible number of devices and in an incomprehensible number of situations. This will need new design principles, new ways of thinking about researching context. Push forward with this yourself, don't wait for it to happen. Designing the notifications, and the actions within them, will become an increasingly important part of product design. We will need to spend as much of our time on this aspect of the experience, as on the experiences within the app. Change how you think and work now, rather than when it is too late. Sketch systems, not screens. Think about who you might integrate with. Integrations as part of a product strategy are increasing, witness the explosion in available APIs, Webhooks and the emergence of services like Zapier and IFTTT. Integrations make things possible that you could never do alone. They give you access to new audiences. Make integrations part of your business plan, product strategy, and product design. I carry around both an iPhone and an Android phone. I often also have my iPad Mini. I wear a Nike Fuelband. I've tried Google Glass whenever I can (in private ;-)). When I can I'll buy an Apple Watch. We all need to dive headfirst into this, eyes open, trying to see what works and fails, trial and error. If you made it this far, thanks for reading. Whilst the trajectories seem clear, much is unknown, we'll all figure this out together. So please let us know your feedback, thoughts, ideas below, and we'll do our best to respond and keep the conversation moving forward. Update: Read our follow up piece: It's not the end of apps . There are many other people writing about these things and I'm indebted to them for sharing their thinking. Here are some key articles if you want to read more: Thanks also to my colleagues at Intercom for contributing to this and making it much much better. Like what you read here? Why not come and work with us? We're hiring for roles in Dublin and San Francisco . Want to read our product management best practices? Download our free book , Intercom on Product Management . It's recommended by folks like Ryan Singer, Hunter Walk, and Dharmesh Shah.",en,89
120,2507,1475614359,CONTENT SHARED,-4996336942690402156,-709287718034731589,-6980904286083816062,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/602.2.11 (KHTML, like Gecko) Version/10.0.1 Safari/602.2.11",SP,BR,HTML,https://medium.freecodecamp.com/live-asynchronously-c8e7172fe7ea?gi=5c95ee418f3e,live asynchronously.,"We'll start by talking about how interruptions - even scheduled ones - can destroy your productivity. But first, a web comic: Did you see what happened? The boss came by and totally wrecked the developer's train of thought. And for what? To notify him that the next time he checked his email, a message would be there for him? When you interrupt a developer, it takes an average of 10 minutes before they can get back to productively coding. So why do 70% of offices these days look like this? Most studies conclude the same thing that this widely-cited paper does: people hate open plan offices . Employers have chosen to sacrifice job satisfaction and productivity, all so they can pack a few extra sardines into $72-per-square-foot San Francisco office spaces. They do this under the guise of lowering barriers to communication. But if you think about it, they should be raising them instead. Because here's what happens when you need to get some real work done in an open plan office: Reaching that flow state Let's talk a bit about how us humans get work done. Is it four hours of crushing it, a lunch break, then four more hours of crushing it? No. It's more like coffee, email, coffee, meeting, coffee, lunch with coworkers, coffee - OK finally time to get some work done! Did you know the average developer only get two hours of uninterrupted work done a day? They spend the other 6 hours in varying states of distraction. But here's what happens during the two hours they have to themselves. They warm up. They check logs, issues, and wrap their heads around what needs to be done. They dive into the code. Their pupils dilate. They enter what psychologist Mihaly Csikszentmihalyi calls a ""flow state."" If you've ever been ""in the groove"" or ""in the zone,"" that's what this is. A happy state of energized focus. Flow. If your job requires even an ounce of creativity, you'll do your best work in one of these flow states. And yes, you can reach a flow state in an open plan office, with noise cancelling headphones cranked up to 11. But it's a lot easier when you're in a quiet, comfortable room by yourself. So Mihaly figures out that task switching utterly devastates your productivity. Something as mundane as getting a text message about dinner will completely wipe out all those things you're juggling in your working memory. It will knock you out of your flow state. Most of Mihaly's fellow researchers agree . There's a serious cost to switching between tasks. Mihaly spends the next 30 years researching flow states. He publishes a ton of papers. He does some consulting. He teaches at Berkeley. He records a TED talk: He writes some books. Here's a good one: But employers, for the most part, don't listen. They continue to cram their teams together into noisy open plan offices. They continue to pepper their teams' days with meetings. They expect their teams to be responsive to emails or Slack, further dashing hopes of ever reaching a flow state and getting some real work done. Do you think Tolstoy could have written War and Peace in an open plan office? Do you think Mozart could have composed The Marriage of Figaro in between stand-ups and one-on-ones? Do you think Torvalds could have designed the Linux Kernel with Slack notifications popping up every 15 seconds? How to live asynchronously Here are three things you can do reclaim your flow state. I'll assume that, like most people, you're crazy busy. You're skeptical of the gazillions of productivity tips you see here on Medium. And you probably don't greet major lifestyle changes with open arms. So I'll introduce these in increasing order of commitment required. One Dalí Clock means you can do it immediately after finishing this article. Four Dalí Clocks means you'll need a full-blown action plan to make the leap. Tip #1: Turn off as many notifications as you can (difficulty: 1 Dalí Clock) Remember when Microsoft Outlook added that feature where it showed you a notification every time you received an email? You shut that off, right? Good. Now shut off pretty much every other notification on your phone and computer. Do you really need to be notified right this instant that a new podcast will be available during your commute home tonight? Or that your aunt liked the photo you took of your lunch two days ago? The only notification you really need is that old standby from 100 years ago: the phone ring. Because if it's really important, people will call. I can't find you in the food court. Call. Your kid threw up at school. Call. The servers are melting down. Call. Some of my friends will still leave their text message notifications on (or their WhatsApp notifications when they're overseas). I did this for a few years, too. But I turned these off a few months ago. So far, planes have not fallen from the sky. As a bonus, when people expect to hear back from you within 24 hours - rather than 10 minutes - they take the time to actually think about what they want to say to you. No more ""hey there"" texts. Or that old favorite: ""can I ask you a question?"" By the way, just for fun, here's the extreme opposite of no notifications: Tip #2: Defend your time by dodging meetings (difficulty: 2 Dalí Clocks) Next time you sit in a meeting, do a quick experiment. Write down all the important things discussed that couldn't have just been mentioned in an email thread. There may be a few. But were those things really worth the 30 to 60 minutes you just spent away from your work? Email brings out the Hemingway in all of us. When someone can't get their point across in an email, it just means they're going to have an even harder time explaining it in person. The next time someone writes you asking if you have time to meet, try responding: ""What do you want to talk about?"" They'll write you back with an answer. Then respond: ""OK - what are your thoughts on that?"" They'll write you back with an answer. Then respond with your own thoughts on the matter if necessary. Or just say: ""OK - sounds good."" Phew. Meeting dodged. Sometimes you get that message from a cheerful stranger on LinkedIn: ""Let's grab coffee and catch up!"" Or that message from a coworker: ""Can I swing by your desk and pick your brain?"" If these people valued your time, they would just tell you what they wanted right up front. But in many cases, they don't know exactly what they want. Don't commit your scarce time to meet with them unless it's clear that they know what they want, and they're willing to tell you before hand. A lot of people may bristle at you not just accepting their meeting request. Be polite and patient, and tell them you're happy to answer their questions right here - in the email chain, or instant message client - wherever here may be. Your time is your most valuable asset. Be protective of it. Don't let others thoughtlessly waste it. Tip #3: Ask for a private office, or to work from home (difficulty: 4 Dalí Clocks) There are tremendous benefits to working remotely. When you have 4 minutes, read this article: A lot of the 13% productivity gain you get when you work remotely is because you're not in a noisy office. If you can't work remotely - or if you are one of the many people who enjoys the energy of the workplace - ask for a private office. If you value your productivity (and your sanity), consider prioritizing this during your job offer negotiation , or the next time you discuss a raise with your boss. Time to think. Time to create. Time enough at last. In closing, I strongly recommend this book by one of the founders of Pixar. You can learn about their experiments with improving their teams' productivity and creativity. Among other things, it explores the evolution of their meetings, their use of open plans, and their transition back to mostly private offices. So what are some ways you keep things asynchronous? Do you have any tips for us? Share them with everyone in the comments section below. Oh, and if you liked this, click the �� below. Follow me for more articles on technology.",en,88
121,730,1462275589,CONTENT SHARED,-730957269757756529,-331066625167168067,-6677202055740773123,,,,HTML,https://medium.com/@vilucas/por-que-a-limita%C3%A7%C3%A3o-da-banda-larga-%C3%A9-uma-forma-de-ignorar-o-futuro-12069206541,por que a limitação da banda larga é uma forma de ignorar o futuro,"Por que a limitação da banda larga é uma forma de ignorar o futuro Com um olhar direcionado para os excessos dos consumidores e com a tentativa das grandes empresas de telefonia e internet de barrarem a expansão de serviços de streaming, a Anatel fecha os olhos para as tendências do mundo da tecnologia. Apesar de o assunto ainda estar aguardando decisões, um ponto ainda foi muito pouco explorado quando falamos da limitação da banda larga no Brasil: os impactos para o uso da tecnologia em um horizonte de médio a longo prazo. Mas antes de entrar a fundo no tema, é preciso entender o que aconteceu. Hoje quando se contrata um plano de banda larga residencial, contrata-se um modelo de acesso aos dados baseado na velocidade do canal. Uma internet de 15 MEGA permite que seu usuário acesse dados da internet em forma de download a uma velocidade de no mínimo 1.5 MB por segundo. Neste modelo ainda, define uma franquia de consumo, por exemplo, 80GB de downloads mensais. As empresas em geral, apesar de definirem estes limites, durante longo tempo permitiam que o usuário consumisse livremente sua banda. Recentemente começou a reduzir a velocidade do download quando este limite era atingido. O usuário não era comunicado e somente saberia da redução através da percepção de má qualidade do link de internet. Toda a discussão existe porque hoje as empresas de telefonia e internet não são capazes de dizer ao usuário que sua franquia se aproxima do fim. Quando a Anatel resolve dar um parecer de que, ainda que temporariamente, as empresas não poderão suspender o serviço quando tal franquia for atingida, até que se permita um controle claro pelo usuário de seu volume consumido, deixa explícito que concorda com a política do bloqueio, se posicionando a favor das empresas provedoras de internet banda larga. Seu presidente, João Rezende, inclusive cita que é inevitável que se realize o limite da franquia, usando como exemplo quem joga online como o grande ""culpado"". O serviço, que já é alvo de incontáveis críticas pela comunidade usuária pelas constantes falhas e variações na velocidade, agora também se vê criticado por querer barrar as evoluções de serviços que se demonstram cada vez mais presentes na vida das pessoas que usam a internet, como Netflix, vídeos em alta resolução, e os próprios jogos online. Pois bem, chegamos ao problema. A tecnologia vem evoluindo constantemente, e a inovação chega cada vez mais rápido. Quando chegam nas lojas, câmeras que fazem vídeos em alta-resolução, e se permite que eles sejam levados à internet, imaginamos que pessoas terão interesse em assistí-los. As televisões, Full-HD, e agora 4K permitem a exibição de vídeos em altíssima qualidade. Ao mesmo tempo, quando as televisões smart nos são oferecidas, com uma série de aplicativos de streaming de vídeo, com conteúdo personalizado, imaginamos que haverá consumidores que pagarão por estes serviços. Quando nos prometem, que a cada dia mais que nossos aplicativos e dispositivos sejam conectados, seja uma geladeira, uma impressora, uma casa inteligente, e que desta forma poderemos realizar nossas atividades de qualquer lugar, focamos no serviço oferecido, e consequentemente entendemos que a tecnologia por trás da transferência de dados esteja disponível e funcione adequadamente. Vamos além: Hoje o mundo tem sido inundado por dados, uma análise constante do comportamento de pessoas e consumidores através de Big Data . Bom, se os dados não podem ser usados em favor dos usuários, não deveriam ser gerados. Dispositivos de Internet das Coisas (IoT - Internet of Things) capturam dados em tempo real que permitem uma série de ações (algumas que ainda nem imaginamos). A limitação da banda larga novamente influenciará para que evoluções nestes dispositivos parem no tempo, afinal, não haverá infra-estrutura por trás que permitam sua expansão. Relógios smart, TVs smart, notebooks, celulares, tablets, video-games das mais novas gerações. Todos eles consideram que a troca de informações e dados com a rede de internet esteja disponível, e em grandes volumes, afinal, deseja-se explorar a interface e usabilidade de seu usuário, sem limites à criatividade. Logo se percebe que as decisões e regulações que se encaminham buscam que o usuário evite a alta transferência de dados, e priorize toda aquela tecnologia que faça menor uso da banda. A exploração da tecnologia, o incentivo à inovação, a experiência do usuário ficam evidentemente em segundo plano. As pessoas à frente destes órgãos focam no problema imediato, sem avaliar todos os arredores que o cercam. É uma visão limitada que causa hoje uma luta da comunidade contra a imposição de regras retrógradas que nem sequer pensam na qualidade do serviço demandada pelas pessoas e nem mesmo na exploração da tecnologia em favor de um futuro que já está mais do que presente. A Internet passou a ser uma necessidade para todo este mundo a ser explorado. E sua limitação fará novamente que o Brasil pare no tempo em termos de inovação, enquanto o mundo se beneficia dela. E os únicos beneficiados serão os provedores do serviço de internet, que poderão aplicar preços abusivos frente à demanda crescente. A regulação dos limites de uso da internet é como fechar os olhos às tendências, e impedir que utilizemos o potencial e benefícios de outras tecnologias em favor de empresas que só querem garantir uma exploração financeira de seus consumidores.",pt,87
122,3023,1485456802,CONTENT SHARED,1348739322889189648,3609194402293569455,-296407187773930883,"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36",SP,BR,HTML,http://epocanegocios.globo.com/Empresa/noticia/2017/01/johnson-johnson-comprara-grupo-suico-por-us-30-bi.html,johnson & johnson comprará grupo suíço por us$ 30 bi,"Produtos da Johnson & Johnson (Foto: Chris Hondros/ Getty Images) A gigante do setor de saúde Johnson & Johnson vai comprar a empresa suíça de biotecnologia Actelion em uma transação em dinheiro de US$ 30 bilhões, que inclui a cisão da unidade de pesquisa e desenvolvimento da Actelion , informaram as empresas nesta quinta-feira. A aquisição dá à J&J acesso aos medicamentos de preço alto e margem elevada para as doenças raras do grupo suíço, ajudando-a a diversificar o portfólio de produtos. A oferta de pagar US$ 280 por ação, após semanas de negociações exclusivas, foi aprovada por unanimidade pelos conselhos de administração de ambas as empresas. O acordo compreende um prêmio de 23% em relação ao preço de fechamento da Actelion na quarta-feira, de 227,4 francos suíços, supera em mais de 80% o valor da ação em 23 de novembro, antes das primeiras notícias de que a maior empresa de biotecnologia da Europa tinha atraído o interesse de compradores. As ações da Actelion saltavam quase 20%, para 274,10 francos por volta das 10:53 (horário de Brasília). (Reportagem de John Miller e John Revill)",pt,87
123,1198,1464866267,CONTENT SHARED,7943088471380012839,-8020832670974472349,2348433893919184737,,,,HTML,https://coreos.com/blog/torus-distributed-storage-by-coreos.html,torus,"Persistent storage in container cluster infrastructure is one of the most interesting current problems in computing. Where do we store the voluminous stream of data that microservices produce and consume, especially when immutable, discrete application deployments are such a powerful pattern? As containers gain critical mass in enterprise deployments, how do we store all of this information in a way developers can depend on in any environment? How is the consistency and durability of that data assured in a world of dynamic, rapidly iterated application containers? Today CoreOS introduces Torus, a new open source distributed storage system designed to provide reliable, scalable storage to container clusters orchestrated by Kubernetes, the open source container management system. Because we believe open source software must be released early and often to elicit the expertise of a community of developers, testers, and contributors, a prototype version of Torus is now available on GitHub , and we encourage everyone to test it with their data sets and cluster deployments, and help develop the next generation of distributed storage. Distributed systems: Past, present, and future At CoreOS we believe distributed systems provide the foundation for a more secure and reliable Internet. Building modular foundations that expand to handle growing workloads, yet remain easy to use and to assemble with other components, is essential for tackling the challenges of computing at web scale. We know this from three years of experience building etcd to solve the problem of distributed consensus - how small but critical pieces of information are democratically agreed upon and kept consistent as a group of machines rapidly and asynchronously updates and accesses them. Today etcd is the fastest and most stable open source distributed key-value store available. It is used by hundreds of leading distributed systems software projects, including Kubernetes, to coordinate configuration among massive groups of nodes and the applications they execute. The problem of reliable distributed storage is arguably even more historically challenging than distributed consensus. In the algorithms required to implement distributed storage correctly, mistakes can have serious consequences. Data sets in distributed storage systems are often extremely large, and storage errors may propagate alarmingly while remaining difficult to detect. The burgeoning size of this data is also changing the way we create backups, archives, and other fail-safe measures to protect against application errors higher up the stack. Why we built Torus Torus provides storage primitives that are extremely reliable, distributed, and simple. It's designed to solve some major problems common for teams running distributed applications today. While it is possible to connect legacy storage to container infrastructure, the mismatch between these two models convinced us that the new problems of providing storage to container clusters warranted a new solution. Consensus algorithms are notoriously hard. Torus uses etcd, proven in thousands of production deployments, to shepherd metadata and maintain consensus. This frees Torus itself to focus on novel solutions to the storage part of the equation. Existing storage solutions weren't designed to be cloud-native Deploying, managing, and operating existing storage solutions while trying to shoehorn them into a modern container cluster infrastructure is difficult and expensive. These distributed storage systems were mostly designed for a regime of small clusters of large machines, rather than the GIFEE approach that focuses on large clusters of inexpensive, ""small"" machines. Worse, commercial distributed storage often involves pricey and even custom hardware and software that is not only expensive to acquire, but difficult to integrate with emerging tools and patterns, and costly to upgrade, license, and maintain over time. Containers need persistent storage Container cluster infrastructure is more dynamic than ever before, changing quickly in the face of automatic scaling, continuous delivery, and as components fail and are replaced. Ensuring persistent storage for these container microservices as they are started, stopped, upgraded, and migrated between nodes in the cluster is not as simple as providing a backing store for a single server running a group of monolithic applications, or even a number of virtual machines. Storage for modern clusters must be uniformly available network-wide, and must govern access and consistency as data processing shifts from container to container, even within one application as it increments through versions. Torus exists to address these cases by applying these principles to its architecture: Extensibility : Like etcd, Torus is a building block, and it enables various types of storage including distributed block devices, or large object storage. Torus is written in Go, and speaks the gRPC protocol to make it easy to create Torus clients in any language. Ease of use : Designed for containers and cluster orchestration platforms such as Kubernetes, Torus is simple to deploy and operate, and ready to scale. Correctness : Torus uses the etcd distributed key-value database to store and retrieve file or object metadata. etcd provides a solid, battle-tested base for core distributed systems operations that must execute rapidly and reliably. Scalability : Torus can currently scale to hundreds of nodes while treating disks collectively as a single storage pool. ""We have seen a clear need from the market for a storage solution that addresses the dynamic nature of containerized applications and can take advantage of the rapidly evolving storage hardware landscape,"" said Zachary Smith, CEO of Packet, a New York-based bare metal cloud provider. ""We're excited to see CoreOS lead the community in releasing Torus as the first truly distributed storage solution for cloud-native applications."" How Torus works At its core, Torus is a library with an interface that appears as a traditional file, allowing for storage manipulation through well-understood basic file operations. Coordinated and checkpointed through etcd's consensus process, this distributed file can be exposed to user applications in multiple ways. Today, Torus supports exposing this file as block-oriented storage via a Network Block Device (NBD). We also expect that in the future other storage systems, such as object storage, will be built on top of Torus as collections of these distributed files, coordinated by etcd. Torus includes support for consistent hashing, replication, garbage collection, and pool rebalancing through the internal peer-to-peer API. The design includes the ability to support both encryption and efficient Reed-Solomon error correction in the near future, providing greater assurance of data validity and confidentiality throughout the system. Deploying Torus Torus can be easily deployed and managed with Kubernetes. This initial release includes Kubernetes manifests to configure and run Torus as an application on any Kubernetes cluster. This makes installing, managing, and upgrading Torus a simple and cloud-native affair. Once spun up as a cluster application, Torus combines with the flex volume plugin in Kubernetes to dynamically attach volumes to pods as they are deployed. To an app running in a pod, Torus appears as a traditional filesystem. Today's Torus release includes manifests using this feature to demonstrate running the PostgreSQL database server atop Kubernetes flex volumes, backed by Torus storage. Today's release also documents a simple standalone deployment of Torus with etcd, outside of a Kubernetes cluster, for other testing and development. What's next for Torus? Community feedback Releasing today's initial version of Torus is just the beginning of our effort to build a world-class cloud-native distributed storage system, and we need your help. Guide and contribute to the project at the Torus repo on GitHub by testing the software, filing issues, and joining our discussions. If you're in the San Francisco area, join us for the next CoreOS meetup on June 16 at 6 p.m. PT for a deep dive into the implementation and operational details of Torus. ""Distributed storage has historically been an elusive problem for cloud-native applications,"" said Peter Bourgon, distributed systems engineer and creator of Go kit. ""I'm really happy with what I've seen so far from Torus, and quite excited to see where CoreOS and the community take it from here!"" Torus is simple, reliable, distributed storage for modern application containers, and a keystone for wider enterprise Kubernetes adoption. CoreOS is hiring If you're interested in helping develop Torus, or solving other difficult and rewarding problems in distributed systems at CoreOS, join us! We're hiring distributed storage engineers .",en,87
124,3093,1487157951,CONTENT SHARED,-6623581327558800021,599868086167624974,-6471874325422590486,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36",MG,BR,HTML,https://www.wired.com/2017/02/spanner-google-database-harnessed-time-now-open-everyone/,"spanner, the google database that mastered time, is now open to everyone","About a decade ago, a handful of Google's most talented engineers started building a system that seems to defy logic. Called Spanner, it was the first global database, a way of storing information across millions of machines in dozens of data centers spanning multiple continents, and it now underpins everything from Gmail to AdWords, the company's primary moneymaker. But it's not just the size of this creation that boggles the mind. The real trick is that, even though Spanner stretches across the globe, it behaves as if it's in one place. Google can change company data in one part of this database-running an ad, say, or debiting an advertiser's account-without contradicting changes made on the other side of the planet. What's more, it can readily and reliably replicate data across multiple data centers in multiple parts of the world-and seamlessly retrieve these copies if any one data center goes down. For a truly global business like Google, such transcontinental consistency is enormously powerful. Before Spanner, this didn't seem possible. Machines couldn't keep databases consistent without constant and heavy communication, and communication across the globe took much too long. You know, the speed of light and all that. Google's engineers needed something like the the ansible , a fictional device that first appeared in Ursula Le Guin's 1966 novel Rocannon's World and became a sci-fi trope. The ansible can instantly send information across any distance, defying both time and space. Spanner isn't the ansible. It can't shrink space. But it works because those engineers found a way to harness time. No one else has ever built a system like this. No one else has taken hold of time in the same way. And now Google is offering this technology to the rest of the world as a cloud computing service . Google believes this can provide some added leverage in its battle with Microsoft and Amazon for supremacy in the increasingly important cloud computing market, just because Spanner is unique. And some agree. ""If they offer it, people will want it, and people will use it,"" says Peter Bailis, an assistant professor of computer science at Stanford University who specializes in massively distributed software systems. But as others point out: Few businesses have the same needs as Google. Trusting Time In the past, if you built a system that spanned hundreds of machines and multiple data centers, you followed an important rule: Don't trust time. If a system involved communication between many machines in many different places, time would vary from machine to machine, just because time-precise time-is a hard thing to keep. Services like the Network Time Protocol aimed to provide machines with a common reference point. But this worked only so well, mainly because networks are slow. It takes time to send the time. For Google, this was a problem. If a database spanned multiple regions, it couldn't ensure that transactions in one part of the world lined up with transactions in another. It couldn't get a truly global picture of its operations. It couldn't seamlessly replicate data cross regions or quickly retrieve replicated data when it was needed. So Google's top engineers found a way to trust time. Part of the trick is that they equipped Google's data centers with a series of GPS receivers and atomic clocks . The GPS receivers, much like the one in your cell phone, grab the time from various satellites orbiting the globe, while the atomic clocks keep their own time. Then they shuttle their time readings to master servers in each data center. These masters constantly trade readings in an effort to settle on a common time. A margin of error still exists, but thanks to so many readings, the masters can bootstrap a far more reliable timekeeping service. ""This gives you faster-than-light coordination between two places,"" says Peter Mattis, a former Google engineer who founded CockroachDB, a startup working to build an open source version of Spanner . Google calls this timekeeping technology TrueTime, and only Google has it. Drawing on a celebrated research paper Google released in 2012, Mattis and CockroachDB have duplicated many other parts of Spanner-but not TrueTime. Google can pull this off only because of its massive global infrastructure. A Changing World To be sure, a few others could build a similar service, namely Amazon and Microsoft. But they haven't yet. With help from TrueTime, Spanner has provided Google with a competitive advantage in so many different markets. It underpins not only AdWords and Gmail but more than 2,000 other Google services, including Google Photos and the Google Play store. Google gained the ability to juggle online transactions at an unprecedented scale, and thanks to Spanner's extreme form of data replication, it was able to keep its services up and running with unprecedented consistency. Now Google wants a different kind of competitive advantage in the cloud computing market. It hopes to convince customers that Spanner provides an easier way of running a global business, a easier way of replicating their data across multiple regions and, thus, guard against outages. The rub is that few businesses are truly global. But Google is betting its new service will give customers the freedom to expand as time goes on. Among them is JDA, a company that helps businesses oversee their supply chains, which is now testing Spanner. ""The volume of data-and velocity with which that data is coming at us-is amplifying significantly,"" says JDA global vice president John Sarvari. Spanner could also be useful in the financial markets, allowing big banks to more efficiently track and synchronize trades happening across the planet. And Google says it's already in talks with large financial institutions about this kind of thing. Traditionally, many banks were wary of handling trades in the cloud for reasons of security and privacy. But those attitudes are softening. A few years ago, Spanner was something only Google needed. Now, Google is banking on change.",en,87
125,1983,1470339055,CONTENT SHARED,-8243488279185272615,-1602833675167376798,-8527471549744389578,,,,HTML,https://pagamento.me/elopar-lanca-o-digio-cartao-para-brigar-com-nubank/,"elopar lança o ""digio"", cartão para brigar com nubank","Elopar lança o Digio , o concorrente do Nubank. Criada através da holding do Bradesco e BB, o grupo Elopar é um dos maiores conglomerados do Brasil. Dentro dele, empresas como Alelo (líder de mercado), Stelo (subadquirente) e Livelo (fidelidade), são os motores do grupo para mover boa parte do mercado financeiro no país. Lá dentro, eles também têm um banco, o CBSS , que acabou de ""parir"" seu novo filho, o Digio . O Digio vai conseguir seguir os passos do Nubank? Alguns palpites enxergam que o Nubank está perto de 1 milhão de cartões emitidos. E ainda tem a fila de espera, que chega às dezenas de milhares. Não há dúvida sobre a escala do ""roxinho"". Ele provou algo que bancos até então, não olhavam como um negócio 100% digital. Há quem diga: ""ah mas os bancos já tinham cartões sem anuidade. O que ele fizeram foi somente empacotar isso num aplicativo bonitinho."" Se a gente olhar os números captados pela empresa e a velocidade de crescimento da carteira de clientes, a combinação parece um tanto complexa de se criar uma fórmula. Não há dúvida que a combinação de conceitos do Vale do Silício (David Vélez, co-fundador da empresa, era responsável pelo fundo Sequoia no Brasil) mais uma potente estratégia de marketing, pode sim criar um pequeno gigante e o Nubank, já é um grande case de aquisição de cliente. Além de serem a fintech mais expressiva da América Latina, o Nubank ainda prepara novas viradas de chave, que vão mudar o jogo completamente, pelo menos para o modelo digital financeiro. Eles conseguiram provar que a emissão massiva de cartões, pode ser feita através de tecnologia e não somente de distribuição de pontos de venda. Isso os bancos tradicionais terão que aprender. O Digio, que tem um respaldo financeiro potente por trás e chega no mercado brasileiro com um grande caminho pela frente, num modelo já testado e validado pelo Nubank, pela startup ContaUm (que divulga ter milhares de cartões emitidos) e também pela chegada de novos bancos como Neon e o próprio Original. O Digio, que foi anunciado ontem, chega com esse respaldo econômico e com o suporte da Visa, exatamente no contraponto do Nubank (que tem a Mastercard do lado). Apesar da Elopar ser formada por Bradesco e BB, a empresa seguirá independente, funcionando dentro da holding assim como as outras empresas do grupo. Como uma divisão do banco CBSS. Diferente do Nubank, o Digio não precisa de convite para aprovar usuários. Assim como os aplicativos já disponíveis do mercado, o Digio faz todo processo de cadastro e gestão financeira dos gastos através de um aplicativo. E não vai cobrar tarifas. Esse é um teste (que o Nubank terá) e uma briga que vamos gostar de assistir. De fato, em termos de comunicação, pode embolar um pouco o jogo (no público alvo) de ambos os cartões. Mas é muito importante o Digio ter na cabeça, que esse é um jogo de produto e marketing.",pt,86
126,1316,1465443590,CONTENT SHARED,2687654465640040976,-1032019229384696495,-67483630648809830,,,,HTML,http://www.theverge.com/2016/6/8/11881786/google-search-material-design-layout-test,google is testing a new material design layout for desktop searches,"Material Design is supposed to be the visual template that unifies Google's products, but it's taken a while to reach every part of the tech company's kingdom - especially the larger products. We saw a Material Design overhaul of YouTube last month , and it seems the company is also testing the updated look on its desktop search. Engadget first spotted the change , and various users on Twitter and Reddit have also reported glimpsing the provisional update. It's not a massive change from the current look of Google's desktop searches, and the main visual difference is segmented search results into separate white cards floating on a gray background. There are a few other minor tweaks, including changing the settings icon from a cog to a three vertical dots, and updating the search button from a white-on-blue magnifying glass to a gray-on-white one. The Google homepage also gets a refresh in line with these changes. Okay, I am seeing @google search result page with Material Design. Looks good. @GoogleDesign #design #Google pic.twitter.com/SiLxOhA5aK - Jay Tyagi (@jay7yagi) May 21, 2016 One of the bigger tweaks is to Google's Knowledge Graph results - those information-dense cards that pop up alongside certain search results with basic facts like biographical details. These usually appear to the side of search results, though sometimes at the top of the page too. In the Material Design update Google is testing, it appears they're being shunted into the main column of search results, along with adverts and image searches. Of course, this is all just provisional at the moment. Google often tests various tweaks to its design with various users (e.g. turning search results from blue to black ), analyzing how the changes affect interaction with its sites. Let us know if you've seen the Material Design update yourself and what you think of it.",en,86
127,2005,1470657286,CONTENT SHARED,-8159730897893673528,1895326251577378793,8303966574908950301,,,,HTML,http://www.techrepublic.com/article/amazon-still-crushing-cloud-competition-says-gartner-magic-quadrant-for-iaas/,"amazon still crushing cloud competition, says gartner magic quadrant for iaas - techrepublic","""Up and to the right"" has become shorthand for ""the winning position."" Usually, a number of companies wrangle for position in the upper-right quadrant of Gartner's Magic Quadrant, a market research report that looks at leaders in an industry. But, in the newest Magic Quadrant for cloud infrastructure services , Amazon sits virtually alone, with only Microsoft Azure to keep it company. But, that's not really news. Amazon Web Services (AWS), after all, has completely dominated cloud infrastructure services for so long that Gartner crowns it the ""safe choice."" What is interesting, however, is just how far everyone else keeps falling behind. Looking at successive years of Gartner's IaaS Magic Quadrant reveals a pack of also-rans retreating into the distance. Years and years of winning In 2014, Gartner discovered that AWS had 5X the utilized capacity of its next 14 nearest competitors combined. By 2015, that lead had jumped to 10X . While we'd expect to see an industry pioneer dominate the early days of that industry, AWS is unique in its ability to continue to overshadow its ""peers"" for years on end. SEE Amazon spills the secrets of its success: Impossible goals and repeated failure Part of this derives from AWS' ability to appeal to developers, its original audience, while also being ""frequently chosen"" by traditional IT, Gartner noted. The analyst firm also declared, ""AWS is the provider most commonly chosen for strategic, organization-wide adoption."" Not surprisingly, then, ""AWS has a diverse customer base and the broadest range of use cases, including enterprise and mission-critical applications,"" with ""an ecosystem of open-source tools, along with more than a thousand technology partners that have licensed and packaged their software to run on AWS, have integrated their software with AWS capabilities, or deliver add-on services."" This jibes well with Amazon CFO Brian Olsavsky's contention that AWS keeps winning because of its superior speed of innovation, depth of functionality, and breadth of ecosystem. But, what about everyone else? Meet the rearview mirror Of course, Microsoft isn't far behind, with the enterprise giant integrating on-prem and in-cloud workloads in ways no one else can. According to Gartner, ""Microsoft's brand, existing customer relationships, history of running global-class consumer internet properties, deep investments in engineering and innovative roadmap have enabled it to rapidly attain the status of strategic cloud IaaS provider."" SEE Why Kubernetes may be a bigger threat to Amazon than Google's cloud Microsoft, in short, is a credible competitor to AWS. Google isn't yet in the same class, but it at least attains ""Visionary"" status with Gartner. Everyone else, however, is a rounding error. For some time, other cloud providers seemed to be closing the gap with AWS. No more. Take a look at these three consecutive Magic Quadrants. Here's 2014: And here's 2015: Finally, here's the cloud infrastructure market in 2016: See what happened? AWS looks like it's treading water, but really it's simply defining true north for everyone else. Microsoft Azure has been closing the gap. Google, for its part, really does appear to be running in place relative to its bigger cloud competitors. And CenturyLink, IBM, Rackspace, etc.? They're all just etcetera, and getting more so with each passing day. Also see",en,86
128,2580,1476787589,CONTENT SHARED,-3058031327323357308,7645894863578715801,7779918589008538147,Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:49.0) Gecko/20100101 Firefox/49.0,SP,BR,HTML,https://dzone.com/articles/the-java-8-api-design-principles,the java 8 api design principles,"This article is featured in the new DZone Guide to Modern Java, Volume II. Get your free copy for more insightful articles, industry statistics, and more. Anyone that writes Java code is an API designer! It does not matter if the coders share their code with others or not, the code is still used; either by others, by themselves or both. Thus, it becomes important for all Java developer to know the fundamentals of good API design. A good API design requires careful thinking and a lot of experience. Luckily, we can learn from other clever people like Ference Mihaly, whose blog post inspired me to write this Java 8 API addendum. We relied heavily on his checklist when we designed the Speedment API. (I encourage you all to read his guide.) Getting it right from the start is important because once an API is published, a firm commitment is made to the people who are supposed to use it. As Joshua Block once said: ""Public APIs, like diamonds, are forever. You have one chance to get it right, so give it your best."" A well-designed API combines the best of two worlds, a firm and precise commitment combined with a high degree of implementation flexibility, eventually benefiting both the API designers and the API users. Why use a checklist? Getting the API right (i.e. defining the visible parts of a collection of Java classes) can be much harder than writing the implementation classes that makes up the actual work behind the API. It is really an art that few people master. Using a checklist allows the reader to avoid the most obvious mistakes, become a better programmer and save a lot of time. API designers are strongly encouraged to put themselves in the client code perspective and to optimize that view in terms of simplicity, ease-of-use, and consistency - rather than thinking about the actual API implementation. At the same time, they should try to hide as many implementation details as possible. Do not Return Null to Indicate the Absence of a Value Arguably, inconsistent null handling (resulting in the ubiquitous NullPointerException) is the single largest source of Java applications' errors historically. Some developers regard the introduction of the null concept as one of the worst mistakes ever made in the computer science domain. Luckily, the first step of alleviating Java's null handling problem was introduced in Java 8 with the advent of the Optional class. Make sure a method that can return a no-value returns an Optional instead of null. This clearly signals to the API users that the method may or may not return a value. Do not fall for the temptation to use null over Optional for performance reasons. Java 8's escape analysis will optimize away most Optional objects anyway. Avoid using Optionals in parameters and fields. Do This: Don't Do This: Do not Use Arrays to Pass Values to and From the API A significant API mistake was made when the Enum concept was introduced in Java 5. We all know that an Enum class has a method called values() that returns an array of all the Enum's distinct values. Now, because the Java framework must ensure that the client code cannot change the Enum's values (for example, by directly writing to the array), a copy of the internal array must be produced for each call to the value() method. This results in poor performance and also poor client code usability. If the Enum would have returned an unmodifiable List, that List could be reused for each call and the client code would have had access to a better and more useful model of the Enum's values. In the general case, consider exposing a Stream, if the API is to return a collection of elements. This clearly states that the result is read-only (as opposed to a List which has a set() method). It also allows the client code to easily collect the elements in another data structure or act on them on-the-fly. Furthermore, the API can lazily produce the elements as they become available (e.g. are pulled in from a file, a socket, or from a database). Again, Java 8's improved escape analysis will make sure that a minimum of objects are actually created on the Java heap. Do not use arrays as input parameters for methods either, since this - unless a defensive copy of the array is made - makes it possible for another thread to modify the content of the array during method execution. Do This: Don't Do This: Consider Adding Static Interface Methods to Provide a Single Entry Point for Object Creation Avoid allowing the client code to directly select an implementation class of an interface. Allowing client code to create implementation classes directly creates a much more direct coupling of the API and the client code. It also makes the API commitment much larger, since now we have to maintain all the implementation classes exactly as they can be observed from outside instead of just committing to the interface as such. Consider adding static interface methods, to allow the client code to create (potentially specialized) objects that implement the interface. For example, if we have an interface Point with two methods int x() and int y(), then we can expose a static method Point.of(int x, int y) that produces a (hidden) implementation of the interface. So, if x and y are both zero, we can return a special implementation class PointOrigoImpl (with no x or y fields), or else we return another class PointImpl that holds the given x and y values. Ensure that the implementation classes are in another package that are clearly not a part of the API (e.g. put the Point interface in com.company. product.shape and the implementations in com.company.product.internal.shape). Do This: Don't Do This: Favor Composition With Functional Interfaces and Lambdas Over Inheritence For good reasons, there can only be one super class for any given Java class. Furthermore, exposing abstract or base classes in your API that are supposed to be inherited by client code is a very big and problematic API commitment. Avoid API inheritance altogether, and instead consider providing static interface methods that take one or several lambda parameters and apply those given lambdas to a default internal API implementation class. This also creates a much clearer separation of concerns. For example, instead of inheriting from a public API class AbstractReader and overriding abstract void handleError(IOException ioe), it is better to expose a static method or a builder in the Reader interface that takes a Consumer<IOException> and applies it to an internal generic ReaderImpl. Do This: Don't Do This: Ensure That You Add the @FunctionalInterface Annotation to Functional Interfaces Tagging an interface with the @FunctionalInterface annotation signals that API users may use lambdas to implement the interface, and it also makes sure the interface remains usable for lambdas over time by preventing abstract methods from accidently being added to the API later on. Do This: Don't Do This: Avoid Overloading Methods With Functional Interfaces as Parameters If there are two or more functions with the same name that take functional interfaces as parameters, then this would likely create a lambda ambiguity on the client side. For example, if there are two Point methods add(Function<Point, String> renderer) and add(Predicate<Point> logCondition) and we try to call point.add(p -> p + "" lambda"") from the client code, the compiler is unable to determine which method to use and will produce an error. Instead, consider naming methods according to their specific use. Do This: Don't Do This: Avoid Overusing Default Methods in Interfaces Default methods can easily be added to interfaces and sometimes it makes sense to do that. For example, a method that is expected to be the same for any implementing class and that is short and ""fundamental"" in its functionality, is a viable candidate for a default implementation. Also, when an API is expanded, it sometimes makes sense to provide a default interface method for backward compatibility reasons. As we all know, functional interfaces contain exactly one abstract method, so default methods provide an escape hatch when additional methods must be added. However, avoid having the API interface evolve to an implementation class by polluting it with unnecessary implementation concerns. If in doubt, consider moving the method logic to a separate utility class and/or place it in the implementing classes. Do This: Don't Do This: Ensure That the API Methods Check the Parameter Invariants Before They Are Acted Upon Historically, people have been sloppy in making sure to validate method input parameters. So, when a resulting error occurs later on, the real reason becomes obscured and hidden deep down the stack trace. Ensure that parameters are checked for nulls and any valid range constrains or preconditions before the parameters are ever used in the implementing classes. Do not fall for the temptation to skip parameter checks for performance reasons. The JVM will be able to optimize away redundant checking and produce efficient code. Make use of the Objects.requireNonNull() method. Parameter checking is also an important way to enforce the API's contract. If the API was not supposed to accept nulls but did anyhow, users will become confused. Do This: DON'T DO THIS: Do not Simply Call Optional.get() The API designers of Java 8 made a mistake when they selected the name Optional.get() when it should really have been named Optional.getOrThrow() or something similar instead. Calling get() without checking if a value is present with the Optional.isPresent() method is a very common mistake which fully negates the null elimination features Optional originally promised. Consider using any of the Optional's other methods such as map(), flatMap() or ifPresent() instead in the API's implementing classes or ensure that isPresent() is called before any get() is called. Do This: Don't Do This: Consider Separating Your Stream Pipeline on Distinct Lines in Implementing API Classes Eventually, all APIs will contain errors. When receiving stack traces from API users, it is often much easier to determine the actual cause of the error if a Stream pipeline is split into distinct lines compared to a Stream pipeline that is expressed on a single line. Also, code readability will improve. Do This: Don't Do This: For more insights on Jigsaw, reactive microservices, and more get your free copy of the new DZone Guide to Modern Java, Volume II!",en,86
129,2496,1475518627,CONTENT SHARED,-6654470039478316910,-4465926797008424436,6720261163003363785,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36",SP,BR,HTML,https://realm.io/news/chiu-ki-chan-advanced-android-espresso-testing/,advanced android espresso,"Espresso is a very powerful UI testing framework for Android. Chiu-Ki Chan describes several techniques to make the most of it, running through: combining matchers ( withParent , isAssignableFrom ) to pinpoint your view; onData and RecyclerViewAction s to test ListView and RecyclerView ; Custom ViewAction and ViewAssertion ; and using Dagger and Mockito to write repeatable tests. What is Espresso? (00:28) Espresso is automatic UI testing ( ""no hands testing!"" ). Once you write the code, Espresso will execute using events: do various things that as a normal user you will do, then verify the test (different from JUnit testing where there is no UI element). In Espresso you can see what is happening. It is a first-party library (Google wrote it under the Android testing support library ). It simulates User interactions (clicking, typing), and it does automatic synchronization of test action with app UI . Espresso will watch the UI thread (the thread that renders all the pixels on screen). Before Espresso, I used ActivityInstrumentationTestCase2 : my test would fail because the button was not drawn yet and I was trying to click on it. You put in some Sleep statements hoping that after Sleeping the pixels will be drawn and then you can click on it. With the automatic synchronization Espresso will loop the Sleep until the UI thread is idle then it will verify and click things. You do not need to Sleep. Espresso has a fluent API (can tack things one after another). The first thing: locate the view that you want to act on. Espresso has the concept of ViewMatchers. Once you locate the view then you can perform actions to it (typing, clicking, swiping). You can check using ViewAssertions. Formula Basic Espresso test: The Hello World app. It has a greet button - when you click on it it will disable it. Using this formula we first need to locate this view: ViewMatcher Espresso comes with a ViewMatcher - withId. In our case we want to look for the view with the R.id.greet_button . We will use the ViewAction click() to click on it. Then, we want to assert that the greet button is no longer enabled. ViewAction In this ViewAssertion (see below), instead of it is not enabled, we are doing it is enabled and then negating it. You can combine different Matchers and Assertions to do exactly what you need. ViewAssertion More info: Espresso library (05:03) Highlighted in yellow (withId, click, matches, isEnabled): comes within Espresso. They are Android-specific: Hamcrest library (06:00) There is also not , from the Hamcrest library (Java library). It has different logic startWith , endWith , if we are dealing with strings and not and allOf . Together with the Espresso patch (Android-specific) and the Java pod you can do very powerful expressions. More Info: Combining Matchers (06:17) Let's say that you wrote this app and you want to verify the Toolbar title is displaying the correct string. Your layout does not contain that view (you do not have the ID). To find this view, we will use Hierarchy Viewer (examines the hierarchy of your app). You have a root node: all the different views hang off it and it shows you the structure (on the screen, not just the path that you set with setContentView ). And the TextView is the one that has the ""My Awesome Title"" string (the one that you want to verify). We have located the view that we want to match. Bad news: There is no ID. But using all the different Matchers, I know that this TextView has a parent (Toolbar). It can do a combination - I will find a view that has two conditions isAssignableFrom : 1) TextView (I am a TextView, I want to find a view that is a TextView), 2) Toolbar (find that one TextView that is under a Toolbar). The reason why we want to do that with two conditions (instead of just the TextView one) is that there can be multiple TextViews. To pinpoint that one particular view I will use allOf Matcher. Then I wrap it into a helper function; every time I want to match a Toolbar I can just call Match Toolbar and then pass a string. Custom Matchers (09:07) You can write a custom Matcher. The previous example works, but it is very fragile (Toolbar class: today, it contains the TextView as the direct child of a Toolbar; tomorrow, they may want to wrap it inside CoordinatorLayout because is not a part of public API). The Toolbar class has a function toolbar.getTitle() which, since it is in the public API, it is going to be stable. Instead of trying to pinpoint the TextView, the ViewMatcher takes on a view that is assignable from the Toolbar class. We are matching on the Toolbar class and we are going to verify that it has the Toolbar title (which is a custom Matcher). With Toolbar title generates a custom Matcher. The BoundedMatcher only works on the Toolbar. We have access to the Toolbar functions. In Match Safely function we wanted to use the getTitle out of the Toolbar and matches the text that we give it. Instead of passing a string you want to match, you are passing a TextMatcher . This way you can use all the Hamcrest test Matchers (such as equalTo ), which is your plain old comparison. Y You can do Prefix Matching, Suffix Matching or Contain String (if you just want the string to be anywhere). If you want to use a function that is not a part of Espresso you can write your own Matcher. More info: ListView operates on a different way. Instead of looking for a view, you are looking for a particular object in the Adapter that is supporting the ListView. That is why your onData takes an object Matcher. You have the same Perform, View Action, Check, View Assertion. And on top, Data options. You can have multiple ListViews. You can pinpoint it in with inAdapterview or you can look inside the item and match on the particular child view. I have a ListView and it has a list of numbers. I want to run a test to verify that item 27 stays 27. I am looking for the item that stays 27, it will scroll and look for it. The basic setup of the app: I have an Adapter (backed by an array of integers), and I will set it as the data for this ListView . The item is a wrapper around a simple integer that has a two string function (the ArrayAdapter knows what to display). Also, when you click on an item the footer TextView it will display that value. We also want to verify that. We are going to verify that the bottom TextView is not displayed. I said, withID this text and I want to match it as not displayed. I am going to use the onData function to look for the item with value 27 inside the AdapterView that has the ID list. Once I find it I will click on it. After I click on it, I will then verify that the bottom TextView is containing the text 27 and also it is also displayed. You have the onView with text ID. And I have performed two checks. Rather than looking for the view again, I can chain them. It will check for the first condition (it has the text 27), and then it will check that it is displayed. You can keep stacking on things, you can perform actions and check, and then perform action. It does not need to go look for the view again. You already have the view in your hand. withValue is a custom Matcher that I wrote. BoundedMatcher takes in an integer and in the Match Safely function it compares to the value of the item using strings. It is a string comparison. We cannot use onData: RecyclerView is a ViewGroup (not AdapterView ). The onData operator only works on AdapterView - it is not just ListView , you can use it with GridView as well. We are going back to using the onView View Matchers. We will use that onView and perform check formula. To click on item is different. We will do the same pre-condition checking that the text is not displayed, but to click on that particular list item we are going to say that onView with ID RecyclerView . We are operating on that RecyclerView level, perform, and then we will use RecyclerViewActions (from the Espresso library). We will have a specific action ( actionOnItemAtPosition(27) ). You look for position item 27 and perform the action click on it. actionOnItemAtPosition is very different (see code below): we are no longer doing Data Matching and we need to look for the position and then perform it all at once in this one giant View Action in it. The rest of the code is the same: we will continue to verify that it displays 27 and it is not hidden. This replaced the middle line (onView). Two ways of thinking, one the AdapterView of looking for things focuses on the data, but if you are doing RecyclerView then you have to go back and focus on the view again. You find the RecyclerView and act on it with the RecyclerViewActions . There are a couple of other ones. If you do not want to look for an at position you can also look for the View Holder which then you have a Matcher that is looking for a specific ViewHolder or you can also do the action item with a ViewMatcher . Make sure that you know they are different. More information: Idling Resource (19:43) Espresso has this notion of the UI queue being idle - no more UI events handled, are queued. That includes clicks, rendering. It also has the notion of idling: the AsyncTask pool is empty. But beyond that it does not know how to wait for your app to be ready to go. Espresso provides a nice framework for you to write your own idling conditions (Idling Resources). Example : Wait until an IntentService is not running ( IntentServiceIdlingResource ). Whenever you write an Item Resource you need to Override three functions: getName , registerIdleTransitionCallback , and isIdleNow . Make sure that you have a unique name (e.g. class name) because you are going to be registering this Idling Resource later, and you use the name as the key. Then register the IdleTransitionCallback you stash that callback into a particular number variable so that in isIdleNow now you can call it when you determine that whatever that you are waiting for is done. You can tell Espresso, ""I am idle, go ahead"". In our particular case we are going to define idle as the IntentService is not running ( isIntentServiceRunning() ). Then I have a callback, I will call it and it will return the boolean. I am going to query the ActivityManager for the specific name (IntentService). If it is there then it is running, I will return true; if I could not find it then I return false. After you have defined your custom Idling resource, you need to register it (most of the time this is JUnit4 syntax). I have an annotation that is before and after - you can register it before your tests run. And then after your test is run unregister it. More info: Dagger and Mockito (31:03) Dagger is a framework for Dependency injection: you have a central repository of classes. We are trying to make tests work: we are doing to use Dagger to provide different objects for app and test; we will use Mockito to mock objects in test. In Dagger2 you are going to define a Component (a collection of different modules that can provide classes to your app and your test). In our case we are going to do something with the clock. It is a classic example because if your app depends on the current time then you cannot verify anything because the current time changes. In the ApplicationComponent we are going to have a real clock module (provides the actual current time) and in the test we are going to have a module that is mocked (using annotation). You have the app's Component and then you have the list of Modules. Dagger is going to then go ahead and do co-generation for you at compile time. You can then call these functions in your app. In sum, you have an ApplicationComponent that provides a clock module and then you also have a TestComponent that provides a mock clock module. The ApplicationComponent only injects into the main activity, it is not aware of the existence of tests. The TestComponent need to inject to both and they are singletons. If you are providing this particular object and then you are changing it in your test the app is going to use the exact same object so that it gets the same value. In your application, when you are onCreate you are creating the application. DaggerDemoApplication_ApplicationComponent is the auto-generated class. It will go through all your annotations, which will allow you to create a component (collection of modules). In your test you are going to be able to use the setComponent function which is defined on the activity, which will provide a different set of modules. With Mockito you can control time. In Joba-Time (a pretty popular dateTime libray for Java) dateTime will give you the current day and time. In your test Mockito allows you to say when someone calls the function getNow on my mock clock return this particular point in time. When you run your test you will be able to do something like this. Maybe you have a TextView that displays today's date. You can verify that it will display the string 2008-09-23 because that is the time the clock is going to provide. Without this mocking, you run the test today you return this value, you run the test tomorrow you return this other value. This is the power of combining Mockito and Dagger and Espresso so that they all work together. You can write test that pretends to be a human, but also in a predictable environment so that you can keep running the test and it will pass unless you wrote a bug in your future code. More info - Blog post abotu Mockito - Example repo - Example using the Shared Preferences Running through the basics (ViewMatcher, ViewAction, ViewAssertion) I showed you how to match the Toolbar using two different ways (combining Matchers, writing your own custom Matcher). I went through ListView and RecyclerView, and how they differ. Spent some time on IdlingResource, and Dagger and Mockito to get you to the next level. I have open souced a complete app . It uses: the Google Plus API, the Nearby API and the Database. At first glance it is really hard to test. It asks you to log into Google Plus. I have to set up a device that has a certain Google Plus account and then I have to log in; next thing it is going to use the Nearby API. It will spell a word depending on the name of the person that you are Google Plus profile logged in as. I used the same technique that I described, Dagger, Mockito to mock stuff. Mockito is more advanced because I need to capture the Google sign in with the listener and then replay it right back. It also has other testing. I have some models, POJO's, I set some function then I verify the conditions. I also have UI-less Instrumentation (to test the Database). I am not executing the UI, but I also need context because it is still going through Android classes. And, of course, it also has Espresso. Thank you! Resources & Q&A (32:28) More links: Q: For the combined Joda-Time and, Mockito and Espresso example, where did you initialize Joda-Time? Did you rely on initializing it with the application? Chiu-Ki: I just call new dateTime. Q: You have to initialize it with a context at some point early in your application lifecycle, and it is suggested that you do it in application onCreate. But I know that we have also been talking about separating test environments so that your, your context is perhaps different than it would be in the normal run time. I am wondering do you have to do anything special to initialize Joda-Time in the test context? Chiu-Ki: I do not and so far nothing blew up, but maybe some other library needs the context. It is not specific to Joda-Time. Currently this set-up only has the clock module component. You will have an additional module that is an Android module which is capable of providing a context. Then in your clock module in the constructor it will take in a context which will be provided by the Android module. And Dagger is smart enough to just do essentially Class Matching. And then the Android module raises its hand, I know how to give you a context. And then it will provide that. And then within the clock module, you will be able to use that. Once you have that context in the constructor of your clock module then you can come back here to the getNow function inside your clock module and then use the context there. Q: Question specific to the Dagger2 example that you showed. The difficulty in Dagger2 is really, there is no module override available in Dagger2. I wanted to know what would be your recommend workaround for being able to override specific dependencies inside of your mock modules without having to completely rewrite, the entire module. You want to have code duplication on the test classes and also on the application. Your example only has the clock module which provides only one dependency, but if you have, 50 dependencies and you only need to override just the one. You would not want to create a test module with 49 real classes and only one mock. What would be the best workaround? Chiu-Ki: It has been a pain point for Dagger2, mostly because it is available in Dagger1! Why did you take that away from us? I have not personally encountered that just because the way I set up my modules, but I have heard people, what they have done is that, you have your app source and then your test source. you can have another source that is the common source which contains the modules that are shared between the test and the app and then when you make your component that is for the app you import those, you include those. And then the test also includes the same ones. And then you only implement each thing that is different once. I heard people do that. Q: I ran into one issue I exactly used the same set-up where I had one class, I had a simple UI which was just fetching a list of GitHub issues at the click of a button and the dependency, the API dependency, was being injected by Dagger and I replaced it by a mock inside one of my modules. I only wanted to verify interaction with that mock once the button's clicked. But because that mock did not return any action that would affect the UI thread, then it was hanging forever. I want to verify interaction with a mock, but I do not want to have to code up an action which is going to affect the UI thread. I just want to verify interaction with a mock. Is there a workaround for things like this? Chiu-Ki: It is a network call, except that you are mocking the result; Is that triggered by a button click or any user action? When does the fetching start? Q: You click the button, it talks to that injected mock. Then that injected mock, you are passing a callback method which will just return a list of issues which will populate an adapter, for example. Chiu-Ki: in that case I would expect it to, instead of hang, go too fast. It did not wait until the mock to come back and was starting to verify things. I was not sure why it was behaving that way. I am not able to diagnose why that happened because that is not the expected behavior that I would be. Q: The fix for me was to take that mock and just use Mockito to manually call the callback that the activity was passing. Chiu-Ki: Espresso testing at that point. You do not really need to, because then nobody is clicking anything. Q: The UI's wired up properly with the back end. You click a button and you want to make sure that when Espresso clicks this button then there is interaction with a mock. Meaning that clicking this button effectively tried to do the API call, you do not care about what the API call returned, but you want to verify that there is some interaction with that mock. Chiu-Ki: Write a blog post, Twitter to me and I will take a look because I feel like there is more to what we can describe. Q: I find one of my major pain points is coming up with complex Macthers for hierarchies that are much more complex than what you showed. I was wondering if you have any general suggestions in finding those complex Matchers perhaps with multiple parents and multiple siblings in a way that will not be fragile. Chiu-Ki: My recommendation would be to simplify your View Hierarchy. Wrap things in the CustomView so that semantically they make more sense. The reason why you have all these things laid out is because probably when you are looking at the UI they are not individual buttons and individual tacks. They are probably logical units. I think it will help not just testing, but your actual app and development if you can group them in logical units. Then at that point then you can hide the internal logic and say, this manual item thing has six things inside, but then I can get them with programmatic calls like the getTitle one. Q: One hardest problems is when elements repeat themselves, such as if you are using a ListView and each item of the ListView has several buttons. It can be difficult to find a container maybe with the text you are looking for and then go back down the chain after that to get to the buttons within the hierarchy. Is there anything that would help sort of expose the chain to get to a path? Chiu-Ki: You can see who is a child of whom; that helps you to expose the structure. One thing that you could do is a helper function. You encapsulate that, every time you are trying to mesh this particular way of setting up you can at least repeat it. I basically smell bad code when you tell me that your view explodes with too many children and too many level deep and it is really difficult to find things. Without going through your specific example it is just really hard to give general advice.",en,86
130,2787,1480422617,CONTENT SHARED,3739926497176994524,8676130229735483748,-2273278933867542008,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36",SP,BR,HTML,http://meiobit.com/355936/a-chegada-arrival-review-sem-spoilers/,resenha - a chegada,"Supondo que haja vida inteligente no Universo além da civilização humana, ela ainda não fez ou não conseguiu fazer contato. Talvez essa vida inteligente alienígena ainda não esteja em um nível tecnológico que possibilite que a detectemos ou simplesmente os ETs estejam com vergonha de dividir o Universo com a gente. Ou também há a triste possibilidade de os humanos serem a forma vida mais inteligente no Universo conhecido : formas de vida extra-terrestre ainda estariam a evoluir. Supondo que haja sim vida inteligente alienígena e desconsiderando a recomendação de Stephen Hawking , como seria um primeiro contato dos ETs com os seres humanos? Essa é a premissa de um bocado de filmes, séries e tantas outras obras de ficção científica: imaginar como seria um primeiro contato com uma civilização alienígena bem desenvolvida que acabou de chegar ao nosso pequeno planeta azul. A maioria dessas histórias, quando estas não tentam aniquilar a raça humana, não foca tanto na dificuldade que é lidar com uma linguagem totalmente desconhecida: em muitas delas os alienígenas são tão avançados que já sabem inglês. Assim fica fácil, mas e quando os ETs falam de um jeito, escrevem de outro e nada parece ser compreensível por humanos? Como saber o propósito dos visitantes em nosso planeta? Essa é a premissa de . Sony Pictures Brasil - A Chegada | Amy Adams | Hoje nos cinemas Baseado no conto (que Story of Your Life de Ted Chiang, o recente filme da Paramount graças a Odin foi distribuído pela Sony Pictures fora da América do Norte) retrata a dificuldade da lingüista Louise Banks (Amy Adams) e do físico Ian Donnelly (Jeremy Renner) em estabelecer um primeiro contato com seres alienígenas. Seres esses que chegaram ao planeta à bordo de uma dúzia de enormes naves em forma de concha. As naves pairam sobre 12 diversos locais de forma aleatória. Vários países mandam diferentes equipes de militares e cientistas para um primeiro contato. O foco de A Chegada é todo sobre o ponto de vista do imenso trabalho lingüístico de Jó da personagem Louise (Adams) em interpretar os símbolos escritos pelos alienígenas, supostamente pacíficos. E a árdua tarefa de convencer os militares a não matar logo de cara os simpáticos visitantes extra-terrestres com 7 pernas sem antes conhecer o propósito da visita. É um trabalho tão grande, a personagem fica tão imersa no idioma dos heptapodes que ele afeta sua percepção de tempo. E infelizmente uma interpretação bem errada acaba por fazer com que os militares de outra região declarem guerra aos ETs. O filme tem claramente um vilão e ele é a estupidez humana, que desconhece limites. A Chegada foi dirigido pelo mesmo Denis Villeneuve do tenso e isso foi excelente: enquanto em Sicario o espectador não tem descanso algum, em A Chegada quem não parece ter descanso é a protagonista. Seja pela curiosidade, seja pela fascinação pelos visitantes. Ou seja pelo medo... dos humanos. A produção escolheu uma excelente locação para a ""concha"" extra-terrestre pairar e a fotografia é simplesmente linda. O filme se passa quase todo em Montana mas escolheram um belo lugar no Canadá. Se bem que uma das naves paira sobre a Venezuela e a fotografia dos telejornais nos mostra o nível de barbárie que é aquela região. Melhor sorte teve Cuba. Enfim, sobre a trilha sonora original, ela muitas vezes nos faz temer os visitantes. E nos envolve quanto ao drama temporal da Louise. O roteiro é assinado pelo Eric Heisserer ( ele é mais conhecido nos filmes de terror ) e só tem um ou outro problema de ritmo. Tal detalhe não chega a comprometer a obra. Considerações A Chegada é basicamente (1997) feito direito. E sem as aloprações do longo Interestelar . O filme vale cada centavo do ingresso e dura o que tem de durar sem desrespeitar a inteligência do espectador. Não é um filme para todos, assim como Sicario também não era. 4,5 de 5 Lois Lanes. Relacionados: A Chegada , Abbott and Costello , Amy Adams , arrival , Blade Runner 2049 , Denis Villeneuve , Eric Heisserer , Ficção Científica , Forest Whitaker , Jeremy Renner , Sapir-Whorf , Sicario , Sony , Sony Pictures , Story of Your Life , Ted Chiang , The Arrival",pt,85
131,1438,1466220945,CONTENT SHARED,5302085907556673015,-1032019229384696495,7678165322893628235,,,,HTML,http://www.theverge.com/2016/6/17/11964786/google-lady-cfo-protest-response,google employees are adding 'lady' to their job titles to fight sexism,"More than 800 members of Google's staff are standing together in a showing against sexism today by appending a single word to their job titles: ""Lady."" Business Insider has details on the protest , which is happening in response to a ludicrous comment made during Alphabet's shareholder meeting last week, when someone referred to company CFO Ruth Porat as the organization's "" lady CFO ."" The internet immediately exploded in outrage over the sexist remark, and now, one week later, Google's staff has found its own way of responding. According to Business Insider , someone made the title change suggestion to an email group, and it quickly caught on. They launched an internal site to promote the protest, which was planned for this Thursday and Friday. It even got this awesome GIF featuring some of Google's newly proposed emoji : Google In a statement to USA Today , Meg Mason, Google's partner operations manager for shopping - or, for today, its lady partner operations manager for shopping - says of the protest, ""I wanted to do something fun and 'Googley' that allowed us all to stand together, and to show that someone's gender is entirely irrelevant to how they do their job."" Women and men within the company are adopting the lady prefix, making the change over these two days in their email signatures and within Google's internal directory, according to Business Insider . It's a smart and simple response that, like Mason says, effectively conveys just how irrelevant gender is to being able to do your job.",en,85
132,1408,1466012967,CONTENT SHARED,-9019233957195913605,-9016528795238256703,8012540432966887758,,,,HTML,http://observatoriodaimprensa.com.br/curadoria-de-noticias/berners-lee-quer-criar-outra-web/,berners-lee quer criar outra web,"Tim Berners-Lee, o cientista que há 27 anos criou a World Wide Web, mais conhecida como apenas Web, pretende desenvolver um novo sistema de interação entre computadores para neutralizar o controle governamental e privado sobre as comunicações entre indivíduos. Berners-Lee reuniu em San Francisco, na Califórnia, um grupo de especialistas em computação para tentar criar uma rede menos sujeita a violações da privacidade e dê aos usuários maior controle sobre seus dados pessoais. Os detalhes sobre o que o grupo de cientistas está pesquisando ainda são pouco conhecidos, mas Lee afirmou que o trabalho parte dos estudos sobre a infraestrutura de programação digital da moeda eletrônica Bitcoin, do site Wikileaks e os programas de troca gratuita de musicas. O projeto foi batizado de Web Descentralizada e foi discutido no Decentralized Web Summit , na segunda semana de junho de 2016. A grande preocupação do grupo de cientistas da Web Descentralizada é o aumento do controle de redes sociais como o Facebook , do sistema de buscas Google e o de comércio eletrônico da Amazon sobre o uso da Web. Mais detalhes na reportagem The Web's Creator Looks to Reinvent It (acesso por meio de cadastramento grátis) Todos os comentários",pt,85
133,1894,1469624606,CONTENT SHARED,8749720044741011597,-2525380383541287600,-7324931632676877948,,,,HTML,https://br.udacity.com/course/how-to-use-git-and-github--ud775/,como usar o git e o github,"Lição 1: Navegando em um Histórico de Commits Nesta lição, você aprenderá sobre alguns tipos diferentes de sistemas de controle de versão e descobrir o que torna Git um grande sistema de controle de versão para programadores. Você também vai começar a praticar usando Git para ver o histórico de um projeto existente. Você vai aprender a ver todas as versões que foram salvas, fazer o checkout de uma versão anterior e comparar duas versões diferentes Lição 2: Criar e Modificar um Repositório Nesta lição, você aprenderá como criar um repositório e salvar versões de seu projeto. Você vai aprender sobre a área de teste, fazendo commit do seu código, branches e mesclagem, e como você pode usá-los para torná-lo mais eficiente e eficaz Lição 3: Usando GitHub para Colaborar Nesta lição, você vai começar a prática usando GitHub ou outros repositórios remotos para compartilhar suas alterações com os outros e colaborar em projetos multi-desenvolvedor. Você vai aprender como fazer e rever uma solicitação de recebimento no GitHub. Finalmente, você vai começar a prática através da colaboração com outros estudantes Udacity para escrever uma história ""criando sua própria aventura"". Projeto: Contribuir com um projeto ao vivo Os alunos irão publicar um repositório que contém suas reflexões do curso e enviar uma solicitação de recebimento a uma História colaborativa ""criando sua própria aventura"".",pt,85
134,1357,1465667023,CONTENT SHARED,-3027055440570405664,7774613525190730745,-5387740831049193382,,,,HTML,http://joelonsoftware.com/items/2016/05/30.html?utm_source=clouddevweekly&utm_medium=email,joel on software,"Introducing HyperDev One more thing... It's been awhile since we launched a whole new product at Fog Creek Software (the last one was Trello , and that's doing pretty well ). Today we're announcing the public beta of HyperDev , a developer playground for building full-stack web-apps fast. HyperDev is going to be the fastest way to bang out code and get it running on the internet. We want to eliminate 100% of the complicated administrative details around getting code up and running on a website. The best way to explain that is with a little tour. Step one. You go to hyperdev.com. Boom. Your new website is already running. You have your own private virtual machine (well, really it's a container but you don't have to care about that or know what that means) running on the internet at its own, custom URL which you can already give people and they can already go to it and see the simple code we started you out with. All that happened just because you went to hyperdev.com. Notice what you DIDN'T do. You didn't make an account. You didn't use Git. Or any version control, really. You didn't deal with name servers. You didn't sign up with a hosting provider. You didn't provision a server. You didn't install an operating system or a LAMP stack or Node or operating systems or anything. You didn't configure the server. You didn't figure out how to integrate and deploy your code. You just went to hyperdev.com. Try it now! What do you see in your browser? Well, you're seeing a basic IDE. There's a little button that says SHOW and when you click on that, another browser window opens up showing you your website as it appears to the world. Notice that we invented a unique name for you. Over there in the IDE, in the bottom left, you see some client side files. One of them is called index.html. You know what to do, right? Click on index.html and make a couple of changes to the text. Now here's something that is already a little bit magic... As you type changes into the IDE, without saving, those changes are deploying to your new web server and we're refreshing the web browser for you, so those changes are appearing almost instantly, both in your browser and for anyone else on the internet visiting your URL. Again, notice what you DIDN'T do: You didn't hit a ""save"" button. You didn't commit to Git. You didn't push. You didn't run a deployment script. You didn't restart the web server. You didn't refresh the page on your web browser. You just typed some changes and BOOM they appeared. OK, so far so good. That's a little bit like jsFiddle or Stack Overflow snippets, right? NBD. But let's look around the IDE some more. In the top left, you see some server side files. These are actual code that actually runs on the actual (virtual) server that we're running for you. It's running node. If you go into the server.js file you see a bunch of JavaScript. Now change something there, and watch your window over on the right. Magic again... the changes you are making to the server-side Javascript code are already deployed and they're already showing up live in the web browser you're pointing at your URL. Literally every change you make is instantly saved, uploaded to the server, the server is restarted with the new code, and your browser is refreshed, all within half a second. So now your server-side code changes are instantly deployed, and once again, notice that you didn't: Save Do Git incantations Deploy Buy and configure a continuous integration solution Restart anything Send any SIGHUPs You just changed the code and it was already reflected on the live server. Now you're starting to get the idea of HyperDev. It's just a SUPER FAST way to get running code up on the internet without dealing with any administrative headaches that are not related to your code. Ok, now I think I know the next question you're going to ask me. ""Wait a minute,"" you're going to ask. ""If I'm not using Git, is this a single-developer solution?"" No. There's an Invite button in the top left. You can use that to get a link that you give your friends. When they go to that link, they'll be editing, live, with you, in the same documents. It's a magical kind of team programming where everything shows up instantly, like Trello, or Google Docs. It is a magical thing to collaborate with a team of two or three or four people banging away on different parts of the code at the same time without a source control system. It's remarkably productive; you can dive in and help each other or you can each work on different parts of the code. ""This doesn't make sense. How is the code not permanently broken? You can't just sync all our changes continuously!"" You'd be surprised just how well it does work, for most small teams and most simple programming projects. Listen, this is not the future of all software development. Professional software development teams will continue to use professional, robust tools like Git and that's great. But it's surprising how just having continuous merging and reliable Undo solves the ""version control"" problem for all kinds of simple coding problems. And it really does create an insanely addictive form of collaboration that supercharges your team productivity. ""What if I literally type 'DELETE * FROM USERS' on my way to typing 'WHERE id=9283', do I lose all my user data?"" Erm... yes. Don't do that. This doesn't come up that often, to be honest, and we're going to add the world's simplest ""branch"" feature so that optionally you can have a ""dev"" and ""live"" branch, but for now, yeah, you'd be surprised at how well this works in practice even though in theory it sounds terrifying. ""Does it have to be JavaScript?"" Right now the server we gave you is running Node so today it has to be JavaScript. We'll add other languages soon. ""What can I do with my server?"" Anything you can do in Node. You can add any package you want just by editing package.json. So literally any working JavaScript you want to cut and paste from Stack Overflow is going to work fine. ""Is my server always up?"" If you don't use it for a while, we'll put your server to sleep, but it will never take more than a few seconds to restart. But yes for all intents and purposes, you can treat it like a reasonably reliably, 24/7 web server. This is still a beta so don't ask me how many 9's. You can have all the 8's you want. ""Why would I trust my website to you? What if you go out of business?"" There's nothing special about the container we gave you; it's a generic VM running Node. There's nothing special about the way we told you to write code; we do not give you special frameworks or libraries that will lock you in. Download your source code and host it anywhere and you're back in business. ""How are you going to make money off of this?"" Aaaaaah! why do you care! But seriously, the current plan is to have a free version for public / open source code you don't mind sharing with the world. If you want private code, much like private repos, there will eventually be paid plans, and we'll have corporate and enterprise versions. For now it's all just a beta so don't worry too much about that! ""What is the point of this Joel?"" As developers we have fantastic sets of amazing tools for building, creating, managing, testing, and deploying our source code. They're powerful and can do anything you might need. But they're usually too complex and too complicated for very simple projects. Useful little bits of code never get written because you dread the administration of setting up a new dev environment, source code repo, and server. New programmers and students are overwhelmed by the complexity of distributed version control when they're still learning to write a while loop. Apps that might solve real problems never get written because of the friction of getting started. Our theory here is that HyperDev can remove all the barriers to getting started and building useful things, and more great things will get built. ""What now?"" Really? Just go to HyperDev and start playing! HyperDev is the developer playground for building full-stack web apps, fast. Want to know more? You're reading Joel on Software , stuffed with years and years of completely raving mad articles about software development, managing software teams, designing user interfaces, running successful software companies, and rubber duckies. About the author. I'm Joel Spolsky, co-founder of Trello and Fog Creek Software , and CEO of Stack Overflow . More about me.",en,84
135,2722,1478623412,CONTENT SHARED,-7423191370472335463,-4465926797008424436,-4234938118093547320,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36",SP,BR,HTML,https://medium.com/android-dev-br/espresso-intents-n%C3%A3o-%C3%A9-m%C3%A1gica-%C3%A9-tecnologia-1fcfc8f21d3b,"espresso intents: não é magia, é tecnologia! - android dev br","Se você leu meu último artigo sobre Testes unitários vs aceitação , já ficou bem claro como é difícil controlar o ambiente de teste. Existem diversas API's do próprio Espresso como ActivityMonitor e IdlingResources que nos ajudam nessa empreitada. Utilizando esses dois recursos, já conseguimos criar excelentes testes de aceitação que nos salvam a vida. Mas, nem tudo é mar de rosas. Imagine um cenário como esse: Como iremos voltar para nosso app? A não ser que criamos uma implementação da câmera com a "" casca "" do seu device, fica bem chato testar esse cenário. E agora, Sica, quem poderá nos ajudar? É exatamente sobre isso que esse artigo fala: o herói Espresso Intents . O problema Um dos principais pilares do nosso querido Android são as . Como o nome diz, uma Intent é quando você tem a intenção (olha só) de fazer algo: um objeto de mensagem que pode ser usado para solicitar uma ação de outro componente de uma aplicação. Por exemplo, para abrir a câmera, precisamos enviar uma Intent solicitando o app de câmera que o usuário tem no aparelho: Há três casos de uso fundamentais na utilização das Intent's : para iniciar sua Activity , Service ou enviar um Broadcast . Ou seja, para iniciar (ou utilizar) qualquer coisa, você irá precisar de uma Intent e seus devidos extras ( B ). Precisamos repetir esse processo o tempo todo: seja para utilizar aplicações do próprio device ou até para aplicativos de terceiros. No ambiente de teste Só de ler o parágrafo acima você já deve ter pensado duas vezes em implementar um teste na sua classe que utiliza esses recursos, né? Bom, já que qualquer ação para inicializar algo necessita de uma Intent , fica muito difícil você ter certeza, nos seus testes, que as coisas estão acontecendo bem. E é exatamente isso que o Espresso Intents resolve: você consegue controlar todas as Intents que são disparadas (ou recebidas) pela sua aplicação. Como funciona O Espresso Intents é uma extensão do Espresso que possibilita você validar e fazer stubbings das Intents que são enviadas e recebidas pela sua aplicação dentro do ambiente de teste. Sabe o ? Então, é tipo isso, só que pra Intents . Só de ser possível comparar com o Mockito , já mostra como essa API é valiosa para nossos testes. Vamos entender a sua estrutura: Adeus ActivityTestRule. Olá IntentsTestRule. Para iniciar nossa Activity , dentro de um teste anotado, declaramos nossa ActivityTestRule como uma do e podemos iniciar qualquer Activity que desejamos: Para inicializar o Espresso Intents , você precisa inicializar a API e liberá-la após o uso: Mas, pra mim, é bem chato lembrar de fazer isso em todo teste. Para a nossa alegria, existe a IntentsTestRule que herda da ActivityTestRule . A IntentsTestRule , por sua vez, facilita a API do Espresso Intents funcionar. Ela inicia o Espresso Intents antes de cada teste anotado com nossa famosa anotação e, quando esse teste terminar, ele irá finalizar a execução de toda a API. Tá, mas o que isso faz? Basicamente te torna um mágico das Intents , conseguindo validá-las e controlá-las como um ótimo Merlin . Stubbing de Intent Agora que você é o Merlin, você tem o poder (lê-se algum programador facilitou isso pra você) de gravar todas as Intents que irão iniciar alguma Activity dentro da sua aplicação. Já que você tem todas elas, o Espresso Intents tem dois métodos que fazem toda essa mágica. Utilizando o mesmo exemplo de abrir a câmera, você consegue utilizar o método para retornar um resultado desejado: Validando uma Intent Nesse caso você estava recebendo uma Intent . Agora, avalia essa situação: Se você deixar essa sua Intent vazar, você irá sair do contexto da sua aplicação, onde seus poderes de Merlin surtem efeito, e entrar no ambiente do nosso amiguinho Android, e para interagir com qualquer elemento da tela você irá precisar do UI-Automator . Só de pensar nesse teste eu já abri a boca de sono. Mas, como iremos testar esse cenário? Aí que temos o outro método mágico, o . Com ele, validar alguma Intent, e a combinando com o método , você é capaz de de isolar totalmente seu ambiente de teste: Agora sim, dá pra testar. Limitações No dia a dia, sinto muito a falta de conseguir interceptar uma Intent para abrir outra Activity, e modificar o conteúdo dentro dela, por exemplo: Isso seria ótimo, já que em um ambiente de CI (emulador) não há imagem alguma. Ou seja, você precisa fornecer essas imagens de alguma maneira. Mas, atualmente, isso não é suportado, o que nos obriga a fazer uma implementação menos ""elegante"" . Projeto de exemplo Calma que todo esse código não vai ficar só em um post não. Criei esse projeto de exemplo que utiliza algumas situações interessantes e como você pode viajar no uso dessa API. Conseguiu implementar algo legal? Não hesite em me mandar um pull request ! Sucesso Aposto que seus testes irão fluir muito melhor com essa extensão maravilhosa do Espresso . Mas não pense que seus poderes acabaram por aqui: no próximo irei falar sobre o vasto reino da Web e como interagir com elas. Não esqueça de compartilhar, comentar ou acrescentar em algo nesse post! Valeu! Links",pt,84
136,1832,1469052884,CONTENT SHARED,7734121175534200554,1748162647671155552,-2382936607572694307,,,,HTML,http://www.meioemensagem.com.br/home/comunicacao/2016/07/19/consultorias-promovem-uma-desvalorizacao-de-nosso-negocio.html,"""consultorias promovem a desvalorização do nosso negócio"" - meio & mensagem","Bárbara Sacchitiello19 de julho de 2016 - 13h00 Márcio Santoro, copresidente da Africa sabe que, daqui a alguns anos, a agência continuará cumprindo sua principal função: comercializar ideias criativas para auxiliar as marcas a venderem mais. De que forma e em quais mídias essa atividade será feita são coisas que o executivo não hesita em reconhecer ainda como incógnitas. Crente de que a indústria vivencia a 'era do já era', termo que usa para traduzir as transformações que a tecnologia vêm trazendo ao cenário da publicidade, Santoro ressalta que não apenas as empresas de comunicação, mas as companhias de todos os segmentos estão sofrendo com as mudanças do mercado. E que, para sobreviver a esse furacão, é primordial estar em sintonia com as transformações sociais. Dividindo o comando da agência com Sérgio Gordilho, o executivo vê a união de diferentes visões como o tempero principal da operação - ingrediente este que, segundo ele, foi um dos fatores que chamaram a atenção da holding Omnicom, que em novembro de 2015 acertou a compra do Grupo ABC , do qual a Africa é parte, em uma das maiores transações recentes da indústria global de comunicação. Nessa entrevista, Santoro fala sobre os impactos dessa negociação para a Africa, defende a confiança como base da relação agência-cliente e critica as consultorias que começam a se infiltrar na indústria criativa. ""Elas promovem uma desvalorização de nosso negócio."" Meio & Mensagem - Desde novembro de 2015, quando o grupo ABC foi adquirido pelo Omnicom, a Africa deixou de ser uma agência controlada por uma empresa de capital nacional. Quais são os prós e contras dessa nova realidade? Márcio Santoro - Das negociações estratégicas que poderíamos ter feito, essa foi a melhor possível. Temos uma relação histórica com o Omnicom, desde a época em que o Nizan (Guanaes, chairman do grupo ABC) e o Guga (Valente, CEO do grupo) fizeram o primeiro negócio na DM9, e também quando criamos a Africa, da qual eles foram sócios minoritários. Na prática, muda pouca coisa porque já sabemos bem como eles trabalham e o grupo tem como característica a liberdade que dá aos gestores locais. Quando olhamos para todas as transformações que estão acontecendo no nosso mercado e vemos que temos ao lado uma fonte de informação como o Omnicom, que tem ferramentas para atravessar esse processo, que está mais avançada e bem preparada na digitalização, reconhecemos que estamos em uma situação altamente privilegiada. Vejo o Nizan, o Guga e nós mesmos com as energias renovadas. O Omnicom foi muito inteligente em ter mantido toda a estrutura do ABC. Não consigo nem identificar algo negativo nesse processo. M&M - Há cinco anos você divide o comando da agência com o Sergio Gordilho (que, antes de ser copresidente, era VP de criação da operação). Como avalia esse modelo de gestão compartilhada? Santoro - Em nossa gestão, é como se eu fosse o mâitre, e o Gordilho, o chef de cozinha. Acho que esse modelo acabou inspirando muitas outras agências, que também adotaram o formato de copresidência. Nosso negócio é propaganda. Vivemos de vender criatividade. Esse é o nosso 'prato'e, por isso, é preciso ter um bom chef, que esteja envolvido e que domine esse trabalho. E também tem de ter alguém que faça o papel do mâitre, olhando para o negócio e para os clientes. Vim da parte de atendimento e temos uma estrutura aqui com sócios e vice-presidentes que estão juntos há muito tempo e que respeitam e compreendem o papel de cada um. É uma relação muito saudável e muito boa por isso. M&M - Quais são as principais transformações na relação entre agência e clientes? Santoro - No fundo, o que mantém essa relação estável é a capacidade de entrega criativa. O que mudou é que, antes, essa criatividade era um reclame na televisão ou uma propaganda em revista. Hoje, o que o cliente espera é uma grande ideia, que consiga navegar em todas as mídias que estão disponíveis aos consumidores e que funcione bem no mobile, na TV e em qualquer canal. Evidentemente que, hoje, para chegar a uma solução criativa pertinente, é preciso conhecer muito o negócio do cliente, que também se sofisticou. A ramificação dos canais de distribuição aumentou, seja para a propaganda, seja para uma marca de roupa. Para criar uma ideia relevante, é preciso se envolver mais - e aí acho que o modelo da Africa foi beneficiado. Criamos equipes exclusivas, com tempo para pesquisar os clientes. As pessoas falam muito da mudança da propaganda, mas todas as indústrias estão mudando. Em 1998, a Kodak tinha cem mil funcionários. Isso acabou, mas a essência da foto continua. Tudo está mudando e temos de saber para que lado correr. Todo o mercado está em xeque. Vivemos a era do 'já era' e o que nos salvará, como indústria, é a criatividade. ""A ramificação dos canais de distribuição aumentou, seja para a propaganda, seja para uma marca de roupa. Para criar uma ideia relevante, é preciso se envolver mais."" M&M - Recentemente o Meio & Mensagem trouxe como manchete uma reportagem sobre o avanço das consultorias no território da comunicação. Você vê essas empresas como concorrentes? Santoro - Tenho um super-respeito pela McKinsey, IBM, Accenture, mas não possoacreditar que eles vão trazer ideias criativas e disruptivas. Acredito que elas nem queiram isso, pois estão voltados para a mídia programática e para um segmento bem tecnológico. Agora acho que para nós, agências de criação, sempre haverá espaço porque, efetivamente, temos a missão de nos envolver ativamente com os clientes e fornecer as melhores ideias. Minha questão com essas empresas é quando elas são, ao mesmo tempo, o médico e farmacêutico. Ou seja, quando elas vão ao cliente, fazem o diagnóstico de um problema e oferecem a própria ferramenta como solução. E também quando essas empresas, que nunca criaram uma campanha, palpitam sobre quanto os nossos clientes estão nos pagando. Aí acho que eles fazem uma destruição de valor do nosso negócio e sou totalmente contra. A íntegra desta entrevista está publicada na edição 1719, de 18 de Julho, exclusivamente para assinantes do Meio & Mensagem , disponível nas versões impressa e para tablets iOS e Android .",pt,84
137,791,1462391771,CONTENT SHARED,764116021156146784,7890134385692540512,1034339681607289113,,,,HTML,https://medium.com/tech-diversity-files/if-you-think-women-in-tech-is-just-a-pipeline-problem-you-haven-t-been-paying-attention-cb7a2073b996,"if you think women in tech is just a pipeline problem, you haven't been paying attention - tech diversity files","If you think women in tech is just a pipeline problem, you haven't been paying attention According to the Harvard Business Review , 41% of women working in tech eventually end up leaving the field (compared to just 17% of men), and I can understand why... I first learned to code at age 16, and am now in my 30s. I have a math PhD from Duke. I still remember my pride in a ""knight's tour"" algorithm that I wrote in C++ in high school; the awesome mind warp of an interpreter that can interpret itself (a Scheme course my first semester of college); my fascination with numerous types of matrix factorizations in C in grad school; and my excitement about relational databases and web scrapers in my first real job. Over a decade after I first learned to program, I still loved algorithms, but felt alienated and depressed by tech culture. While at a company that was a particularly poor culture fit, I was so unhappy that I hired a career counselor to discuss alternative career paths. Leaving tech would have been devastating, but staying was tough. I'm not the stereotypical male programmer in his early 20s looking to ""work hard, play hard"". I do work hard, but I'd rather wake up early than stay up late, and I was already thinking ahead to when my husband and I would need to coordinate our schedules with daycare drop-offs and pick-ups. Kegerators and ping pong tables don't appeal to me. I'm not aggressive enough to thrive in a combative work environment. Talking to other female friends working in tech, I know that I'm not alone in my frustrations. When researcher Kieran Snyder interviewed 716 women who left tech after an average tenure of 7 years, almost all of them said they liked the work itself, but cited discriminatory environments as their main reason for leaving. In NSF-funded research, Nadya Fouad surveyed 5,300 women who had earned engineering degrees (of all types) over the last 50 years, and 38% of them were no longer working as engineers. Fouad summarized her findings on why they leave with ""It's the climate, stupid!"" This is a huge, unnecessary, and expensive loss of talent in a field facing a supposed talent shortage. Given that tech is currently one of the major drivers of the US economy, this impacts everyone. Any tech company struggling to hire and retain as many employees as they need should particularly care about addressing this problem. Your company is NOT a meritocracy and you are NOT ""gender-blind"" Nobody wants to think of themselves as being sexist. However, a number of studies have shown that identical job applications or resumes are evaluated differently based on whether they are labeled with a male or female name. When men and women read identical scripts containing entrepreneurial pitches or salary negotiations, they are evaluated differently. Both men and women have been shown to have these biases. These biases occur unconsciously and without intention or malice. Here is a sampling of just a few of the studies on unconscious gender bias: Investors preferred entrepreneurial ventures pitched by a man than an identical pitch from a woman by a rate of 68% to 32% in a study conducted jointly by HBS, Wharton, and MIT Sloan. ""Male-narrated pitches were rated as more persuasive, logical and fact-based than were the same pitches narrated by a female voice."" In a randomized, double-blind study by Yale researchers, science faculty at 6 major institutions evaluated applications for a lab manager position. Applications randomly assigned a male name were rated as significantly more competent and hirable and offered a higher starting salary and more career mentoring, compared to identical applications assigned female names. When men and women negotiated a job offer by reading identical scripts for a Harvard and CMU study, women who asked for a higher salary were rated as being more difficult to work with and less nice , but men were not perceived negatively for negotiating. Psychology faculty were sent CVs for an applicant (randomly assigned male or female name), and both men and women were significantly more likely to hire a male applicant than a female applicant with an identical record. In 248 performance reviews of high-performers in tech , negative personality criticism (such as abrasive, strident, or irrational) showed up in 85% of reviews for women and just 2% of reviews for men. It is ridiculous to assume that 85% of women have personality problems and that only 2% of men do. Most concerningly, a study from Yale researchers shows that perceiving yourself as objective is actually correlated with showing even more bias . The mere desire to not be biased is not enough to overcome decades of cultural conditioning and can even lend more credence to post-hoc justifications. Acknowledging that you have biases that conflict with your values does not make you a bad person. It's a natural result of our culture. The important thing is to find ways to eliminate them. Blindly believing your company is a meritocracy not only does not make it so, but will actually make it even harder to address implicit bias. Bias is typically justified post-hoc. Our initial subconscious impression of the female applicant is negative, and then we find logical reasons to justify it. For instance, in the above study by Yale researchers if the male applicant for police chief had more street smarts and the female applicant had more formal education, evaluators decided that street smarts were the most important trait, and if the names were reversed, evaluators decided that formal education was the most important trait. Good News and Bad News The Bad News... Because of the high attrition rate for women working in tech, teaching more girls and women to code is not enough to solve this problem. Because of the above well-documented differences in how men and women are perceived, training women to negotiate better and be more assertive is also not enough to solve this problem. Female voices are perceived as less logical and less persuasive than male voices . Women are perceived negatively for being too assertive. If tech culture is going to change, everyone needs to change, especially men and most especially leaders. The professional and emotional costs to women for speaking out about discrimination can be large (in terms of retaliation, being perceived as less employable or difficult to work with, or companies then seeking to portray them as poor performers). I know a number of female software engineers who will privately share stories of sexism with trusted friends that we are not willing to share publicly because of the risk. This is why it is important to proactively address this issue. There is more than enough published research and personal stories from those who have chosen to publicly share to confirm that this is a widespread issue in the tech industry. ...and the Good News Change is possible. Although these are schools and not tech companies, Harvey Mudd and Harvard Business School provide inspiring case studies. Strong leaders at both schools enacted sweeping changes to address previously male-centric cultures. Harvey Mudd has raised the percentage of computer science majors that are women to 40% (the national average is 18%). The top 5% of Harvard Business School graduates rose from being approximately 20% women to closer to 40% and the GPA gap between men and women closed, all within one year of making a number of comprehensive, structural changes. So What Can We Do About It? These recommendations on what companies could do to improve their cultures are based on a mix of research and personal experience. My goal is to have a positive focus, and I would love it if you walked away with at least one concrete goal for making constructive change at your company. Train managers It is very common at tech start-ups to promote talented engineers to management without providing them with any management training or oversight, particularly at rapidly growing companies where existing leadership is stretched thin. These new managers are often not aware of any of the research on motivation, human psychology, or bias. Untrained, unsupervised managers cause more harm to women than men , although regardless, all employees would benefit from new managers receiving training, mentorship, and supervision. Formalize hiring and promotion criteria In the Yale study mentioned above regarding applicants for police chief, getting participants to formalize their hiring criteria before they looked at applications (i.e. deciding if formal education or street smarts was more important) reduced bias. I was once on a team where the hiring criteria were amorphous and where the manager frequently overrode majority votes by the team because of ""gut feeling"". It seemed like unconscious bias played a large role in decisions, but because of our haphazard approach to hiring, there was no way of truly knowing. Leaders, speak up and act in concrete ways Leadership sets the values and culture for a company, so the onus is on them to make it clear that they value diversity. Younger engineers and managers will follow their perceptions of what executives value. In the cases of positive change at Harvey Mudd and Harvard Business School , leadership at the top was spearheading these initiatives. Intel is going to begin tying executives' compensation to whether they achieve diversity goals on their teams. As Kelly Shuster, director for the Denver chapter of Women Who Code has pointed out, leaders have to get rid of employees who engage in sexist or racist behavior . Otherwise, the company is at risk of losing talented employees, and is sending a message to all employees that discrimination is okay. Don't rely on self-nominations or self-evaluations There is a well-documented confidence gap between men and women. Don't rely on people nominating themselves for promotions or to get the most interesting projects, since women are less likely to put themselves forward. Google relies on employees nominating themselves for promotions and data revealed that women were much less likely to do so (and thus much less likely to receive promotions). When senior women began hosting workshops encouraging women to nominate themselves, the number of women at Google receiving promotions increased . Groups are more likely to pick male leaders because of their over-confidence , compared to more qualified women who are less confident. Don't rely heavily on self-evaluations in performance scoring. Women perceive their abilities as being worse than they are, whereas men have an inflated sense of their abilities . Formally audit employee data Confirm that men and women with the same qualifications are earning the same amount and that they are receiving promotions and raises at similar rates (and if not, explore why). Make sure that gendered criticism ( such as calling a woman strident or abrasive ) is not used in performance reviews. The trend of tech companies releasing their diversity statistics is a good one, but given the high industry attrition rate for women, they should also start releasing their retention rates broken down by gender. I would like to see companies release statistics on the rates at which women are given promotions or raises compared to men, and how performance evaluation scores compare between men and women. By publicly sharing data, companies can hold themselves accountable and can track changes over time. Don't emphasize face time A culture that rewards facetime and encourages people to regularly stay late or eat dinner at the office puts employees with families at a disadvantage (particularly mothers), and research shows that working excess hours does not actually improve productivity in the long-term since workers begin to experience burn out after just a few weeks. Furthermore, when employees burn out and quit, the cost of recruiting and hiring a new employee is typically 20% of the annual salary for that position . Create a collaborative environment Stanford research studies document that women are more likely to dislike competitive environments compared to men and are more likely to select out of them, regardless of their ability. Given that women are perceived negatively for being too assertive, it is tougher for women to succeed in a highly aggressive environment as well. Men who speak up more than their peers are rewarded with 10% higher ratings, whereas women who speak up more are punished with 14% lower ratings . Creating a competitive culture where people must fight for their ideas makes it much tougher for women to succeed. Offer maternity leave Over 10% of the 716 women who left tech in Kieran Snyder's research left because of inadequate maternity leave. Several were pressured to return from leave early or to be on call while on leave. These women did not want to be stay-at-home-parents, they just wanted to recover after giving birth. Just as you would not pressure someone to return to work without recovery time after a major surgery, women need time to physically heal after delivering a baby. When Google increased paid maternity leave from 12 weeks to 18 weeks, the number of new moms who quit Google dropped by 50% . Some final thoughts... A note on racial bias There is a huge amount of research on unconscious racial bias , and tech companies need to address this issue. As Nichole Sanchez, VP of Social Impact at GitHub, describes, calls for diversity are often solely about adding more white women , which is deeply problematic. Racial bias adds another intersectional dimension to the discrimination that women of color experience. In interviews with 60 women of color who work in STEM research , 100% of them had experienced discrimination, and the particular negative stereotypes they faced differed depending on their race. A resume with a traditionally African-American sounding name is less likely to be called for an interview than the same resume with a traditionally white sounding name. I do not have the personal experience to speak about this topic and instead encourage you to read these blog posts and articles by and about tech workers of color on the challenges they've faced: Erica Joy (Slack engineer, former Google engineer), Justin Edmund (designer, Pinterest's 7th employee), Aston Motes (Engineer, Dropbox's 1st employee), and Angelica Coleman (developer advocate at Zendesk, formerly at Dropbox). Now I'm currently teaching software development at all-women Hackbright Academy , a job that I love and that suits me perfectly. I want all women to have the opportunity (and I mean truly have the opportunity, without implicit or explicit discrimination) to learn how to program - knowing software development provides so many career and financial possibilities; it's intellectually rewarding and fun; and being a creator is deeply satisfying. Although I know many women with frustrating experiences of sexism, I also know women who have found companies where they're happily thriving. I'm glad for the attention tech's diversity problem has been receiving and I am hopeful about continued change. Thanks for review, edits, and discussion to: Jeremy Howard and Angie Chang.",en,83
138,2574,1476722765,CONTENT SHARED,1992928170409443117,-4028919343899978105,6702709297756973694,"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36",SP,BR,HTML,http://googlediscovery.com/2016/10/11/google-vai-reduzir-em-50-consumo-de-memoria-do-chrome/,google vai reduzir em 50% consumo de memória do chrome | google discovery,"O Google anunciou que a versão 55 do Chrome vai incluir um novo motor JavaScript que irá reduz significativamente o consumo de memória no navegador. A empresa afirma que seus testes internos, que incluem páginas populares como o New York Times, Twitter, Reddit e YouTube, irão utilizar 50% menos RAM, em média, do que a atual versão 53 do Chrome. O upgrade está programado para acontecer no dia 06 de dezembro para a versão estável do Chrome, mas os usuários de versões Beta, Canary e DEV poderão desfrutar em primeira mão. A equipe do Chromium espera tornar o Chrome mais funcional em dispositivos menos sofisticados, principalmente aqueles com menos de 1 GB de memória.",pt,83
139,1099,1464181070,CONTENT SHARED,6437568358552101410,-1032019229384696495,3498647633704503701,,,,HTML,http://techcrunch.com/2016/05/25/tech-in-brazil-is-booming-despite-the-countrys-political-troubles/,tech in brazil is booming despite the country's political troubles,"It was a balmy night in Rio de Janeiro and a pretty J.P. Morgan Private Wealth representative was buying drinks for newly paper-rich entrepreneurs. Nearby at the pool of the city's only Relais et Chateaux property mingled investors who had flown in from New York, San Francisco, London and Berlin. The next morning the Hotel Santa Teresa's projector beamed countless up-and-to-the-right charts as entrepreneurs presented their well-funded ideas to the participants of Founders Forum Brazil 2012. Few of those companies still exist today. The hype of that era is long gone, overwhelmed by a political and economic crisis that regularly fills the front pages of global newspapers. However, unfazed by the high-profile failures of many foreign-backed startups, technology in Brazil continued its forward march. While Brazil's overall GDP fell 4 percent in 2015, the tech industry has been largely immune to the slowdown, growing 20 percent from 2014-2015. Contrary to conventional wisdom, venture investments in Latin America have also consistently grown from those heady days, reaching US$594 million in 2015, up from US$387 million in 2012, according to the Latin America Venture Capital Association. Surprisingly, recession-racked Brazil consumed 63 percent of the region's total investment, with the city of Sao Paulo receiving the lion's share of VC dollars. The reality is that the recession and political crisis created a clear divergence of fortunes among Brazil's technology companies. Many of the companies that presented at Founders Forum Brazil 2012 were plays on the emergence of Brazil's middle class. The entrepreneurs and investors were wagering that these newly empowered consumers would snap up furniture, shoes and baby supplies at record rates. Unfortunately, an economy fueled by ever-looser credit and constantly increasing government spending could not last. As in most countries, recessions lead to belt-tightening among companies and consumers. Just as expected, the technology companies that thrived are those that offer consumers and businesses a way to improve efficiency and sustainably reduce their costs. Examples include small business SaaS ERPs like Conta Azul and Omie , consumer productivity apps like Gympass and GuiaBolso , and companies that facilitate B2B transactions like Intelipost and Loggi . The relative unconcern about the recession among Brazilian technology entrepreneurs was evident last week at the award gala hosted by Latam Founders Network , a group made up of founders, executives and investors from across the region. Camera crews from all of Brazil's major television stations were there to cover what Brazil's leading business magazine, EXAME, dubbed the ""Oscar for Startups."" Companies competed in eight categories, including B2C, Best Investor, and Most Innovative. (Full disclosure: My company, Gaveteiro.com.br, was nominated in the B2B category). Past winners have included Printi , Nubank and iFood , all of which went on to raise large rounds. Taking home the prizes were companies that fit the trend of efficiency-driving technology. Contabilizei , SaaS accounting software, won in the B2B category. Dr. Consulta , a chain of low-cost, technology-enabled health clinics serving poor areas, won Most Impactful. Finally, Pipefy , a SaaS workflow tool with global ambitions, won Most Innovative. Entrepreneur of the Year went to Bruno Pierobon , CEO of Zup , a company focused on helping companies deal with all the complexities of systems integration and outsourcing services. Investors, too, have recognized this trend. Kaszek Ventures , one of the region's largest funds, has invested heavily in B2B and consumer finance (credit card interest rates in Brazil can pass 200 percent a year). Eight of their last 10 investments listed in CrunchBase have been in these two categories. Ditto for Redpoint eventures , another large fund focused on Brazil that took home this years' Investor of the Year award. No one knows the final outcome of Brazil's political crisis - until now it has been an unpredictable cross of Gabriel Garcia Marquez's surrealism and Netflix's House of Cards. What we do know, however, is that efficiency-driving innovation will have its place, come whatever market environment. Those entrepreneurs and investors who can thrive in a tough macroeconomic scenario are surely well-positioned for long-term success in the world's seventh-largest economy.",en,83
140,1158,1464693171,CONTENT SHARED,1441248639512899483,3609194402293569455,4752652671493723871,,,,HTML,http://www.diolinux.com.br/2016/05/one-dollar-board-um-projeto-brasileiro.html,one dollar board: um projeto brasileiro para revolucionar a educação,"Com o objetivo de popularizar o acesso inicial a Internet das coisas (IoT) e robótica para crianças e adultos de países em desenvolvimento evitando o analfabetismo tecnológico, surgiu a One Dollar Board. Uma placa para entrar no mundo da eletrônica e da programação, compatível com o popular software livre e gratuito IDE Arduino, acompanhada de um manual de instruções impresso nela mesma (on-board). Criada para fazer parte da lista básica de materiais escolares de crianças, a One Dollar Board tem o propósito de fazer a criança sair da escola já sabendo programar, assim como aprende os conteúdos básicos, matemática, gramatica, geografia, etc Com o objetivo de incentivar a programação de hardware, você que é palestrante, programador, professor, gosta de robótica e quer fazer seu amigo interessar-se por eletrônica e iniciar-se na Internet das Coisas (IoT). Agora você pode presenteá-lo sem precisar gastar muito e iniciá-lo facilmente neste universo. Se você quer gerar um impacto na vida das pessoas presenteando-as ou quer realizar uma doação, o investimento será menor comparado com os resultados efetivos. O One Dollar Board será comercializada com um preço acessível para qualquer pessoa do mundo, tendo como foco principal os países em desenvolvimento. Ela contém o básico para iniciar projetos e ser funcional. O que você faria com um Dólar? O que você faria com 1 Dólar? Quer uma dica? Por quê não se aventurar e descobrir a programação de uma maneira mais lúdica, tendo possibilidades de trabalhar com robótica e Internet das coisas? A One Dollar Board nasceu para ser livre, o que permite aumentar seu impacto na sociedade possibilitando à qualquer pessoa modificar a versão original ou melhorar para outras aplicações comerciais ou não comercias. Ela possui licença Open Source Hardware O projeto ainda está buscando apoio financeiro para realizar seus objetivos, você pode colaborar e ver mais detalhes da campanha na página da One Dollar Board no IndieGOGO. O projeto nasceu nas Campus Party 2015 de São Paulo através das mãos de Claudio Olmedo. Curtiu a ideia? Então apoie o projeto e compartilhe as informações para que mais pessoas possam saber desta iniciativa. Viu algum erro ou gostaria de adicionar alguma sugestão a essa matéria? Colabore, clique aqui.",pt,82
141,816,1462470341,CONTENT SHARED,-5570129644089964821,-48161796606086482,3791045604731925300,,,,HTML,https://novoed.com/prototyping-2016-1,design kit: prototyping,"The Course Course Description: Human-centered design is a process that starts with the people you're designing for and ends with new solutions that are tailor-made to suit their needs. Prototyping is a crucial part of this process. Building prototypes gives you a chance to get your ideas out of your head and into the hands of the people you're designing for. By getting feedback early and often, and continuing to improve your idea, you'll be well on your way to getting useful ideas and solutions out into the world. This four-week course builds on Design Kit: The Course for Human-Centered Design , a class that has been taken by thousands of teams around the world. The curriculum will strengthen your existing knowledge of the human-centered design process and help you to become a more innovative problem solver by building up your prototyping skills. You'll walk away knowing how to prototype products, services, and environments, and get constructive feedback along the way. What You'll Learn: After completing this course, you will be able to: Practice the mindsets and methods of human-centered design so that you can become a more effective, innovative problem solver, specifically through prototyping Produce tangible results quickly and bring your ideas to life through early, rough prototypes Experiment with the power of prototyping in multiple forms-from products to services to environments Gain valuable feedback from the people you're designing for, and how that feedback can inform and be integrated into new iterations Demonstrate how the cycle of prototyping, feedback, and iteration leads to concept refinement How the Course Works: You will work through the course with a group of 2-6 people that we will refer to as your ""design team."" It's strongly recommended you form a team before the start of the course with people located nearby, with whom you can meet in-person, but you will also have the opportunity to find a team using the course platform once the course begins. You will learn the prototyping process by applying it to a real-world design challenge that we will provide for you. Each week you will explore the main concepts of prototyping through readings, case studies, and short videos. Then you'll be expected to meet in-person with your design team to get your hands dirty building prototypes and practicing other relevant human-centered design methods. Throughout the course, you'll have the opportunity to interact and gain inspiration from design teams around the world also taking the course. Due to the size of this course, instructors will be available to answer questions, but cannot provide feedback on individual assignments. The course occurs entirely online and there are no set meeting times that you are required to log in, but you will set meeting times to meet with your own team. You can log in and access the course materials at any time between the course opening and closing dates. Building Your Design Team: Many of the workshop activities will work best if your group has 2-6 members (ideally at least 4). Recruit your friends, colleagues, or family members Find a group through the course platform once it starts More Information Approximately 4 hours per week:",en,81
142,2776,1479901922,CONTENT SHARED,8428597553954921991,268671367195911338,-4581094555157170117,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.98 Safari/537.36",SP,BR,HTML,http://www.huffingtonpost.com/advertising-week/time-to-re-think-design-t_b_12455924.html,time to re-think design thinking,"Olof Schybergson, CEO and Shelley Evenson, Head Of Organizational Evolution At Fjord, Design and Innovation From Accenture Interactive Faced by growing competition and nimbler start-ups , many organizations are struggling. They suffer from a crisis of innovation. Unable to differentiate their brands, their products and their services in a digitally disruptive world, organizations' future success depends on better managing and responding to change. Their very existence hinges on their ability to continuously and rapidly innovate. In order to do so successfully, they must place people at the heart of everything they do. They must harness the power of design. Business leaders once distinguished business strategy from customer experience but, today, that mindset is changing: business strategy has become experience strategy. In fact, 89% of companies recently surveyed by Gartner claim that experience will be their primary basis for competitive advantage this year. These shifts, along with growing evidence that design-centric companies are outperforming the market average, are fueling private and public sector interest in design - an agile and collaborative discipline that enables human-centered innovation to be brought to market, fast. The appetite for design thinking to reframe experience has never been greater. More corporations are opening their eyes to the power of design thinking as a way to solve the crisis of innovation. They see disruptive companies like AirBnB get this. But misguided efforts -- however well-intentioned -- may do more harm than good. The truth is, design thinking has become broken in today's digital age. The current interpretation of design thinking is often shallow and, as widely understood, not the answer. Simply put, design thinking is not enough. True success comes from building a complete design system, and no organization can build such a system on design thinking alone. Here's how to do it. The fact is, design thinking only has value when combined with design doing and supported by a strong design culture. You can't be good at only one or two. The Design Rule of 3 constitutes the three fundamental rules that underpin every successful design system employed by leading organizations, across sectors. When optimized and deployed in unison, organizations can effectively unlock the full potential of design to transform not only their own value and performance but peoples' experiences of the products or services they provide. Re-Thinking Design Thinking Design thinking should bring a quest for truth, empathy with people, and a systematic reframing of the business challenge-zooming in and out of the opportunity space, and providing a strategic compass to help executives understand how to reorient their businesses. Co-creation has to be integral. An organization must be willing and able to break down organizational silos to enable it. Fundamentally, design thinking must align a design perspective with business realities and technical possibilities. Key Tenants of Modern-Day Design Thinking: Co-Creation Top Down, Bottom Up Design Prototyping Continuous Feedback & Testing Measuring customer delight is essential, by employing Net Promoter Score ( NPS ) or an equivalent. At Fjord, we use our research-based Love Index to quantify and understand people's engagement. The Love Index is an actionable tool that allows you to understand what people feel about your offering, and why.Mastery requires both C-suite and grassroots support; this simultaneous top-down and bottom-up commitment is essential for design thinking to become broadly embedded across an organization. Staff training and ""learning by doing"" project-based experiences will help get you there. Crucially, successful design thinking must also include an element of making - early experience prototypes are important to validate thinking and align teams. Proponents of design thinking often get caught up in the methodologies (""how to get there"") versus the actual destination. Hands-on creation is often forgotten in today's rush to apply design thinking organization-wide. Financial software giant Intuit is a powerful example of an organization that has optimized design thinking and the context in which it lives. VP and Executive Creative Director Suzanne Pellican oversees the company's Design for Delight program that aims to inject design thinking into the company's DNA. She's trained and cultivated a community of 200 innovators who've run more than 1,000 workshops over five years to change the way people work across every function. By becoming design-driven, Intuit shifted from what Pellican has described as ""the best run, no growth company in the Valley"" to ""a 30-year-old startup."" Make no mistake, design thinking is crucial to improving everything from the value of a company's offering to reimagining the employee experience. But misunderstanding what it is and how best to apply it risks sidelining design thinking into just another passing management fad. The reason is simple: design thinking is just the beginning -- a catalyst. What's critical is to convert theory into reality to catalyze change. This is where design doing comes in. Not Just Design Thinking, but Doing Design doing is where design thinking meets the real world. While design thinking should be embedded across the organization, design doing must involve design experts. It must be driven by people passionate about the craft of design across its every application, powered by design practices such as rapid iteration and real-world testing. As the digitization of everything takes hold, the medium for design doing constantly evolves. Instead of designing for print or TV, today we design for mobile consumption and voice interaction. Instead of creating static products and websites, we design living services tailored for each individual, and powered by data. We are designing for experience in an ever-broadening context. This requires interdisciplinary teams of designers collaborating with experts as diverse as data scientists and developers. "" Design thinking is nothing without design doing. "" This broader context brings unprecedented complexity. Great design cuts through the clutter and helps prioritize and progressively reveal -- without dumbing down. The ability to simplify and make complex systems easy, engaging and intuitive for people is one of the critical contributions of good design (and great designers) at a time when the strategic business value of simplicity has never been greater. An emotional connection to brands, products, and services is also crucial for success. Rather than focus narrowly on Minimum Viable Product, the aim should be to imagine and shape a Minimum Lovable Product. Great design did not always come easy in the engineering-obsessed culture of Google. But after Larry Page took reigns as CEO in 2011, the company started crafting a common design language for experience that, for the first time, unified a vast collection of offerings into one coherent family. C-suite support, willingness to invest and strategic commitment created a program led by a core team of designers that enabled and applied great design doing across Google's diverse initiatives. In this way, an everyone-for-themselves approach to design was replaced by design becoming a central guiding force for the organization. The results speak for themselves. Google products are now perceived by many as having made the most strides at improving design, according to KPCB's 2016 Design In Tech report. Some 64% of those surveyed rated Google as ""most improved"" when asked which tech companies were best improving their design. Only 33% said so of Apple. Design doing can generate powerful results. But it can only do so within a considered and optimized organizational culture conducive to innovating and doing design well. This brings us to the last rule in the Design Rule of 3. Why Design Culture is All Design culture is equally important as design thinking and design doing because it enables the others. Creating a design culture is no trivial undertaking. It requires organizational commitment and patience. This is where most organizations stumble. Brilliant people will fail if the environment in which they work doesn't foster creativity, collaboration and innovation. Fostering a Design Culture: Diverse teams including change agents Learning & evolution of individuals Flexible physical spaces Given that great ideas emerge from diverse teams, change agents should be recruited as ambassadors and implementers of cultural transformation. Care must be taken to ensure they are set up for success. Money alone will no longer secure the best designers. People want environments in which they are challenged, continuously learn and make impact. Our own Fjord Evolution team helps our clients create a learning design environment, a culture that will encourage the best design talent to come, learn, thrive -- and stay. Flexible and open workspace is important to facilitate the best design work; and visual representations of ideas and their impact are a powerful tool. Design and innovation requires full mind and body engagement. Photo: Werner Huthmacher, Fjord Berlin It's relatively easy to copy a good business idea today, and technology solutions are cheaper and more flexible than ever. Differentiation through a clever business model or a novel technology is challenging. Culture, however, is hard to emulate. A vibrant design culture can be the best and most sustainable differentiator for an organization. Commerzbank, the global banking and financial services company, is one organization working to build such a culture through a considered deployment of design. Having initially considered buying a design consultancy to import a fully-fledged design function, it decided to change its culture from within. Work is now underway to create an internal design function that will embody and enact the company's new vision. The design space and design team is being built and managed by Fjord, as their strategic partner, until the bank's management team takes over day-to-day operations. As they're discovering, fostering a culture of design is an organizational and mindset shift that does not happen overnight. It requires commitment to a multi-year process, one that constantly evolves. Benefits of an Effective Design System In today's world, digital may look like the driving force of change but, in reality, people are at the heart of digital. Design, by its very nature, creates a culture obsessed with people willing to listen and learn from multiple vantage points, ignore hierarchies and experiment until finding the best solution. This explains why human-centricity is what powers successful organizations, across sectors, and why design is a fundamental part of their DNA. Harnessed properly, design can boost an organization's performance and value because enduring customer relationships are a natural outcome. Other evidence points to a happier workforce too. Bottom line -- mindset matters. Acquiring design thinking methods is a great first step, but must be followed through with changing how products and services are conceived and delivered, every day in every way. Effecting that transformation means simultaneously creating the right culture. Good intentions can only become reality if underpinned by a considered, effective design system built on solid foundations: the Design Rule of 3.",en,81
143,1910,1469719009,CONTENT SHARED,1881534532776527237,-1032019229384696495,3028199142413699225,,,,HTML,http://thecooperreview.com/non-threatening-leadership-strategies-for-women/,9 non-threatening leadership strategies for women,"In this fast-paced business world, female leaders need to make sure they're not perceived as pushy, aggressive or competent. One way to do that is to alter your leadership style to account for the (sometimes) fragile male ego. Should men accept powerful women and not feel threatened by them? Yes. Is that asking too much? IS IT? Sorry I didn't mean to get aggressive there. Anyhoo, here are 9 non-threatening leadership strategies for women. #1 When setting a deadline, ask your coworker what he thinks of doing something, instead of just asking him to get it done. This makes him feel less like you're telling them what to do and more like you care about his opinions. #2 When sharing your ideas, overconfidence is a killer. You don't want your male coworkers to think you're getting all uppity. Instead, downplay your ideas as just ""thinking out loud,"" ""throwing something out there,"" or sharing something ""dumb,"" ""random,"" or ""crazy."" #3 Pepper your emails with exclamation marks and emojis so you don't come across as too clear or direct. Your lack of efficient communication will make you seem more approachable. #4 If a male coworker steals your idea in a meeting, thank him for it. Give him kudos for how he explained your idea so clearly. And let's face it, no one might've ever heard it if he hadn't repeated it. #5 When you hear a sexist comment, the awkward laugh is key. Practice your awkward laugh at home, with your friends and family, and in the mirror. Make sure you sound truly delighted even as your soul is dying inside. #6 Men love explaining things. But when he's explaining something and you already know that, it might be tempting to say ""I already know that."" Instead, have him explain it to you over and over again. It will make him feel useful and will give you some time to think about out how to avoid him in the future. #7 Pointing out a mistake is always risky so it's important to always apologize for noticing the mistake and then make sure that no one thinks you're too sure about it. People will appreciate your ""hey what do I know?!"" sensibilities. #8 When collaborating with a man, type using only one finger. Skill and speed are very off-putting. #9 When all else fails, wear a mustache so everyone sees you as more man-like. This will cancel out any need to change your leadership style. In fact, you may even get a quick promotion!",en,81
144,1324,1465499895,CONTENT SHARED,7400903238402587728,-1387464358334758758,2157275533176536828,,,,HTML,http://googlediscovery.com/2016/06/06/inbox-by-gmail-ganha-formatacao-de-texto/,inbox by gmail ganha formatação de texto | google discovery,"Em resposta as solicitações dos usuários, o Google anunciou hoje a disponibilidade de um novo recurso de formatação de texto no Inbox by Gmail para desktop. Os usuários poderão escolher entre 11 tipos de letras, 4 tamanhos pré-definidos e uma grande variedade de cores para marcar o texto. Além das clássicas opções de negrito, itálico e sublinhado. é bacharel em administração de empresas e fundador da FragaNet Networks - empresa especializada em comunicação digital e mídias sociais. Em seu portfólio estão projetos como: Google Discovery, TechCult, AutoBlog e Arquivo UFO. Também foi colunista de tecnologia no TechTudo, da Globo.com.",pt,81
145,1739,1468340950,CONTENT SHARED,-8043889175123948305,-4465926797008424436,3133301208369799012,,,,HTML,http://keeptesting.com.br/2014/11/04/dicas-avancadas-de-ruby-capybaracucumber/,dicas avançadas de ruby (capybara/cucumber),"Esse post foi escrito originalmente em inglês ( ) e achei que seria bom ter a versão na minha lingua nativa :) Como de constume, continuo vendo um pessoal dizendo que automação não é desenvolvimento, logo não usam técnicas como DRY (dont repeat yourself) ou coisas bacanas da linguaguem que escolheram usar para automatizar seus testes. Para ajudar essas pobres almas, eu juntei mais algumas dicas avançadas para que você deixe seu código mais bonito, semântico e o tempo que você vai economizar para estender seus testes, você poderá gastar ficando mais tempo no break do café :) 1 - Você deveria usar o Bundler Uma das coisas legais do ruby é seu gerenciador de depenência chamado bundler . Com ele você tem o controle de quais gems o seu projeto de teste depende. No diretório raiz do seu projeto, crie um arquivo chamado Gemfile (sem extensão) . Uma cópia do conteúdo pode ser vista abaixo (adicione outras gems que você usa em seus scripts) salve o arquivo e chame o comando abaixo no mesmo diretório. O bundler vai pegar a última versão de todas as gem's do arquivo Gemfile e irá instalar para você. Caso ela já esteja instalada, vai mostrar na tela a seguinte mensagem: ""Using gem XXXX"". Se você quer uma versão específica de uma gem, pode usar a sintaxe ao lado Quer saber mais sobre o bundler ? . 2 - Se possível, evite usar IF's O condicional if é o método mais comum utilizado em todas as linguagens e é facil de usar, porém com o tempo e o aumento de complexidade do seu script, fica fácil se perder em tantos if's jogados por ai. Muitos ifs tornam o código difícil de manter, ler e estender. Se você tem uma condicional simples, não tem problema usar if mas se ela é um pouco maior, tente usar case . Veja o exemplo abaixo: Usando case : Não é nem pela quantidade de linhas, mas de como o código é exibido. Vamos concordar que é bem mais semântico e fácil de entender 3- Não declare variáveis vazias( x = nill).. Muitas pessoas que vieram do mundo JAVA , constumam declarar variáveis vazias para poder popular mais tarde dentro de um loop ou dentro de uma condicionais ( if ou case ) . Pelo amor de deus não faça mais isso.. Se você está tentando preencher uma variável dentro de uma condicional, tente usar a sintaxe abaixo: if : case : Reparou que eu não faço myVariable = ""true""? Pois cada condicional tem um valor de retorno implícito e ele preenche a variável com esse valor de retorno. 3.1 Você pode expandir essa dica para métodos também: Viu? sem usar o método return ..e funciona tranquilamente :) 3.2 Tente usar inline / if's tenários É o mesmo que: Ou você pode usar a forma tenária de se escrever: 4 - Coloque seus processos comuns em métodos de suporte. Com cucumber você já consegue reutilizar STEP's (isso é nativo da biblioteca e é genial) porém algumas vezes existem processos que não entram nesses steps e você pode utilizar eles frequentemente. Exemplo: Você precisa fazer logoff da aplicação para testar se o processo de teste trocar senha está funcionando. . O Step disso poderia ser: ""E o usuário loga novamente para confirmar a alteração de senha"" . O step não é um processo específico de logoff porém utiliza os passos para fazer um logoff. Então por que não criar um método específico para ele? Ou exemplo poderia ser um método que faz um calculo que é geralmente utilizado em grande parte dos testes? Dentro da pasta support , qualquer arquivo .rb que você colocar lá dentro, os métodos poderão ser acessados de qualquer arquivo de teste dentro do seu projeto Mais fácil que fritar ovo! 5 - Utilize valores padrão para argumentos dos seus métodos Em tempos sombrios onde o selenium 1 era beta e eu estava aprendendo a programar em java. Meus métodos orientados a objetos utilizados para dar suporte aos meus testes tinham 3..5.. 10 argumentos EU pensava que era normal por causa da complexidade dos testes... o restante da história eu vou deixar de lado para poupar vocês da dor e agonia. Primeiro de tudo, tente evitar fazer métodos que use mais que 4 argumentos. Também verifique quais argumentos são passíveis de um valor padrão. Por exemplo, vamos imaginar que você precise chamar um método para salvar um log no banco de dados e algumas vezes você precisa jogar exibir na tela alguma mensagem no console. Na maior parte do tempo, você não irá exibir nada no console. Em tempos passados eu faria desta forma: Como falado acima, grande parte do tempo não será exibido nada no console, então por que temos que chamar o método savelog sempre com o argumento ""console"" false?Por que não deixar esse argumento com falor padrão = false? Vamos refatorar! Repare que só deixei explícito o argumento console igual a ""true"" e quando eu não mando nada, ele usa o padrão do método 5.2 Se você precisar mandar mais que 4 argumentos, procure enviar um hash. Imagine que você tem um método assim: Se você precisa o método, mas não precisa passar todos os argumentos, mesmo com valor padrão você precisaria colocar algo para identificar a ordem certa dos paramêtros. (arg3 e arg4 tem que ser nill para identificar o arg5) Feio né? Sabia que podemos chamar um método com argumentos em hash? Bem mais bonito e semântico! E perceba que não está na ordem de antigamente, pois ele identifica o nome do parametro pela chave do hash! mas espera um pouco... o arg3 e arg4 vai exibir nill? vai dar pau? Vai exibir ""nill"" e isso nos leva a última dica :) 5.3 - Tratando variáveis nulas (nill) Lembra do exemplo anterior? puts options[:arg3] e puts options[:arg4] vai ser exibido nill. Mas e se eu não quero exibir nill ? É possível setar um valor default para caso o argumento não venha OU venha _nill Agora caso não venha o parâmetro requisitado, ele irá preencher com uma mensagem genérica ""não veio o argumento""! Por enquanto é só. Espero que tenham gostado! Dúvidas, críticas, café, sugestão.. sinta-se a vontade para comentar! Sobre o Autor: Leonardo Galani é Agile Tester, dev, gamer, dj and etc. Mantém o fórum | (profile)| (blog em inglês) Please enable JavaScript to view the comentários por Disqus. comments powered by",pt,80
146,2962,1484567664,CONTENT SHARED,-7120369133608664808,3915038251784681624,2839973453847029371,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36",SP,BR,HTML,http://cio.com.br/gestao/2017/01/16/e-chegada-a-hora-do-desenvolvimento-de-software-orientado-por-hipoteses/,é chegada a hora do desenvolvimento de software orientado por hipóteses - cio,"Embora pouco conhecido, o conceito de desenvolvimento de software e negócios Hypothesis-Driven Development (ou ""Desenvolvimento Orientado por Hipóteses"") não é estranho à maioria dos profissionais. Basta recordar as aulas de metodologia científica. Recordando: observa-se o mundo ao redor, formula-se uma explicação prévia, ou hipótese, para explicar determinado fenômeno. Então, testamos essa hipótese em um teste controlado. Caso o resultado esperado seja obtido, nossa teoria é confirmada, servindo de base para novas explorações. Caso não, a hipótese inicial é abandonada e novas alternativas são buscadas. A experimentação é a base do método científico, uma forma sistemática de explorar o mundo. Embora os experimentos científicos sejam associados a laboratórios, eles podem ocorrer em todo o lugar, a qualquer tempo, inclusive no desenvolvimento de software e novas propostas de negócio. Praticar a metodologia de ""Desenvolvimento Orientado por Hipóteses"" é pensar sobre o desenvolvimento de novas ideias, produtos e serviços - incluindo mudanças organizacionais - como uma série de experimentos para determinar se um resultado esperado foi alcançado. O processo é realizado até que o resultado esperado seja obtido ou a ideia se mostre inválida. Precisamos passar a entender uma solução proposta para determinado problema como uma hipótese, especialmente no desenvolvimento de um novo produto ou serviço - que envolva o mercado que se está mirando, o modelo de negócio, como os códigos irão se comportar e até como o consumidor irá usá-lo. Não fazemos mais projetos, mas sim experimentos. As estratégias de Customer Discovery (Descoberta do Consumidor) e Lean Startup são empregadas para testar suposições acerca dos consumidores. O princípio da experimentação também se aplica ao desenvolvimento orientado a testes - nós criamos o teste antes, depois usamos o teste para validar se a programação está correta. Em última instância, o desenvolvimento de um produto ou serviço é um processo de teste de uma hipótese de um comportamento em um ambiente ou mercado almejado. Os principas benefícios da abordagem de experimentação são as evidências mensuráveis e o contínuo aprendizado. Apreender é o que ganhamos ao conduzir um experimento. O que esperávamos realmente aconteceu? Se não, quais inputs podem nos ajudar a direcionar o que devemos fazer a seguir? Ou seja, para aprender, temos que utilizar a metodologia científica para investigar um fenômeno, adquirindo novos conhecimentos e corrigindo e integrando os conhecimentos prévios à nossa visão geral. Com o amadurecimento da indústria de software, temos agora novas oportunidades de alavancar capacidades fundamentais como entrega e design contínuo para maximizar nosso potencial de aprender rápido o que funciona ou não. Ao empregarmos a abordagem experimental para descobrir informações, nós podemos testar mais rapidamente as soluções propostas versus os problemas identificados nos produtos e serviços que queremos desenvolver. Com o objetivo de otimizar a efetividade da solução dos problemas certos, ao invés de se tornar uma fábrica de recursos que continuamente constrói soluções. Um exemplo de empresa para qual trabalhamos que utiliza a metodologia de Desenvolvimento Orientado a Hipóteses é a Lastminute.com. A equipe do projeto formulou uma hipótese que previa que consumidores apenas tinham predisposição a pagar um preço premium para um hotel de acordo com o período do dia em que eles fazem a reserva. Tom Klein, CEO e presidente da Sabre Holdings, compartilhou essa história de como eles aumentaram a taxa de conversão em 400% em uma semana. Combinar práticas como o ""Desenvolvimento Orientado a Hipóteses"" e a ""Entrega Contínua"" acelera a experimentação e amplifica o conhecimento validado. Isso permite que aceleremos a taxa de inovação enquanto reduzimos sensivelmente os custos, deixando os concorrentes para trás. Idealmente, visamos atingir o seguinte processo: mudanças radicais, que nos permitem identificar relações de casualidade entre mudanças em nossos produtos e serviços, com seus respectivos impactos nas métricas. ""Desenvolvimento Orientado a Hipóteses"" é uma grande oportunidade para testar o que você pensa ser um problema, antes de começar a trabalhar na solução. (*) Barry O'Reilly é co-autor do livro ""Lean Enterprise""",pt,80
147,144,1459524591,CONTENT SHARED,2709926970543371965,-1130272294246983140,-5732531255984918367,,,,HTML,http://www.revistacapitolina.com.br/a-importancia-dos-filmes-de-mulherzinha/,a importância dos filmes de mulherzinha - capitolina,"Outro dia conversava com amigos - todos homens - sobre dois filmes que estavam concorrendo a diversas premiações. Um era O Regresso ( The Revenant, 2015 ) e o outro Brooklyn (2015). O primeiro conta a história de um homem que após ser atacado por um urso e traído pelos seus companheiros de viagem, precisa lutar para sobreviver na floresta no meio de um inverno rígido. O segundo conta a história de uma menina irlandesa que migra sozinha para os EUA no final dos anos quarenta. A grande discussão era que eu não senti absolutamente nada além de tédio vendo o primeiro filme. Não tiro o mérito técnico do filme, realmente é muito bem feito, mas pra mim, é mais um homem branco tentando sobreviver, e já vi tantos. Homem branco tentando sobreviver no mar, no fogo, em Marte, já deu pra mim. Enquanto o outro eu amei, chorei do início ao fim. Falou demais comigo esse filme. Meus colegas não aceitavam isso, Regresso era o melhor filme do ano e pronto, não deveria nem ter sido indicado. Depois de quebrar diversos argumentos deles, finalmente me falaram por que Brooklyn era tão inferior: é por ser filme de mulherzinha. Essa discussão me fez pensar muito sobre esse tipo de filme. Dramas centrados em mulheres ou até mesmo as famosas comédias românticas, obras que visam o público feminino, assim como tudo que é destinado à gente ou faz parte do nosso universo é considerado inferior. Parei para pensar nos grandes filmes que são considerados clássicos do cinema ou até nos filmes recentes que ganharam premiações e são considerados ""grandes filmes"" e a maioria esmagadora, passa longe de ser destinado a mulheres. A maioria esmagadora é centrada em homens e suas histórias, nós mulheres estamos ali só para acompanhar e no máximo ajudar eles a alcançarem seus objetivos. Esses são filmes ""sérios"". Nossas histórias são besteiras, são filmes de mulherzinha. Pegue um diretor como Woody Allen, por exemplo. Seus filmes são bem concentrados no diálogo e ele fala até bastante de amor, fez filmes como Noivo neurótico, noiva nervosa ( Annie Hall, 1977) , Manhattan (1979), Tudo pode dar certo (Whatever Works, 2009) , mas na maioria das vezes o ponto de vista é masculino, com um personagem principal homem. Ele (Allen) é considerado um gênio, um mestre do cinema. Agora uma diretora como Nora Ephron, que também fazia filmes muito concentrados em diálogos, que também falava de amor, como Sintonia do Amor (Sleepsless in Seattle, 1993) e Harry e Sally - Feitos um para o outro (When Harry Met Sally, 1989) , esses são filmes de mulherzinha, esses não são tão grandiosos quanto os do Woody Allen, por quê? Voltando ao Brooklyn : li uma crítica feita por um jornalista homem que detonava o filme e basicamente resumia o filme a uma garotinha que quando acha um namorado fica tudo bem e que seu maior desafio é escolher entre dois amores e como isso era pequeno e não valia a pena ser visto no cinema. Primeiro que Brooklyn é muito mais que isso, o filme é sobre amadurecer, sobre quando crescemos precisamos fazer escolhas e lidar com o fato de quando escolhemos algo, inevitavelmente, estamos abrindo mão de outra coisa e que isso causa um sofrimento e uma angústia muito forte. Será que se fosse um homem passando por tudo isso que ela passa, o jornalista conseguira enxergar essa complexidade? Tendo a achar que sim. Segundo que, mesmo se o filme fosse só ela sofrendo as idas e vindas do amor, por que assistir a mulheres lidando com amores, felicidades e sofrimento relacionados a isso é tão menos importante do que ver homens sobrevivendo na selva? Quando eu vi o filme 500 dias com ela ( 500 Days of Summer, 2009 ), fiquei bem irritada e não sabia exatamente o porquê. O filme é uma comédia romântica que conta a história de um rapaz que se apaixona por uma menina que não acredita em amor e como os dois lidam com essa situação. Temos aqui uma troca de papel de gêneros que estamos acostumados a ver no cinema, que é até interessante. Aqui temos o homem super apaixonado e a mulher não; a mulher não curtindo ter relacionamentos, não sabendo lidar com isso direito. Além da personagem feminina ser uma clássica manic-pixie-dream-girl , descobri o que me irritava: esse filme é considerado ""cool"", não é como uma comédia romântica comum, ele é interessante, porque é o homem que está sofrendo por amor e, de repente, quando o homem tem que lidar com isso o filme fica interessante e merece ser comentado. Esse não é o único filme faz isso. Posso citar vários, a maioria são considerados filmes ""indie"", sensíveis e estão em festivais como o Sundance e outros. Não tiro a validade deles e até gosto da maioria, só questiono por que as comédias românticas clássicas com mulheres sofrendo e vivendo por amor não estão nesses festivais. Isso me faz pensar também como é um reflexo da vida real. Quando vemos um homem passando por um término difícil, sofrendo ou tendo que lidar com uma rejeição como ele é sensível e como ela foi uma idiota por ter dispensado ele, já quando é uma mulher tendo que lidar com a mesma coisa, ela é uma maluca que está com raiva, ou coitada, ela é uma desesperada que não para de correr atrás de homem, tem que superar logo. E como esse julgamento diferenciado às vezes faz com que nós mulheres escondamos nossos sentimentos, algo que só nos faz mal . Não quero com esse texto fazer todo mundo amar comédias românticas e filmes de mulherzinha. Entendo também como todos esses filmes podem ser problemáticos, começando pela imensa falta de representatividade, porque só falamos aqui de casais héteros e mulheres brancas, magras e cis. O mundo das mulheres não gira somente em torno dos homens e de amores, tem muito mais a ser explorado e grande parte desses filmes deixam de ir mais a fundo e ficam na superfície. O que eu quero é não cair mais nesse falso julgamento de que para ser um grande filme sério e com méritos a história tem que ser centrada em um homem. Precisamos olhar para os filmes que tratam do mundo feminino com outros olhos. Dando a mesma importância e valor que damos aos outros. Precisamos principalmente incentivar mulheres a entrarem nesse clube do Bolinha que é o cinema e deixar elas se tornarem roteiristas e diretoras, para elas poderem contar as nossas histórias.",pt,80
148,1596,1467296491,CONTENT SHARED,2267321579404324900,881856221521045800,-1609125166981192463,,,,HTML,https://developer.ibm.com/bluemix/2016/02/01/migrating-from-parse-to-bluemix/,migrating from parse to bluemix,"This week Parse.com , Facebook's Mobile Backend as a Service offering, shocked both their customers and the development community by announcing that they are sunsetting the service to ""focus resources elsewhere."" Luckily, they're not leaving current customers high and dry - they are giving developers a year's notice before the service is shut down, providing data migration tools to MongoDB , and open-sourcing parse-server , a Parse.com API-compatible router package for Express.js on top of Node.js. If you're a Parse customer looking to move your app infrastructure to someplace secure, then you're in luck. Bluemix is the place for you. In this post, I spell out the process of migrating an application from Parse.com to Bluemix. Let's get started! Create your Parse-on-Bluemix application The first thing you need to do is create an app on Bluemix . If you don't already have a Bluemix account, you can sign up here (it's free to try!). Once you're logged in, it's time to create a new Node.js app on Bluemix to host the Parse server. Click the CREATE APP button on your dashboard. Then, when prompted, select the WEB application option. This doesn't mean that you can't connect a mobile app to it; this option just sets up the backend application server, without provisioning any Bluemix mobile services like Mobile Client Access . The web option is used to deliver REST API calls, but does not support any enhanced mobile security or analytics features. Next you need to specify the type of application you want to create. Bluemix supports a variety of server-side processing options; in this case you want to select ""SDK for Node.js"" and then CONTINUE. Next you are prompted to enter an application name. Go ahead and enter one. (Application names must be unique across Bluemix.) When you click FINISH, the app is staged, and you are presented with instructions and next steps. At this point, click ""Overview"" to go back to the app's dashboard. Add the MongoDB service to your Bluemix application The next step is to create a MongoDB instance to migrate the data from Parse.com. From the app's dashboard select ADD A SERVICE OR API. Scroll down to the ""Data and Analytics"" section and select ""MongoDB by Compose"" to add a MongoDB instance to your application. You are then presented with details for configuring a MongoDB instance. Open up a new browser window for the next step - leave Bluemix open, because you're going to need to come back here to finalize the MongoDB configuration. Create and configure the MongoDB instance on Compose.io The MongoDB instance on Bluemix is available through Compose.io, a recent IBM acquisition. To create and configure the MongoDB instance, click on this Compose link and navigate to compose.io. If you don't already have a Compose account, you need to create one (they're free for 30 days!). Once you're logged in, click on the deployments icon (top icon on the left side) and select the MongoDB option to create a new MongoDB deployment. You need to specify a deployment name and region, then click ""Add deployment"" to create the MongoDB instance. For this example I created a deployment called ""parse-migration"". Next you need to create a database instance. Once the deployment is created, click on ""Add Database"" and create a new database. In this sample I created a database called ""myDatabase"". Once the database is created, you need to add a user account to access the database. On the database screen, select the ""Users"" menu option on the left side, and then click ""Add User"" to create a new user. Next click on the ""Admin"" (gear) link on the left side to view the connection info for your database instance - you're going to need this in the next step. Go back to the Bluemix MongoDB screen and specify the MongoDB host string, port, and login credentials, then click CREATE to finalize the MongoDB configuration within your Bluemix app. The host string is in this format: For this sample app, it is: The port is 10373 , and the username and password are the ones for the account that was just created. Migrating data from Parse.com to your MongoDB instance At this point you're ready to migrate data out of Parse.com, and into the MongoDB instance you just created. Detailed instructions are available in the Parse.com migration guide . Log into Parse.com and select the app that you want to migrate. Go into ""App Settings"", then select ""General"", and click the ""Migrate"" button to begin the process. Note: You must be in the new/beta view to see this option. To begin the migration process, a modal dialog asks you to specify your database connection string. The connection string is in the format: So, in the example app, it is: Click ""Begin the migration"". In a few minutes the migration will complete, and you can return to your MongoDB database on Compose.io to see the migrated data. Note: Migration times vary depending upon the amount of data you need to migrate. Configuring your Bluemix application Next, let's configure some environment variables to use in the Node.js application. On the Bluemix app's dashboard select the ""Environment Variables"" option on the left menu, then select USER-DEFINED. Add environment variables for APP_ID , MASTER_KEY , and REST_KEY that correspond with the App Keys from Parse.com. Also add a PARSE_MOUNT key that contains the path where the services will be exposed on Node.js. For this last one, just use the value /parse . On Parse.com, you can access the ID and Key values by going to the app, selecting ""App Settings"", and then selecting ""Security & Keys"". Save these environment variables. Now we're ready to deploy the Node.js application. Deploy your Bluemix application The parse-server API is a router for Express.js (an application framework for Node.js). It can be easily leveraged in a new Node.js/Express.js app, or dropped into an existing Express.js app to expose the Parse.com API. There is a sample application in the parse-server repository that you can copy to get started. Or, you can take the easy route. IBM Cloud Data Services Developer Advocate Mike Elsmore has put together a base Node.js application that can be quickly deployed that already supports the Bluemix environment configuration. You can click the ""Deploy to Bluemix"" button below to deploy this project directly to Bluemix in a single click, or head over to github.com/ibm-cds-labs/parse-on-bluemix to view the source code. If you want to deploy the application manually, then clone it to your local machine and use the Cloud Foundry API cf push command to push it to Bluemix. You application will be deployed and staged, and it's now ready for consumption. Testing the deployment You should now test the deployment using curl commands from the command line to verify that the migration and deployment were successful. Note: the actual URL depends on your data model. If things were successful, you should see data returned from your parse server instance: Or you can post data to the server to test write ability: What Next? If you are using Parse Cloud Code, then you can copy your Cloud Code files into the Node.js project. (See the ""cloud"" directory in the git project.) For additional details, be sure to check Parse.com's migration guide. Configure the client apps Once you've stood up the new back-end system for your app(s), you're ready to point the mobile client applications to the new back-end infrastructure. The most recent version of the Parse.com SDK introduced the ability to change the server URL (at least version 1.12 for iOS, 1.13.0 for Android, 1.6.14 for JS, 1.7.0 for .NET). Limitations and warnings Parse.com has done a great job providing you with tools to migrate your apps off their platform, but be warned - not all features of the Parse.com platform are available in the open-source parse-server. Push notifications are not implemented. Luckily, the push notification implementation can be modified to support IBM Bluemix's Push notification service , so your app does not have to suffer loss of functionality. In addition, Parse.com's App Links do not have a direct replacement in the parse-server. Many of these offerings have separate Node.js npm modules that can be leveraged, but each requires additional development effort; they are not drop-in ready. App settings are impacted by this transition. This includes social logins, user authentication and sessions. App analytics, the client dashboard, in-app purchases, and jobs are also not supported. Luckily, the Bluemix catalog has many of these features covered, plus a whole lot more. If you run into any cases where the parse-server seems to just ""do nothing"", meaning services don't seem to exist or they fail silently, then double-check your PARSE_MOUNT environment variable and MongoDB connection/host string to make sure they are in the proper formats mentioned above. For more details on transitioning away from Parse.com, be sure to read the sunset announcement , the database migration tool , and the parse-server migration guide .",en,80
149,1768,1468496385,CONTENT SHARED,2727743992157210358,268671367195911338,876332810118518702,,,,HTML,http://thefinancialbrand.com/59403/digital-banking-transformation-future/,the future of digital banking is now,"Banking executives convened to discuss the opportunities and challenges of digitization in banking. Their consensus was that becoming a 'digital bank' is no longer just an option... it's a necessity. Digital transformation isn't new - it's been happening in different industries over the past 20 or 30 years - with different waves occurring across various industry segments. In the 1990s, music, retailing, photography and video were all impacted by new entrants who used digital capabilities to change the way services were delivered and consumed. In the 2000s, TV, travel and recruitment were impacted, with the advent of YouTube, online travel sites and job posting boards. The 2010s find industries like retailing experiencing their second wave of digitization (first seen in the 1990's) while financial services finally begins to discover the opportunities and challenges of digitization. Beyond simply making current processes digital, today's transformation is dramatically impacting the way in which customers interact with brands. Instead of visiting physical facilities, consumers in all industries are beginning their shopping and buying experiences online or with a smart phone, altering all phases of the traditional customer journey. A new report, produced by and Oracle Financial Services Global Business Unit , entitled ' Digital Transformation - The Challenges and Opportunities Facing Banks ' looks at the current and future impacts of digitization within the banking industry. This report is the culmination of a series of three 'Think Tank' sessions hosted by these organizations. Impact of Digital Transformation While a great deal of attention is given to the individual players that are impacted by digital transformation (Kodak, Blockbuster, Borders, etc.),there is often quite a large financial impact on overall industry segments. As stated in the Efma/Oracle report, ""A clear example can be seen in the music industry, where the global market now is only worth about half of its value in 2000. Another example is with the newspaper print advertising market, which is about a third of what it used to be."" Not only is there a major financial impact to digital transformation. The report also found that digital transformation occurs significantly faster at a much lower cost than transformations of industries in the past. In addition, because of the lower cost of entry, more players can be informed in the process (enter fintech start-ups). The consequence is a more rapid pace of change. For example, as mentioned in the report, in Brett King's book, 'Banking 3.0', he chooses a level of 50 million users as the definition of a target figure for a market. For planes and cars to reach this level took over 60 years. Credit cards took 28 years but more recently, contactless credit cards took only four years to reach 50 million users, while Facebook and Twitter took only 3 and 2 years respectively. ""So, the pace of change is getting much faster and the financial services industry has to adapt if banks are to succeed and survive,"" states the research. Role of Fintechs It is well documented that there is a mass influx of new competitors in the financial services space globally, with massive funding of both new start-ups and innovations by large, established technology companies like Google, PayPal, Facebook and Amazon. According to the research study, fintechs have primarily focused on three segments of the financial services industry: payments, lending and personal finance. The two main reasons why these are the primary segments that the fintechs are pursuing include: They are areas with significant fees and that also have a strong push towards a digital interaction. Fintechs want to be able to work cheaper, faster and clearer and to provide a better transparency of what's happening. Despite the aggressive competitive environment, the vast majority of fintech firms lack scale and market awareness. While part of this can be attributable to a lack of successful marketing by all but a few of the new entrants, the major hurdles for fintech firms include a lack of capital, no legacy customer base, the trust level afforded legacy financial organizations and the understanding of regulatory and compliance issues by traditional banking firms. As a result, many fintech firms (and legacy banking organizations) are pursuing partnerships. ""The fintech companies have the advantage in terms of speed, agility, and the capacity to understand and quickly build a very good user experience. However, they don't have the legacies that banks have and they have a completely different mindset - and with the lack of scale and trust, it's not as easy as it might seem for fintechs to move forward without banks,"" says the report. Ultimately, some fintechs will obtain a banking license, others will partner with larger banks or perhaps later will be acquired by larger financial institutions, while others may do a bit of both. Key Digital Strategies for Banking Oracle has observed different approaches that banks have been using to help to drive a digital strategy as well as to react and deal with some of the fintech companies. The report explores four key digital strategies that banks have been using with varying success. These are: Launching a digital brand - This might involve positioning a new brand differently from the existing one, or developing a set of processes that enable the new digital brand to compete in a different way. Many different banks have done this across a lot of different markets. Examples include Fidor Bank in Germany, UBank in Australia and mBank in Poland. Digital brands focus on simplicity of design and ease of use through digitization. They also compete on price because as digital-only players, they can become more aggressive in this area, as they have much lower costs than traditional banks. Many of these brands have achieved relatively modest scale. Digitizing processes - This is a key area where traditional banking organizations can compete with fintech firms. To do so, legacy banks must digitize both front-office and back-office processes based on the expectations that have been set by all of the other digital brands (not just financial services). The key digital processes from the consumer's perspective include customer onboarding, originations and relationship pricing. Modernizing the digital experience - The development of a digital enterprise is based around the four 'Ps' - Product, Price, People and Place. Place (which can be physical or virtual) refers to the need to enhance the digital experience at the point where the customer is. Unfortunately, the digital experience of many financial services companies tend to be rather dated. Key customer-facing digitization includes use of HTML5, responsive design, the ability to fully support all mobile devices, the integration of the Internet of Things, and even open APIs. Launching new digital capability - When looking at a new capability, organizations may want to deliver something completely new outside their overarching mobile app, such as money movement apps, mobile wallets or the use of data as 'currency'. Oracle also believes banks need to start thinking more seriously about how they can take advantage of innovations such as virtual reality, FitBit or the Internet of Things, as all of these will play an integral role in the future lives of consumers. They believe banking organizations also need to reexamine and transform the role of the branch, so that it becomes more focused on customer service and advice rather than transactions. The Time to Take Action It is clear that differentiation - and competitive advantage - is occurring based on the ability of financial institutions to embrace and implement digitization. The firms that aggressively pursue the digitization of both back and front-offices will be in a better position to compete for the increasingly digital consumer and will be able to reduce costs (and increase revenue) based on this transition. As stated in the Efma/Oracle report, ""Overall, although the financial services sector isn't unique in terms of suffering from the effects of disruption, banks can no longer afford to sit back and do nothing. There are numerous new technologies that they should be taking advantage of - and they could learn a great deal from other industries who have already had to face disruption. Despite the problem of regulation, there are also still countless valuable insights that they could can gain from different sources of customer information that would enable them to sell more and also to provide better advice."" It is becoming abundantly clear that becoming a 'digital bank' is quickly becoming 'table stakes' as opposed to a nice benefit or feature of today's banking experience. Access the Report Oracle teamed up with the Efma to learn about the digital strategies that banks are implementing to tackle these challenges in a series of virtual Think Tanks that were attended by 40+ banks from across EMEA and JAPAC. EFMA is a global not-for-profit organisation that brings together more than 3,300 retail financial services companies from over 130 countries - including almost a third of all large retail banks worldwide. Download and read the 32-page report, ""Digital Transformation"" here . Jim Marous is co-publisher of The Financial Brand and publisher of the Digital Banking Report , a subscription-based publication that provides deep insights into the digitization of banking, with over 150 reports in the digital archive available to subscribers. You can follow Jim on Twitter and LinkedIn . All content © 2016 by The Financial Brand and may not be reproduced by any means without permission.",en,80
150,2647,1477438797,CONTENT SHARED,8769121796537771237,2542290381109225938,3896024018019760972,Android - Native Mobile App,FL,US,HTML,http://www.revistaapolice.com.br/2016/10/susep-divulga-nota-de-esclarecimento/,susep divulga nota de esclarecimento sobre a youse seguros,"Em atenção à propaganda e à forma de comunicação empregadas pela Youse, em especial no seu sítio eletrônico na Internet, a Susep - autarquia federal responsável pela supervisão dos mercados de seguros, resseguros, previdência complementar e capitalização, esclarece: 1. A Youse não é sociedade seguradora autorizada a funcionar pela Susep, consequentemente, não possui produtos aprovados pela Susep, nem tampouco autorização para comercializar produtos de seguro, nessa condição; 2. A referida empresa já foi notificada pela autarquia quanto à propaganda e à comunicação empregadas, por induzirem a erro potenciais consumidores, afrontando direito básico do consumidor, conforme prevê o art. 6°, inciso III, da Lei n° 8.078/90 (Código de Defesa do Consumidor); 3. A Caixa Seguros Holding S.A ingressou com pedido de autorização de constituição para seguradora denominada Youse, sob o processo Susep n° 15414.001677/2016-46, cuja análise ainda se encontra em andamento, portanto, pendente de aprovação; 4. As reclamações e denúncias apresentadas à Susep a respeito da comercialização de produtos pela empresa estão sob análise da Diretoria de Supervisão de Conduta. Com o objetivo de continuar protegendo os interesses dos cidadãos e demais agentes envolvidos com os mercados de seguros, resseguros, previdência complementar e capitalização, a Susep coloca-se à disposição para prestar esclarecimentos adicionais. Joaquim Mendanha de Ataídes Superintendente da Susep Fonte: Susep L.S. Revista Apólice",pt,80
151,3104,1487597538,CONTENT SHARED,-6872546942144599345,-1393866732742189886,-6350745898785551312,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36",MG,BR,HTML,https://medium.com/@husayn.hakeem/my-experience-with-googles-associate-android-developer-certification-40b34b27ebc7,my experience with google's associate android developer certification,"In this article I talk about my personal experience through the Google Android Certification exams. I'm expressing my opinion, don't take it for facts. Also, any information given here about the Google Android Certification is also available in more details on its official website . On the new year's eve, I decided to take the Google Android Certification. It was an abrupt decision, and frankly speaking I didn't know what to expect, even though my interest for android development had began 2 years prior to this, I had only just started working professionaly as an android developer 4 months prior to making this decision. Before doing so, I did a bit of research online, read about the certification on its officiel web site and forums and discussed it a bit with my team lead at work. About the Google Android Certification The Google Android Certification covers the following sections: Testing and debugging Application User Interface (UI) and User Experience (UX) Fundamental Application Components Persistent Data Storage Enhanced System Integration So basically a candidate taking this certification is expected to have an understanding of the full lifecycle of Android applications development, from clearly defining and understanding the app's main functionalities, to designing and developing UX friendly interfaces, passing by being able to debug the app during its development process, and finally to testing the app. The applicant is also expected to have a minimum knowledge of data persistance, Android's 4 main application components (Activity, Service, etc) and when & how to use them, and finally how to manage app notifications and widgets. The certification exam is divided into two main parts: The programming phase and the interview phase . The programming phase is to be completed and submitted within 48 hours after starting it. I had read on a couple of forums that it shouldn't take more than 1 day to complete it, personally I was able to complete it in about 4 hours, while taking my time and taking one pause. The interview phase was also fun, I was asked a couple of questions to which I felt I answered quite good. The durations between the phases of the certification, their results and receiving the digital badge aren't long at all, from what I had read on a couple of forums I got the impression that the process would take forever, but to my surprise it was quite rapid and smooth, and the response from the support team was quite fast, they got back to me a couple of hours after I had sent them some remarks regarding the first exam. So all in all, was it worth it? I'd definitely yes, for the following reasons: As a fresh graduate and working as a junior Android developer, the certification definitely made me look more ""mature"" to recruiters, especially that it is provided by Google, which gives it, if anything, more authenticity and credibility. Being certified guarantees and testifies that I have -at the least- moderate skills in the basics of Android development. The exams aren't difficult at all (they weren't for me at least), but it's a great way to expose your android coding skills and put them to the test. The experience was fun, I didn't know what to expect before taking the certification, so I was expecting the process to be a bit stressful, but it wasn't at all, I really enjoyed both phases of the certification. During the first phase of programming, I encountered a malfunction in the provided starter code. I'm not sure whether this was done intentionally or not, but I was able to find it, fix it and report it to the appropriate team. Proving again that your skills are put to the test during the exam. The certification isn't expensive at all, at the time I took it it was $100. What I didn't appreciate? I wouldn't go as far as to say that there are things I didn't like in the certification process, but I think a couple of things can definitely be improved. I wish there were different Android certifications with different levels of difficulty. This would be more interesting as it would put one's skills to the test in a more challenging way. It would be better if the Android certification test (at least the first phase of coding) was done in designated exam centers preferably without internet access, making it impossible for candidates to copy code off the internet and forcing them to rely solely on their skills. So, to sum up? To sum up, I'd say that my experience taking the Google Android Certification was quite a good one, if you're thinking of taking it I'd say go for it! Especially if you're a novice to android development. If on the other hand you're been into Android app development for a while (a couple of years for example), I think your experience and apps you've worked on would speak louder for your skills than the certification would.",en,80
152,2576,1476729411,CONTENT SHARED,-1787737696395220629,-4028919343899978105,6702709297756973694,"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36",SP,BR,HTML,http://www.b9.com.br/67542/tech/laboratorio-da-disney-cria-robo-que-pula/,laboratório da disney cria robô que pula,"Como se não bastasse o robô capaz de subir pelas paredes , a Disney inventou um novo robô. Dessa vez os laboratórios da empresa criaram um robô que pula. Veja no vídeo acima. No topo da sua perna única, ficam o computador, os sensores e uma bateria, enquanto que a perna é feita de um mecanismo de molas paralelas. O computador é capaz de calcular e executar pelo menos 19 pulos antes dele perder o equilíbrio e cair, mas ainda assim é um progresso interessante para uma companhia investindo na área. A sugestão do The Verge é que ele seja transformado em um brinquedo o mais rápido possível - e se não for incomodar muito, que seja um robô do Tigrão, do Ursinho Puff.",pt,80
153,1609,1467340692,CONTENT SHARED,7851022564818849560,6644119361202586331,-2763188495283035571,,,,HTML,https://www.linkedin.com/pulse/workplace-diversity-brazil-theres-still-long-way-go-ruy-shiozawa,workplace diversity in brazil: there's still a long way to go,"Portuguese: Here, at Great Place to Work Brazil, we're always encouraging companies to promote diversity in the workplace. A great example of that was our latest GPTW Women Event , which took place on March 8th. Although we were able to identify important advances related to diversity then, there's still a lot of work to do as well . We are used to commend Brazil for its diverse cultural background. We like to believe that people from all tribes, colors and groups can live together around here. That's all very nice in theory but, as usual, the reality is uglier: Yes, Brazil is a big cultural cauldron, but unfortunately it's one still boiling with prejudice - gender, ethnicity, sexual orientation, age, disability... they all suffer from some sort of bias or stigma. And those are amplified, especially in the workplace . To keep it shorter, we'll talk now about the first three groups and compare theory and practice. Let's start with gender inequality . Equal job opportunities for men and women are one of the many fronts in this battle. Looking at GPTW's Best Workplaces in Brazil, we can say we're really close to equality - in a superficial level. However... We already talked about this in our blog a couple of times. The higher we go in the companies' hierarchies, less women we are able to find. And the more excuses we hear: women aren't as willing to relocate because of family - wouldn't you too if society had historically told you that you're the one supposed to take care of the household? They could be on leave for months due to pregnancy (a decision that tends to involve a man too). And even absurdities like women not being able to convey credibility to investors who are, in their large majority, (guess who?) men. If we are tired of hearing this nonsense, imagine how women feel about it. And we still haven't even touched salary inequality. Even among the companies recognized in our GPTW Women Event, a woman is paid, on average, 35% less than a man with the same position . And there are excuses for this too, but frankly they don't even deserve to be written here. Enough to say it's another byproduct of a biased society that allows ignorance to perpetuate ad infinitum . Talking about ignorance, let's demystify one of the main excuses for ethnic inequality in the workplace , shall we? It goes somehow like this: ""there are less black people than white people in leadership positions because they tend to be part of an underprivileged population with less access to education and, therefore, not qualified enough to be an executive leader in today's competitive market."" First of all, using this is an excuse makes it seem like a fact of life; one we should simply accept and move on. No! It's actually something rotten in our society that needs to be fixed right away . But for the reality deniers out there, let's pretend this is an acceptable argument for a second , just so we can smash it into oblivion once and for all. According to DIEESE (Inter-Union Department of Statistics and Socio-Economic Studies), 11,8% of the black population in Brazil have a college degree. Among the non-black population, this number jumps to 23,4% (almost twice as much). At a first glance, it looks like the argument makes sense. But once again, that's true only in a superficial level. Borrowing some data from Previ ( Banco do Brasil 's pension fund), we find out that only 2% of the top and middle management positions in the 114 Brazilian biggest companies are filled by darker-skinned professionals. Therefore, there are around twice as many white people than black people in our population with a college degree, but that distribution (2:1) jumps to 49:1 in companies' leadership positions . Weird, huh? Also weird is the fact that most companies argue they have practices to eliminate prejudice related to race/ethnicity and to develop non-Caucasian leadership. Looking at those numbers, there's a simple message I can see: It's time for a change! Those practices are clearly ineffective . Besides, that same DIEESE research points that white professionals in the industry and the trade sectors earn around 20% more than black professionals among people without complete elementary school or lesser education. Among professionals with a college degree, the difference grows to nearly 40% . It makes me wonder how much less a black woman is paid when compared to a white man. And now, to end this horror show: a research conducted by the recruiting and hiring company Elancers revealed that 7% of the interviewed companies have declared they would not hire a homosexual candidate under no circumstances whatsoever , and 11% of them would only hire that candidate if they knew there was no chance that employee could reach a position of influence/visibility (such an executive). And remember: those are the companies that declared so. How many more felt the same way but didn't have the guts to admit it? In other words, if you're a homosexual man or woman in Brazil and you want a job where you'll be treated fairly, at least next to one in five companies in the country are not for you . But that doesn't happen only around here! In an experiment conducted by the Harvard University, nearly identical résumés were sent to around 1,700 recruiters in the USA. The only difference among the documents: in half of them, it was also added that the candidate had previously worked as treasurer in the college's LGBT group. The result: the résumé without that extra piece of information got four times more calls to a job interview . And the problems do not end in the hiring process: according to the LGBT Out Now research, only 3 in 10 gay executives in Brazil speak openly about their sexual orientation at work . They're afraid for their jobs if they come out. And companies are losing money over it: the same research also points that among openly gay employees, 75% consider themselves to be productive. Among the ""closeted"" ones, only 46% felt the same way. It's hard to focus on work if you're spending all your energy to pretend being someone different than you are . And we haven't even talked about transgenders , who can barely hope to find any job in a company, let alone a decent one with perspective to grow. And this leads to very serious consequences: according to ANTRA (National Association of Transgenders and Cross-Dressers), 90% of the transgender and cross-dresser population in Brazil need to prostitute themselves as a mean to survive. So enough saying Brazil isn't a sexist country. Or racist. Or homophobic. It's all of that (and alarmingly so) inside and outside of the workplace. Enough with all the excuses! In order to solve any problem, it's necessary to admit its existence in the first place. And that's what Brazilian companies (and society as a whole) need to do right now. Whatever it is we're doing, it is not enough . We have to do more. Much more. From our side, we'll be watching more and more attentively. And we'll be more and more demanding of the companies - which is why you can expect more actions from Great Place to Work Brazil to support diversity, such as GPTW Women. Together we can work to eliminate prejudice from the workplaces and, hopefully, our society.",en,79
154,1180,1464790328,CONTENT SHARED,-8813724423497152538,-1602833675167376798,-3784159144403077709,,,,HTML,http://www.baguete.com.br/noticias/25/04/2016/itau-e-pioneiro-do-blockchain,itaú é pioneiro do blockchain,"O Itaú é o primeiro banco da América Latina a participar do consórcio R3, uma iniciativa que reúne 42 duas instituições financeiras de todo o mundo para desenvolver projetos relacionados a tecnologias de registros compartilhados baseados em Blockchain. Banco é pioneiro na América Latina. Foto: divulgação. A iniciativa,comandada pela startup americana R3CEV, começou em setembro do ano passado com nove bancos: Barclays, BBVA, Commonwealth Bank of Australia, Credit Suisse, Goldman Sachs, J.P. Morgan,Royal Bank of Scotland, State Street e UBS. Um blockchain é um banco de dados distribuído, no qual novos registros de transação estão linkados entre si por marcadores de tempo compartilhados. Cada bloco, acessível por todos os participantes, contém o registro de uma série de transações. A tecnologia ficou conhecida por ser a base do bitcoin, criptomoeda eletrônica que tem sido uma febre no setor financeiro nos últimos anos, mas o princípio pode ser utilizado numa série de sistemas financeiros ou qualquer outra situação que demande registros públicos confiáveis. ""A tecnologia tem outras aplicações que possibilitam novos modelos de negócios, agilidade e eficiência em um ambiente seguro, de forma descentralizada, garantindo rastreabilidade das transações e autenticidade das partes envolvidas"", aponta o Itaú em nota. No que vai de ano, o R3 já anunciou três testes bem sucedidos envolvendo bancos participantes da iniciativa. O último deles contou com tecnologias da Eris Industries, IBM, Intel e Chain para facilitar a negociação de instrumentos de débito. Esse experimento foi a continuação de um primeiro feito com 11 bancos, usando Ethereum hospedado na nuvem Azure da Microsoft. ""Estamos certos de que essas inovações trarão benefícios para nossos clientes e ganhos reais de eficiência para o setor como um todo"", afirma Márcio Schettini, diretor-geral de Tecnologia e Operações do Itaú. O Itaú é o maior banco privado do Brasil, com 90 mil colaboradores, mais de 5 mil agências e PABs, e quase 26 mil caixas eletrônicos.",pt,79
155,2363,1474048569,CONTENT SHARED,-4655195825208522542,-3595444231792050977,-5218411221113627439,,,,HTML,http://blog.caelum.com.br/java-9-na-pratica-jigsaw/,java 9 na prática: jigsaw,"Há muito tempo se diz sobre modularizar a plataforma Java . É um plano que começou desde antes do Java 7, foi uma possibilidade no Java 8 e por fim, para permitir mais tempo de desenvolvimento, revisão e testes, foi movido para o Java 9. O projeto Jigsaw, como foi chamado, é composto por uma série de JEPs . Algumas delas inclusive já disponíveis no Java 8, como os conhecidos Compact Profiles . A idéia por trás do projeto não é só criar um sistema de módulos, que poderemos usar em nossos projetos, mas também aplicá-lo em toda a plataforma e JDK em busca de melhor organização e desempenho. we propose to design and implement a standard module system for the Java SE Platform and to apply that system to the Platform itself, and to the JDK. The module system should be powerful enough to modularize the JDK and other large legacy code bases, yet still be approachable by all developers. Neste novo post da serie Java 9 prático não só veremos que o projeto já existe e está integrado aos últimos builds, como também vamos explorar e implementar nosso próprio projeto modular. Tudo ao estilo hands on, claro - preparem seus compiladores! Preparando o seu ambiente para o Java 9 modular O Jigsaw foi integrado ao build do JDK 9 desde a versão ea+111, portanto tudo que você precisa para usá-lo é baixar uma versão igual ou superior a ela. Eu recomendo bastante que, mesmo que você já tenha uma versão relativamente atual, baixe sempre a última antes de qualquer teste. Você pode fazer o download aqui . A versão que estou usando neste post é do build 134 (mais atual de agora). Criando um hello world tradicional No lugar de já começar criando o projeto modular com Jigsaw, vamos criar um projeto da forma tradicional e depois migrá-lo, assim a diferença entre as duas abordagens deve ficar bastante clara. Nos exemplos do post vou fazer tudo pela linha de comando e ir compartilhando os snippets, mas para acompanhar, você pode e deve usar o gerenciador de arquivos do seu sistema operacional e qualquer editor de sua preferência. Os passos são simples, começando pela criação dos diretórios do projeto e de seus pacotes. O projeto se chamará hello-jigsaw e as classes Java devem ficar no pacote br.com.caelum.jigsaw . Podemos fazer tudo com o comando: Claro que, se você preferir, pode criar usando o gerenciador de arquivos do seu sistema operacional favorito. O importante é que, no final, a estrutura de diretórios fique assim: Tudo ok. Então agora podemos criar a classe JigSaw.java , dentro de src/br/com/caelum/jigsaw : Aqui tem um vídeo com todos os passos desse processo, caso queira dar uma olhada. E também vou deixar um link para fazer download ou navegar pelos arquivos com esse estado do post. Compilando e executando, ainda da forma atual Para compilar o projeto podemos usar o comando javac e pronto. Nada de especial aqui. Feito isso, caso não exista nenhum erro, podemos executar com: Repare que, como já criamos dentro de um pacote e da pasta source , foi preciso passar o parâmetro -cp (classpath) apontando para ela e também usar o nome completo da classe, que inclui o pacote. Talvez você não esteja tão acostumado com isso, afinal a maioria de nós não compila classes na linha de comando no dia a dia, mas não tem nenhuma novidade por enquanto. Ao executar, o output, conforme esperado, será a mensagem: Olá Java 9 modular! . Você pode ver aqui o vídeo do processo de compilação e execução. Agora finalmente vamos para a parte divertida, que será migrar o projeto para que seja modular! Migrando para um projeto modular A migração não é nada complicada. Você precisa basicamente criar uma nova pasta com o nome do seu módulo, que por convenção, é o pacote base do seu projeto. Você pode pensar no nome do módulo como algo parecido com o groupId de um projeto do Maven. É aberto, você pode escrever qualquer coisa, mas idealmente deve seguir uma convenção. Em nosso caso o módulo será chamado de br.com.caelum.jigsaw : Ok. O próximo passo será mover todos os arquivos do projeto, de dentro da pasta src, para dentro deste novo diretório do módulo. Você pode fazer isso com o comando mv do terminal ou simplesmente mover a pasta br pelo seu gerenciador de arquivos com tudo que tem dentro para o diretório do módulo. Na linha de comando seria assim: Agora falta criar um arquivo chamado module-info.java , também dentro deste diretório do módulo - tudo ficará nele a partir de agora. Esse arquivo será o responsável pela declaração do módulo e futuramente as suas dependências. A configuração é mínima: Tudo pronto, no final de todos os passos nosso projeto deve ficar assim: Vou deixar aqui os links do post nesta fase também, caso queira acompanhar: [ video , download do código , navegar pelo projeto ] Nosso projeto agora é modular! Nada complicado, não é mesmo? Vamos ver agora como compilar e executar um projeto com esse novo formato. Compilando um projeto modular O comando javac para um projeto modular é um pouco diferente. Ele terá a seguinte estrutura: Onde <1> é o diretório onde ficará o código compilado, <2> é o caminho para o seu arquivo module-info.java , e <3> é o caminho completo para o seu arquivo .java. O comando completo para compilar esse nosso projeto ficará assim: Execute e, se nenhum erro de compilação for exibido, você verá que a nova pasta mods foi criada com todo o código compilado. Inclusive os metadados do module-info. O projeto ficou assim: Agora temos o diretório src, com o código fonte, e o diretório mods , com o código compilado. Executando um projeto modular Para executar você usará o comando java , como sempre, mas agora com esse formato: Onde <1> é o nome do diretório onde está o código compilado do nosso módulo, ou seja, a pasta /mods . E <2> o nome completo ( full qualified name ) da classe que será executada. Um detalhe importante é que, agora que estamos trabalhando de forma modular, o nome da classe recebe também o nome do módulo como prefixo, logo antes do nome do pacote. O comando para executar a classe JigSaw ficará assim: E a saída, assim como esperado, será o print: Olá Java 9 modular! Aqui vão os links de [ video , download do código , navegar pelo projeto ] Adicionando um alert do Swing Por enquanto usamos apenas um System.out no projeto, apenas recursos do pacote padrão java.lang . Vamos experimentar usar um importe de alguma outra API do próprio Java pra ver o que acontece? Uma forma simples seria usando um alert do Swing para imprimir a mensagem: Maravilha, agora basta compilar utilizando o mesmo comando de antes e ops... Você pode ver aqui um video com esses passos e o erro gerado. O que houve? A mensagem diz que o pacote javax.swing não existe! Claro que o Swing não foi removido no Java 9... mas a partir do momento em que compilamos nosso projeto modular, o compilador só usará os módulos que declararmos como dependência no arquivo module-info.java . Até então não tinhamos nenhuma. O JRE modular A primeira coisa que eu fiz ao baixar o build do Java 9 com Jigsaw foi abrir o diretório onde ele está instalado e ver o que mudou. Se você fizer isso aí na sua casa, vai perceber que no lugar da pasta jre/lib , onde ficavam todos os .jar s da JRE, você encontrará um diretório jmods com todos os módulos. E não são poucos. The original goal of this Project was to design and implement a module system focused narrowly upon the goal of modularizing the JDK, and to apply that system to the JDK itself. We expected the resulting module system to be useful to developers for their own code, and it would have been fully supported for that purpose, but it was not at the outset intended to be an official part of the Java SE Platform Specification. O pensamento foi: se vamos criar um sistema de módulos na plataforma Java, por que não modularizar o próprio JDK? E assim foi feito, todo o código foi reorganizado em módulos. Você pode ver todos executando o seguinte comando: Você pode ver todos na imagem deste link . Um detalhe que você deve ter percebido é que, assim como não era preciso importar o java.lang para usar as classes System , String e outras, você também não precisa declarar nenhum módulo. Essas classes estão presentes no módulo padrão, java.base . Outro detalhe importante é que você não é obrigado a usar Java modular, claro. Você só precisa declarar de quais módulos a sua aplicação depende se ela for modular... caso exista o arquivo module-info . Declarando a dependência entre módulos As classes do Swing , como o JOptionPane que usamos, estão em um módulo chamado java.desktop . Para ensinar ao compilador que precisamos desse módulo em nosso projeto, basta declará-lo no arquivo module-info.java . A mudança é simples, o código ficará assim: E pronto! Você não precisa de nenhuma configuração adicional, basta usar o mesmo comando para compilar e agora ele saberá de onde vem o pacote javax.swing . Zero erros. Agora executamos o código e o alert será exibido: Aqui vão os links de [ video , download do código , navegar pelo projeto ] É claro que, assim como você não precisa executar os comandos javac e java manualmente na linha de comando hoje, passando o classpath e caminho para as dependências do projeto, você também não vai precisar fazer isso com o projeto modular. As IDEs vão adicionar suporte para que isso fique transparente e para que você não se preocupe tanto com os detalhes. O post foi um overview rápido sobre um assunto bastante abrangente, então, sem dúvida alguma novos posts entrando em detalhes que não foram discutidos aqui virão em breve. Não deixe de deixar suas perguntas e sugestões de assuntos aqui como comentário, são elas que estão guiando os posts da serie Java 9. E aí, o que achou do Jigsaw? Tem outros assuntos que gostaria de ler aqui no blog sobre Java 9?",pt,78
156,3108,1487608319,CONTENT SHARED,-2402288292108892893,-4465926797008424436,-8412615285329455721,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.54 Safari/537.36",SP,BR,HTML,http://www.codeatest.com/como-testar-excecoes-em-java-junit/,como testar exceções em java com o junit - code a test,"Introdução Neste post vou mostrar de forma sucinta e prática como testar exceções em Java, utilizando o framework JUnit 4. Vou demonstrar 3 formas de escrever testes unitários que verificam o comportamento de exceções. Nossa classe para ser testada As 3 formas de se testar exceções em Java serão feitas em cima da classe mostrada abaixo. O código é simples. Faz a transferência de um valor entre uma conta origem e uma conta destino. Se o saldo da conta origem não for suficiente para a realização da transferência, uma exceção será lançada. Abaixo o código da exceção. Vamos aos testes. Como testar exceções em Java #1: @Test(expected=...) O JUnit 4 permite definirmos, opcionalmente, como argumento da anotação @Test o expected . O expected nos permite especificar uma exceção que esperamos que seja lançada pelo código sendo testado. O teste só tem sucesso se a exceção for lançada, caso contrário temos uma falha. O exemplo abaixo mostra o código de teste com o expected . O código de teste acima deve lançar uma exceção, pois o valor que desejamos transferir - 2000 - é maior que o saldo da conta origem: 1500. O argumento expected=SaldoInsuficienteException.class diz ao JUnit que esperamos que o teste ao executar lance uma exceção do tipo SaldoInsuficienteException . Veja que não precisamos fazer asserções nesse código; o argumento expected de @Test faz esse papel. Embora o teste anterior seja legal e bastante conciso, ele não atende a um ponto: e se quisermos verificar a mensagem que foi lançada? Nesse caso, vamos ter que usar uma abordagem diferente. Vamos a ela. Como testar exceções em Java #2: blocos try {} catch() Uma outra forma de como testar exceções em Java é utilizando um bloco try{} catch . Essa forma era bastante utilizada no JUnit 3, antes de aparecer a funcionalidade @Test(expected=...) . Embora essa forma de se testar exceções não seja mais tão utilizada, ela ainda é útil quando queremos verificar a mensagem de detalhe da exceção disparada. Isso ocorre, por exemplo, se a mensagem da exceção vai ser exibida na camada de apresentação para o usuário. Nesse caso, é legal validarmos se a mensagem é aquela que esperamos. Vamos ao código de teste com try{} catch() . A preparação e a condição de falha é a mesma do teste anterior. A diferença está na forma que verificamos a ocorrência da exceção. Veja as duas linhas em destaque: A primeira contém o Assert.fail() ; se o código de teste executar essa linha, indica que ele falhou, pois, o teste deveria lançar uma exceção; É no catch() que esperamos que a execução do teste vá. Na segunda linha em destaque, utilizamos o assertEquals para verificar se a mensagem da exceção ( ex.getMessage() ) é a que esperamos. Esse código atende muito bem nossas expectativas. Mas não sei o que vocês acham, mas na minha opinião não é muito elegante utilizar try {} catch() para testar a ocorrência de uma exceção e validar sua mensagem. Pois bem. O JUnit 4 tem uma forma mais elegante de resolver isso. Como testar exceções em Java #3: @Rule e ExpectedException O Junit 4.7 introduziu o conceito de Rules . As Rules , de maneira geral, permitem adicionar comportamentos que serão executados antes e depois de cada método de teste. O JUnit já vem com algumas test rules predefinidas e permite, também, criarmos as nossas próprias Rules . Uma das test rules oferecidas pelo JUnit é a ExpectedException . É ela que vamos usar em nosso próximo exemplo. Vamos mostrar o código. Uma ExpectedException é uma rule que nos permite verificar se o nosso código lança uma determinada exceção. As linhas em destaque no trecho de código anterior fazem uso dessa rule : As duas primeiras linhas destacadas mostram a declaração da rule . Para isso, utilizamos a anotação @Rule e a classe ExpectedException . Como se pode ver, a varíavel excecaoEsperada é inicializada com o o valor ExpectedException.none() ; essa inicialização é para informar que, por padrão, nenhuma exceção é esperada. A próxima linha em destaque - excecaoEsperada.expect() - modifica o comportamento padrão definido anteriormente, informando qual o tipo de exceção esperamos: SaldoInsuficienteException ; A última linha destacada nos permite verificar a mensagem da exceção ( excecaoEsperada.expectMessage ). Cabe ressaltar algumas coisas sobre os test rules (variáveis anotadas com @Rule ) para que elas funcionem adequadamente: A variável deve ser pública; A variável não pode ser estática ( static ); A variável deve ser um subtipo de TestRule . Se quiser saber um pouco mais sobre as test rules recomendo a leitura do excelente artigo Testes isolados com jUnit Rules do Rafael Ponte. Conclusão Apresentei, neste post, 3 formas diferentes de como testar exceções em Java utilizando o framework JUnit. Eu, pessoalmente gosto da primeira e terceira formas, pois o código fica mais limpo com as mesmas. Utilizo a primeira quando não preciso verificar a mensagem da exceção. A última, por sua vez, utilizo quando preciso garantir que a mensagem de exceção é a esperada. Todo o código desse post pode ser encontrado nesse link do meu GitHub . Abraços e até a próxima.",pt,78
157,668,1461879742,CONTENT SHARED,-6467708104873171151,-1032019229384696495,-1941773591979139720,,,,HTML,http://justcuriousblog.com/2016/04/5-reasons-your-employees-arent-sharing-their-knowledge/,5 reasons your employees aren't sharing their knowledge,"I rarely hear someone say ... WOW! We have so many people sharing on [insert name of enterprise platform]! Everyone's problems are getting solved so quickly nowadays! Instead, I hear plenty of ... We have [insert name of enterprise platform] at work, but no one really uses it ... What's going on? It's 2016! People like sharing online, right? I mean, look what happens in an internet minute nowadays ... 347,222 tweets are sent. 293,000 Facebook statuses are updated. 527,760 Snapchat photos are shared. 2,780,000 YouTube videos are viewed. If sharing as enabled by social technology has become a standard component of everyday life, why can't organizations take advantage of these behaviors and scale knowledge sharing in the workplace? For me, it comes down to a simple answer: its not the same . The technology may look familiar, and the desired behaviors may be similar. But, there are a few key considerations that most organizations ignore when attempting to generate shared organizational knowledge. It's by no means impossible ... It's just a lot more difficult than most people think. Here are 5 reasons your employees aren't sharing their workplace knowledge ... and a few things you can do about it ... #1 - Sharing tech isn't work tech ... When and where are you asking employees to share their knowledge? By WHEN, I'm referring to the expectation that sharing is actually part of the job - not an extra, nice-to-have task. Few organizations hold employees accountable for sharing their knowledge as a core responsibility and are therefore left with only the information from designated subject matter experts, communications teams and a few ""go-getters"" who derive personal value from such sharing. By WHERE, I'm referring to the technology component, often embodied by an intranet or enterprise social network (ESN). Yes, knowledge sharing is more about organizational culture than it is about technology, but right-fit technology must be applied to enable sharing at the speed and scale needed in the modern workplace. So, what tools do your employees need to use to do their jobs every day? Email? POS? SRM? Are your intranet or ESN ever part of that list? Employees may go to your SharePoint or Google Drive now and then to download a file, but these tools typically aren't getting their eyes every day like email and your SRM. Therefore, employees will be less inclined to contribute given minimal traffic time and value-add as part of their regular tasks. #2 - Networks require scale ... I'm admittedly not a Facebook fan. I'm more of a Twitter guy. So why am I still on Facebook? Well, that's where the crowd is. I have a feeling there are plenty of Facebook users with that same predicament. I've tried plenty of other social networks with similar capabilities, but I keep coming back to Facebook because that's where the people with whom I want to engage are sharing. The same is true at work. ESNs and intranets require a flow of constant user activity to deliver value to a single user. Would you stay on Facebook if your newsfeed was empty for a month? A week? A day? People share and consume content where they can engage with the desired audience at the moment of need. To solve problems quickly by leveraging the community, the community has to be there - all the time. Enterprise knowledge sharing rarely hits this necessary level of scale and utility and therefore cannot sustain value. #3 - Most sharing tech is transient ... What was that thing that person said on Facebook the other day? What group did they upload that file to? If you can't remember who said it or when they shared it, good luck finding it on a traditional social network. Newsfeed-oriented tools aren't built for long-term knowledge retention. They're inherently transient, showing users what's going on NOW within the network. This translates into the workplace with popular tools like Yammer. It's also the reason I'm not super-enthused by Facebook at Work. Most organizations introduce separate tools like SharePoint for long-term documentation. This establishes a disconnected knowledge experience for employees. They can share what they know over HERE, but the info they need on the job is over THERE. This model puts employees at a sharing disadvantage, as they are usually required to pass new knowledge through hierarchical channels to get it added to a formal document repository for long-term use. Too often, this privilege is limited to designated SMEs or senior managers - who are several steps removed from the employee/customer experience. Shared knowledge can't attain the scale and longevity necessary to become valuable to the community, and formal documentation processes cannot keep up with the pace of the modern workplace. #4 - Knowledge sharing requires just enough structure ... Have you tried to search your organization's intranet? Can you actually find what you need - quickly? Or are you stuck with PDFs and PowerPoint presentations from 7 years ago? The internet doesn't just organize itself. We have the teams from Google, YouTube and Wikipedia to thank for making the world's shared knowledge searchable. Unfortunately, most organizations don't apply a similar structured approach to internal knowledge sharing. Either people have no ability to share, or the gates are flung open and everyone can pile on however they'd like. This is why your SharePoint instance has become a jumbled mess of locked-down team sites and confusing file directories. Employees shouldn't just be expected to know how to effectively contribute their knowledge to the larger community. Every team shouldn't be granted permission to organize their small corner of shared knowledge however they see fit. Regardless of title or desire, most people just don't understand how to best structure knowledge sharing at an enterprise scale to meet the needs of individual employees. Therefore, they do what they know and install hierarchical folder structures organized by department and supported by rigid process with a dose of overarching ownership mentality. #5 - Why should they ...? It all comes down to motivation and culture. Frankly, if they aren't held accountable to it as part of their roles, why should employees share their knowledge? If the sharing experience isn't super simple - like it is in the real world - why should they dedicate the time? If they aren't likely to get any value back from other employees' sharing, why should they make the effort? If no one is going to recognize their contribution, why contribute? And if they aren't TRUSTED to do the right thing, why would they broadcast their work for everyone to pick apart? ""If you build it, he will come"" worked out pretty well for Kevin Costner, but it has never proven to be a solid strategy for knowledge sharing in the workplace. Difficult - but not impossible. Over the past few years, I've witnessed organizations get around these challenges and begin to realize true value from shared knowledge. Not just business value thanks to the improved ability to solve problems via the community but also cultural value derived from increased employee engagement. Remember - SMEs and formal documentation are a great start, but most of your organization's HOWs - the way the work really gets done every day - are locked in the minds of your employees. You NEED them to share so you can keep up with your competition and maximize your company's long-term potential. How have companies overcome these issues and helped their employees share their knowledge? Here are a few practical tips. Start with trust. As I already mentioned, knowledge sharing is more about culture than technology. Before getting started with strategy, assess the role trust plays in your organization and take steps to address any related issues. Move knowledge sharing closer to the workflow. Help employees talk about the work ON the work. Select technology and enable processes that merge sharing with on-the-job information reference in a single seamless experience. Select right-fit technology. Search is the killer app when it comes to shared knowledge. Leverage technology that looks and feels like Wikipedia, YouTube and other prevalent real-world sharing tools. Provide just enough support. Don't just install technology and process. Find the people in your organization who understand enterprise knowledge sharing - regardless of formal role or past experience - and give them the keys (and the accountability). Make it about their peers. I have come to realize that employees are almost always more willing to share when the act is positioned as a way to help their coworkers, not just the organization or customers - who also benefit anyway. Recognize contribution. Not only should sharing be a core component of their roles, but employees should also be continuously recognized for the value they are contributing through shared knowledge. This could be as simple as mentioning key contributors during group meetings or take on a more strategic form with the use of integrated game mechanic, such as leaderboards, badges, and redeemable points. Get the managers to do it too . This isn't just about frontline employees sharing what they know. Managers must leverage the same behaviors for consuming and sharing knowledge for their own benefit and to reinforce the desired behaviors for their teams. Are you seeing similar challenges in your organization? Have you found ways to motivate your employees to share their knowledge? What technology have you found most helpful when trying to generate and organize shared knowledge?",en,78
158,1387,1465950308,CONTENT SHARED,-4205346868684833897,-1443636648652872475,6340331642204025925,,,,HTML,http://techcrunch.com/2016/06/14/google-launches-springboard-an-ai-powered-assistant-for-its-enterprise-customers/,"google launches springboard, an ai-powered assistant for its enterprise customers","Google has unwrapped two significant announcements for its enterprise customers, the most notable of which is the rollout of Springboard, a new digital assistant to help enterprise companies that make use of Google's services for business. Springboard, which has been in testing with ""a small set of customers,"" is a little like Google Now for enterprise workers. That's to say that it offer a single search interface which utilises artificial intelligence to surface information within a user's suite of Google products - such as Google Drive, Gmail, Calendar, Google Docs and more. That's important because, according to Prabhakar Raghavan, VP of Engineering for Google Apps, ""the average knowledge worker [currently] spends the equivalent of one full day a week searching for and gathering information."" Beyond search, Springboard also provides ""useful and actionable information and recommendations"" to users throughout their work day. The second side of today's news is a new design for Google Sites, a product that acts like an information portal for housing internal company information like quarterly reports, or newsletters. Now, when designing a Google Site, users have drag and drop editing and real-time collaboration, creation features that have become standard in other services like Google Docs and Google Sheets. Finally, in terms of presentation, Google Sites has been revamped so that the content fits to any kind of screen, be it a smartphone, laptop or 30-inch monitor. These changes are rolling out to early adopter programs that existing Google Apps for Work customer can join - the Springboard program is here and Google Sites program is here - while the search giant has teased that it has ""a lot more in store"" for both services.",en,78
159,2573,1476722551,CONTENT SHARED,2715453133655798791,-4028919343899978105,6702709297756973694,"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36",SP,BR,HTML,https://www.engadget.com/2016/10/17/the-simpsons-vr-google-cardboard/,'the simpsons' celebrates 600 episodes with a vr couch gag,"For the opening of Treehouse of Horror XXVII , which aired last night, viewers were treated to a Planet of the Apes parody called Planet of the Couches. Fans could (and still can) download the Google Spotlight Stories app (available on both iOS and Android ) to engage with a 360-degree experience that responds to their movement. The couch gag normally runs for 45 seconds, but Planet of the Couches offers an extra two minutes of bonus content. We won't ruin it for you but there are six different scenes that include a Moe's ""Cavern"" and a snarky quip from The Comic Book Guy. Just make sure to explore all 360 degrees to catch all of the clever references and in-jokes. The experience is best viewed with a viewer like Cardboard, but you can still enjoy the VR short by opening the Spotlight Stories app and physically moving your iPhone or Android device around (it just won't be as immersive). Get out your #GoogleCardboard , the 600th episode couch gag VR experience via @Google starts now! pic.twitter.com/ip3jLs0N8D - The Simpsons (@TheSimpsons) October 17, 2016",en,77
160,2039,1470922612,CONTENT SHARED,-5628897645967553681,-5527145562136413747,-145525363878254760,,,,HTML,http://itspronouncedmetrosexual.com/2015/03/the-genderbread-person-v3/,the genderbread person v3 | it's pronounced metrosexual,"Taking the lessons learned from version 2 , and applying them to sexuality in a more meaningful way, here's my best rendition of the genderbread person yet. If you want to see a more in-depth explanation using the first version of this graphic I made, check out my post Breaking through the binary: Gender explained using continuums . Someone pointed out that I hadn't shared this here yet, even though it's been out for awhile. I am in disbelief about how long it's taken me to get this up on the site. I really don't know what happened - time flies? I created the first iteration of my version 3 of the genderbread person when I published my book back in 2013. I've posted it on Facebook , it's been translated a few times, it's even been Santafied , and don't have much of an explanation to share for why I've neglected to share it here. But... here it is! Sorry? Sorry. What's new? There are two big differences from v2 to the most recent v3: separating romantic & sexual attraction, and the labeling of the continua. I'm also sharing it here with a more simplified version that I encourage folks to use for Powerpoints or to supplement any articles they're writing, where the additional verbiage isn't necessary. Separating romantic and sexual attraction is a more accurate way of describing some of the ways we all experience attraction (or don't), and it's also an effort to make the graphic more inclusive of asexual folks. Check out this great article on Asexuality.org about sexual and romantic attraction if you'd like to read more. Labels have always been one of the toughest parts of of the genderbread person , because in my versions I'm trying to simultaneously reinforce folks current understandings (or former, or problematic ) of gender, while expanding and building upon their understanding, enhancing it, and making it a more honest depiction of the ways we all actually experience gender. Using the language that reinforces problematic and limiting understandings of gender as a means to expand understanding feels a lot like using the master's tools to deconstruct the master's house , and in this version I'm trying to get away from as much language as possible. Here's a testament to that ""less language is better"" sentiment, a more Powerpoint-friendly version you're welcome to use: What's the same? It's still totally uncopyrighted and yours to use however you'd like. No need to ask for permission. It's still based on the deconstruction of gender into identity, expression, and sex, building on my second and first versions of the genderbread person, which are my spin on the model that's been around for a couple decades, which was based on the work of feminists from the past century. And it's still, I hope, as adorable as ever. Join the Discussion",en,77
161,2842,1481114167,CONTENT SHARED,8869347744613364434,997469202936578234,2204632498906085879,"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36",MG,BR,HTML,https://www.infoq.com/br/articles/Java_Garbage_Collection_Distilled,java garbage collection essencial,"Serial, Parallel, Concurrent, CMS, G1, Young Gen, New Gen, Old Gen, Perm Gen, Eden, Tenured, Survivor Spaces, Safepoints e as centenas de flags de inicialização da JVM. Deixam tudo confuso quando se está tentando otimizar o garbage collector para obter a taxa de transferência e latência necessária para a aplicação Java? A documentação sobre a coleta de lixo parece um manual para aeronaves. Cada botão e seletor está explicado detalhadamente, mas em nenhum lugar é possível encontrar um guia sobre como voar. Este artigo vai tentar explicar as vantagens e desvantagens na escolha e ajuste dos algoritmos do garbage collector em uma determinada carga de trabalho. O foco será nos coletores da Oracle Hotspot JVM e do OpenJDK que são de uso mais comum. No final, outras JVMs comerciais serão discutidas para mostrar alternativas. Vantagens e desvantagens O sábio povo continua nos dizendo: ""não se consegue nada de graça"". Quando queremos algo geralmente temos que abrir mão de algo em troca. Quando se trata da coleta de lixo trabalhamos com 3 principais variáveis que definem metas para os coletores: Taxa de transferência: O total de trabalho realizado por uma aplicação como uma proporção do tempo gasto no GC. Configurando a taxa de transferência para ‑XX:GCTimeRatio=99, 99 é o padrão que equivale a 1% do tempo do GC. Latência: O tempo gasto pelos sistemas em resposta a eventos que são afetados por pausas introduzidas através da coleta de lixo. Configure a latência para pausas do GC com -XX: MaxGCPauseMillis = <n>. Memória: A quantidade de memória que nossos sistemas usam para guardar o estado, que é frequentemente copiado e movido quando estão sendo geridos. O conjunto de objetos ativos mantidos pela aplicação em qualquer tempo é conhecido como Live Set. Tamanho máximo da heap (Maximum heap size) -Xmx<n> é um parâmetro de ajuste para configurar o heap size disponível para uma aplicação. Observação: Muitas vezes o Hotspot não pode alcançar essas metas e vai silenciosamente continuar sem nenhum aviso, mesmo após ter falhado por uma grande margem. Latência é a distribuição entre eventos. Pode ser aceitável ter um aumento médio de latência para reduzir o pior caso de latência, ou torná-los menos frequentes. Não devemos interpretar o termo ""real-time"" como menor latência possível; mas sim que se refere a ter uma latência determinística, independentemente da taxa de transferência. Para cargas de trabalho de algumas aplicações, a taxa de transferência é a meta mais importante. Um exemplo seria um processamento em lote de longa duração, não importa se o processamento em lote é ocasionalmente interrompido por alguns segundos enquanto a coleta de lixo é executada, desde que o processamento em lote possa ser concluído logo. Para praticamente todas as outras cargas de trabalho, desde aplicações interativas com humanos a sistemas de comércio financeiro, se um sistema não responder após alguns segundos ou milissegundos, isso pode significar um desastre. Em negócios financeiros frequentemente vale a pena negociar alguma taxa de transferência em troca de latência consistente. Podemos também ter aplicações que são limitadas pela quantidade de memória física disponível e ter que se manter presente, neste caso temos que melhorar o desempenho em ambos, latência e taxa de transferência. A seguir algumas vantagens e desvantagens: Em grande parte o custo da coleta de lixo, como um custo amortizado, pode ser reduzido fornecendo ao garbage collector algoritmos com mais memória. Os piores casos observados de pausas induzidas por latência devido à coleta de lixo podem ser reduzidos limitando o live set e mantendo a heap pequena. A frequência com que as pausas ocorrem pode ser reduzida pelo gerenciamento da heap e o tamanho das gerações, e controlando a taxa de alocação de objetos. A grande frequência de pausas pode ser reduzida pela execução do GC concorrentemente com a aplicação, às vezes à custa da taxa de transferência. Ciclo de vida dos Objetos Os algoritmos de coleta de lixo são frequentemente otimizados na expectativa de que a maioria dos objetos viva por um curto período de tempo, enquanto que relativamente poucos vivem por muito tempo. Na maioria das aplicações, os objetos que vivem por um período significativo de tempo tendem a constituir uma porcentagem muito pequena dos objetos alocados ao longo do tempo. Na teoria, esse comportamento observado na coleta de lixo é conhecido como ""mortalidade infantil"" ou ""hipótese geracional fraca"". Por exemplo, loops em Iterators são em sua maioria de curta duração enquanto que Strings são efetivamente imortais. A experiência tem demonstrado que os garbage collectors geracionais normalmente podem suportar uma taxa de transferência maior que os collectors não geracionais, portanto são quase sempre usados em servidores JVMs. Pela separação das gerações de objetos, sabemos que a região na qual os novos objetos são alocados é provavelmente muito escassa para objetos vivos. Portanto, um collector que varre procurando por objetos vivos nessa região e os copia para outra região de objetos mais velhos pode ser muito eficiente. Os coletores de lixo do Hotspot gravam a idade de um objeto de acordo com o número de ciclos do GC que sobreviveu. Observação: Se uma aplicação sempre gera um monte de objetos que vivem por um tempo bastante longo, então é de se esperar que essa aplicação gaste uma porção significativa deste tempo com a coleta de lixo, assim como tenha que se gastar um tempo significativo ajustando o coletor de lixo do Hotspot. Isto é devido à redução da eficiência do GC que acontece quando o ""filtro"" geracional é menos efetivo, e ao custo resultante de coletar gerações vivas há mais tempo com uma maior frequência. Gerações mais velhas são menos escassas e, como resultado, a eficiência dos algoritmos de coleta das gerações mais velhas tendem a ser bem ruim. Os Garbage Collectors geracionais tendem a operar em dois ciclos de coletas distintos: as coletas Menores (Minor collections), em que os objetos de curta duração são coletados, e as menos frequentes coletas Maiores (Major collections), em que as regiões mais velhas são coletadas. Eventos do tipo Para-O-Mundo (Stop-The-World) As pausas sofridas pelas aplicações durante a coleta de lixo são devidas aos, como são conhecidos, ""eventos do tipo stop-the-world"". Por razões práticas de engenharia, para que o garbage collector opere, é necessário parar a aplicação em execução periodicamente para que a memória possa ser gerenciada. Dependendo dos algoritmos, diferentes coletores dispararão o evento stop-the-world em específicos pontos da execução, variando o tempo da parada. Para fazer uma aplicação parar totalmente é necessário pausar todas as threads em execução. Os garbage collectors fazem isso avisando as threads para parar quando elas estiverem em um ""ponto seguro"", que é o ponto durante a execução do programa no qual é conhecido por todos os GC e todos os objetos na heap estão consistentes. Dependendo do que a thread estiver executando, pode demorar algum tempo até ela alcançar um ponto seguro. Pontos seguros são normalmente estabelecidos nos retornos de método e nos finais dos loops, mas podem ser otimizados para alguns pontos diferentes, tornando-os dinamicamente mais raros. Por exemplo, se uma Thread está copiando um grande array, clonando um objeto grande, ou executando um loop, pode demorar muitos milissegundos antes de um ponto seguro ser atingido. O tempo para atingir um ponto seguro é uma preocupação importante em aplicações com baixa latência. Este tempo pode ser visualizado habilitando a flag ‑XX:+PrintGCApplicationStoppedTime junto com as outras flags do GC. Observação: Para aplicações com um número grande de threads em execução, quando um evento stop-the world ocorre, o sistema sofrerá uma pressão enorme assim que as threads estiverem liberadas dos pontos seguros. Por isso que algoritmos meno sependentes de eventos stop-the-world são potencialmente mais eficientes. Organização da Heap no Hotspot Para entender como os diferentes coletores funcionam é melhor explorar como a Heap é organizada para suportar os coletores geracionais. Os objetos que vivem durante um tempo longo o suficiente são eventualmente promovidos para a tenured space . A perm gen é o lugar em que a Runtime (JVM) armazena os objetos ""conhecidos"" por serem efetivamente imortais, tais como Classes e Strings estáticas. Infelizmente o uso comum do carregamento de classes de forma contínua em muitas aplicações motiva a equivocada suposição de que por trás da perm gen as classes são imortais. No Java 7 as Strings internas foram movidas da perm gen para a tenured , e a partir do Java 8 a perm gen não existirá mais, e não será abordada neste artigo. A maioria dos coletores comerciais não usa uma perm gen separada e tende a tratar todos os objetos de longa duração na tenured . Observação: As áreas virtuais permitem que os coletores ajustem o tamanho das regiões para cumprir as metas de taxa de transferência e latência. Os coletores mantêm estatísticas para cada fase da coleta e ajustam o tamanho da região na tentativa de cumprir as metas. Alocação de Objetos Para evitar disputas, cada thread é atribuída a um buffer de alocação de thread local (Thread Local Allocation Buffer - TLAB) no qual os objetos são alocados. O uso de TLABs permite que a alocação de objetos possa escalar de acordo com número de threads, evitando disputas de um único recurso na memória. A alocação de objetos via TLAB é uma operação muito barata, a TLAB simplesmente aponta o ponteiro para o tamanho do objeto que leva cerca de 10 instruções na maioria das plataformas. A alocação da pilha de memória para o Java é ainda mais barato do que usar o malloc do runtime existente no C. Observação: Visto que a alocação individual de objetos é muito barata, a taxa à qual coletas menores devem ocorrer é diretamente proporcional à taxa de alocação do objeto. Quando uma TLAB está exausta uma thread simplesmente socilita uma nova para o Eden . Quando o Eden está cheio uma coleta menor é iniciada. Pode acontecer de não ser possível alocar grandes objetos (-XX:PretenureSizeThreshold=n) na young gen e assim eles terão de ser alocados na old gen , por exemplo um grande array. Se o limite configurado é menor que o tamanho do TLAB, então os objetos que couberem no TLAB não serão criados na old gen . O novo coletor G1 manipula grandes objetos diferentemente e o mesmo será discutido mais adiante. A cópia para o tenured space é conhecida como uma promoção ou amadurecimento. Promoções ocorrem para objetos que são suficientemente velhos (-- XX:maxTenuringThreshold ), ou quando o survivor space estoura. Objetos vivos são objetos que podem ser acessados pela aplicação, quaisquer outros objetos que não possam ser acessados podem portanto ser considerados mortos. Em uma coleta menor, a cópia dos objetos vivos é feita primeiramente pelo que é conhecido como GC Roots, e de forma iterativa copia qualquer objeto acessível para o survivor space. GC Roots normalmente incluem referências da aplicação e campos estáticos internos da JVM, e pilhas de threads, tudo o que efetivamente aponta para gráficos de objetos acessíveis da aplicação. O cartão de mesa do Hotspot é um array de bytes no qual cada byte é usado para marcar a potencial existência de referências entre gerações em uma região correspondente a 512 bytes da old gen . As referências são armazenadas na heap, a ""barreira de armazenamento"" de código marcará os cartões para indicar que uma potencial referência da old gen para a new gen possa existir na região 512 byte associada. No momento da coleta, o cartão de mesa é usado para procurar por referências entre gerações, que efetivamente representam GC Roots adicionais para a new gen . Entretanto um custo fixo significativo nas coletas menores é diretamente proporcional ao tamanho da old gen . Existem duas survivor spaces na new gen da Hotspot, que se alternam em suas regras em seus "" espaço destino "" e "" espaço origem "". No início de uma coleta menor, o "" espaço destino "" survivor space é sempre vazio, e atua como uma área de cópia para a coleta menor. O survivor space da coleta menor anterior é parte do ""espaço origem"", que também inclui o Eden , local que os objetos vivos que precisam ser copiados podem ser encontrados. O custo de um GC de coleta menor é geralmente dominado pelo custo da cópia de objetos para os survivor spaces e tenured spaces . Objetos que não sobrevivem a coleta menor estão efetivamente livres para serem tratados. O trabalho realizado durante a coleta menor é diretamente proporcional ao número de objetos vivos encontrados, e não ao tamanho da new gen . O tempo total gasto executando a coleta menor pode ser praticamente reduzida a metade cada vez que o tamanho do Eden é dobrado. A memória portanto pode ser trocada por taxa de transferência. A duplicação do tamanho Eden resultará no aumento do tempo de coleta por ciclo, mas isso é relativamente pequeno se tanto o número de objetos a serem promovidos como o tamanho de geração mais velha é constante. Observação: Na Hotspot as coletas menores são eventos stop-the-world. Isso está rapidamente se tornando uma questão importante conforme nossas heaps ficam maiores com mais objetos vivos. Já vemos a necessidade de coletas concorrentes da young gen para atingirmos as metas de pausa de tempo. Coletas maiores O coletor da old gen vai tentar adivinhar quando é necessário coletar para evitar um fracasso na promoção da young gen . Os coletores monitoram um limite de preenchimento para a old gen e começam a coletar quando este limite é ultrapassado. Se este limite não é suficiente para atender as necessidades de promoção, então um ""FullGC"" é acionado. O FullGC envolve a promoção de todos os objetos vivos da new gen seguidas por uma coleta e compactação da old gen . A falha de promoção é uma operação muito cara e os objetos promovidos a partir deste ciclo deve ser desfeitos para que o evento FullGC possa ocorrer. Observação: Para evitar a falha na promoção será necessário ajustar o preenchimento que a old gen permite para acomodar promoções (‑XX:PromotedPadding=<n>). Observação: Quando a Heap precisa aumentar um FullGC é acionado. Esse redimensionamento da Heap por FullGCs podem ser evitado ajustando -Xms e -Xmx para o mesmo valor. Além do FullFC, uma compactação da old gen é provávelmente o maior evento stop-the-world que uma aplicação vai experimentar. O tempo para esta compactação tende a crescer linearmente com o número de objetos vivos no tenured space . Coleta em série Coletor paralelo Em sistemas com muitos processadores o Coletor Paralelo Antigo vai dar melhor vazão que qualquer coletor. Ele não impacta em uma aplicação em execução até a coleta acontecer, e então vai coletar em paralelo usando múltiplas threads usando um algoritmo mais eficiente. Isso faz o Coletor Paralelo Antigo muito eficiente para aplicação em batch. O custo de coletar as gerações old é afetado pelo número de objetos mantendo uma maior extensão do que o tamanho da heap. Portanto a eficiência do Coletor Paralelo Antigo pode ser aumentada para alcançar uma maior taxa de transferência, fornecendo mais memória e aceitando maiores, mas em número menor, pausas para coleta. Espere coletas menores mais rápidas com este coletor porque a promoção para o tenured space é um simples operação de cópia e acerto de ponteiros. Para servidores de aplicações o Coletor Paralelo Old deve ser a porta de entrada. Entretanto se as pausas para coletas maiores são maiores que a aplicação pode tolerar então será preciso considerar empregar um coletor concorrente que colete os objetos na tenured concorrentemente enquanto a aplicação está em execução. Observação: Espere pausas em ordem de 1 a 5 segundos por GB de informação viva em máquinas modernas enquanto a old gen é compactada. Coletor Concurrent Mark Sweep (CMS) O coletor Concurrent Mark Sweep (CMS) (-XX:+UseConcMarkSweepGC) é executado na old gen coletando objetos maduros que não são mais acessíveis durante a coleta maior. Ele é executado concorrentemente na aplicação com o objetivo de manter espaço livre o suficiente na old gen , então falhas de promoção na young gen não ocorrem. A falha de promoção irá desencadear um FullGC. O CMS segue um processo de várias etapas: Marcação inicial <stop-the-world>: Procura GC Roots; Marcação concorrente: Marca todos os objetos acessíveis pelo GC Roots; Pré-limpeza concorrente: Verifica referências de objetos que foram atualizadas e objetos que foram promovidos durante a fase de remarcação concorrente; Remarcação <stop-the-world>: Captura referências de objetos que foram atualizados desde a fase de pré-limpeza; Varredura concorrente: Atualiza as listas livre com a recuperação da memória ocupada por objetos mortos; Reinicio concorrente: Reinicia a estrutura de dados para a próxima execução. Assim que os objetos maduros se tornam inacessíveis, o espaço é recuperado pelo CMS e colocado em listas livres. Quando a promoção acontece, deve ser pesquisado um local de tamanho adequado para as listas livres para o objeto promovido. Isso aumenta o custo da promoção e assim aumenta o custo da coleta menor comparado ao Coletor Paralelo. Observação: O CMS não é um coletor de compactação, o que ao longo do tempo pode resultar na fragmentação da old gen. A promoção do objeto pode falhar, pois um objeto grande pode não caber nos locais disponíveis na old gen . Quando isso acontece uma mensagem ""promoção falha"" é registrada e um FullGC é acionado para compactar os objetos maduros vivos. Para tais compactações orientadas ao FullGCs, espere pausas piores que das coletas maiores usando o Coletor Paralelo Antigo porque CMS usa uma simples thread para a compactação. O CMS é na maior parte concorrente com a aplicação, que tem um número de implicações. Primeiro, o tempo da CPU é consumido pelo coletor, deste modo reduzindo a CPU disponível para a aplicação. O total de tempo requerido pelo CMS aumenta linearmente com o montante de objetos promovidos para o tenured space . Segundo, para algumas fases concorrentes do ciclo do GC, todas as threads tem que ser trazidas para um ponto a salvo para o GC Roots marcar e realizar uma remarcação paralela para verificar se há mutações. Observação: Se uma aplicação percebe mutações significantes nos objetos maduros, então uma fase de remarcação pode ser significante, nos extremos pode levar mais tempo do que uma compactação completa com o Coletor Paralelo Antigo. O CMS faz do FullGC um evento menos frequente à custa da redução da taxa de transferência, coletas menores são mais caras, e mais marcantes. A redução na taxa de transferência pode ser qualquer coisa entre 10% a 40% em relação ao Coletor Paralelo, dependendo da taxa de promoção. O CMS também exige 20% mais memória para acomodar estruturas de dados adicionais e ""lixo flutuante"" que pode ser perdido durante a marcação simultânea que será transferida para o próximo ciclo. Altas taxas de promoção e fragmentação resultante podem às vezes ser reduzidas pelo aumento do tamanho de ambos a young gen e old gen . Observação: O CMS pode sofrer ""falhas de modo concorrente"", que pode ser visto nos logs, quando deixa de coletar a uma taxa suficiente para manter-se com a promoção. Isso pode ser causado quando a coleta começa tarde, que podem ser definidos por meio de ajuste. Mas isso também pode acontecer quando a taxa de coleta não consegue acompanhar a alta taxa de promoção ou a alta mutação de objetos de algumas aplicações. Se o taxa de promoção ou mutação da aplicação é tão alta, então sua aplicação pode necessitar de algumas mudanças para reduzir a pressão da promoção. Adicionando mais memória para um sistema deste tipo, por vezes, pode piorar a situação, porque o CMS teria mais memória para examinar. Coletor Garbage First (G1) O Garbage First (G1) (-XX:+UseG1GC) é um novo coletor introduzido no Java 6 e agora oficialmente suportado no Java 7. É um algoritmo parcialmente concorrente que também tenta compactar o tenured space em pequenas e incrementais pausas stop-the-world para tentar minimizar os eventos FullGC que aflige o CMS por causa da fragmentação. G1 é um coletor geracional que organiza a heap diferentemente dos outros coletores dividindo-o em regiões de tamanho fixo de efeito variável, ao invés de regiões contíguas com o mesmo objetivo. O G1 adota a abordagem de concorrentemente marcar regiões para rastrear referências entre regiões, e concentra a coleta nas regiões com mais espaço livre. Estas regiões são então coletadas em uma pausa stop-the-world descarregando os objetos vivos para uma região vazia, assim compactando no processo. Objetos maiores que 50% da região são alocados em uma região monstruosa, que é um múltiplo do tamanho da região. A alocação e coleta de objetos monstruosos podem ser muito caros para o G1, e até agora teve pouco ou nenhum esforço de otimização aplicada. O desafio com qualquer coletor de compactação não é o de mover objetos mas o de atualizar as referências destes objetos. Se um objeto é referenciado em várias regiões, então atualizar estas referências podem demorar significativamente mais do que mover o objeto. O G1 rastreia quais objetos em uma região tem referências de outras regiões via ""Remembered Sets"" (Conjuntos lembrados). Se um Remembered Set tornar-se grande, então o G1 pode desacelerar significativamente. Quando liberando objetos de uma região para outra, o tamanho dos eventos stop-the-world associados tendem a ser proporcionais ao número de regiões com referências que precisam ser escaneadas e potencialmente corrigidas. Manter os Remembered Sets aumenta o custo das coletas menores resultando pausas maiores do que visto com o Coletor Paralelo Antigo ou o CMS para coleções menores. O G1 é configurado baseado na latência -XX:MaxGCPauseMillis=<n>, valor padrão = 200ms. A meta irá influenciar a quantidade de trabalho realizado em cada ciclo somente na base nos melhores esforços. Estabelecer metas em dezenas de milissegundos é mais fútil, e da forma como foi escrito, tentar alvejar dezenas de milissegundos não foi o foco do G1. O G1 é em geral um bom coletor para grandes pilhas que tem a tendência de se tornar fragmentado quando uma aplicação pode tolerar pausas entre 0.5 e 1.0 segundo para compactações incrementais. O G1 tende a reduzir a frequência dos piores casos de pausa visto no CMS devido a fragmentação ao custo das coletas menores e compactação incremental da old gen . Mais pausas acabam sendo obrigatórias para o desenvolvimento regional ao invés de compactações de pilhas cheias. Como o CMS, o G1 pode deixar de manter-se com as taxas de promoção, e voltará para um FullGC stop-the-world. Assim como o CMS tem ""concorrente modo de falha"", G1 pode sofrer uma falha de evacuação, visto nos logs como ""estouro de espaço"". Isso ocorre quando não existem regiões livres na qual os objetos possam ser evacuados, que é similar a uma falha de promoção. Se isso ocorrer, tente usar uma grande pilha e mais threads de marcação, mas em alguns casos mudanças na aplicação são necessárias para reduzir os custos de alocação. Um problema desafiador para o G1 é lidar com objetos populares e regiões. Compactação incremental stop-the-world funciona bem quando regiões tem objetos vivos que não são fortemente referenciados em outras regiões. Se um objeto ou região é popular então o Remembered Set será grande e o G1 evitará coletar estes objetos. Eventualmente ele pode não ter escolha, que resultará em muitas pausas frequentes de média duração até que a heap seja compactada. Coletores Concorrentes Alternativos CMS e G1 são frequentemente chamados de coletores concorrentes. Quando olhamos o trabalho total realizado é claro que a young gen , promoção e até mesmo o trabalho da old gen não é concorrente ao todo. O CMS é mais concorrente para a old gen ; O G1 é muito mais do que um coletor incremental do tipo stop-the-world. Ambos CMS e G1 tem significantes e regulares ocorrências dos eventos stop-the-world, e nos piores cenários que frequentemente tornam-nos impróprios para rigorosas aplicações de baixa latência, como uma negociação financeira or interfaces reativas ao usuário. Coletores alternativos como Oracle JRockit Real Time, IBM Websphere Real Time e Azul Zing estão disponíveis. Os coletores JRockit e Websphere levam vantagem na latência na maioria dos casos sobre o CMS e G1, mas frequentemente veem limitações na faixa de transferência e continuam sofrendo significantes eventos stop-the-world. Zinf é o único coletor Java conhecido por este autor que pode ser verdadeiramente concorrente para coleta e compactação enquanto mantém uma alta taxa de transferência para todas as gerações. Zing tem algumas frações de milisengundos em eventos stop-the-world, mas há trocas de fases no ciclo de coleta que não estão relacionados ao tamanho do conjunto vivo. JRockit RT pode alcançar tempos de pausa típicos em dezenas de milisegundos para altas taxas de alocação contidas no tamanho da pilha, mas as vezes tem que deixar para traz a pausa para a compactação completa. Websphere RT pode alcançar pausas de poucos milisegundos através de taxa de alocação restrita e tamanho de conjuntos vivos. O Zing pode alcançar pausas de frações de milisegundos com altas taxas de alocação por ser concorrente por todas fases, incluindo durante as coletas menores. O Zing tem condições de manter esse comportamento consistente graças ao tamanho da heap, permitindo que o usuário aplique grandes tamanhos de heap conforme preciso, acompanhando a taxa de tranferência do aplicativo ou a necessidade dos estados dos objetos, sem medo de maiores tempos de pausa. Para todos os coletores concorrentes focados na latência é necessário dar um pouco de taxa de transferência e ganho de rastro. Dependendo da eficiência do coletor concorrente pode-se dar um pouco de taxa de transferência, mas está sempre adicionando rastro significativo. Se for realmente concorrente, com poucos eventos stop-the-world, então mais núcleos de CPU serão necessários para habilitar a concorrência e manter a vazão. Observação: Todos coletores concorrentes tendem a funcionar mais eficientemente quando há espaço suficiente para a alocação. Como regra de ponto de partida tente reservar uma heap com pelo menos duas ou três vezes o tamanho dos conjuntos vivos, para a operação ser eficiente. Entretanto, o espaço necessário para manter as operações concorrentes aumentam com a taxa de transferência da aplicação, e está associada a alocação e taxas de promoção. Então para aplicações de maior vazão, uma heap maior para um conjunto vivo proporcional pode ser justificado. Dados os enormes espaços de memória disponíveis para os sistemas de hoje o uso da memória raramente é um problema no lado do servidor. Monitorando e Ajustando a Coleta de Lixo Para entender como a aplicação e o garbage collector estão se comportando, inicie a JVM com as seguintes configurações: Então carregue os logs dentro de uma ferramenta como Chewiebug para realizar as analises. Para ver a natureza dinâmica do GC, execute o JVisualVM e instale o plugin Visual GC que permitirá a visualização do GC em ação, como por exemplo na aplicação a seguir. Para entender o que o GC da aplicação precisa é necessário carregar testes representativos que possam ser rodados repetidamente. À medida que se familiariza com a forma como cada coletor trabalha, então execute os teste de carga com diferentes configurações até chegar na taxa de transferência e latência desejadas. É importante medir a latência da perspectiva do usuário final. Isso pode ser alcançado capturando o tempo de resposta de cada requisição do teste em um historograma, e é possível ler mais sobre isso aqui . Se houver picos de latência que estão fora do range aceitável, então tente relacioná-las com os logs do GC para determinar se o problema é o GC. É possível que outras questões possam causar os picos de latência. Outra ferramenta útil a considerar é o jHiccup que pode ser usado para acompanhar as pausas dentro da JVM e através do sistema como um todo. Se os picos de latência são devidos ao GC, então invista em ajustar o CMS ou G1 para ver se suas métricas de latência podem ser cumpridas. As vezes isso não é possível devido a alta alocação e taxas de promoção combinadas com os requisitos de baixa latência. Os ajustes do GC podem se tornar um exercício altamente necessário e que muitas vezes requer mudanças de aplicativos para reduzir as taxas de alocação ou ciclo de vida dos objetos. Se for o caso, então avalie a economia de tempo e recursos gastos no ajustes do GC e mudanças na aplicação com a compra de um dos concorrentes comerciais de compactação de JVMs como JRockit Real Time Azul ou Zing.",pt,77
162,1249,1464967629,CONTENT SHARED,-4798431998955209742,-709287718034731589,-2752546383439495719,,,,HTML,http://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/,what color is your function?,"I don't know about you, but nothing gets me going in the morning quite like a good old fashioned programming language rant. It stirs the blood to see someone skewer one of those ""blub"" languages the plebians use, muddling through their day with it between furtive visits to StackOverflow. (Meanwhile, you and I, only use the most enlightened of languages. Chisel-sharp tools designed for the manicured hands of expert craftspersons such as ourselves.) Of course, as the author of said screed, I run a risk. The language I mock could be one you like! Without realizing it, I could let have the rabble into my blog, pitchforks and torches at the ready, and my fool-hardy pamphlet could draw their ire! To protect myself from the heat of those flames, and to avoid offending your possibly delicate sensibilities, instead, I'll rant about a language I just made up. A strawman whose sole purpose is to be set aflame. I know, this seems pointless right? Trust me, by the end, we'll see whose face (or faces!) have been painted on his straw noggin. A new language Learning an entire new (crappy) language just for a blog post is a tall order, so let's say it's mostly similar to one you and I already know. We'll say it has syntax sorta like JS. Curly braces and semicolons. if , while , etc. The lingua franca of the programming grotto. I'm picking JS not because that's what this post is about. It's just that it's the language you, statistical representation of the average reader, are most likely to be able grok. Voilà: Because our strawman is a modern (shitty) language, we also have first-class functions. So you can make something like like: This is one of those higher-order functions, and, like the name implies, they are classy as all get out and super useful. You're probably used to them for mucking around with collections, but once you internalize the concept, you start using them damn near everywhere. Maybe in your testing framework: Or when you need to parse some data: So you go to town and write all sorts of awesome reusable libraries and applications passing around functions, calling functions, returning functions. Functapalooza. What color is your function? Except wait. Here's where our language gets screwy. It has this one peculiar feature: 1. Every function has a color. Each function-anonymous callback or regular named one-is either red or blue. Since my blog's code highlighter can't handle actual color, we'll say the syntax is like: There are no colorless functions in the language. Want to make a function? Gotta pick a color. Them's the rules. And, actually, there are a couple more rules you have to follow too: 2. The way you call a function depends on its color. Imagine a ""blue call"" syntax and a ""red call"" syntax. Something like: If you get it wrong-call a red function with *blue after the parentheses or vice versa-it does something bad. Dredge up some long-forgotten nightmare from your childhood like a clown with snakes for arms under your bed. That jumps out of your monitor and sucks out your vitreous humour. Annoying rule, right? Oh, and one more: 3. You can only call a red function from within another red function. You can call a blue function from with a red one. This is kosher: But you can't go the other way. If you try to do this: Well, you're gonna get a visit from old Spidermouth the Night Clown. This makes writing higher-order functions like our filter() example trickier. We have to pick a color for it and that affects the colors of the functions we're allowed to pass to it. The obvious solution is to make filter() red. That way, it can take either red or blue functions and call them. But then we run into the next itchy spot in the hairshirt that is this language: 4. Red functions are more painful to call. For now, I won't precisely define ""painful"", but just imagine that the programmer has to jump through some kind of annoying hoops every time they call a red function. Maybe it's really verbose, or maybe you can't do it inside certain kinds of statements. Maybe you can only call them on line numbers that are prime. What matters is that, if you decide to make a function red, everyone using your API will want to spit in your coffee and/or deposit some even less savory fluids in it. The obvious solution then is to never use red functions. Just make everything blue and you're back to the sane world where all functions have the same color, which is equivalent to them all having no color, which is equivalent to our language not being entirely stupid. Alas, the sadistic language designers-and we all know all programming language designers are sadists, don't we?-jabbed one final thorn in our side: 5. Some core library functions are red. There are some functions built in to the platform, functions that we need to use, that we are unable to write ourselves, that only come in red. At this point, a reasonable person might think the language hates us. It's functional programming's fault! You might be thinking that the problem here is we're trying to use higher-order functions. If we just stop flouncing around in all of that functional frippery and write normal blue collar first-order functions like God intended, we'd spare ourselves all the heartache. If we only call blue functions, make our function blue. Otherwise, make it red. As long as we never make functions that accept functions, we don't have to worry about trying to be ""polymorphic over function color"" (polychromatic?) or any nonsense like that. But, alas, higher order functions are just one example. This problem is pervasive any time we want to break our program down into separate functions that get reused. For example, let's say we have a nice little blob of code that, I don't know, implements Dijkstra's algorithm over a graph representing how much your social network are crushing on each other. (I spent way too long trying to decide what such a result would even represent. Transitive undesirability?) Later, you end up needing to use this same blob of code somewhere else. You do the natural thing and hoist it out into a separate function. You call it from the old place and your new code that uses it. But what color should it be? Obviously, you'll make it blue if you can, but what if it uses one of those nasty red-only core library functions? What if the new place you want to call it is blue? You'll have to turn it red. Then you'll have to turn the function that calls it red. Ugh. No matter what, you'll have to think about color constantly. It will be the sand in your swimsuit on the beach vacation of development. A colorful allegory Of course, I'm not really talking about color here, am I? It's an allegory, a literary trick. The Sneetches isn't about stars on bellies, it's about race. By now, you may have an inkling of what color actually represents. If not, here's the big reveal: Red functions are asynchronous ones. If you're programming in JavaScript on Node.js, everytime you define a function that ""returns"" a value by invoking a callback, you just made a red function. Look back at that list of rules and see how my metaphor stacks up: Synchronous functions return values, async ones do not and instead invoke callbacks. Synchronous functions give their result as a return value, async functions give it by invoking a callback you pass to it. You can't call an async function from a synchronous one because you won't be able to determine the result until the async one completes later. Async functions don't compose in expressions because of the callbacks, have different error-handling, and can't be used with try/catch or inside a lot of other control flow statements. Node's whole shtick is that the core libs are all asynchronous. (Though they did dial that back and start adding ___Sync() versions of a lot of things.) When people talk about ""callback hell"" they're talking about how annoying it is to have red functions in their language. When they create 4089 libraries for doing asynchronous programming , they're trying to cope at the library level with a problem that the language foisted onto them. I promise the future is better People in the Node community have realized that callbacks are a pain for a long time, and have looked around for solutions. One technique that gets a bunch of people excited is , which you may also know by their rapper name ""futures"". These are sort of a jacked up wrapper around a callback and an error handler. If you think of passing a callback and errorback to a function as a concept , a promise is basically a reification of that idea. It's a first-class object that represents an asynchronous operation. I just jammed a bunch of fancy PL language in that paragraph so it probably sounds like a sweet deal, but it's basically snake oil. Promises do make async code a little easier to write. They compose a bit better, so rule #4 isn't quite so onerous. But, honestly, it's like the difference between being punched in the gut versus punched in the privates. Less painful, yes, but I don't think anyone should really get thrilled about the value proposition. You still can't use them with exception handling or other control flow statements. You still can't call a function that returns a future from synchronous code. (Well, you can , but if you do, the person who later maintains your code will invent a time machine, travel back in time to the moment that you did this and stab you in the face with a #2 pencil.) You've still divided your entire world into asynchronous and synchronous halves and all of the misery that entails. So, even if your language features promises or futures, its face looks an awful lot like the one on my strawman. (Yes, that means even Dart , the language I work on. That's why I'm so excited some of the team are experimenting with other concurrency models .) I'm awaiting a solution C# programmers are probably feeling pretty smug right now (a condition they've increasingly fallen prey to as Hejlsberg and company have piled sweet feature after sweet feature into the language). In C#, you can use the await keyword to invoke an asynchronous function. This lets you make asynchronous calls just as easily as you can synchronous ones, with the tiny addition of a cute little keyword. You can nest await calls in expressions, use them in exception handling code, stuff them inside control flow. Go nuts. Make it rain await calls like a they're dollars in the advance you got for your new rap album. Async-await is nice, which is why we're adding it to Dart. It makes it a lot easier to write asynchronous code. You know a ""but"" is coming. It is. But... you still have divided the world in two. Those async functions are easier to write, but they're still async functions . You've still got two colors. Async-await solves annoying rule #4: they make red functions not much worse to call than blue ones. But all of the other rules are still there: Synchronous functions return values, async ones return Task<T> (or Future<T> in Dart) wrappers around the value. Sync functions are just called, async ones need an await . If you call an async function you've got this wrapper object when you actually want the T . You can't unwrap it unless you make your function async and await it. (But see below.) Aside from a liberal garnish of await , we did at least fix this. C#'s core library is actually older than async so I guess they never had this problem. It is better. I will take async-await over bare callbacks or futures any day of the week. But we're lying to ourselves if we think all of our troubles are gone. As soon as you start trying to write higher-order functions, or reuse code, you're right back to realizing color is still there, bleeding all over your codebase. What language isn't colored? So JS, Dart, C#, and Python have this problem. CoffeeScript and most other languages that compile to JS do too (which is why Dart inherited it). I think even ClojureScript has this issue even though they've tried really hard to push against it with their core.async stuff. Wanna know one that doesn't? Java. I know right? How often do you get to say, ""Yeah, Java is the one that really does this right.""? But there you go. In their defense, they are actively trying to correct this oversight by moving to futures and async IO. It's like a race to the bottom. C# also actually can avoid this problem too. They opted in to having color. Before they added async-await and all of the Task<T> stuff, you just used regular sync API calls. Three more languages that don't have this problem: Go, Lua, and Ruby. Any guess what they have in common? Threads. Or, more precisely: multiple independent callstacks that can be switched between . It isn't strictly necessary for them to be operating system threads. Goroutines in Go, coroutines in Lua, and fibers in Ruby are perfectly adequate. (That's why C# has that little caveat. You can avoid the pain of async in C# by using threads.) Remembrance of operations past The fundamental problem is ""How do you pick up where you left off when an operation completes""? You've built up some big callstack and then you call some IO operation. For performance, that operation uses the operating system's underlying asynchronous API. You cannot wait for it to complete because it won't. You have to return all the way back to your language's event loop and give the OS some time to spin before it will be done. Once it is, you need to resume what you were doing. The usual way a language ""remembers where it is"" is the callstack . That tracks all of the functions that are currently being invoked and where the instruction pointer is in each one. But to do async IO, you have to unwind discard the entire C callstack. Kind of a Catch-22. You can do super fast IO, you just can't do anything with the result! Every language that has async IO in its bowels-or in the case of JS, the browser's event loop-copes with this in some way. Node with its ever-marching-to-the-right callbacks stuffs all of those callframes in closures. When you do: Each of those function expressions closes over all of its surrounding context. That moves parameters like iceCream and caramel off the callstack and onto the heap. When the outer function returns and the callstack is trashed, it's cool. That data is still floating around the heap. The problem is you have to manually reify every damn one of these steps. There's actually a name for this transformation: continuation-passing style . It was invented by language hackers in the 70s as an intermediate representation to use in the guts of their compilers. It's a really bizarro way to represent code that happens to make some compiler optimizations easier to do. No one ever for a second thought that a programmer would write actual code like that . And then Node came along and all of the sudden here we are pretending to be compiler back-ends. Where did we go wrong? Note that promises and futures don't actually buy you anything, either. If you've used them, you know you're still hand-creating giant piles of function literals. You're just passing them to .then() instead of to the asynchronous function itself. Awaiting a generated solution Async-await does help. If you peel back your compiler's skull and see what it's doing when it hits an await call you'd see it actually doing the CPS-transform. That's why you need to use await in C#: it's a clue to the compiler to say, ""break the function in half here"". Everything after the await gets hoisted into a new function that it synthesizes on your behalf. This is why async-await didn't need any runtime support in the .NET framework. The compiler compiles it away to a series of chained closures that it can already handle. (Interestingly, closures themselves also don't need runtime support. They get compiled to anonymous classes. In C#, closures really are a poor man's objects .) You might be wondering when I'm going to bring up generators. Does your language have a yield keyword? Then it can do something very similar. (In fact, I believe generators and async-await are isomorphic. I've got a bit of code floating around in some dark corner of my hard disc that implements a generator-style game loop using only async-await.) Where was I? Oh, right. So with callbacks, promises, async-await, and generators, you ultimately end up taking your asynchronous function and smearing it out into a bunch of closures that live over in the heap. Your function passes the outermost one into the runtime. When the event loop or IO operation is done, it invokes that function and you pick up where you left off. But that means everything above you also has to return. You still have to unwind the whole stack. This is where the ""red functions can only be called by red functions"" rule comes from. You have to closurify the entire callstack all the way back to main() or the event handler. Reified callstacks But if you have threads (green- or OS-level), you don't need to do that. You can just suspend the entire thread and hop straight back to the OS or event loop without having to return from all of those functions . Go is the language that does this most beautifully in my opinion. As soon as you do any IO operation, it just parks that goroutine and resumes any other ones that aren't blocked on IO. If you look at the IO operations in the standard library, they seem synchronous. In other words, they just do work and then return a result when they are done. But it's not that they're synchronous in the sense that it would mean in JavaScript. Other Go code can run while one of these operations is pending. It's that Go has eliminated the distinction between synchronous and asynchronous code . Concurrency in Go is a facet of how you choose to model your program, and not a color seared into each function in the standard library. This means all of the pain of the five rules I mentioned above is completely and totally eliminated. So, the next time you start telling me about some new hot language and how awesome its concurrency story is because it has asynchronous APIs, now you'll know why I start grinding my teeth. Because it means you're right back to red functions and blue ones.",en,77
163,1149,1464631078,CONTENT SHARED,5270696484536580646,3609194402293569455,3334185486919070217,,,,HTML,http://www.inc.com/brian-de-haaff/8-insanely-simple-productivity-hacks-.html,8 insanely simple productivity hacks,"You earnestly set goals and try to meet them each day . But something always seems to get in the way and upset your progress. I know because it happens to me too. It could be your colleague's weekend recap that throws off your Monday morning, or the distraction of calls and texts throughout the day. When your best-laid plans face off with the reality of your workday, reality often wins. That disconnect between your goals and your productivity is likely cause for major frustration. You're not alone -- studies show many people are overworked and underperforming. A 2015 survey of 2,000 workers conducted by Staples Business Advantage revealed that 37 percent of employees believe more workplace flexibility would help them be productive. And about 50 percent point to email as the main culprit for reduced productivity. While there are no shortcuts to hard work, there are practical ways to get more done in the time you have. But first, you must take back control of your workday and start setting yourself up for success. These eight hacks can help you accomplish more meaningful work in half the time: 1. Rise and shine. You are most productive in the first two hours after you get up, so plan your most challenging and meaningful work for that time of day. Arrive before the rest of the team and dive into your work -- you will be ready to take a quick coffee break and catch up with everyone once they arrive. 2. Make mini-goals. Far-reaching goals are admirable, but they can seem so distant they appear unreachable. Help yourself realize goals by breaking them down into smaller, manageable mini-goals. These smaller ""wins"" will help you make progress and boost your confidence. 3. Stop multitasking. Multitasking is not only ineffective, but it reduces the quality of your work, causes more stress, and can even reduce your IQ. Stop trying to do everything at once. Prioritize essential work, and schedule time each day to deal with non-urgent matters like answering routine emails. 4. Tell others. Clearly communicate to your co-workers when you are heads-down on a task and need to work undisturbed. Block time in your calendar, put up a do-not-disturb sign, or otherwise set your status to ""busy"" to avoid social interruptions. 5. Seek quiet. An open floor plan is a noisy work environment, making it challenging for anyone to perform even simple tasks. Purchase noise-canceling headphones, find a quiet corner or an empty boardroom, or work remotely . Remember: The ding of an incoming text message is enough to disrupt your focus. Silence your phone during critical work, and put it out of sight. 6. Schedule breaks. If you work nonstop, your work will suffer and so will you. The ideal work-to-break ratio? One study says 52-minute work sprints followed by 17-minute breaks. Make your break-time meaningful -- stretch or go for a walk outside if you can. The important thing is to take time away from your screen and think about something else. 7. Measure progress. At the end of the workday, step back and take stock. What did you accomplish, and how do these accomplishments line up with your goals? What were your low points? What got in your way, and what can you do better tomorrow? By measuring your progress, you can spot patterns and opportunities to revamp how you work. 8. Prep for tomorrow. Devote the last 15 minutes of your day (no matter when that is) to creating a to-do list for the next day. That way, you can jump right into the work the next morning. This is especially important before you are going to take a bit of time off, like over the weekend -- and you will be happy to discover your list on Monday morning. It takes discipline and hard work to unlearn bad habits, reestablish boundaries, and regain control over your productivity. But the effort will be worth it. You will be proud of how much more you can accomplish daily, as well as the meaningful progress you will make toward those long-range goals . So when the next day rolls around and you're faced with a chatty co-worker or a buzzing phone, you now have a plan to silence them both -- and get more done.",en,77
164,1898,1469649566,CONTENT SHARED,1005751836898964351,3429602690322213789,6505036434267960593,,,,HTML,https://www.linkedin.com/pulse/seria-stranger-things-uma-obra-de-arte-do-algoritmo-da-gustavo-miller,seria stranger things uma obra de arte do algoritmo da netflix?,"Matei Stranger Things no último final de semana, sete dias após o seu lançamento na Netflix, tomado pelos milhares posts nostálgicos que floodaram minhas redes sociais sobre a série que emula os filmes de aventura infantil dos anos 80. Antes de ser mais um a dizer que os criadores foram geniais ao colocarem em 8 episódios todos os nossos filmes favoritos de infância, ninguém chegou a pensar que essa série talvez seja a maior obra de arte do algoritmo da Netflix? Pensem aqui: com base nos hábitos de seus assinantes, há cinco anos o algoritmo deles mostrou que os filmes do Kevin Spacey eram muito vistos, assim como os dirigidos por David Fincher, e que uma série britânica dos anos 90 sobre os bastidores sujos do Parlamento tinha uma interessante legião de seguidores. Daí saiu a adaptação americana para House of Cards. Desta vez temos uma série que costurou ET com Conta Comigo, Alien com Carrie, Contatos Imediatos do Terceiro Grau com Evil Dead, Goonies com Poltergeist, Além da Imaginação com Chamas da Vingança... Tudo isso estrelado por dois dos atores mais populares da década perdida: Winona Ryder e Matthew Modine. A Netflix costuma se fechar quando o assunto é a maneira como eles exploram dados e criam produtos a partir dos insights que o big data sugere. O algoritmo deles, que já ajudou (mas não é crucial, deixemos claro) a ressuscitar seriados como Arrested Development e Full House, além de estreitar cada vez mais a parceria com a Marvel Studios ( vide a penca de novas séries anunciadas na Comic-Con ), é algo tão valioso dentro da empresa que ela criou internamente o Netflix Prize , que justamente premia quem melhorar a capacidade daqueles números darem previsões de consumo. Em 2013, 75% dos assinantes escolhiam o que assistir ali dentro por base das recomendações da empresa. E hoje? A Netflix não divulga. Uma de minhas frases favoritas sobre a empresa, por sinal, é a de que há "" 33 milhões de versões diferentes da Netflix"" . Conhecendo um pouco como a Netflix analisa todos os nossos hábitos ali, aposto um quindim que tem muito big data por trás de Stranger Things. E que nenhuma das 99973 referências da série estão ali por acaso. Basta ver a reação apaixonada de todo mundo nas redes sociais na semana passada.",pt,76
165,379,1460554787,CONTENT SHARED,4761910285123871012,-1032019229384696495,-6165125069895538182,,,,HTML,https://wit.ai/blog/2016/04/12/bot-engine?hn,bot engine,"Today we are releasing the first step of our vision for conversational bots: an early beta of Bot Engine. Since we joined Facebook 15 months ago, the Wit community grew from 6,000 to 21,500 developers. Many of you are already making bots, leveraging Wit to parse messages into structured, actionable data. We are also building Facebook M , your personal assistant in Messenger. We've learned a lot in the process. Making a bot seems easy until you try. And then, the curse of combinatorics bites you. Understanding the user intent is a necessary step, but that's not enough. How should the bot respond? What actions should it perform? What question should it ask? This is the problem we are trying to help solve with Bot Engine. Here is how: Key concept 1: Stories To make a bot, there are two schools: rules or machine learning. (Everybody claims rules are bad and their bot is powered by AI, but when you really look under the hood, the core is often imperative.) Machine learning is of course more desirable, but the problem is the training dataset. Training a Wit intent with a dozen examples works well, and it's easy to leverage the community to get more examples. But in order to entirely learn the business logic of a bot of medium complexity, we would need many, many thousands of example conversations. Rules (or any kind of imperative approach, including plain script/program) are kind of the opposite. The good thing with rules is, you can have a demo working after you write two rules. As long as you follow the script carefully, your bot will work and your audience will be impressed. But as you discover new ""paths"" in the dialog, you'll add more and more rules, until one day everything collapses. You're doomed by the curse of combinatorics. Any new rule conflicts with old rules that you totally forgot the reason for. Your bot cannot improve anymore. Bot Engine is trained with Stories. Stories are example conversations. On top of user's and bot's messages, Stories also contain the bot actions: When you create your bot, you just start with a few stories that describe the most probable conversations paths. At this stage, Bot Engine will build a machine learning model that deliberately overfits the stories dataset. Practically, it means that stories will behave almost like rules. It enables you to start beta testing your bot and collect conversations. You will then turn these conversations into new stories (exactly like you turn logs into new expressions in the original Wit). The more stories you have, the better the model becomes. Unlike rules, Stories can ignore each other; when you discover a new use case, you can add a new story without the need to take into account all previous stories. With this approach, we are trying to get the best of both worlds: rules when you get started and don't have much data, and AI once your dataset grows. Key concept 2: Action Trained by Stories, Bot Engine predicts the next action your bot may execute at each step of the conversation. This action is executed on your side, with your code. We've made this decision because we believe that developers need total freedom of platform and execution scope for their bot's actions. You should be able to use any programming language of your choice and call any API you need. That wouldn't be practical if actions were executed on our side. An action typically modifies the context of the conversation. You pass the new context to Bot Engine when you need to predict the next action. Actions also help relieve the pressure on the machine learning model: they encapsulate some business logic that would require a huge dataset to learn end-to-end. It enables Wit to train efficient models before you have hundreds of Stories. Key concept 3: Inbox In line with the original Wit philosophy , we think that you really discover your users' expectations and logic only after you release a first version of your bot. As a result, the initial build should be as lightweight as possible, and the platform optimized around continuous learning and improvement based on actual usage. In Bot Engine, we are extending the concept of the Wit Inbox to the conversation logs: we will provide an easy way to turn logs into new Stories. This is the learning loop. Each time you add a new Story, your bot's models are rebuilt in real time. We are trying to solve a very hard problem, and we are not pretending that we have the definitive solution. There are many kinds of bots, and many different ways to address the problem. This is an early beta: our hope is that you won't hesitate to contact us when you are trying to do something that doesn't seem to be well handled. We are committed to spend a significant amount of time supporting the community and learning more about the problem. While at first glance it may look easier to just use scripts, AIML, or slot-based workflow templates to build bots, we believe that these approaches soon lead to bottlenecks. Bot Engine might not be as easy as putting a few rules together in a basic bot, but it's designed to handle scaling complexity with simplicity ( special dedication to our hero Rich Hickey ), as additional stories don't need to comply with a complex net of existing rules. Some of us at Wit have been building bots for 15 years. We've tried many things that don't work, and we're still looking for what could work. Please give Bot Engine a try , and give us your feedback! Alex Lebrun & Team Wit Note: If you already have a Wit app - First of all, thanks! We are thrilled to continue to serve you. - Your existing app will continue to work as before. The /message API is unchanged. - You won't have the Stories tab in your existing app. We'll add a migration process very soon. - If you create a new app, you'll get the Stories tab. You will also notice a change in the Console: we've merged the concepts of intents and entities under an Understanding tab, and slightly updated the training UX . Follow @WitNL",en,76
166,1749,1468376407,CONTENT SHARED,4375556914674736641,-1836083230511905974,3650195981289591968,,,,HTML,http://codigo-google.blogspot.com.br/2016/07/firebase-tag-manager-360.html?m=1&_utm_source=1-2-2,código google: introdução da próxima geração do google tag manager e do tag manager 360 para mobile apps,"O Google Tag Manager é conhecido por facilitar a implementação e o gerenciamento de analítica, remarketing, acompanhamento de conversão e outros tipos de tags de sites e aplicativos. E com a introdução do Firebase - a nova plataforma da Google para desenvolvedores para dispositivos móveis Android e iOS -, usar o Tag Manager ou o Tag Manager 360 para configurar a medição interna no aplicativo nunca foi tão fácil nem tão poderoso! No Google I/O , anunciamos que estamos expandindo o Firebase para torná-lo uma plataforma unificada para desenvolvedores para dispositivos móveis projetada para facilitar mais do que nunca a criação de aplicativos usando produtos e serviços da Google - e o Google Tag Manager é um destes serviços! A versão mais recente do Tag Manager e do Tag Manager 360 para aplicativos móveis foi projetada para funcionar com o Firebase e estender seus recursos para desenvolvedores e profissionais de marketing. Instrumentação interna do aplicativo unificada No coração do Firebase está o Firebase Analytics - um produto de analítica ilimitado e gratuito projetado especificamente para aplicativos móveis. Mas o Firebase Analytics é mais do que um produto de analítica, é uma forma unificada para os desenvolvedores medirem tudo o que estiver acontecendo em um aplicativo, desde os principais direcionadores da empresa até a interação detalhada do usuário. Isto cria uma fonte única confiável para atividades internas do aplicativo, que você pode compartilhar com outros recursos do Firebase e produtos da Google. Para o Tag Manager, isto torna o Firebase Analytics a nova camada de dados, o que significa que qualquer um que use o Firebase Analytics está pronto para começar a usar o Tag Manager sem precisar reprogramar. Para começar com o Google Tag Manager e o Tag Manager 360, basta se inscrever no Firebase, acessar o Tag Manager para configurar um novo contêiner do Firebase e, depois, adicionar o Firebase Analytics e o Google Tag Manager ao seu aplicativo. Tudo que você estiver medindo com o Firebase Analytics estará pronto para usar em tags, gatilhos e variáveis no Tag Manager. Medição dinâmica de aplicativos O Firebase Analytics facilita a medição do que está acontecendo no seu aplicativo. Mas o que acontece se você identificar errado um evento ou esquecer de adicionar um parâmetro fundamental? Ao adicionar o Tag Manager ou o Tag Manager 360 ao aplicativo, você pode fazer alterações na configuração de medição sem precisar perder tempo no processo de atualização do aplicativo. Como os profissionais de marketing experientes sabem, sem o gerenciamento de tags, mesmo as mudanças mais básicas em tags podem exigir muito tempo e esforço, além de coordenação entre as equipes de marketing e desenvolvimento e de retirar recursos de outros projetos. Com o Tag Manager e o Firebase, você poderá desacoplar as mudanças de medição dos ciclos de desenvolvimento - e, ao fazê-lo, simplificará o modo com que as equipes de marketing e desenvolvimento trabalham juntas. Um SDK, muitas opções Apesar de o Firebase visar facilitar ao máximo a criação de aplicativos e medir o comportamento do usuário, ele não se destina a ser uma solução para todos os problemas. Os desenvolvedores e profissionais de marketing muitas vezes escolhem usar soluções diversas de vários provedores nos aplicativos. O Google Tag Manager e o Tag Manager 360 podem ajudar a fazer com que estas ferramentas distintas façam sentido. Com o Firebase Analytics, fica fácil medir o que está acontecendo no aplicativo - sem limitar você a um único conjunto de ferramentas. O Google Tag Manager e o Tag Manager 360 permitem que você envie os dados para diversas outras ferramentas de analítica, tanto da Google - como o Google Analytics - quanto de outros parceiros. Estamos muito felizes em anunciar parcerias de fornecimento de tags com diversas das soluções de atribuição do aplicativo, incluindo Kochava , Tune , adjust , AppsFlyer , Apsalar e muito mais. O Google Tag Manager sempre foi respeitado por seu compromisso com ser agnóstico em relação aos provedores para medição web e está animado em estender o mesmo compromisso para os aplicativos móveis. E nossos parceiros também estão! ""Sempre fomos apaixonados por apoiar os desenvolvedores, garantindo que o Kochava sempre estivesse integrado às melhores ferramentas. Por isso estamos tão felizes em fornecer suporte pronto para uso para o Google Tag Manager pelo Firebase."" - Charles Manning, Presidente da Kochava Se você não está vendo o parceiro que procura, não se desespere. Estamos adicionando mais parceiros continuamente por meio do Programa de modelos de tag de provedores . Comece quando quiser Se já estiver usando o Google Tag Manager ou o Tag Manager 360 para aplicativos móveis, não se preocupe, seus contêineres existentes e os SDKs atuais continuarão trabalhando da mesma maneira de antes. Mas, como com qualquer versão importante de recurso, recomendamos atualizar para a versão mais recente do Google Tag Manager para aplicativos móveis - e com o Firebase junto - o mais cedo possível. Assim, você pode ter certeza de que extrairá o máximo da experiência de gerenciamento de tags móveis. Pronto para o Google Tag Manager? Saiba mais e comece hoje!",pt,76
167,2230,1472566708,CONTENT SHARED,-5044204125574973395,3829784524040647339,-5600479164389645980,,,,HTML,"http://link.estadao.com.br/noticias/gadget,samsung-lanca-dispositivo-para-deixar-carro-conectado-a-partir-de-chip-4g,10000071168",samsung lança dispositivo para deixar carro conectado a partir de chip 4g - link - estadão,"A Samsung lançou, durante a última semana, uma série de acessórios para acompanhar a chegada ao mercado norte-americano de seu mais recente smartphone, o Galaxy Note 7. Entre fones de ouvido e uma versão melhorada dos óculos de realidade virtual Gear VR, um dispositivo se destaca: o Connect Auto. Parecido com um pendrive ou um adaptador de tomada, o dispositivo poderá ser utilizado em qualquer carro para conectar o veículo, a partir de um chip habilitado para transmitir dados via redes 4G. Para funcionar, o dispositivo deve ser conectado à porta OBD II - utilizada nos carros dos últimos 20 anos para fornecer dados sobre o automóvel de forma eletrônica. Com isso, o Connect Auto consegue receber informações do veículo, e enviá-las para o usuário. Anunciado em fevereiro, o Connect Auto consegue disponbilizar sinal de Wi-Fi para até 10 dispositivos, desde que tenha um chip 4G em seu interior. Por enquanto, o aparelho está sendo vendido nos EUA exclusivamente para usuários da operadora AT&T - o preço varia de acordo com o plano contratado, mas o preço cheio do dispositivo é de US$ 109. Ainda não há previsão para o lançamento no Brasil.",pt,75
168,996,1463494497,CONTENT SHARED,4369833742675497700,268671367195911338,-9042980566649783979,,,,HTML,http://www.mckinsey.com/industries/financial-services/our-insights/transforming-life-insurance-with-design-thinking,transforming life insurance with design thinking,"Better addressing the evolving needs of consumers can help incumbents win their loyalty-and protect against new competitors. For generations, the life insurance industry delivered its promise of financial security with the help of strong actuarial functions, working through intermediated distribution channels. Complex products, limited services, rigid processes, and cumbersome consumer interactions did not necessarily hamper business success. In the past decade, however, the rules of the game have changed. Today's consumers reward transparency, speed, and flexibility, new competitors are looming on the horizon, and the low-interest-rate environment makes the traditional business model a thing of the past. This challenge is reflected in the sector's financial performance: the life industry has grown only 3.1% p.a. globally in the past decade (and only 2% p.a. in Europe), significantly lagging behind other mature industries such as banking or manufacturing, which have achieved a 5 to 6% p.a. growth rate. European life insurance delivered an after-tax ROE of around 8.6% between 2010 and 2015E -in line with the performance of banks (8.6%) and slightly above that of asset management (7.6%)-however still below other mature industries such as retailing (13.1%). To some fintechs, noninsurance incumbents, and venture capitalists, the industry's challenges suggest opportunity. The life insurance value chain is increasingly losing share to these players, who are chipping away at the profit pool of incumbents (Exhibit 1). How can incumbent life insurers keep pace in today's fast-moving competitive environment and meet customers' changing needs? There are three major ingredients of the ""winning recipe"": simplified, compelling product design, a streamlined cost base, and delightful customer journeys. This article focuses on the third ingredient, and describes a new approach called ""design thinking (and doing)"" that connects every aspect of the business, from marketing to distribution, underwriting, and claims. It is a method that goes beyond the traditional mantra of customer centricity and aims to change not only a company's processes but also its people. Incumbent life insurers that don't keep pace will falter; some might disappear. As Klaus Schwab, chairman of the World Economic Forum, famously put it: ""In the new world, it is not the big fish which eats the small fish, it's the fast fish which eats the slow fish."" Size or complexity cannot be an excuse for sluggishness: the top global insurers typically have a market cap of around USD 60 to 80 billion, while Alphabet's (Google) is well over USD 500 billion. The disconnect between life insurers' value proposition and today's customers Imagine Susan, a 34-year-old high school teacher who is expecting her first child and hence decides to buy a life insurance policy. First, it is very unlikely that a provider will proactively reach out to her. If she tries to shop for a policy online, she may be intimidated by complex products and technical terminology that is confusing and makes her feel incompetent. If she looks for an agent and is lucky enough to find one that she feels she can trust, she is likely to have concerns about how much the policy will really cost, the meaning of all the fine print, and whether the agent is more interested in her needs or a quick commission. If Susan overcomes these doubts and decides to buy a policy, she will begin what may be an arduous application process: lengthy forms with sensitive medical questions, weeks of waiting, and little transparency on where things stand. She may wonder why the process is so complicated and time-consuming when many companies in other industries offer convenient, fast (and mostly digital) services. If she has questions on her coverage after buying the product or wants to change her policy, she will likely struggle with the limited self-service options offered by the insurers and may find that her agent has moved on. Such pain points may make Susan abandon the process or take her business elsewhere, as illustrated by the most common business leakage drivers in Exhibit 2. Susan's journey illustrates some of the big challenges facing life insurance: Low engagement . Life insurers have long struggled to engage prospective clients and nurture relationships with existing ones. The product spurs high customer interest but low engagement, leading to significant untapped demand. Distribution is often intermediated through brokers, independent financial advisors, or banks, putting distance between insurers and their customers. And typical customers have very few interactions with their provider compared to other industries, such as banking (Exhibit 3). This is a major barrier to reducing attrition and enabling cross- and upselling. Limited ability to meet the preferences of Generation Y (and the new needs of Generation X) . Generation Y, the millennials now coming of age, will comprise close to half of the insurance customer pool within the next ten years. They expect highly interactive digital experiences, complete price transparency as well as fast and even instant delivery. When seeking information, they rely less on friends and family, looking instead to social communities and online reviews. Thanks to the ""equalizing"" effect of smartphones, the members of Generation X are adopting many of these behaviors rapidly. Few life insurers have the management mindset, functional firepower, or digital talent to develop propositions that meet these preferences. They also have difficulty adjusting to their fast-paced changes. These gaps frustrate consumers-and open the door to agile innovators. Legacy cost structures and IT systems . Life insurance carries a large, inflexible stock of customers/policies, and it is hard to change products or systems when some of the policies on the books were sold 30 years ago. At many incumbents, this means a clunky IT landscape that is difficult and costly to transform, especially given an expense ratio of 9 to 10%. The rigidity of the in-force book creates an environment where old processes never die and costs keep building over time. As a result, industry expense ratios have fallen only slightly in the past ten years; for key European markets, the total expense ratio declined by only 0.5 percentage points between 2000 and 2013. The dilemma of the slow fish . Insurers have historically been slow to change since their business is built on stability and risk aversion. Product development cycles often stretch to a year, and most IT upgrades follow a sequential ""waterfall"" approach. Accountability is diluted, which leads to management by committee and slows delivery. In our recent proprietary Digital Quotient™ assessment (a comprehensive tool that rates an organization's digital maturity across roughly 20 management practices), insurers lag behind digital leaders especially in terms of performance steering and measurement (Exhibit 4). As a consequence, speed of decision making and agility suffer. The life industry has not stood still in the face of these challenges, of course. Many incumbents have launched efforts to transform themselves. Most have experimented with digital initiatives, agile work modes, customer immersion, and so on. Some have set up incubators or accelerators to explore new, customer-centric solutions outside their core operations. Only a few are making the more painful change of transforming their IT platforms to enable digital delivery. And hardly any have truly transformed their basic value propositions or created genuinely customer-centric cultures. Initiatives in this direction often remain cosmetic or produce artifacts that never see the light of day. Applying design thinking to systematically rework customer journeys How can incumbent life insurers reorient themselves around the customer? One powerful approach is to apply the principles of customer-centric ""design thinking,"" which can deliver a compelling end product and be disruptive enough to transform a company's culture along the way. The methodology is based on the insight that optimizing individual touch points is insufficient to deliver a truly satisfactory overall journey-what is required is an end-to-end redesign. It is also informed by the understanding that consumers today do not separate products or services from the experience of buying and owning them. As a result, the entire ""package"" needs to be carefully designed. A few common misconceptions need correcting up front. Design thinking is not just about creating pretty interfaces or replacing paper with digital records. What it really means is applying creative, nonlinear approaches to reinvent how customers interact with the business. It touches on all functions, from product development through underwriting to IT. The essence of this new perspective is to view the customer experience as a source of competitive advantage and taking action-and making investments-accordingly. Applying design thinking to the customer journey in life insurance requires a fundamental shift along four dimensions: Instilling customer empathy . Good design requires real empathy that goes beyond what customers want: it reflects why they want it. In life insurance, fintechs are successfully attacking incumbents in this arena. The sleek experience provided by PolicyGenius, Knip, Acorns, and other start-ups shows that they are anchored in customer preferences. Like with Uber, Airbnb, and many others, the heart of their success is not the digital tool, but the experience it enables. Real empathy allows designers to respond to true underlying needs, not superficial, stated interests. By doing this, it spurs breakthrough innovation. In fact, we believe it is the only way an incumbent insurer can be sure of delivering more than a ""me-too"" customer experience. This kind of empathy requires specific research tools that typically have an ethnographic flavor, such as ""shadowing,"" ""follow me home,"" or ""shopalongs""-but empathy is about more than just collecting ""soft"" insights. A critical part of instilling empathy across the organization is translating it into hard operational metrics that can be tracked and managed (e.g., response time to under-writingrelated inquiries, number of broker or agent calls that get resolved without a call transfer, share of rejections where no alternative product was offered). In this sense, improving customer experience metrics has to become a ""contact sport"" where the entire organization works together to move the same set of numbers. Getting comfortable with an iterative approach . To use the design methodology, insurers have to feel comfortable releasing ""imperfect"" products that offer basic functionalities and services with the goal of obtaining immediate customer feedback. This approach helps avoid costly mistakes down the road and aims to bring new solutions to the market in less than 16 weeks rather than 9 to 12 months. It also means that the product is never really complete: it undergoes constant iteration. In VC and start-up jargon, it is the ""minimum viable product"" refined in multiple consecutive user tests. As an example, in testing a digital application tool, designers found that brokers were most excited about the ""accelerator"" button that allowed them to fast track three policies per month for their high-potential clients. By contrast, bank-based advisors liked a feature that prompted them to hand over the tablet to the customer to answer the sensitive medical questions themselves. To drive this kind of highly responsive innovation, many insurers need to develop a two-speed IT architecture that permits experimentation without disrupting the architecture that supports a massive in-force book. Replacing functional siloes with agile cross-functional teams . Life insurers are conservative organizations built around functions, such as Risk, Underwriting, and Sales. These offer deep expertise, but are too rigid to respond to a rapidly changing environment. In a functional setup, no one owns the full customer experience. In fact, it typically takes one to two weeks (and many work sessions) to create a complete view of the end-to-end customer experience and pain points. What is the solution? Agile, cross-functional teams that are self-motivating and entrepreneurial. They follow a new cadence, including structured, transparent, weekly sprints with daily stand-ups, review meetings, and retrospectives, to learn and take on more accountability. The cultural gap between the traditional and new setup is immense, and incumbents find it hard to navigate the transition. What is important is to anchor this thinking at the top. Having one person monitor every aspect of design thinking could work in the transition phase. It need not be a stand-alone position-one of the existing managers, such as the Chief Marketing Officer, can take on the role. The key is to have a senior leader oversee the end-to-end customer journey. ""Braiding"" business and IT . Traditionally, the business has led IT. Sales and product managers defined their requirements, and IT (and operations) executed. Today's environment has propelled Chief Operating and Chief Technology Officers to a more critical role. Since they are typically the ones who get firsthand exposure to what fintechs and digital providers are experimenting with, they have become the door openers to new possibilities. Operational and IT leaders are often tempted to take over the company's digital agenda, but this approach can easily backfire. Digital and customer transformations stall if leaders cannot show how they deliver value. IT needs the business leaders to link the investment to a clear customer need (and business case). On the other hand, business leaders trying to bypass IT out of frustration with its speed or capabilities doesn't work either. This creates isolated, greenfield solutions that are hard to integrate back into the IT landscape. Common examples are product navigators, insurance coverage calculators and advice tools that are not linked into a single vision or IT infrastructure. The pragmatic middle way is a ""braided"" approach where operations, IT, and business leaders work in close partnership to define and execute a customer-centric strategy. Hallmarks of this approach include an honest dialog about the starting position as well as real-time investment in defining a joint digital ambition. Turning design thinking into results Life insurers can adopt design thinking with an agile, sprint-based approach that allows for a full experience redesign in 8 to 12 weeks (for one product proposition). A design wave of this kind has three critical components: Immersion . As described earlier, customer immersion involves a rich set of interviews, shadowing, and desk observations of end customers and distribution partners. It is critical to engage a broad set of stakeholders in this phase: customer immersion should be mandatory for everyone who works on the design. It is also important to differentiate between traditional customer research and immersion, coach the team on the difference between both, and show how to uncover and document fresh insights. A sign of success is if the team members start quoting the customers they have spoken to and tackle service issues through the eyes of customer ""personas"" they have created. The outcome of the immersion is a detailed depiction of the customer journey that highlights pain points and links them to hard operational metrics, such as policy processing time, number of rejections without an alternative offer, or average number of interactions with clients per year. Ideation . Ideation starts with brainstorming around the target journey, devoid of any internal or external constraints. The goals are to resolve basic pain points and servicing issues as well as to develop ""signature moments"" that can drive customer loyalty and cross-selling. Examples could be emotionally charged moments, such as year-end thank-you cards to the policy beneficiaries to remind them that they are protected. Other features like a simplified 2-minute upfront risk check to provide immediate price guidance, or a real-time application status tracker could be very compelling as well. Surprise and delight elements, such as accelerated paths for simple applications, can also be powerful. The vision then needs to be translated into pragmatic actions, many of which will not be digital in nature, such as underwriting services or turnaround times. A systematic leakage analysis along the customer journey provides the basis for prioritizing across these initiatives. In a next step, the initiatives are translated into mock-ups of processes that allow visualization of the new customer experience. At the end of the ideation phase, the crude mock-ups become detailed prototypes. Iteration . Iterations are built on user tests and are ideally done in short, weekly sprints. The willingness to let go of ""pet"" features and ideas if they fail the customer test is a critical prerequisite in this phase. While the designs are being iterated, the technical implementation can begin in an agile way to deliver value to the customer (and road test the new design) in the shortest time possible. Importantly, iteration continues after the launch of the new proposition or service. As the organization moves towards a test-and-learn approach, it needs to use market feedback to spur the next iteration. Since today's consumers expect frequent updates and new features in nearly every product and service, the insurer's innovation work is never done. Transforming into an analytics-driven insurance carrier Read the article Hallmarks (and impact) of a revamped customer journey The end product of the redesign process is a new customer experience that can combine radical changes with surprise elements and simple operational improvements. It is an experience that our protagonist Susan can enjoy and even recommend to friends. What could this look like? Susan might be proactively contacted by an insurer with a short message that explains the value of buying a life insurance policy given her family situation, thus saving her extensive research. The outreach could include a link to tool that maps her needs (and demographics) in a quick, ""human"" and fun way-and delivers an instant offer with a buying rationale that feels personal to her (even if it is ""just"" a smart algorythm). When visiting the insurer's Web site, she might be able to select her agent based on shared interests or background (which is proven to build trust-and hence sales successs). If her policy is not issued instantly, she can track her application status online, and see the estimated day when her policy will be (e)mailed to her. When the policy arrives, it comes with an emotional gesture that ""rewards"" her for taking care of an important financial to do for her family. Examples could be a token gift or a branded card with the product name and coverage amount that she can share with her beneficiary. Obscure jargon is banned from all communications. Once Susan becomes a customer, she gets quarterly account updates with just the two to three headline data she cares about. What is more, she can adapt the policy on a self-service site without having to download an app, and she can choose to buy additional products without having to reenter all her data. These elements are just examples and focused purely on the end customer. Our work with different insurance clients has shown that there is typically a portfolio of 40 to 60 experience elements along the journey that can be built in to create a delightful experience-not only for end customers, but also for agents and brokers. The financial impact is significant. Depending on the starting situation, an overhaul of the journey can generate a 50 to 100% increase in new premium. Life insurers in Europe and across the globe need a jolt to keep pace with a new breed of customers and attackers. We believe that design thinking can be a fresh approach that provides this, because it allows transformation that spans functional lines and changes the way people think and act. It is also a proven tool that has already delivered results in many other industries. Incumbent life insurers are now called upon to take action and make the much-needed change happen. About the author(s) Jochen Kühn and Ildiko Ring are partners in McKinsey's Zurich office, where Maximilian Straub is a consultant. Markus Berger-de Leon, digital partner, is head of McKinsey Digital Labs Studio Germany. The authors wish to thank Mark Bothorn for his contributions to this article.",en,75
169,768,1462319974,CONTENT SHARED,-6603351162010419903,-3500661007957156229,-6855475010330807605,,,,HTML,http://info.versionone.com/webinar-safe-4-value-streams-email.html?mkt_tok=eyJpIjoiTXpjM05HVXhNRGc0TkdOayIsInQiOiJGWk90angzRjl4V0VLXC9iaW9KQzcrMm8yaXk4TlwvSW1SdkFEWTdBeG9oT3pBVDZBNjVNaWV1Zlo5YzR5UFwvTGdtQVRqTTR5VTdUWE8rZExkb0U3YTN0ZE80c2VvM2E4XC9HZUdpRm9ZenRCN1k9In0%3D,versionone agilelive™ webinar series| safe® expert webinar,"Alex Yakyma SAFe Fellow & Principal Consultant Scaled Agile, Inc. Alex Yakyma is a methodologist, trainer, and consultant who has been applying lean and agile practices in the software industry for over a decade. Alex joined Dean Leffingwell back in 2006 and since then has been actively working on the Scaled Agile Framework and its numerous implementations in the field. Alex's broad prior experience as a program manager in highly distributed multi-cultural environments allows him to perfect his clients' operational capabilities at the program level, their cross-program coordination, and their portfolio strategy.",en,74
170,2315,1473684520,CONTENT SHARED,-8511291357261863413,6960073744377754728,-8019369593564010761,,,,HTML,http://ronjeffries.com/articles/016-09ff/defense/,dark scrum,"Let's talk about ""Dark Scrum"". Too often, at least in software, Scrum seems to oppress people. Too often, Scrum does not deliver as rapidly, as reliably, as steadily as it should. As a result, everyone suffers. Most often, the developers suffer more than anyone. The theme behind a lot of my thinking lately is this: Kent Beck, my original ""Agile"" mentor, once said that he invented Extreme Programming to make the world safe for programmers. It turns out that the world is not yet safe for programmers. Scrum can be very unsafe for programmers. To paraphrase Ken Schwaber, one of the co-creators of Scrum, in another context: ""That makes me sad"". Scrum, done even fairly well, is a good framework. I'm quite sincere about that. It's good to have a strong connection to the business deciding what needs to be done, and what needs to be deferred. It's good to have all the skills you need to build the product right on the team. It's good to build a tested tangible product every couple of weeks, and it's good to show it to stakeholders, and it's good to review how things went and how to improve them. I've studied, worked with, and thought about Scrum for years, and everything about it is really quite good. Not perfect, but really quite good. Unfortunately, that's Scrum as a concept, Scrum as an ideal, Scrum done as intended. Like every good idea, the reality sometimes falls short. Sometimes it falls far short. I call the result ""Dark Scrum"". Dark Scrum: Some Typical Abuses of Scrum Let's look at how things can go wrong in Dark Scrum, in just a few steps. In this section we observe a team experiencing Dark Scrum. The Dark Scrum leaders mean well: they're just doing it wrong. Self-organization is slow to happen. Clearly it takes some time to get good at Scrum. It has new roles and activities. Even more difficult, it asks that we take on new values. We're supposed to let our developers self-organize to do the work. It's easy to call the Scrum meetings and call ourselves by Scrum roles. It's not at all easy to really walk the walk. Scrum generally starts with very few people trained, even fewer understanding it, and a lot of people who think they know what they're supposed to do. It should be no surprise if they do it wrong, and very often they do. Often, today's power holders believe that their job is to decide what their people should do, tell them to do it, and see that they do. Scrum teaches otherwise: explain what needs to be done, then stand back and let the people self-organize to do it. People can't just click over into Scrum's new mode of operation. It takes time to unlearn the old ways. It takes time to learn new habits, time to learn to trust the team. It takes time for the team to learn how to be trusted: in a way that's the underlying message of this article. Scrum's training begins this learning process for the people who get the training. Dark Scrum begins when people who know their old job, but not their new Scrum job, begin to do the Scrum activities. It goes like this: Awesome, we can help the team every day! Every day, the team is supposed to get together and organize the day's work. This practice, the ""Daily Scrum"", is imposed on the typical team. There might be one person in the room, the ScrumMaster, who has been told how it should be done. The programmers haven't been told. Quite often, even the Product Owner hasn't been told. Almost certainly other power holders haven't been told. But the power holder already knows his job. His job is to stay on top of what everyone is doing, make sure they're doing the right things, and redirect them if they're not. How convenient that there's a mandatory meeting where he can do that, every single day! The result: instead of the team rallying around their joint mission and sorting out a good approach for the day, someone else drags information of of them, processes it in their head, and then tells everyone what to do. Since nothing ever goes quite as we expected yesterday morning, this improper activity often comes with a lot of blame-casting and tension. Dark Scrum oppresses the team every day. Self-organization cannot emerge. We also have convenient frequent planning! Every Sprint, the Scrum Product Owner is supposed to meet with the team, and describe what's needed. The team then figures out what they can do to meet the needs, and begins to build it. Well, that's the theory. In practice, there may not even be a business-side Product Owner. Even if there is, odds are they are not trained in how to be a Product Owner. Well, the power holders may be new to Scrum, but they know a lot about how to handle this problem. So every two weeks they show up and tell these programmers what they have to build. Oh, those programmers will push back. They are lazy, and recalcitrant. But the power holders keep the pressure on, because that's how you manage people. So they tell the team what to do and they'd better do it. Of course, power holders have little or no idea how to program, and the programmers usually at least have some clue. They have no idea what the state of the code is, and the programmers usually do know. The programmers are supposed to decide how much work to take on in Scrum, but in Dark Scrum, power holders know better. They pile it on. They see that as their job: drive the team. The results never vary. The team sincerely tries to do what is demanded. They know there's no point in saying it's impossible, even though it is. They'll just get berated for being lazy, stupid, trouble-makers. So they do their best and it isn't good enough. At the end of the Sprint, results are not what was demanded. Magic has once again not happened. The developers have failed again. Fortunately we have just the chance to set them straight: the Sprint Review! We get to critique what's done - and what's not. Every Sprint, stakeholders look at what the team has accomplished and provides guidance on how to go forward. Great theory, rarely done in practice when organizations are not expert in Scrum. In practice the Dark Sprint Review begins by someone reminding everyone what the team ""promised"" to do. (That is, what was demanded right before the team said ""We'll try"". That's a promise, isn't it?) Then we look at the pitiful failure the team brings us. Guess what. The organization demanded more than could be done, and they didn't get it. The team tried, and in trying, they took every shortcut they could think of to get all those unreasonable requests finished. They skimped on testing. They skimped on design. They even worked when they were too tired to think straight. They fell short and what they did accomplish wasn't very good. The team failed. Again. Fortunately, Dark Scrum has power holders, Product Owners, and stakeholders for this effort. They make sure the programmers are made fully aware of how badly they've done. That will surely inspire everyone to do better next time. To help with that, we have the Sprint Retrospective! We get to tell them how to improve ... In the Sprint Retrospective, the team looks back at the preceding Sprint, and at prior history. They observe what has gone well and what has not gone well. They figure out how to improve their process for next time. In practice, Dark Scrum leaders help them with that. We remind the team that they fell short, despite how closely we were managing them. We make it clear to these developers that their failure - and it is a failure - was surely due to a combination of laziness and incompetence. To inspire the team to do better, we point out the consequences of not improving, which will almost certainly include the rolling of heads. That always gets their attention. Sometimes the team will offer suggestions. Let me prepare you right now for that. Here are some things they might say, and how you might answer them: Developers : We need to do more testing to reduce the bug count. Power Holder : No. You're already falling behind on features. You need to code smarter. Stop putting in bugs and you won't have to remove them. We need features, not tests! Developers : The design is getting crufty. We need to refactor to improve it. Power Holder : No. Why did you write a crappy design in the first place? No one told you to write a crappy design. Stop doing that and fix what's wrong. There's a weekend coming up. Do it then! Developers : The requirements are unclear, they don't get clarified, and then what we build gets rejected at the last minute. Power Holder : We hired you to be smart and figure things out. You're supposed to self-organize to solve these problems. Quit sitting on your asses and build what we want! You get the idea. Keep these people's nose to the grindstone, and their feet to the fire. That has always been what it takes to get software done. Scrum doesn't change that. Wow, that was depressing. Well, of course, Scrum is actually trying to change all that. But until the hearts and minds of the organization actually change, there'll be too much of the old-style management happening ... and now it happens every couple of weeks ... often every single day! What can we developers do? The things that happen in Dark Scrum are abuses. They really are in opposition to what Scrum tries to teach us. No real Scrum expert would recommend any of these things. People who do these things are not ""doing it right"". Everyone agrees about that. Still, Dark Scrum does happen, and it happens far too often. In my view, abuses are almost inevitable until people really learn Scrum's principles and values. It might seem that there's nothing a development team can do but accept oppression. Fortunately, that's not the case. The good news is that there's something very powerful that we can do. The bad news is that it isn't easy. The other good news, though, is that it's mostly about programming and we're usually pretty good at that. Much of the pressure from Product Owner and/or other power holders comes because they can't see clearly what we're working on, what we're actually getting done. If we can make progress crystal clear, we can change how we're treated. If the product has a lot of known defects, our leadership will assume that there are even more, and they'll be afraid. In their fear, they'll put us under even more pressure, and since fear often manifests as anger, they'll be angry with us. Often, very angry. If we're working on design or infrastructure before features, features seem to come out slowly. This makes our leaders afraid that we'll be late or fail to deliver important features. Again, we are subjected to fear and anger. If we slow down because our design is falling behind, we produce fewer features. Fewer features, more fear, more anger, more pressure. When leadership cannot see working software, they become afraid about the future - and rightly so. They show that fear, invariably, in ways that are not helpful, and that are generally painful. Developers can stop that from happening. The only way I know is to deliver software. Real, live, working software. With visible features! The Power of the Increment Scrum asks us to deliver a tested, integrated, working, shippable increment of the product, at the end of every Sprint. We'll talk shortly about how to do that. For now, let's imagine that we can do it: we have the ability to deliver a working product increment every couple of weeks. Much of the fear will go away by itself, because leadership can see real results. Some will remain, however, because quite likely real results lag behind their hopes and dreams. Everyone hopes for a sports car, but sometimes you only get a bike. Maybe you wanted a pony, but you only get a dog, not even a cat, which a man could at least respect. (But I digress.) Still, with a solid increment in hand, tested, integrated, containing the most important features they've given us to do, we can change the conversation: Power Holders : You're not getting enough done: you have to do more. Developers : We're doing all we can, and we'll keep trying to improve. Meanwhile, all this works and is usable. It might be wise to use our real rate of delivery to predict how fast we'll go. It might be best to have us work on the most important backlog items first. It might be best to trim each item down to be as lean as possible. This won't just work magically right away. But it will begin to help, and the more true and real our product increment is, the more it will influence people to look at reality and base their management on it. Contrary to what we often think, our leaders are not stupid. They're doing the best they can with the information they have. If we can give them better information, in the form of working software, they'll begin to use that information. Using that information, they'll resort to less pressure and less abuse. Again, this isn't magic and it won't happen overnight. However, I've seen it happen time and again. Other than in the most dysfunctional organization on earth - and only a few of you readers actually work there - it will work, if we keep at it. There's a catch. There's always a catch. The catch here is that in order to change the conversation, in order to reduce pressure and abuse, the increment must actually work. It really has to be tested, integrated, ready to ship, containing all the backlog items we've undertaken to do. That isn't easy. Worse yet, we didn't learn in Computer Science how to do it. We didn't learn in Software Engineering how to do it. We can't learn how to do it at IT Tech Institutes, and we won't find it in Your Favorite Language for Dummies . There are practices that are specially formed to allow teams to build software incrementally, keeping it well tested, well designed, focused on features, and ready to go at all times. If we're going to change Dark Scrum into Scrum as She Should Be Played , we need to learn these practices. Fortunately, they're not that hard to pick up, although like all programming, they do require practice and hard work if we want to be good at them. Later in this series we'll look at them in more detail, but let's look at some of them briefly here: Acceptance Testing It is quite common for there to be disputes, at the end of a Sprint, about whether the team did or did not build what the Product Owner ""wanted"". These disputes are wasteful and they tend to drive Product Owner and developers apart. We want them working closely together, not fighting with each other. One very strong practice is for the PO and the developers to agree on concrete, executable acceptance tests for each story. It can be productive to think of these as ""examples"". Team: OK, we want the system to show all the organizations' accounts that are 30 days in arrears, and a total of the arrearage. Here's a table of accounts, and a table showing what the identified accounts would look like, with the sum. PO: No, that's not quite right. We don't want to show them at thirty, only beyond thirty. Each little example becomes an acceptance test for the story or backlog item in question. By convention, if the tests don't run, the team has not finished that story. Conversely, if the tests do run, the Product Owner and team agree that we have done what was asked. It's possible, of course, that the Product Owner won't like what she sees. But now the conversation has changed. She can't legitimately say ""That's not what I wanted"". It is what she wanted when we agreed. She has learned something and now wants something different. That's just fine, it's the PO's job to find her way to the best possible product. Working from concrete acceptance tests helps us understand what's being asked for, helps us know when we've accomplished it, and helps us know when we're refining our understanding and asking for something to be changed. As an important bonus, when we automate these examples, we can run all of them and show with certainty that the product is still doing what we agreed it should do. Incremental Design In a Scrum project, we're supposed to deliver a tested, working, potentially shippable Product Increment after each Sprint. Obviously, we can't have the whole design figured out at the beginning: we only know roughly what the product will even be. Equally obviously, we need to wind up with a good design at the end. Therefore, the design of our software must be created incrementally, a bit at a time. That's OK: when the product is small and incomplete, it doesn't need much of a design. As it grows, it needs a larger, better, more robust design. We just need to grow the design as we grow the product. ""Just"" . The most dangerous word in software development. Building up the design as we go is difficult. It requires skill in design, skill in testing, and skill in refactoring. We'll explore those here below, and in other articles in this series. Over time, we'll point you to some resources on the subject. But the fundamental truth remains: if we are going to deliver a Product Increment every Sprint, we must find a way to do good design, incrementally. Refactoring The best way we know today to do incremental design is called ""Refactoring"". Refactoring is the process of improving the design of existing code - without breaking it ! Refactoring isn't rewriting. Certainly if we have a program with a bad design, we can write a new program, using a better design. Probably I should say here ""just write"", because that trick rarely works. But in principle we could. It would take a long time, however, and in Scrum we don't have a long time: we need a better design by the end of the current Sprint. Refactoring is a process of transforming the code, usually in very small steps, to make it better. Modern IDEs help us with this: most of them offer a number of automated refactorings. They can extract a commonly-used bit of code and turn it into a method or function. They can move code from one class to another. They can rename things, or change the signature of a method. Using refactoring well, we can incrementally improve our design as we go. The operative word is "" well "", however. We do have to build up that skill. Incremental design improvement is necessary. Refactoring is how we do it. Programmer Testing We need programmer tests in addition to acceptance tests. Every feature we build is composed of many programming steps. In any of of those steps, we could make a mistake. We avoid these mistakes using programmer testing. One very good form of programmer testing is Test-Driven Development, which goes like this: Think of some small next step on the way to our feature. Write a test that should run when that step is complete. Run it: make sure it doesn't work now. Build the code to make the step complete. Run the test: make sure it works now. Check the code to see if it looks clear enough. If not, refactor it. Run the test again to be sure everything still works. Repeat until the feature is done. This isn't the only way we can do things, but it's one of the best we know right now. One way or another, it's very helpful to have a lot of little tests supporting each larger acceptance test, because the little tests tell us which bit we got wrong, while the acceptance test just screams ""Something Broke!"" These tests also help us with larger refactorings. I suppose a perfect TDD programmer could do all her design improvement in the TDD cycle, but my experience is that sometimes I don't notice a design problem right away. When that happens, I may need to put in some extra refactoring effort the next time I'm working with that code. Just like when we develop a feature, our tests can help us be sure we haven't broken anything with our design change. Continuous Integration Scrum requires a running, thoroughly-tested, usable increment, containing the value of all the backlog items from previous Sprints: a complete, running, tested, operable program. Obviously, this requires that all the team's work must wind up integrated, tested, and working together by the end of the Sprint. Delayed integration uncovers problems, many of which require substantial debugging. The longer we wait, the worse it gets. Conversely, if we integrate after making small working changes, it becomes much easier. An automated build process, running the system's test suite, turns out to be an essential part of a Scrum team's development process. For most teams, the more frequently they build, the better things go. Building ""all the time"" seems to go best, which is why this practice is called Continuous Integration. Surviving Dark Scrum TL;DR There is a common, perhaps inevitable Dark Scrum stage in many Scrum installations; Dark Scrum situations follow a common pattern; The Dev Team can make things better for themselves, and can move Dark Scrum toward real Scrum, a better place to be. If you'd like to provide input, my email is open at ronjeffries at acm dot org .",en,74
171,1107,1464205754,CONTENT SHARED,-6980020268309524947,-1032019229384696495,3498647633704503701,,,,HTML,https://www.acquia.com/blog/cross-channel-user-experiences-drupal/16/05/2016/3294766,cross-channel user experiences with drupal,"Last year around this time, I wrote that The Big Reverse of Web would force a major re-architecture of the web to bring the right information, to the right person, at the right time, in the right context. I believe that conversational interfaces like Amazon Echo are further proof that the big reverse is happening. New user experience and distribution platforms only come along every 5-10 years, and when they do, they cause massive shifts in the web's underlying technology. The last big one was mobile, and the web industry adapted. Conversational interfaces could be the next user experience and distribution platform - just look at Amazon Echo (aka Alexa), Facebook's messenger or Microsoft's Conversation-as-a-Platform . Today, hardly anyone questions whether to build a mobile-optimized website. A decade from now, we might be saying the same thing about optimizing digital experiences for voice or chat commands. The convenience of a customer experience will be a critical key differentiator. As a result, no one will think twice about optimizing their websites for multiple interaction patterns, including conversational interfaces like voice and chat. Anyone will be able to deliver a continuous user experience across multiple channels, devices and interaction patterns. In some of these cross-channel experiences, users will never even look at a website. Conversational interfaces let users disintermediate the website by asking anything and getting instant, often personalized, results. To prototype this future, my team at Acquia built a fully functional demo based on Drupal 8 and recorded a video of it. In the demo video below , we show a sample supermarket chain called Gourmet Market. Gourmet Market wants their customers to not only shop online using their website, but also use Echo or push notifications to do business with them. We built an Alexa integration module to connect Alexa to the Gourmet Market site and to answer questions about sale items. For example, you can speak the command: ""Alexa, ask Gourmet Market what fruits are on sale today"". From there, Alexa would make a call to the Gourmet Market website, finding what is on sale in the specified category and pull only the needed information related to your ask. On the website's side, a store manager can tag certain items as ""on sale"", and Alexa's voice responses will automatically and instantly reflect those changes. The marketing manager needs no expertise in programming -- Alexa composes its response by talking to Drupal 8 using web service APIs. The demo video also shows how a site could deliver smart notifications. If you ask for an item that is not on sale, the Gourmet Market site can automatically notify you via text once the store manager tags it as ""On Sale"". From a technical point of view, we've had to teach Drupal how to respond to a voice command, otherwise known as a ""Skill"", coming into Alexa. Alexa Skills are fairly straightforward to create. First, you specify a list of ""Intents"", which are basically the commands you want users to run in a way very similar to Drupal's routes. From there, you specify a list of ""Utterances"", or sentences you want Echo to react to that map to the Intents. In the example of Gourmet Market above, the Intents would have a command called GetSaleItems . Once the command is executed, your Drupal site will receive a webhook callback on /alexa/callback with a payload of the command and any arguments. The Alexa module for Drupal 8 will validate that the request really came from Alexa, and fire a Drupal Event that allows any Drupal module to respond. It's exciting to think about how new user experiences and distribution platforms will change the way we build the web in the future . As I referenced in Drupalcon New Orleans keynote , the Drupal community needs to put some thought into how to design and build multichannel customer experiences. Voice assistance, chatbots or notifications are just one part of the greater equation. If you have any further thoughts on this topic, please share them in the comments.",en,74
172,2975,1484837692,CONTENT SHARED,3091351089612339864,3609194402293569455,-8817271067166899413,"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36",SP,BR,HTML,https://www.sitepoint.com/whats-the-best-programming-language-to-learn-in-2017/,what's the best programming language to learn in 2017?,"Many of you will reflect on your skill set and career choices as we embark on the new year. There are numerous sources of ""best language"" statistics, so that's where we'll begin ... Stack Overflow Developer Survey More than 56,000 developers in 173 countries completed the Stack Overflow Developer Survey during 2016. Here are the most-used technologies : The survey also asked what developers loved most : and what developers most dreaded: Perhaps more useful are the technologies developers are interested in learning: Stack Overflow Top Tech Stack Overflow also collated statistics for questions, answers and votes : PYPL Popularity The PYPL Popularity of Programming Languages Index uses data from Google Trends to determine how often language tutorials are searched online: TIOBE Index, January 2017 The TIOBE Programming Community Index rates languages using search engine results to provide a ranking percentage: The biggest riser during 2016 was Go , which leapt from nowhere to 2.3% (#13). Java fell 4.19%, but it remains almost double C's score. What Do Surveys Tell Us? Surprisingly little. Results are interesting but often contradictory, and data collection methods are limited: Search engine results can favor older, more problematic or more widespread languages. Few would expect VisualBasic to appear above JavaScript. Online surveys are limited to a specific audience. Stack Overflow is populated by reasonably knowledgeable developers who have encountered problems in popular languages and frameworks. Historical usage patterns do not necessarily indicate future trends. Node.js did not exist a decade ago. In the mid-1990s, Perl or C were the most viable options for server-side development. For example, all surveys rank Java higher than PHP. Java is often adopted for education and used to develop command line, desktop and native Android applications. Yet WordPress powers 27.3% of the web and is written in PHP. PHP is used on 82.4% of web servers compared to just 2.7% for Java. PHP was developed for the web and has a more widespread adoption on the platform. There's nothing wrong with Java but, if you want a web development career, PHP could serve better. Probably. Depending on where you live and work. And the industry you work in. And what you do. Surveys are flawed, so perhaps we can seek ... Other Developer Opinions I've been writing ""best language"" articles for several years and they attract numerous comments. Everyone has an opinion, and that's great. Yet everyone is wrong. No developer has experience in every language. Some will have good knowledge of several, but no one can offer an unbiased choice. Whatever language a developer chooses and uses daily becomes their preferred option. They will passionately defend that decision because, if they can't, they'd switch to something else. Other developers can offer lessons learned from their experiences. That is useful information, but you're unlikely to have identical aspirations. To flip this on its head, seek opinions from developers who've been forced to use a particular language or framework: the majority will hate that technology. Why trust someone else to make a decision for you? If we can't rely on surveys or the opinions of others, where does it lead? ... There's no ""Best Language"" If you learn to drive a car, that knowledge can be transferred to driving a bus, a truck or a tractor. Similarly, most computer languages implement input, output, variables, loops, conditions and functions. Learn the basics of any language and learning another becomes considerably easier. It's mostly a different syntax . You cannot choose the ""wrong"" language; all development knowledge is good knowledge. Perhaps picking COBOL for an iOS game isn't the best choice, but you'd quickly discover it was impractical and learn something about the language which was useful elsewhere. The hardest part of any learning process is making a start ... Are You Asking the Right Questions? Those with some programming experience know where they've been struggling. The gaps in their knowledge are more obvious: If you're spending too much time manually manipulating spreadsheet data, invest some effort in learning its macro language. If you've been developing a website and are unhappy with the layout, improving your CSS knowledge is an obvious next step. If you're developing a server application and need to store data, learning SQL or a NoSQL alternative is a logical option. Those asking ""what language should I learn?"" are probably new to the software industry. A comparably vague question would be ""what clothes should I wear?"" . No one can answer until they appreciate your age, gender, size, taste, preferences, country, local weather, customs, decency laws, where it will be worn, etc. It's impossible to suggest a language without knowing: whether you're genuinely interested programming what problems you want to solve what hardware and systems are available to you what time and learning opportunities you have, and all the variables associated with the factors above. No one wakes up and decides to embark on a professional development career without any programming experience. If you're genuinely interested in development, pick a small project, choose a language, dig out some tutorials and get going. A few places to start on SitePoint ... Then Keep Learning Despite stating that other developer opinions won't align with your situation, I will offer a morsel of advice to SitePoint's primary web development audience: If you're primarily a front-end developer, attempt back-end coding. Try PHP, Node.js, Ruby or whatever piques your interest, then add SQL to your skill set. If you're primarily a back-end developer, learn HTML, CSS and JavaScript. Browser APIs and data formats such as JSON are also beneficial. Frameworks don't count! Learn the basics of the language first. That knowledge will remain invaluable regardless of the ever-changing whims, opinions and tool sets used by the development community. You may not want to become a full-stack developer but, at the very least, it will help you appreciate the work of others and make a better contribution to your project. Best of luck. Stop procrastinating. Stop reading articles like this. Just start coding!",en,74
173,2888,1482157149,CONTENT SHARED,7065704533945771463,1895326251577378793,-5664675415604649356,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.98 Safari/537.36",SP,BR,HTML,http://idgnow.com.br/internet/2016/12/19/nova-regra-do-banco-central-pode-significar-o-fim-do-nubank-entenda/,nova regra do banco central pode significar o fim do nubank; entenda - idg now!,"Uma das principais fintechs do Brasil, a Nubank pode fechar as portas caso o Banco Central confirme nesta terça-feira, 20/12, uma nova regra que mudaria de forma drástica o prazo de pagamento das empresas de cartão de crédito para os comerciantes no país. A medida afetaria todas as companhias do segmento, mas seria pior justamente para as menores, que não possuem o mesmo capital para financiamento como bancos gigantes. Em entrevista para a Agência Estado, a cofundadora do Nubank, Cristina Junqueira, criticou duramente a ideia do governo federal de reduzir de 30 dias para 2 dias o prazo para esse pagamento. Atualmente, os lojistas brasileiros levam 30 dias para receber das emissoras de cartão de crédito, prazo bem maior do que nos EUA, onde esse período é de apenas dois dias. ""Atualmente, um cliente que usa o cartão pagará a fatura, em média, 26 dias depois. Assim, o Nubank, como emissor, receberá o dinheiro apenas após este prazo. Com o dinheiro, pagamos o adquirente (operador do cartão), que leva mais dois ou três dias para pagar o varejista. Isso dá o prazo de 30 dias"", afirma Cristina, que descreve a possível mudança como ""apocalíptica"" para a fintech que já emitiu mais de 1 milhão de cartões no Brasil desde 2014. Caso a mudança do prazo, cuja intenção de redução foi oficializada recentemente pelo presidente Michel Temer, seja realmente colocada em prática, o Nubank teria de pagar a operadora do cartão antes mesmo de o cliente pagar a fatura. Segundo Cristina, já foram feitas algumas simulações internas e nem mesmo um prazo de 15 dias seria suficiente para a continuidade do Nubank, que se destaca no mercado por não cobrar taxas e permitir o gerenciamento do uso do cartão por meio de um app para smartphones. ""Nós já fizemos algumas simulações. Com dois dias é apagar a luz e fechar a porta. Com 15 dias, a gente precisaria de quase R$ 1 bilhão de capital adicional do dia para a noite"", decreta.",pt,74
174,2867,1481666562,CONTENT SHARED,6569463984699537820,3609194402293569455,5472228981666337194,Mozilla/5.0 (Windows NT 10.0; WOW64; rv:50.0) Gecko/20100101 Firefox/50.0,SP,BR,HTML,https://googlediscovery.com/2016/12/13/google-anuncia-android-things-plataforma-para-internet-das-coisas/,"google anuncia android things, plataforma para a internet das coisas | google discovery","O Google anunciou hoje o Android Things , sua nova plataforma baseada no Android que vem sendo desenvolvida para trabalhar com dispositivos conectados à internet (aka ""Internet das Coisas""). Além de contar com APIs do Android e serviços do próprio Google, Android Things utiliza grande parte dos esforços do Projeto Brillo , e as ferramentas de desenvolvedor do Android, como o Android Studio, Android SDK, Google Play Services e serviços de computação em nuvem do Google. ""Agora qualquer desenvolvedor Android pode rapidamente construir um dispositivo inteligente usando APIs do Android e serviços do Google, enquanto permanece altamente seguro com as atualizações direto do Google"", publicou o buscador. A gigante de Mountain View também anunciou um update da plataforma Weave que permite aos dispositivos se conectarem à nuvem e interagir com serviços como o Google Assistant. ""Fabricantes de aparelhos como a Philips Hue e Samsung SmartThings já usam Weave, e vários outros como Belkin Wemo, LiFX, Honeywell, Wink, TP-Link e First Alert estão implementando a tecnologia"". publicou Wayne Piekarski , Developer Advocate.",pt,73
175,1593,1467291890,CONTENT SHARED,-2828628091730682303,7645894863578715801,-6062980417313435691,,,,HTML,http://martinfowler.com/articles/serverless.html,serverless architectures,"Mike Roberts Mike is an engineering leader living in New York City. While spending much of his time these days managing people and teams he also still manages to code, especially in Clojure, and has Opinions about software architecture. He is cautiously optimistic that Serverless architectures may be worth some of the hype they are current receiving. Serverless is a hot topic in the software architecture world. We're already seeing books , open source frameworks , plenty of vendor products , and even a conference dedicated to the subject. But what is Serverless and why is (or isn't) it worth considering? Through this evolving publication I hope to enlighten you a little on these questions. A couple of examples UI-driven applications Let's think about a traditional 3-tier client-oriented system with server-side logic. A good example is a typical ecommerce app (dare I say an online pet store?) Traditionally the architecture will look something like this, and let's say it's implemented in Java on the server side, with a HTML / Javascript component as the client: With this architecture the client can be relatively unintelligent, with much of the logic in the system - authentication, page navigation, searching, transactions - implemented by the server application. With a Serverless architecture this may end up looking more like this: This is a massively simplified view, but even with this there are a number of significant changes that have happened here. Please note this is not a recommendation of an architectural migration, I'm merely using this as a tool to expose some Serverless concepts! We've deleted the authentication logic in the original application and have replaced it with a third party BaaS service. Using another example of BaaS, we've allowed the client direct access to a subset of our database (for product listings), which itself is fully 3rd party hosted (e.g. AWS Dynamo.) We likely have a different security profile for the client accessing the database in this way from any server resources that may access the database. These previous two points imply a very important third - some logic that was in the Pet Store server is now within the client, e.g. keeping track of a user session, understanding the UX structure of the application (e.g. page navigation), reading from a database and translating that into a usable view, etc. The client is in fact well on its way to becoming a Single Page Application . Some UX related functionality we may want to keep in the server, e.g. if it's compute intensive or requires access to significant amounts of data. An example here is 'search'. For the search feature instead of having an always-running server we can implement a FaaS function that responds to http requests via an API Gateway (described later.) We can have both the client, and the server function, read from the same database for product data. Since the original server was implemented in Java, and AWS Lambda (our FaaS vendor of choice in this instance) supports functions implemented in Java, we can port the search code from the Pet Store server to the Pet Store Search function without a complete re-write. Finally we may replace our 'purchase' functionality with another FaaS function, choosing to keep it on the the server-side for security reasons, rather than re-implement it in the client. It too is fronted by API Gateway. Message-driven applications A different example is a backend data-processing service. Say you're writing a user-centric application that needs to quickly respond to UI requests, but secondarily you want to capture all the different types of activity that are occurring. Let's think about an online ad system - when a user clicks on an advertisement you want to very quickly redirect them to the target of the ad, but at the same time you need to collect the fact that the click has happened so that you can charge the advertiser. Traditionally, the architecture may look like this. The 'Ad Server' synchronously responds to the user - we don't care about that interaction for the sake of this example - but it also posts a message to a channel that can be asynchronously processed by a 'click processor' application that updates a database, e.g. to decrement the advertiser's budget. In the Serverless world this looks like: There's a much smaller difference to the architecture here compared to our first example. We've replaced a long lived consumer application with a FaaS function that runs within the event driven context the vendor provides us. Note that the vendor supplies both the Message Broker and the FaaS environment - the two systems are closely tied to each other. The FaaS environment may also process several clicks in parallel by instantiating multiple copies of the function code - depending on how we'd written the original process this may be a new concept we need to consider. Unpacking 'Function as a Service' We've mentioned the FaaS idea a lot already but it's time to dig into what it really means. To do this let's look at the opening description for Amazon's Lambda product. I've added some tokens to it, which I then expand upon. AWS Lambda lets you run code without provisioning or managing servers. (1) ... With Lambda, you can run code for virtually any type of application or backend service (2) - all with zero administration. Just upload your code and Lambda takes care of everything required to run (3) and scale (4) your code with high availability. You can set up your code to automatically trigger from other AWS services (5) or call it directly from any web or mobile app (6) . Fundamentally FaaS is about running back end code without managing your own server systems or your own server applications. That second clause - server applications - is a key difference when comparing with other modern architectural trends like containers and PaaS (Platform as a Service.) If we go back to our click processing example from earlier what FaaS does is replace the click processing server (possibly a physical machine, but definitely a specific application) with something that doesn't need a provisioned server, nor an application that is running all the time. FaaS offerings do not require coding to a specific framework or library. FaaS functions are regular applications when it comes to language and environment. For instance AWS Lambda functions can be implemented 'first class' in Javascript, Python and any JVM language (Java, Clojure, Scala, etc.). However your Lambda function can execute another process that is bundled with its deployment artifact, so you can actually use any language that can compile down to a Unix process (see Apex later on.) FaaS functions do have significant architectural restrictions, especially when it comes to state and execution duration, and we'll come to that soon. Let's consider our click processing example again - the only code that needs to change when moving to FaaS is the 'main method / startup' code, in that it is deleted, and likely the specific code that is the top-level message handler (the 'message listener interface' implementation), but this might only be a change in method signature. All of the rest of the code (e.g. the code that writes to the database) is no different in a FaaS world. Since we have no server applications to run deployment is very different to traditional systems - we upload the code to the FaaS provider and it does everything else. Right now that typically means uploading a new definition of the code (e.g. in a zip or JAR file), and then calling a proprietary API to initiate the update. Horizontal scaling is completely automatic, elastic, and managed by the provider. If your system needs to be processing 100 requests in parallel the provider will handle that without any extra configuration on your part. The 'compute containers' executing your functions are ephemeral with the FaaS provider provisioning and destroying them purely driven by runtime need. Let's return to our click processor. Say that we were having a good day and customers were clicking on 10 times as many ads as usual. Would our click processing application be able to handle this? For example did we code to be able to handle multiple messages at a time? Even if we did would one running instance of the application be enough to process the load? If we are able to run multiple processes is auto-scaling automatic or do we need to reconfigure that manually? With FaaS you need to write the function ahead of time to assume parallelism, but from that point on the FaaS provider automatically handles all scaling needs. Functions in FaaS are triggered by event types defined by the provider. With Amazon AWS such stimuli include S3 (file) updates, time (scheduled tasks) and messages added to a message bus (e.g. Kinesis ). Your function will typically have to provide parameters specific to the event source it is tied to. With the click processor we made an assumption that we were already using a FaaS-supported message broker. If not we would have needed to switch to one, and that would have required making changes to the message producer too. Most providers also allow functions to be triggered as a response to inbound http requests, typically in some kind of API gateway. (e.g. AWS API Gateway , Webtask ) . We used this in our Pet Store example for our 'search' and 'purchase' functions. State FaaS functions have significant restrictions when it comes to local (machine / instance bound) state. In short you should assume that for any given invocation of a function none of the in-process or host state that you create will be available to any subsequent invocation. This includes state in RAM and state you may write to local disk. In other words from a deployment-unit point of view FaaS functions are stateless . This has a huge impact on application architecture, albeit not a unique one - the 'Twelve-Factor App' concept has precisely the same restriction . Given this restriction what are alternatives? Typically it means that FaaS functions are either naturally stateless - i.e. they provide pure functional transformations of their input - or that they make use of a database, a cross-application cache (e.g. Redis), or network file store (e.g. S3) to store state across requests or for further input to handle a request. Execution Duration FaaS functions are typically limited in how long each invocation is allowed to run. At present AWS Lambda functions are not allowed to run for longer than 5 minutes and if they do they will be terminated. This means that certain classes of long lived task are not suited to FaaS functions without re-architecture, e.g. you may need to create several different coordinated FaaS functions where in a traditional environment you may have one long duration task performing both coordination and execution. Startup Latency At present how long it takes your FaaS function to respond to a request depends on a large number of factors, and may be anywhere from 10ms to 2 minutes. That sounds bad, but let's get a little more specific, using AWS Lambda as an example. If your function is implemented in Javascript or Python and isn't huge (i.e. less than a thousand lines of code) then the overhead of running it in should never be more than 10 - 100 ms. Bigger functions may occasionally see longer times. If your Lambda function is implemented on the JVM you may occasionally see long response times (e.g. > 10 seconds) while the JVM is spun up. However this only notably happens with either of the following scenarios: Your function processes events infrequently, on the order of longer than 10 minutes between invocations. You have very sudden spikes in traffic, for instance you typically process 10 requests per second but this ramps up to 100 requests per second in less than 10 seconds. The former of these may be avoided in certain situations by the ugly hack of pinging your function every 5 minutes to keep it alive. Are these issues a concern? It depends on the style and traffic shape of your application. My former team has an asynchronous message-processing Lambda app implemented in Java which processes hundreds of millions of messages / day, and they have no concerns with startup latencey. That said if you were writing a low-latency trading application you probably wouldn't want to use FaaS systems at this time, no matter the language you were using for implementation. Whether or not you think your app may have problems like this you should test with production-like load to see what performance you see. If your use case doesn't work now you may want to try again in a few months time since this is a major area of development by FaaS vendors. API Gateway One aspect of FaaS that we brushed upon earlier is an 'API Gateway'. An API Gateway is an HTTP server where routes / endpoints are defined in configuration and each route is associated with a FaaS function. When an API Gateway receives a request it finds the routing configuration matching the request and then calls the relevant FaaS function. Typically the API Gateway will allow mapping from http request parameters to inputs arguments for the FaaS function. The API Gateway transforms the result of the FaaS function call to an http response, and returns this to the original caller. Amazon Web Services have their own API Gateway and other vendors offer similar abilities. Beyond purely routing requests API Gateways may also perform authentication, input validation, response code mapping, etc. Your spidey-sense may be buzzing about whether this is actually such a good idea, if so hold that thought - we'll consider this further later. One use case for API Gateway + FaaS is for creating http-fronted microservices in a Serverless way with all the scaling, management and other benefits that come from FaaS functions. At present tooling for API gateways is achingly immature and so while defining applications with API gateways is possible it's most definitely not for the faint-hearted. Tooling The comment above about API Gateway tooling being immature actually applies, on the whole, to Serverless FaaS in general. There are exceptions however - one example is Auth0 Webtask which places significant priority on Developer UX in its tooling. Tomasz Janczuk gave a very good demonstration of this at the recent Serverless Conference. Debugging and monitoring are tricky in general in Serverless apps - we'll get into this further in subsequent installments of this article. Open Source One of the main benefits of Serverless FaaS applications is transparent production runtime provisioning, and so open source is not currently as relevant in this world as it is for, say, Docker and containers. In future we may see a popular FaaS / API Gateway platform implementation that will run 'on premise' or on a developer workstation. IBM's OpenWhisk is an example of such an implementation and it will be interesting to see whether this, or an alternative implementation, picks up adoption. Apart from runtime implementation though there are already open source tools and frameworks to help with definition, deployment and runtime assistance. For instance the Serverless Framework makes working with API Gateway + Lambda significantly easier than using the first principles provided by AWS. It's Javascript heavy but if you're writing JS API Gateway apps it's definitely worth a look. Another example is Apex - a project to 'Build, deploy, and manage AWS Lambda functions with ease.' One particularly interesting aspect of Apex is that it allows you to develop Lambda functions in languages other than those directly supported by Amazon, e.g. Go. What isn't Serverless? So far in this article I've defined 'Serverless' to mean the union of a couple of other ideas - 'Backend as a Service' and 'Functions as a Service'. I've also dug into the capabilities of the second of these. Before we start looking at the very important area of benefits and drawbacks I'd like to spend one more moment on definition, or at least defining what Serverless isn't. I've seen some people (including me in the recent past) get confused about these things and I think it's worth discussing them for clarity's sake. Comparison with PaaS Given that Serverless FaaS functions are very similar to 12-Factor applications, are they in fact just another form of 'Platform as a Service' (PaaS) like Heroku ? For a brief answer I refer to Adrian Cockcroft If your PaaS can efficiently start instances in 20ms that run for half a second, then call it serverless. -- Adrian Cockcroft In other words most PaaS applications are not geared towards bringing entire applications up and down for every request, whereas FaaS platforms do exactly this. OK, but so what, if I'm being a good 12-Factor App developer there's still no difference to how I code? That's true, but there is a big difference to how you operate your app. Since we're all good DevOps-savvy engineers we're thinking about operations as much as we are about development, right? The key operational difference between FaaS and PaaS is scaling . With most PaaS's you still need to think about scale, e.g. with Heroku how many Dynos you want to run. With a FaaS application this is completely transparent. Even if you setup your PaaS application to auto-scale you won't be doing this to the level of individual requests (unless you have a very specifically shaped traffic profile), and so a FaaS application is much more efficient when it comes to costs. Given this benenfit, why would you still use a PaaS? There are several reasons but tooling, and maturity of API gateways, are probably the biggest. Furthermore 12-Factor Apps implemented in a PaaS may use an in-app readonly cache for optimization, which isn't an option for FaaS functions. #NoOps Serverless doesn't mean 'No Ops'. It might mean 'No internal Sys Admin' depending on how far down the serverless rabbit hole you go. There are 2 important things to consider here. Firstly 'Ops' means a lot more than server administration. It also means at least monitoring, deployment, security, networking and often also means some amount of production debugging and system scaling. These problems all still exist with Serverless apps and you're still going to need a strategy to deal with them. In some ways Ops is harder in a Serverless world because a lot of this is so new. Second even the Sys Admin is still happening - you're just outsourcing it with Serverless. That's not necessarily a bad thing - we outsource a lot. But depending on what precisely you're trying to do this might be a good or a bad thing, and either way at some point the abstraction will likely leak and you'll need to know that human sys admins somewhere are supporting your application. Charity Majors gave a great talk on this subject at the recent Serverless Conference and I recommend checking it out once it's available online. Until then you can read her write-up here and here . Stored Procedures as a Service Another theme I've seen is that Serverless FaaS is 'Stored Procedures as a Service'. I think that's come from the fact that many examples of FaaS functions (including some I've used in this article) are small pieces of code that wrap access to a database. If that's all we could use FaaS for I think the name would be useful but because it is really just a subset of FaaS's capability then thinking of FaaS in such a way is an invalid constraint. That being said it is worth considering whether FaaS comes with some of the same problems of stored procedures, including the technical debt concern Camille mentions in the referenced tweet. There are many lessons that come from using stored procs that are worth reviewing in the context of FaaS and seeing whether they apply. Some of these are that stored procedures: Often require vendor-specific language, or at least vendor-specific frameworks / extensions to a language Are hard to test since they need to be executed in the context of a database Are tricky to version control / treat as a first class application Note that not all of these may apply to all implementations of stored procs, but they're certainly problems that I've come across in my time. Let's see if they might apply to FaaS: (1) is definitely not a concern for the FaaS implementations I've seen so far, so we can scrub that one off the list right away. For (2) since we're dealing with 'just code' unit testing is definitely just as easy as any other code. Integration testing is a different (and legitimate) question though which we'll discuss later. For (3), again since FaaS functions are 'just code' version control is OK. But as to application packaging there are no mature patterns on this yet. The Serverless framework which I mentioned earlier does provide its own form of this, and AWS announced at the recent Serverless Conference in May 2016 that they are working on something for packaging also ('Flourish'), but for now this is another legitimate concern. This is an evolving publication, and I shall be extending it over the coming days and weeks to cover more topics on serverless architecture including the benefits and drawbacks of this approach, and where we might see serverless evolve over the next year or two. To find out when we publish these installments, keep an eye on the site's RSS feed , my twitter feed , or Martin's twitter feed.",en,73
176,2212,1472434664,CONTENT SHARED,1060159005880386235,-8132559109129514792,-8668326016989888923,,,,HTML,http://blogs.wsj.com/cio/2015/05/26/internet-of-things-drives-outcome-based-business-models/,internet of things drives outcome-based business models,"By placing intelligent hardware where digital and physical worlds intersect, business leaders and CIOs can gain quantifiable, end-to-end insights into the outcomes their customers are trying to achieve, and use those insights to differentiate themselves competitively and to enter new markets, Guest Contributor Paul Daugherty writes.",en,73
177,2861,1481626644,CONTENT SHARED,-1878128207048892154,-2979537012405607453,-2564952476915635344,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.98 Safari/537.36",MG,BR,HTML,https://blog.codinghorror.com/the-ten-commandments-of-egoless-programming/,the ten commandments of egoless programming,"The Ten Commandments of Egoless Programming, as originally established in Jerry Weinberg's book The Psychology of Computer Programming : No matter how much ""karate"" you know, someone else will always know more. Such an individual can teach you some new moves if you ask. Seek and accept input from others, especially when you think it's not needed. Critique code instead of people - be kind to the coder, not to the code. As much as possible, make all of your comments positive and oriented to improving the code. Relate comments to local standards, program specs, increased performance, etc. The human principles of software are truly timeless; The Psychology of Computer Programming was written way back in 1971, a year after I was born!",en,73
178,1192,1464813325,CONTENT SHARED,-3161714324304758767,-1479311724257856983,676862310426775888,,,,HTML,http://news.mit.edu/2015/csail-deep-learning-algorithm-predicts-photo-memorability-near-human-levels-1215,"deep-learning algorithm predicts photos' memorability at ""near-human"" levels","Researchers from MIT's Computer Science and Artificial Intelligence Laboratory (CSAIL) have created an algorithm that can predict how memorable or forgettable an image is almost as accurately as humans - and they plan to turn it into an app that subtly tweaks photos to make them more memorable. For each photo, the ""MemNet"" algorithm - which you can try out online by uploading your own photos - also creates a heat map that identifies exactly which parts of the image are most memorable. ""Understanding memorability can help us make systems to capture the most important information, or, conversely, to store information that humans will most likely forget,"" says CSAIL graduate student Aditya Khosla, who was lead author on a related paper. ""It's like having an instant focus group that tells you how likely it is that someone will remember a visual message."" Team members picture a variety of potential applications, from improving the content of ads and social media posts, to developing more effective teaching resources, to creating your own personal ""health-assistant"" device to help you remember things. Part of the project the team has also published the world's largest image-memorability dataset, LaMem. With 60,000 images, each annotated with detailed metadata about qualities such as popularity and emotional impact, LaMem is the team's effort to spur further research on what they say has often been an under-studied topic in computer vision. The paper was co-written by CSAIL graduate student Akhil Raju, Professor Antonio Torralba, and principal research scientist Aude Oliva, who serves as senior investigator of the work. Khosla will present the paper in Chile this week at the International Conference on Computer Vision. How it works The team previously developed a similar algorithm for facial memorability . What's notable about the new one, besides the fact that it can now perform at near-human levels, is that it uses techniques from ""deep-learning,"" a field of artificial intelligence that use systems called ""neural networks"" to teach computers to sift through massive amounts of data to find patterns all on their own. Such techniques are what drive Apple's Siri, Google's auto-complete, and Facebook's photo-tagging, and what have spurred these tech giants to spend hundreds of millions of dollars on deep-learning startups. ""While deep-learning has propelled much progress in object recognition and scene understanding, predicting human memory has often been viewed as a higher-level cognitive process that computer scientists will never be able to tackle,"" Oliva says. ""Well, we can, and we did!"" Neural networks work to correlate data without any human guidance on what the underlying causes or correlations might be. They are organized in layers of processing units that each perform random computations on the data in succession. As the network receives more data, it readjusts to produce more accurate predictions. The team fed its algorithm tens of thousands of images from several different datasets, including LaMem and the scene-oriented SUN and Places (all of which were developed at CSAIL). The images had each received a ""memorability score"" based on the ability of human subjects to remember them in online experiments. The team then pitted its algorithm against human subjects by having the model predicting how memorable a group of people would find a new never-before-seen image. It performed 30 percent better than existing algorithms and was within a few percentage points of the average human performance. For each image, the algorithm produces a heat map showing which parts of the image are most memorable. By emphasizing different regions, they can potentially increase the image's memorability. ""CSAIL researchers have done such manipulations with faces, but I'm impressed that they have been able to extend it to generic images,"" says Alexei Efros , an associate professor of computer science at the University of California at Berkeley. ""While you can somewhat easily change the appearance of a face by, say, making it more 'smiley,' it is significantly harder to generalize about all image types."" Looking ahead The research also unexpectedly shed light on the nature of human memory. Khosla says he had wondered whether human subjects would remember everything if they were shown only the most memorable images. ""You might expect that people will acclimate and forget as many things as they did before, but our research suggests otherwise,"" he says. ""This means that we could potentially improve people's memory if we present them with memorable images."" The team next plans to try to update the system to be able to predict the memory of a specific person, as well as to better tailor it for individual ""expert industries"" such as retail clothing and logo design. ""This sort of research gives us a better understanding of the visual information that people pay attention to,"" Efros says. ""For marketers, movie-makers and other content creators, being able to model your mental state as you look at something is an exciting new direction to explore."" The work is supported by grants from the National Science Foundation, as well as the McGovern Institute Neurotechnology Program, the MIT Big Data Initiative at CSAIL, research awards from Google and Xerox, and a hardware donation from Nvidia.",en,71
179,2025,1470832760,CONTENT SHARED,-1573329182923097618,7774613525190730745,6403122829531428804,,,,HTML,http://searchsecurity.techtarget.com/feature/DevOps-security-requires-new-mindset-and-tools-for-visibility-automation,"devops security requires new mindset and tools for visibility, automation","In 2011, Intuit Inc. embarked on an effort to completely revamp its QuickBooks Online accounting service for small businesses with better integration of payroll, customer relationship management and third-party services. With its shift toward software as a service, Intuit decided to transform its desktop production model -- development, QA and IT operations -- so developers could quickly prototype incremental functionality to address customers' demands. As part of the effort, which culminated in the announcement of a completely new service in September 2013, the financial software company focused on responsive development and continuous rollout. In other words, they needed DevOps. Consumer awareness of massive data breaches started around the same time period. Online attackers scooped up at least 60 million email addresses belonging to Fortune 100 clients of marketing firm Epsilon Data Management in 2011. The same year, hackers caused disruptions in Sony's PlayStation Network for more than three weeks and made off with credentials and information for 77 million PSN and Qriocity accounts. And the colossal Target breach, in which attackers stole 110 million records, capped off 2013. With the introduction of CRM features and payroll information, it's little wonder, then, that Intuit put a major focus on DevOps security. There's no recipe book for integrating security into DevOps' continuous integration and continuous delivery (CI/CD) cycle, says Shannon Lietz, senior manager for cloud security engineering at Intuit. With 3,000 developers, Intuit faced enormous problems because the security and agile DevOps teams continued to lead separate existences. ""We realized that the DevOps teams were throwing [responsibility] over the wall to security, and we [security] had all the information; we knew all the attacks that were coming in, and the DevOps people...did not have the information to make the decisions,"" she says. ""And we said, 'Alright, we have to figure out how to fix this problem.' It was a head-banging experience."" Fast-tracking DevOps security Lietz is not alone. Other major firms that created cloud services, or moved their services to the cloud, dealt with similar issues. Video-streaming service Netflix and Etsy, a marketplace for crafters, are well-known proponents of DevOps security. Both companies also pioneered many of the efforts to secure Agile software development . ""There were a million lessons to be learned along the way and some pretty spectacular failures,"" says Zane Lackey, a former director of security engineering for Etsy, who left and founded Signal Sciences in 2014 with Etsy colleagues Andrew Peterson and Nick Galbreath. With the online craft marketplace pushing out 50 deploys in a day, Etsy faced several challenges in fast-tracking DevOps security testing and providing visibility to developers. ""How do we get coverage and visibility over the applications and APIs that we are trying to defend?"" Lackey says. ""And how can we couple that up with the ability to really block attacks against apps that can really scale and be used in production?"" Phoenix rises Developers have always pursued faster development methods. With the publication of the Manifesto for Agile Software Development in 2001, and the move from software to cloud services, the practice of developers working more closely with IT operations to automate more of their workflow has taken off. ""DevOps is different,"" says Rich Mogull, CEO of security consultancy Securosis. ""It is as much of a philosophy as an approach. DevOps is about extending the concepts of an assembly line to figure how we get code into the hands of customers."" The DevOps concept -- popularized in The Phoenix Project (IT Revolution Press, 2013), a novel authored by DevOps proponents Gene Kim, George Spafford and Kevin Behr -- has gone from niche to mainstream in the past decade. According to RightScale's ""2016 State of the Cloud Report : DevOps Trends,"" 74% of the 1,060 IT professionals surveyed worldwide indicated their organizations are adopting DevOps for software development, up from 66% in 2015. However, it is not companywide -- 60% of DevOps adoption is by specific business units or project teams. The fundamental problem? While agile DevOps pushes small batches of code out in days -- and in some cases hours -- security teams work on projects that take weeks and months. To make DevOps more secure, security teams need to have ongoing collaboration with DevOps teams, providing coders with visibility and feedback and automating security checks as early in the development process as possible. Can security be automated into a set of actions like DevOps? Whatever you call it -- DevOps security, DevOpsSec or Rugged DevOps , which relies on peer-pressure tactics to make developers feel good or bad about their code -- here are four ways to tightly integrate security throughout the CI/CD cycle: 1. Don't be a gatekeeper When Lackey joined Etsy in 2011, the company already had a strong focus on the continuous deployment model, in which any code that passed the automated testing phase was released into production. The question was how to create a security organization that could really enable the company to quickly push out product features, but in a secure way. ""For security, that feels very scary at first because we have always operated as a gatekeeper,"" Lackey says. ""But the problem is that gatekeeper mentality means, in this modern world, becoming a blocker. Security cannot be the blocker that slows down the business because the developers will route around you."" Training security to not say ""no"" all the time is only part of the answer. Historically, security has focused on longer projects -- static analysis that required days to run, analyze and build a remediation list -- and that slow feedback loop blocks development in its own ways. ""In the old SDLC [secure development lifecycle] model, we built a lot of security controls that were very heavyweight and very top-down,"" Lackey says. ""And they required a lot of investment to tune and work correctly; and in the end, we would get a report once a month or once a week."" Getting beyond that requires a commitment from the security team to work on DevOps security with developers. Etsy, for example, reused the antivirus toolkit in its continuous integration environment to test source code. 2. Give the developer visibility Developers want to quickly push out code. When security teams return scan results that show flaws from code checked weeks ago, it requires developers to slow down or, often, to ignore the results. Instead, security teams should give developers quick visibility into the vulnerabilities in their code. Such visibility helps to tighten the feedback loop, says Joshua Corman, former CTO at Sonatype and co-founder of Rugged Software. (Corman is now the director of the Cyber Statecraft Initiative for the Atlantic Council, which works on international conflict and cooperation.) Twitter, for example, created a tool called Brakeman to quickly scan submitted code for vulnerabilities and immediately notify the developers. A report, originally in email, lets the developers know when they checked in vulnerable code and how to fix it. ""They found that tightening the feedback loop helped the developers not hate security; it was integrated into their development process,"" Corman says. ""One of the big problems with application security is that there is a time lag between when a bug is inserted and when it is detected."" Providing quick feedback at code check-in is extremely important. Intuit, for example, uses a golden master for its Amazon Machine Image (AMI) infrastructure, and whenever a developer pushes code and creates a new image, their scanners return a letter grade for the security of the image. At Etsy, Lackey and his team built tools that would deliver security results directly to the developer's queue of defects. That way, security was not a separate process, but part of the development cycle (See ""The tools they use""). ""We can't just say that security is everyone's responsibility and then not give them visibility or tools for security,"" he says. ""The way in which you actually deliver on that as a security team is by bringing visibility to everyone."" 3. Automate security checks and run them all the time Developing as part of the DevOps CI/CD model means that security has to be as close to continuous as possible. Netflix, one of the pioneers of CI/CD development, created a collection of programs that would frequently test its systems to make sure they had no vulnerabilities, were configured correctly and complied with company policy. Each program was called a monkey , and the automation suite became Netflix's Simian Army . Using monkeys -- or automation in general -- a company can assess that accounts are configured correctly, that the most secure and up-to-date version of third-party software is used and that new code is detected, analyzed and vetted through an automated red team process, says Adrian Cockcroft, former cloud architect and director of web engineering at Netflix. ""You need tooling that will break a build if the developer is violating any of these policies,"" he says. ""Once you have built something that you think is good, then it's posted to the testing environment and is automatically checked."" While basing your infrastructure in the cloud is not mandatory, the service-oriented nature of the cloud -- with its APIs -- makes accessing the code and infrastructure much easier for security teams. A significant part of securing DevOps is using that infrastructure to allow programs to automatically check code and the infrastructure. The service-oriented architecture of the cloud means that applications tend to be less monolithic and, as a result, have smaller codebases. Programs with a million lines of code are not as common anymore. ""Because the size of the codebase has been considerably reduced, the number of bugs has been reduced as well,"" says Meera Subbarao, director of the secure development practice at Cigital. ""However, security has not changed. We still see the bugs in these programs with 200,000 lines of code."" Cigital in May released The Agile Security Manifesto , which suggests four principles to build security into the Agile process. BSIMM 6 -- the Building Security In Maturity Model co-authored by the company's CTO Gary McGraw, a former columnist for this magazine -- was released in October. 4. Don't punish mistakes or blame the developer When developers move quickly, mistakes are made. And often the security team is the group that finds the errors. Blaming developers results in negative feedback and worsens relations between developers and security teams, says Lackey. Moreover, developers are not the only ones who will be making mistakes. DevOps security is such a new discipline that most security teams are learning by doing, he adds. ""I certainly made plenty of mistakes along the way,"" Lackey says. ""There were not a lot of other people doing it at the time, but that is how we learned to experiment."" In the end, security groups need to remember that any strategy to secure the agile DevOps cycle that attempts to change how developers work or interferes with their goals will likely fail, says Securosis' Mogull. ""Security needs to learn the developers' language and understand their requirements,"" he says. ""If they do not ship on time, they are in trouble; and for them, that's what matters."" About the author: Robert Lemos is an award-winning technology journalist who has reported on computer security and cybercrime for 20 years. He currently writes for several publications focused on information security issues.",en,71
180,257,1460038639,CONTENT SHARED,374352050712569304,1895326251577378793,-3394884316493790889,,,,HTML,http://arquiteturadeinformacao.com/user-experience/o-tenue-equilibrio-da-equipe-que-desenha-produtos-digitais/,o tênue equilíbrio da equipe que desenha produtos digitais,"É sempre fascinante escutar os relatos com riqueza de detalhes que as pessoas contam sobre a concepção, desenvolvimento e lançamento de um novo produto ou serviço, seja em uma grande empresa ou pequena de amigos. Todavia, existe uma questão intrínseca na concepção de produtos, que é vital para o sucesso, mas que para algumas pessoas, passa despercebida quando a solução já está no mercado. Podemos chamar esta questão de ""equilíbrio dos elementos"" de uma equipe de produto. Hoje, muitas empresas de diferentes portes trabalham com equipes multidisciplinares e isso é cada vez mais comum em todos os nichos de mercado, principalmente porque diferentes profissionais com experiências distintas, apresentam maior probabilidade de alcançar o máximo de inovação em soluções para os projetos. Entretanto, alinhar uma equipe em prol de um objetivo universal e com responsabilidades claras é um desafio permanente pelo qual todos que trabalham com produtos ou serviços, seja digital ou físico, já passaram ou vão passar algum dia. O que torna crítico o entendimento dos papéis envolvidos nesse processo, suas armadilhas e as idiossincrasias de cada área. Partindo da ótica de equipes de produtos digitais, habitualmente temos três grupos chaves que trabalham em conjunto: negócios, design e desenvolvimento. Se você trabalha no ramo há algum tempo já deve ter visto variações dos termos, ao invés de negócios, product management ou ao invés de design, setor de criação e assim por diante. Esses três elementos formam o núcleo responsável por boa parte dos produtos que existem hoje no mercado de varejo digital, desde um website à uma aplicação multiplataforma. Alguns exemplos de nomes que as empresas costumam utilizar: Em toda equipe, obviamente, existe algum tipo de conflito seja ela multidisciplinar ou não, mas aqui exploraremos os mais comuns em cada um dos núcleos. Vamos notar que boa parte nasce do pouco entrosamento das equipes. Uma raiz comum de conflitos é falta de entendimento do que é ""trabalho em equipe"". Considerando a educação formal o ponto de partida para o trabalho que exercemos hoje, infelizmente boa parte de nós saiu de escolas e/ou faculdades fazendo trabalhos em grupo, sem o foco no que realmente importa, a colaboração e sintonia entre as pessoas, que é a essência do trabalho em equipe. Podemos dizer inclusive, que os trabalhos em equipe mais essenciais são executados por times esportivos ou integrantes de uma força militar, porque em ambos os exemplos, se o time não estiver em sintonia dificilmente ganhará a partida ou a batalha. Pode-se pensar também que a cultura de certas empresas espera que seu comportamento seja ""competitivo"" de acordo com as metas estabelecidas aos colaboradores, mas não se pode esquecer que a cultura é feita de pessoas e essas podem trazer o seu melhor, com o engajamento certo. Listo aqui, as principais situações que pude vivenciar e/ou presenciar em equipes de desenvolvimento de produtos. Esses exemplos apresentam nuances para cada time ou empresa. Infelizmente os desequilíbrios se repetem, contribuindo para a saturação dos três elementos principais e em alguns casos extremos podem até gerar a dissolução de equipes. Desequílibrio em Negócios Negócios ou Gerência é naturalmente a ponta com maior força, não só pelas atribuições de liderança ou planejamento do produto, mas porque é onde são armazenados e interpretados os números mais relevantes do produto para a companhia: vendas! Os números de vendas ou dados relevantes não são comunicados periodicamente - parece óbvio deixar a equipe na mesma sintonia, sempre respeitando o devido sigilo a cada tipo de produto, mas alguns tipos de gerenciamento de produto preferem manter a discrição dos dados, essa preferência é um equívoco, principalmente quando os demais membros da equipe só tomam conhecimento dos números em uma reunião de urgência. Esse tipo de ""informação estratégica"" revelada em cima da hora, pode ser motivo de muito estresse para a equipe. Não são definidos os principais índices de performance do produto (KPIs) - é interessante frisar esse fator, pois os índices devem existir em qualquer planejamento de produto, portanto se ele não existe é preciso rever como o projeto está sendo gerenciado, pois impactará o que se espera que a equipe entregue. O planejamento/roadmap do produto vem pronto - talvez esse seja o fator mais contraditório, nos atuais tempos de implantação em massa de . O ato de conceber ou planejar um produto sem ajuda da equipe que vai desenvolvê-lo é algo muito arriscado, pois a capacidade técnica e o tempo dos envolvidos influencia na qualidade e nos prazos de entrega das soluções prometidas. Desequílibrio em Desenvolvimento A área de Desenvolvimento já foi considerada o coração e a mente de empresas de tecnologia de renome. No entanto, a tecnologia é o elemento que tem mais interdependência dos demais elementos da equipe, pois é uma área que requer um direcionamento ou objetivo ligado às necessidades das pessoas e isso é algo que dificilmente nasce da própria tecnologia em si. O produto é projetado apenas com as tecnologias vigentes - de acordo com o desafio proposto pelo produto, é necessário testar novas soluções tecnológicas ou a mescla de linguagens de programação, mas algumas equipes técnicas dificultam esse processo, pois isso requer mais horas de estudos, mudança de infra-estrutura ou planejamento para implementação em larga escala e diversas correções de bugs. As interfaces são baseadas em máquina-máquina e não homem-máquina - algumas companhias, para agilizar seu processo produtivo, utilizam frameworks ou configurações padrões de software, porém algumas vezes esses ""comportamentos"" não atendem de forma assertiva os usuários daquele produto. Padrões agilizam o desenvolvimento, mas para se ter certeza de como as pessoas interagem não existe atalho, só testando com potenciais ou reais usuários do produto. A tecnologia dita o ritmo de todas as etapas do produto - é sabido que a tecnologia dita o ritmo hoje das inovações, mas nem sempre seus potenciais usuários ou clientes estão dispostos a absorvê-la com toda a sede que você gostariam de vender. Além disso, nem todos os processos do desenvolvimento do produto são quantitativos, passíveis de serem resolvidos em segundos por um algoritmo O fator humano deve ser considerado, mesmo que isso signifique que seus desenvolvedores devam ficar parados em uma sprint. Desequílibrio em Design Apesar da área de gerência/negócios ter noções da importância de prover a melhor experiência para seus clientes, existe um conflito de interesses na medida em que Negócio é cobrado pelo ""o que"" vai ser feito, enquanto Design é cobrado por ""como"" vai chegar ao cliente. Os papéis apresentam algumas sobreposições, porém as demandas e/ou expertises tem pontos de vista distintos sob o produto. As pesquisas não são estratégicas para o produto - algumas técnicas apresentam grande versatilidade de aplicação. Todavia é tentador para os profissionais da área escolherem técnicas por costume ou por confiança na execução, somando pouco ao produto final. Falta de foco no que causa maior impacto - o problema desse desequilíbrio é fazer com o que o produto possa apresentar apenas melhorias estéticas e sem impacto para as demais áreas envolvidas ou inovações. Importante também, correlacionar as metas do produto com as pesquisas ou técnicas aplicadas, para que os demais comecem a entender o valor da melhor experiência do usuário. O designer pensa as soluções sozinho - agindo de forma independente sem a colaboração ou compartilhamento de ideias com os demais elementos, a possibilidade de soluções inatingíveis ou apenas conceituais ocorrem com frequência, além disso podem minar a construção da visão de uma empresa centrada no usuário. Na tentativa de gerar inovação e uma real perspectiva dos clientes, algumas empresas como AirBnb tem colocado a gerência/negócios para cuidar do ponto de vista do cliente , o que talvez ocasione um conflito de interesses, já que esse setor está impregnado com a visão da melhor forma de vender mais, onde não raramente isso pode afetar a privacidade, paciência e a melhor experiência de uso para os clientes. Indo além do desequilíbrio Sabe-se também que viabilizar equipes de produto em equilíbrio é difícil e existem até convenções que promovem debates sobre isso. Portanto, uma grande equipe não é apenas medida pela velocidade de entrega das tarefas ou a qualidade do produto no mercado e sim a harmonia e colaboração dos elementos envolvidos, dirimindo os deslizes mais comuns e tornando os produtos significativos para as pessoas que o desenvolveram. O trabalho em equipe precisa ser discutido mais vezes pelos profissionais de produtos digitais, porque poucas pessoas que trabalham no desenvolvimento de produtos entendem que o equilíbrio desses três elementos é o que torna o produto/serviço algo significativo para os membros da equipe. Portanto, sem o engajamento que surge do ""pertencimento"" a algo que ""vale a pena"", com uma visão concreta de valor, há grandes chances de ocorrerem inúmeros conflitos ou desmotivação da equipe ao longo do processo. Contudo, o equilíbrio desses três elementos é almejado, mas poucas vezes é algo alcançado sem o empenho de todos envolvidos, pois cada uma das pontas tem um ponto de vista que precisa ser complementado com os outros elementos, talvez a empatia seja a melhor representação do centro dos três elementos. E qualquer um pode começar hoje essa mudança no seu local de trabalho, através de uma simples e direta comunicação, indo além dos problemas de transparência ou hierarquia em prol de um melhor dia-dia. Boa sorte na sua jornada! Se interessou pelo assunto?",pt,71
181,2850,1481455576,CONTENT SHARED,7088167897470452815,6013226412048763966,2517113148561143287,"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36",SP,BR,HTML,https://www.tonyrobbins.com/coaching/life-coach-vs-therapist/,"life coach vs. therapist, learn the difference | tony robbins","One of the most common misconceptions about life coaching is that it is therapy in disguise - or, worse yet, therapy from an unlicensed practitioner. In reality, life coaching is truly its own unique service designed to help ambitious achievers meet the outcomes that will bring them success and fulfillment. Here are some of the differences between life coaching and therapy, and a basic guide for when each service is appropriate. Defining terms Therapy, also called counseling or psychotherapy, is a long-term process in which a client works with a healthcare professional to diagnose and resolve problematic beliefs, behaviors, relationship issues, feelings and sometimes physical responses. The idea behind therapy is to focus on past traumas and issues to change self-destructive habits, repair and improve relationships and work through painful feelings. In this sense, therapy focuses on the past and on introspection and analysis. Life coaching is a process which may be long- or short-term. In life coaching, a client works with a coach who is not a healthcare professional in order to clarify goals and identify obstacles to success and problematic behaviors in order to create action plans to achieve desired results. It takes the client's current starting point as an acceptable neutral ground and is more action-based from that point onward. Similarities and differences between therapy and life coaching The fundamentals of life coaching are what distinguish it from therapy. Life coaches do not diagnose, while therapists determine illnesses and pathologies so they can be clinically treated. Therapists analyze their client's past as a tool for understanding present behaviors, whereas life coaches simply identify and describe current problematic behaviors so the client can work to modify them. In other words, therapists focus on ""why"" and coaches work on ""how."" Therapists help clients explore and understand their subconscious and unconscious mind. Their goal in this exploration is deep understanding. Life coaches focus on results and actions. Their goals can be measured with key performance indicators and specific behavioral outcomes and goals. Therapy and life coaching do share certain traits and aims, however. Both therapists and life coaches work to enable clients to make positive changes in their lives and become more productive. While therapists do diagnose and treat from a healthcare perspective, not all therapy clients are ill; many healthy people seek the services of both therapists and life coaches. Therapists may at times work with specific results in mind, such as the cessation of a particular problematic behavior. Despite occasional areas of overlap, however, the work and processes of therapists and life coaches are distinct. Want to learn more? Check out the Life Coach v. Therapy Infographic ! Should I seek out a therapist or a life coach? Naturally, the decision to seek out a therapist or a life coach is a very personal one. It might help to imagine yourself getting ready to climb a mountain. You could either hire an expert sherpa and guide for your expedition or a doctor. Which should you choose? If you are physically unwell, or would be in danger if you even attempted the climb, a sherpa and guide wouldn't do you any good. You need to be at a baseline level of good health before you can make the climb at all, so if you're not, you might need to see the doctor before trying something that challenging. However, if you're healthy and just need someone to help you with climbing strategy, carrying the load of supplies and finding the best path, the sherpa and guide is the best bet. The therapist is like the doctor in this example. He or she gets you well enough to take on major challenges in your life by exploring your mental and emotional well-being. The life coach is like the sherpa and guide. He or she has an expert knowledge of your climb and can help you reach the summit. A life coach would be able to offer guidance by: Clarifying and achieving personal and professional goals Creating business plans Working to improve communication skills Achieving financial independence and security Achieving a work/life balance Starting a new business or growing a current business A therapist, on the other hand, focuses their conversation on ways to: Recover from past traumas Explore why past relationships (business or personal) have been destructive Work through depression or anxiety that affect your ability to function at home or work Survive a divorce or loss of a loved one Although life coaches and therapists occasionally help clients with similar problems, their work is not the same. In order to get the right kind of professional expertise, it is crucial to know which kind of guidance will serve you best. Life coaching isn't simply a watered-down version of therapy. It is a dynamic discipline designed to help motivate and inspire people to achieve more than they believe is possible.",en,71
182,2414,1474647970,CONTENT SHARED,7226503561499839165,7645894863578715801,8514890134781099260,,,,HTML,http://techblog.netflix.com/2016/09/zuul-2-netflix-journey-to-asynchronous.html,"zuul 2 : the netflix journey to asynchronous, non-blocking systems","We recently made a major architectural change to Zuul , our cloud gateway. Did anyone even notice!? Probably not... Zuul 2 does the same thing that its predecessor did -- acting as the front door to Netflix's server infrastructure, handling traffic from all Netflix users around the world. It also routes requests, supports developers' testing and debugging, provides deep insight into our overall service health, protects Netflix from attacks, and channels traffic to other cloud regions when an AWS region is in trouble. The major architectural difference between Zuul 2 and the original is that Zuul 2 is running on an asynchronous and non-blocking framework, using Netty. After running in production for the last several months, the primary advantage (one that we expected when embarking on this work) is that it provides the capability for devices and web browsers to have persistent connections back to Netflix at Netflix scale. With more than 83 million members, each with multiple connected devices, this is a massive scale challenge. By having a persistent connection to our cloud infrastructure, we can enable lots of interesting product features and innovations, reduce overall device requests, improve device performance, and understand and debug the customer experience better. We also hoped the Zuul 2 would offer resiliency benefits and performance improvements, in terms of latencies, throughput, and costs. But as you will learn in this post, our aspirations have differed from the results. Differences Between Blocking vs. Non-Blocking Systems To understand why we built Zuul 2, you must first understand the architectural differences between asynchronous and non-blocking (""async"") systems vs. multithreaded, blocking (""blocking"") systems, both in theory and in practice. Zuul 1 was built on the Servlet framework. Such systems are blocking and multithreaded, which means they process requests by using one thread per connection. I/O operations are done by choosing a worker thread from a thread pool to execute the I/O, and the request thread is blocked until the worker thread completes. The worker thread notifies the request thread when its work is complete. This works well with modern multi-core AWS instances handling 100's of concurrent connections each. But when things go wrong, like backend latency increases or device retries due to errors, the count of active connections and threads increases. When this happens, nodes get into trouble and can go into a death spiral where backed up threads spike server loads and overwhelm the cluster. To offset these risks, we built in throttling mechanisms and libraries (e.g., Hystrix ) to help keep our blocking systems stable during these events. Multithreaded System Architecture Async systems operate differently, with generally one thread per CPU core handling all requests and responses. The lifecycle of the request and response is handled through events and callbacks. Because there is not a thread for each request, the cost of connections is cheap. This is the cost of a file descriptor, and the addition of a listener. Whereas the cost of a connection in the blocking model is a thread and with heavy memory and system overhead. There are some efficiency gains because data stays on the same CPU, making better use of CPU level caches and requiring fewer context switches. The fallout of backend latency and ""retry storms"" (customers and devices retrying requests when problems occur) is also less stressful on the system because connections and increased events in the queue are far less expensive than piling up threads. Asynchronous and Non-blocking System Architecture The advantages of async systems sound glorious, but the above benefits come at a cost to operations. Blocking systems are easy to grok and debug. A thread is always doing a single operation so the thread's stack is an accurate snapshot of the progress of a request or spawned task; and a thread dump can be read to follow a request spanning multiple threads by following locks. An exception thrown just pops up the stack. A ""catch-all"" exception handler can cleanup everything that isn't explicitly caught. Async, by contrast, is callback based and driven by an event loop. The event loop's stack trace is meaningless when trying to follow a request. It is difficult to follow a request as events and callbacks are processed, and the tools to help with debugging this are sorely lacking in this area. Edge cases, unhandled exceptions, and incorrectly handled state changes create dangling resources resulting in ByteBuf leaks, file descriptor leaks, lost responses, etc. These types of issues have proven to be quite difficult to debug because it is difficult to know which event wasn't handled properly or cleaned up appropriately. Building Non-Blocking Zuul Building Zuul 2 within Netflix's infrastructure was more challenging than expected. Many services within the Netflix ecosystem were built with an assumption of blocking. Netflix's core networking libraries are also built with blocking architectural assumptions; many libraries rely on thread local variables to build up and store context about a request. Thread local variables don't work in an async non-blocking world where multiple requests are processed on the same thread. Consequently, much of the complexity of building Zuul 2 was in teasing out dark corners where thread local variables were being used. Other challenges involved converting blocking networking logic into non-blocking networking code, and finding blocking code deep inside libraries, fixing resource leaks, and converting core infrastructure to run asynchronously. There is no one-size-fits-all strategy for converting blocking network logic to async; they must be individually analyzed and refactored. The same applies to core Netflix libraries, where some code was modified and some had to be forked and refactored to work with async. The open source project Reactive-Audit was helpful by instrumenting our servers to discover cases where code blocks and libraries were blocking. We took an interesting approach to building Zuul 2. Because blocking systems can run code asynchronously, we started by first changing our Zuul Filters and filter chaining code to run asynchronously. Zuul Filters contain the specific logic that we create to do our gateway functions (routing, logging, reverse proxying, ddos prevention, etc). We refactored core Zuul, the base Zuul Filter classes, and our Zuul Filters using RxJava to allow them to run asynchronously. We now have two types of filters that are used together: async used for I/O operations, and a sync filter that run logical operations that don't require I/O. Async Zuul Filters allowed us to execute the exact same filter logic in both a blocking system and a non-blocking system. This gave us the ability to work with one filter set so that we could develop gateway features for our partners while also developing the Netty-based architecture in a single codebase. With async Zuul Filters in place, building Zuul 2 was ""just"" a matter of making the rest of our Zuul infrastructure run asynchronously and non-blocking. The same Zuul Filters could just drop into both architectures. Results of Zuul 2 in Production Hypotheses varied greatly on benefits of async architecture with our gateway. Some thought we would see an order of magnitude increase in efficiency due to the reduction of context switching and more efficient use of CPU caches and others expected that we'd see no efficiency gain at all. Opinions also varied on the complexity of the change and development effort. So what did we gain by doing this architectural change? And was it worth it? This topic is hotly debated. The Cloud Gateway team pioneered the effort to create and test async-based services at Netflix. There was a lot of interest in understanding how microservices using async would operate at Netflix, and Zuul looked like an ideal service for seeing benefits. While we did not see a significant efficiency benefit in migrating to async and non-blocking, we did achieve the goals of connection scaling. Zuul does benefit by greatly decreasing the cost of network connections which will enable push and bi-directional communication to and from devices. These features will enable more real-time user experience innovations and will reduce overall cloud costs by replacing ""chatty"" device protocols today (which account for a significant portion of API traffic) with push notifications. There also is some resiliency advantage in handling retry storms and latency from origin systems better than the blocking model. We are continuing to improve on this area; however it should be noted that the resiliency advantages have not been straightforward or without effort and tuning. With the ability to drop Zuul's core business logic into either blocking or async architectures, we have an interesting apples-to-apples comparison of blocking to async. So how do two systems doing the exact same real work, although in very different ways, compare in terms of features, performance and resiliency? After running Zuul 2 in production for the last several months, our evaluation is that the more CPU-bound a system is, the less of an efficiency gain we see. We have several different Zuul clusters that front origin services like API, playback, website, and logging. Each origin service demands that different operations be handled by the corresponding Zuul cluster. The Zuul cluster that fronts our API service, for example, does the most on-box work of all our clusters, including metrics calculations, logging, and decrypting incoming payloads and compressing responses. We see no efficiency gain by swapping an async Zuul 2 for a blocking one for this cluster. From a capacity and CPU point of view they are essentially equivalent, which makes sense given how CPU-intensive the Zuul service fronting API is. They also tend to degrade at about the same throughput per node. The Zuul cluster that fronts our Logging services has a different performance profile. Zuul is generally receiving logging and analytics messages from devices and is write-heavy, so requests are large, but responses are small and not encrypted by Zuul. As a result, Zuul is doing much less work for this cluster. While still CPU-bound, we see about a 25% increase in throughput corresponding with a 25% reduction in CPU utilization by running Netty-based Zuul. We thus observed that the less work a system actually does, the more efficiency we gain from async. Overall, the value we get from this architectural change is high, with connection scaling being the primary benefit, but it does come at a cost. We have a system that is much more complex to debug, code, and test, and we are working within an ecosystem at Netflix that operates on an assumption of blocking systems. It is unlikely that the ecosystem will change anytime soon, so as we add and integrate more features to our gateway it is likely that we will need to continue to tease out thread local variables and other assumptions of blocking in client libraries and other supporting code. We will also need to rewrite blocking calls asynchronously. This is an engineering challenge unique to working with a well established platform and body of code that makes assumptions of blocking. Building and integrating Zuul 2 in a greenfield would have avoided some of these complexities, but we operate in an environment where these libraries and services are essential to the functionality of our gateway and operation within Netflix's ecosystem. We are in the process of releasing Zuul 2 as open source. Once it is released, we'd love to hear from you about your experiences with it and hope you will share your contributions! We plan on adding new features such as http/2 and websocket support to Zuul 2 so that the community can also benefit from these innovations. - The Cloud Gateway Team ( Mikey Cohen , Mike Smith , Susheel Aroskar , Arthur Gonigberg , Gayathri Varadarajan , and Sudheer Vinukonda )",en,71
183,1576,1467205950,CONTENT SHARED,3075564241645350154,-1032019229384696495,6023728345695403953,,,,HTML,https://techcrunch.com/2016/06/28/decentralizing-iot-networks-through-blockchain/,decentralizing iot networks through blockchain,"Imagine a washer that autonomously contacts suppliers and places orders when it's low on detergent, performs self-service and maintenance, downloads new washing programs from outside sources, schedules its cycles to take advantage of electricity prices and negotiates with peer devices to optimize its environment; a connected car, smart enough to find and choose the best deal for parts and services; a manufacturing plant where the machinery knows when to order repairs for some of its parts without the need of human intervention. All these scenarios - and many more - will be realized thanks to the Internet of Things (IoT). Already, many of the industries that historically didn't fit well with computers have been transformed by the billions of IoT devices connected to the internet; other industries will follow suit as billions more enter the fray . The possibilities are virtually countless , especially when the power of IoT is combined with that of other technologies, such as machine learning. But some major hurdles will surface as billions of smart devices will want to interact among themselves and with their owners. While these challenges cannot be met with the current models that are supporting IoT communications, tech firms and researchers are hoping to deal with them through blockchain , the technology that constitutes the backbone of the famous bitcoin. The problem with the centralized model Current IoT ecosystems rely on centralized, brokered communication models, otherwise known as the server/client paradigm. All devices are identified, authenticated and connected through cloud servers that sport huge processing and storage capacities. Connection between devices will have to exclusively go through the internet, even if they happen to be a few feet apart. While this model has connected generic computing devices for decades, and will continue to support small-scale IoT networks as we see them today, it will not be able to respond to the growing needs of the huge IoT ecosystems of tomorrow. Existing IoT solutions are expensive because of the high infrastructure and maintenance cost associated with centralized clouds, large server farms and networking equipment. The sheer amount of communications that will have to be handled when IoT devices grow to the tens of billions will increase those costs substantially. Even if the unprecedented economical and engineering challenges are overcome, cloud servers will remain a bottleneck and point of failure that can disrupt the entire network. This is especially important as more critical tasks such as human health and life will become dependent on IoT . There's no single platform that connects all devices. Moreover, the diversity of ownership between devices and their supporting cloud infrastructure makes machine-to-machine (M2M) communications difficult. There's no single platform that connects all devices and no guarantee that cloud services offered by different manufacturers are interoperable and compatible. Decentralizing IoT networks A decentralized approach to IoT networking would solve many of the questions above. Adopting a standardized peer-to-peer communication model to process the hundreds of billions of transactions between devices will significantly reduce the costs associated with installing and maintaining large centralized data centers and will distribute computation and storage needs across the billions of devices that form IoT networks. This will prevent failure in any single node in a network from bringing the entire network to a halting collapse. However, establishing peer-to-peer communications will present its own set of challenges, chief among them the issue of security. And as we all know, IoT security is much more than just about protecting sensitive data. The proposed solution will have to maintain privacy and security in huge IoT networks and offer some form of validation and consensus for transactions to prevent spoofing and theft. The blockchain approach Blockchain offers an elegant solution to the peer-to-peer communication platform problem. It is a technology that allows the creation of a distributed digital ledger of transactions that is shared among the nodes of a network instead of being stored on a central server. Participants are registered with blockchains to be able to record transactions. The technology uses cryptography to authenticate and identify participating nodes and allow them to securely add transactions to the ledger. Transactions are verified and confirmed by other nodes participating in the network, thus eliminating the need for a central authority. The ledger is tamper-proof and cannot be manipulated by malicious actors because it doesn't exist in any single location, and man-in-the-middle attacks cannot be staged because there is no single thread of communication that can be intercepted. Blockchain makes trustless, peer-to-peer messaging possible and has already proven its worth in the world of financial services through cryptocurrencies such as Bitcoin, providing guaranteed peer-to-peer payment services without the need for third-party brokers. Tech firms are now mulling over porting the usability of blockchain to the realm of IoT. The application of blockchain to IoT isn't without flaws and shortcomings. The concept can directly be ported to IoT networks to deal with the issue of scale, allowing billions of devices to share the same network without the need for additional resources. Blockchain also addresses the issue of conflict of authority between different vendors by providing a standard in which everyone has equal stakes and benefits. This helps unlock M2M communications that were practically impossible under previous models, and allows for the realization of totally new use cases. Concrete uses of blockchain in IoT The IoT and blockchain combination is already gaining momentum, and is being endorsed by both startups and tech giants. IBM and Samsung introduced their proof-of-concept system, ADEPT , which uses blockchain to support next-generation IoT ecosystems that will generate hundreds of billions of transactions per day. In one of the first papers to describe the use of blockchain in IoT , IBM's Paul Brody describes how new devices can be initially registered in a universal blockchain when assembled by the manufacturer, and later transferred to regional blockchains after being sold to dealers or customers, where they can autonomously interact with other devices that share the blockchain. The combination of IoT and blockchain is also creating the possibility of a circular economy and liquefying the capacity of assets, where resources can be shared and reused instead of purchased once and disposed after use. An IoT hackathon hosted by blockchain platform leader Ethereum put the concept of blockchain-powered IoT to test, in which some interesting ideas were presented , including in the domain of energy sharing and electricity and gas billing. Filament is another startup that is investing in IoT and blockchain with a focus on industrial applications such as agriculture, manufacturing and oil and gas. Filament uses wireless sensors, called Taps, to create low-power autonomous mesh networks for data collection and asset monitoring, without requiring a cloud or central network authority. The firm uses blockchain technology to identify and authenticate devices and also to charge for network and data services through bitcoin. Chain of Things is a consortium that is exploring the role of blockchain in dealing with scale and security issues in IoT. In a recent hackathon held in London, the group demonstrated the use of blockchain and IoT in a case study involving a solar energy stack designed to provide reliable and verifiable renewable data, speeding up incentive settlements and reducing opportunities for fraud. The system facilitates the process in which a solar panel connects to a data logger, tracks the amount of solar energy produced, securely delivers that data to a node and records it on a distributed ledger that is synced across a broader global network of nodes. Caveats and challenges The application of blockchain to IoT isn't without flaws and shortcomings, and there are a few hurdles that need to be overcome. For one thing, there's dispute among bitcoin developers over the architecture of the underlying blockchain technology, which has its roots in problems stemming from the growth of the network and the rise in the number of transactions. Some of these issues will inevitably apply to the extension of blockchain to IoT. These challenges have been acknowledged by tech firms , and several solutions, including side-chains, tree-chains and mini-blockchains, are being tested to fix the problem. Processing power and energy consumption is also a point of concern. Encryption and verification of blockchain transactions are computationally intensive operations and require considerable horsepower to carry out, which is lacking in many IoT devices. The same goes for storage, as ledgers start to grow in size and need to be redundantly stored in network nodes. And, as Machina Research analyst Jeremy Green explains , autonomous IoT networks powered by blockchain will pose challenges to the business models that manufacturers are seeking, which includes long-term subscription relationships with continuing revenue streams, and a big shift in business and economic models will be required. It's still too early to say whether blockchain will be the definite answer to the problems of the fast-evolving IoT industry. It's not yet perfect; nonetheless, it's a very promising combination for the future of IoT, where decentralized, autonomous networks will have a decisive role. Featured Image: Morrowind / Shutterstock",en,71
184,932,1463090835,CONTENT SHARED,2916072977192006313,-2626634673110551643,5975453253989656448,,,,HTML,https://bitsonblocks.net/2016/05/09/confused-by-blockchains-revolution-vs-evolution/,confused by blockchains? revolution vs evolution,"This article attempts to explain the difference between the revolutionary disruptive innovation of bitcoin and the evolutionary efficiency innovations of industry workflow tools , and why calling them both ""blockchains"", even as a generic term, is incredibly confusing. For the rest of this post, I will use the phrase ""industry workflow tools"" instead of industry blockchains, as some of the emerging solutions being proposed in this space are not blockchains (eg, R3's Corda is not a blockchain but DAH's solutions are - however, both companies are proposing industry workflow tools). Just as it's not helpful to call Twitter and Microsoft Sharepoint ""database companies"" although they both use variations of databases, it's not helpful to call cryptocurrencies, cryptocurrency companies, blockchain platforms and industry workflow tool companies ""blockchain companies"", although this frequently happens in the popular press. Why? Because you don't want to create misunderstandings like ""But I thought you could only use 140 characters in Sharepoint."". To be clear, both cryptocurrencies and industry workflow tools both have admirable objectives in their own ways for their own purposes, as do both Twitter and Sharepoint. Disruptive innovation: Public Cryptocurrencies The purpose of bitcoin , according to Satoshi Nakamoto's original whitepaper is to create "" A purely peer-to-peer version of electronic cash [which] would allow online payments to be sent directly from one party to another without going through a financial institution "". This is new and radically different to anything that has ever existed before. It is meant to enable: value to be held electronically without any third party being involved, and value to be transmitted without a specific third party being able to censor the transaction at will. The problem statement is: How do we use technology to create a financially inclusive system that anyone can participate in? The proposed solution is: Efficiency innovation: Industry Workflow Tools The purpose of successful incumbent institutions is to maintain and improve on their position by keeping customers happy, increasing revenues, reducing costs, becoming more efficient - ie to maintain a competitive, well-run business. The problem statement is: How do we use technology to improve our business and add shareholder value? The proposed solution is: Use industry workflow tools These are incredibly different, more or less polarised problem statements, requiring incredibly different and targeted solutions. Blockchains have somehow been caught in the middle. So why the conflation? Why are people confused about these entirely separate problems and solutions? Somehow in all the hype and PR, industry workflow tools still seem to have retained some of the connotations of bitcoin, when industry workflow tools and cryptocurrencies have almost the opposite ideology! Industry workflow tools and bitcoin are both separately exciting and innovative, but for very different reasons. How did the confusion happen? 2013 In 2013 The 'thing' was bitcoin. No one really talked about ""blockchain"" apart from talking about bitcoin's blockchain, ie the replicated ledger of all bitcoin transactions. It was called ""the blockchain"" because there was only one. (I am excluding the alt-coins like Litecoin, Dogecoin and other digital tokens as they only take up a tiny fraction of the mind-share of bitcoin). 2014 In 2014, because bitcoins were described as a new currency, the financial industry started to take notice. Bitcoins were pitched to the financial industry as an investment (buy bitcoins because the price will go up) and a trading asset (buy and sell bitcoins because you can make money) and a strategy (integrate bitcoin functionality because your customers will want it). The financial services industry as a whole wasn't interested in an anonymous, open, unregulated self-declared 'currency', backed by no government or central bank. They had no mandate to invest, and no framework to price it or understand it. Associations between bitcoins and scams, drugs and underground markets made the concept even less tasteful for traditional financial service industry participants. Even those who saw some potential were mostly put off by the small size and illiquidity of the market. 2015 In late 2014/15 the narrative moved away from bitcoins, towards blockchains and distributed ledgers (replicated ledgers without necessarily having chains of blocks). Institutions were saying ""We're not interested in bitcoin, but we are interested in the data-sharing technology behind it, the Blockchain"". In summary, "" Bitcoin bad, blockchain good "". The industry responded. In an attempt to garner interest, funding, customers, and higher company valuations, many bitcoin companies started rebranding as blockchain companies using more or less a text find-and-replace strategy (find the word 'bitcoin' and replace it with 'blockchain'). This was also to avoid the negative connotations of the word 'bitcoin', and to participate in the interest in the word 'blockchain'. I have heard the phrase ""we send it on/over/using blockchain"" or ""using blockchain technology"" as a deliberate tactic not to use the word bitcoin, and to hide what is going on. This misleads customers, investors, and regulators. This deliberate misdirection is still currently pursued by some companies who use bitcoins, who assert that they are blockchain-powered when really they mean that they transfer value by buying, transferring, and selling bitcoins. One of the arguments is that by using the word blockchain instead of bitcoin, they attract less regulatory scrutiny and have a higher chance of opening a bank account, needed for fiat deposits. This has hurt the industry by creating confusion, and in retrospect is incredibly short-sighted. This also hurt the companies who were genuinely using blockchain technology (not bitcoins) to attempt to solve other problems. I call this The Blockchain Blunder . Later, journalists, industry leaders, politicians, figureheads, consultants, bloggers and dinner party speakers started waving their hands and talking about ""blockchain"" (without ever specifying which one) being a solution for everything from disrupting banks to saving banks, from replacing 3rd parties to creating more efficient 3rd parties, and of course enabling financial inclusion. More pundits jumped on the bandwagon and regurgitated barely-researched content, creating the echo chamber of confusion. Chaos ensued. Some technology vendors pivoted from bitcoin (not well paid) to industry workflow tools (better paid) and tried to retrofit iterations of bitcoin's blockchain technology into perceived financial service problems, often without understanding the problems and the context of the financial service problems in the first place. Incumbents in turn needed a 'blockchain strategy' and started doing proof of concepts by taking well known, well understood problems with well known, well understood solutions, and attempting to apply blockchain solutions to them. This makes sense if the purpose is to get hands dirty and explore the technology, though it doesn't make sense from a pure IT architecture perspective. This is where we stand in Q2 2016. What has the effect been? Bitcoin: a way for people to pay each other across the world without interference from financial institutions. Industry workflow tools: mechanisms to share and update data between entities without a central point of control, creating efficiencies for incumbent industry participants. By referring to these both as blockchains, the connotations have got jumbled up. Here's how it should look: Some benefits, reality checks, and points to consider Now that we understand the difference between cryptocurrencies such as bitcoin, and industry workflow tools, let's explore where the benefits lie. Industry workflow tools will benefit incumbents A shared or distributed ledger/fabric/communication tool/chat-app is only useful to business if it makes businesses better, more efficient, more competitive. The promise of the proposed industry workflow technologies is to join up participants and create a strategic advantage for them. The benefits of participation are potentially both cost reduction (cheaper IT) and oligopoly-consolidation (let's maintain our advantage, together). It's a no-brainer for incumbent financial institutions to pursue what this technology can bring. Industry workflow tools may benefit regulators Regulators may want to insist on being able to 'plug in' to the workflow tools to get a better understanding of what is going on under their watch - something they have wanted to do for a long time. The transparency of asset ownership promised by industry workflow tools may have positive implications for systemic risk understanding and reduction. However regulators should watch out for creating a new systemic risk - is something being created which is globally too big to fail? Are regulators enabling a monopoly or oligopoly? Industry workflow tools may speed up asset settlement, but traders don't want real-time gross settlement. There is a difference between same-day settlement (T+0) and Real Time Gross Settlement. Same day settlement Same day settlement is beneficial to participants as it means that you get what you bought more quickly. Instead of waiting 3 days (T+3) to own your shares, the shares would be yours by the end of the same trading day (T+0). This frees up your pre-pledged collateral for use in another trade. The technology to reduce the time taken to settle equities from T+3 to T+0 (ie 3 days to same-day) has been available ever since we have known how to edit a row in a database and tell someone else about it quickly. The Kuwait and Saudi exchanges operate on T+0 according to a member of NASDAQ staff. It's not the technology that has prevented the change, It's the market structure, practices, regulation, and habits . The DTCC made this point clear in their blockchain whitepaper . So if it's T+0 settlement is not a technology problem, why will using newer technology help? Real Time Gross Settlement This is where trades are settled individually in as close to real time as possible, and there is no 'netting'. Gross settlement means when I buy some shares one minute and sell them the next, we settle both trades in full (we don't ""settle the difference""). However, netting is efficient. In real life you net wherever possible - for example if you buy your friend dinner, then later she buys you dinner, you wouldn't insist that you pay each other back the full amounts on the restaurant bills - no, instead you settle the difference, ie 'net' them off against each other. Market players don't want real time gross settlement . Being able to trade in and out of positions during the day and only settle up at the end of the day provides a lot of benefit: having to settle every trade would reduce the fun that can be had and money to be made. It would also increase the amount of collateral that would need to be posted. There is confusion between what bitcoin does (fairly real-time settlement of a BTC-denominated payment) and industry workflow tools being pitched as a solution to frictionless, real-time gross asset settlements (that traders don't want). Industry workflow tools may make innovation harder and more expensive Unless built extremely carefully, a system affecting multiple participants could be harder to upgrade and could ossify more quickly than a system run by a single entity. If you think it's hard to innovate a single banking app, just think how hard it will be to innovate one that touches multiple banks. Who would coordinate it? Who bears the costs? For analogy, the internet uses a couple of protocols called TCP/IP. There has been amazing stuff built on top. However, changing TCP/IP itself is incredibly hard, partly because there is so much software and firmware built on top of it. What about the 3rd parties who were going to be disintermediated? Today, 3rd parties set the rules and also enforce them, with some enjoying a quasi monopolistic status, and reaping the pricing benefits that that allows. With the industry utilities being built, the role of the 3rd parties may change a bit - they could become technology service centres and standards agencies. I don't think they will be disintermediated. Perhaps with these workflow tools, control and execution of the rules will lie with the participants (the banks running the nodes), and the 3rd parties will end up setting the rules and coordinating the upgrades: someone has to. Incumbent 3rd parties are shaping the narrative and working hard to make sure they remain relevant. The promise of disintermediation is driven by bitcoin & cryptocurrency side, not from industry workflow tools. Cryptocurrencies and Financial inclusion The promise of bitcoin and its bag of technological tricks gave us for the first time in the history of the world a way for two online people anywhere on the planet to send value electronically without needing to onboard or rely on specific third parties . For or better or for worse, this is truly a step towards financial inclusion . But here's the rub: ask a policymaker what they want, they'll say Financial Inclusion. Ask them what they don't want, and many will say bitcoin: the most financially inclusive tool we have ever seen . Bitcoin is the most financially inclusive technology that exists today. Industry workflow tools have no direct impact on financial inclusivity. So what are next steps? I am an incumbent, what should I do? Make best use of technology You should be using the best technology to increase revenues, reduce costs, increase efficiency, keep customers happy, and deliver profits to shareholders It's important to use technology to remain competitive Get involved with industry workflow tools, or you might miss out! Defend against the disruptive innovation of cryptocurrencies Aside from political lobbying, the only defence against disruption is to get down and play on the disruptor's terms. To compete with bitcoin you need to create an open, permissionless, censorship-resistant payment network that is better than bitcoin. That may be difficult to do while maintaining a banking licence Perhaps one long-term play might be investing in bitcoin firms (actual cryptocurrency firms, not 'blockchain solution providers') I am interested in disrupting the financial industry, what should I do? Keep working on the public blockchains, the unprofitable stuff, the stuff that people are uncomfortable talking about Keep making the open networks better, solving the hard problems, proving the skeptics wrong Don't lose focus: you are not trying to solve the incumbent industry's problems. Industries aren't disrupted by having better widgets built for them. Do you really need to dance with the devil? Don't spend your VC money too quickly pre-empting traction. It will probably take longer than you think! You are trying to create a new way of doing things - for better or for worse, and however it may end up Be careful what you enable. I am sitting on the sidelines, what should I do? Keep reading, talking, learning, and understanding. Challenge the hand-wavers if something doesn't sound or feel right Keep watching the evolution of this space, the future is exciting! What does the future hold? The disruptive innovators will keep working on bitcoin and variants, improving them, solving problems. Disruptive companies will come and go, the majority will fail. The incumbents will keep working to remain competitive, keep their customers happy, and deliver profits to shareholders. They will ignore bitcoin because it's not what they think their customers want, and the market is too small ! Then, when bitcoin or its progeny gets good enough, something will happen. Disruption is uncomfortable, it's dirty, it's subversive, it's ""Oh shit, we can't do anything about this"". There will be intense lobbying and a concerted effort to ban or make the technology illegal. The technology will win. This is the story of disruption , which has been told over and over again. The financial service providers in a cryptocurrency world will look very different to today.",en,70
185,2240,1472727922,CONTENT SHARED,7187598944032799890,-2979537012405607453,-4362351633011051065,,,,HTML,http://fossbytes.com/top-15-facebook-open-source-projects-you-must-know/,top 15 facebook open source projects you must know,"I am starting with Facebook as I am always impressed with all the project they have open sourced till date and how other companies including fossBytes uses some of these technologies. Facebook uses, maintains, and contributes to a significant number of major projects- in areas as diverse as native mobile tools, big data systems, client-side web libraries, backend runtimes and infrastructure, and through the Open Compute Project, server and storage hardware. Facebook's GitHub account alone, now has more than 90 repos comprising over 40,000 commits and that have collectively been forked 15,000 times. Facebook contribution to open source can be largely categorized into Mobile, Web, Back-end and Infrastructure. Top open source projects made in these categories are: Mobile: Buck is a high performance build system for Android that encourages creation of small, reusable modules consisting of code and resources. Because Android applications are predominantly written in Java, Buck also functions as a Java build system. Rebound is a Java library that models spring dynamics. Rebound spring models can be used to create animations that feel natural by introducing real world physics to your application. Rebound uses the same spring constants as making it easy to convert Origami interaction mockups directly into your Android application. Origami is a tool for designing modern user interfaces. Quickly put together a prototype, run it on your iPhone or iPad, iterate on it, and export code snippets your engineers can use. Stetho is an all new debugging platform for Android. It enables the powerful Chrome Developer Tools which is implemented using a client/server protocol which the Stetho software provides for your application. Once your application is integrated, simply navigate to chrome://inspect and click ""Inspect"" to get started! Infer Facebook is a static analysis tool to detect bugs in Android and iOS apps before they ship. If you give Infer some Objective-C, Java, or C code, it produces a list of potential bugs. Anyone can use Infer to intercept critical bugs before they have shipped to people's phones, and help prevent crashes or poor performance. Infer targets critical bugs such as null pointer exceptions, resource leaks and memory leaks. Web: React Js is a declarative, efficient, and flexible JavaScript library for building user interfaces. Lots of people use React as the V in MVC. Since React makes no assumptions about the rest of your technology stack, it's easy to try it out on a small feature in an existing project. HHVM (Hip Hop VM) is an open-source virtual machine designed for executing programs written in Hack and PHP. HHVM uses a just-in-time (JIT) compilation approach to achieve superior performance while maintaining the development flexibility that PHP provides. It has realized more than a 5x increase in throughput for Facebook compared with Zend PHP 5.2. HipHop is most commonly run as a standalone server, replacing both Apache and modphp, but it can also run standalone scripts from the command line. Flux is the application architecture that Facebook uses for building client-side web applications. It complements React's composable view components by utilizing a unidirectional data flow. It's more of a pattern rather than a formal framework, and you can start using Flux immediately without a lot of new code. Flow adds static typing to JavaScript to improve developer productivity and code quality. The goal of Flow is to find errors in JavaScript code with little programmer effort. Flow relies heavily on type inference to find type errors even when the program has not been annotated - it precisely tracks the types of variables as they flow through the program. fb-flo is a Chrome extension that lets you modify running apps without reloading. It's easy to integrate with your build system, dev environment, and can be used with your favorite editor. Jest is unit testing framework for JavaScript. It is built on top of the Jasmine test framework, using familiar expect(value).toBe(other) assertions. It automatically mocks CommonJS modules returned by require(), making most existing code testable. Nuclide is a suite of packages for to provide IDE-like functionality for a variety of programming languages and technologies. It is designed to provide a unified developer experience for engineers throughout the company - whether they work on native iOS apps, on React and React Native code, or on Hack to run on our HHVM web servers. Back-end: Presto is an open source distributed SQL query engine for running interactive analytic queries against data sources of all sizes ranging from gigabytes to petabytes. Facebook uses Presto for interactive queries against several internal data stores, including their 300PB data warehouse. Over 1,000 Facebook employees use Presto daily to run more than 30,000 queries that in total scan over a petabyte each per day. Osquery gives you a SQL interface to try out new queries and explore your operating system. With the power of a complete SQL language and dozens of useful tables built-in, osquery is an invaluable tool when performing incident response, diagnosing system operations problem, or troubleshooting a performance issue. Deploy a security tool that also enables developers and administrators. RocksDB builds on LevelDB to be scalable to run on servers with many CPU cores, to efficiently use fast storage, to support IO-bound, in-memory and write-once workloads, and to be flexible to allow for innovation. Not only softwares, but Facebook has also built various custom hardware competent to meet its ever increasing scale under Open Compute Project . Unlike Google and Microsoft, Facebook not only contributes its research work but also the end implementation to open source community. Let us know in comments your views towards Facebook's open source contribution. Check out our other articles on open source projects here .",en,70
186,1334,1465558374,CONTENT SHARED,3818189513627822856,-8020832670974472349,-6121624872835463728,,,,HTML,http://blog.cloudfour.com/autofill-what-web-devs-should-know-but-dont/,"autofill: what web devs should know, but don't","Autofill: What web devs should know, but don't Many people know that you can scan your credit credit in Mobile Safari. But how many web developers know how to create a form that supports that feature? Not many I'd bet. Photo source: Auto-Fill Credit Card Forms Using Your iPhone's Camera in iOS 8 . Used with permission. It doesn't help that Apple has provided zero documentation on how the scan credit card feature works. But there is another factor at play here. The scan credit card feature is a subset of browser functionality that web developers have long ignored: autofill. It's understandable why web developers haven't paid much attention to autofill. When you're filling out forms with test data on a regular basis, autofill tends to get in the way. But autofill is an important feature for our users. Google has found that "" users complete forms up to 30% faster "" when using autofill. So let's learn how autofill works, how to build forms that support cross browser autofill, and take advantage of new features like scanning credit cards. How does autofill work? Until recently, there were no standards when it came to autofill. Each browser implemented their autofill features differently and there was little documentation on how a browser determines what content a field expects. Despite this chaotic situation, browsers seem to have settled on two main approaches: 1. Pre-determined autofill fields Chrome, Opera and Safari have all taken the approach of identifying high-value form fields and providing a way to manage what the browser will autofill for those fields. For example, Opera provides autofill for addresses and credit cards. These can be managed in the preferences as shown above. Chrome, Opera and Safari all differ on which fields they provide autofill for, but the basic fields needed to complete a checkout process are well supported. Most users never have to see or edit these preferences in order to utilize autofill. The browser watches the person filling out forms and when it recognizes an address or a credit card, it will ask if the user wants them to save that information to reuse later. 2. Autofill any field If the previous approach is like a scalpel applied to preselected fields only, this second approach is a chainsaw cutting down every field in view. When a form is submitted, Microsoft Edge and Firefox will store the value submitted along with the value of the name attribute. If the browser sees a field in the future with a matching name attribute, it will provide autofill options. Firefox also appears to look at the id in addition to the name attribute Because there are security and privacy concerns with this approach, the autocomplete off value has long been supported to prevent the browser from storing and autofilling sensitive information. Which approach is better? While the second approach works for more fields, as a developer, I much prefer the pre-determined autofill fields approach. It makes it much easier to figure out what information the browser has to autofill. It is also easier to set up test profiles. Plus, with the second approach, you actually need to submit a form in order for the browser to store values to use with autofill. The browser won't remember your answers without the form submission. It also makes me nervous to think that the browser might store credit card information in a non-encrypted way if it can't clearly identify the type of field. Given how concerned Microsoft and Mozilla are about security and privacy, I'm certain they've put protections in place. But I personally feel more secure looking at an autofill preferences pane and seeing credit card information clearly separated and understood by the browser. All that said, I don't know what end users prefer. The second system works in more places, but I've seen quite a few support questions where people have been trying to remove their autofill options from the browser history. It will be interesting to see how Edge and Firefox change when they began to support the new autofill standard. One behavior to watch for Sometimes browsers require more than one field of a certain type before they will present you with autofill options. For example, in the following animated GIF, Safari won't offer to autofill the single cardholder field, but it will offer to autofill it once there is a second card number field. However, if the only field being collected is the card number, then Safari will offer the autofill option. My experience has been that creating isolated test cases for single fields can be challenging because of this behavior. At one point in my testing, I encountered Opera requiring three fields before it would autofill, but I can no longer recreate that behavior. This should never be an issue for a user so long as your form is written to support autofill (more on this soon), but I note it here in case you're attempting to troubleshoot autofill and encounter this behavior. The standards-based approach to autofill Thankfully, there is a way forward for autofill. HTML5 recently expanded the autocomplete attribute to provide hints to the browser about what content a field expects. The autocomplete attribute has been around for several years. It started with two values: on and off . By default, the autocomplete attribute is set to on which means that the browser is free to store values submitted and to autofill them in the form. However, some fields are poor candidates for autofill behavior. In that case, the autcomplete attribute can be set to off to tell the browser that this field should be not autofilled. Recently, additional values have been added as options for the autocomplete attribute. These new options are called autofill detail tokens . These tokens tell the browser what information the field is looking for. One type of token is called the autofill field names. The autofill field names tell the browser what type of information a field expects. For example, one of the autofill field names is organization . The HTML5 specification says that organization refers to: Company name corresponding to the person, address, or contact information in the other fields associated with this field If you wanted to tell the browser to autofill the organization field, your code would look something like this: The HTML5 specification has a great table listing all 53 possible autofill field names , what their intended use is, and what type of input must be used with it. That's autocomplete at its simplest, but it gets more powerful and a bit more complex. Shipping and billing The value of the autocomplete attribute is actually a space-separated list of tokens. So for example, if you wanted to collect shipping information, you would prepend the autocomplete value with the ""shipping"" token like so: The billing token works exactly the same way as shipping . Telephones, email and instant messaging Another token option applies to telephone, emails and instant messaging. For those services, there is an optional token to indicate if the autofill field name is referring to home , work , mobile , fax or pager . For example: Broad versus narrow autofill field names The specification provides for both broad and narrow autofill field names for many of the types of information. For example, in addition to the tel field which would be a single input containing a full telephone number, there are also: The specification authors encourage you to use the broad autofill field names as often as possible: Generally, authors are encouraged to use the broader fields rather than the narrower fields, as the narrower fields tend to expose Western biases. For example, while it is common in some Western cultures to have a given name and a family name, in that order (and thus often referred to as a first name and a surname), many cultures put the family name first and the given name second, and many others simply have one name (a mononym). Having a single field is therefore more flexible. I agree with this recommendation. And as a practical matter, it means that it is important to pay attention to the table of values and make sure you're using the right one for the field you're working with. Sections The final feature of the new autocomplete attribute tokens is the ability to declare an arbitrary section to group fields. A section is defined using a token that starts with section- . What comes after the dash is up to you. The specification provides this example of sections: All the tokens Put all together, we've now got a much more complex set of tokens for the autocomplete attribute. And the order of the tokens matters. First, you're either using on and off values OR you're using autofill field names. You can't use them at the same time. If you're using the autofill tokens, the order is: And keep in mind that [home|work|mobile|fax|pager] only applies for the telephone, email and chat autofill field names. The longest possible set autofill token might look something like this: Yay for standards! We're done, right? I'm afraid not. My hope is that eventually all of the browsers will support the expanded autocomplete standard, but that's not the current situation. I tested browsers on mobile and desktop to see what attributes the autofill appeared to honor. This is what I found: Thus far, only Chrome and Opera clearly support the new autocomplete features. Safari appears to have partial support, but because they don't have documentation, I can't tell if this is intentional or simply a regular expression match that happens to be looking at the autocomplete attribute as well as name and other attributes. Safari's strange behavior Since the release of iOS 8's credit card scanning feature, web developers have been reading tea leaves trying to divine what combination of markup Safari is looking for. Some suggest that you have to have specific values in the name field . Others found values in ids work . Even labels seem to matter : The Cardholder's name is quite a bit more tricky. We messed around with various IDs for a long time and almost gave up. We couldn't figure out an ID that would cause Card Scan to populate it. After much frustration we finally discovered that the TEXT of the label associated with the field matters. Setting the label text to ""Name on card"" was the magic trick. I've done quite a bit of testing, and I'm still not confident that I understand fully what Safari is doing. However, I've seen enough to be able to draw some basic conclusions: Contact and address fields support autocomplete Safari recognizes the form I created that only includes autocomplete attributes . The moment I start typing in the first field, it offers to fill in the form with my contact information. This all works as expected with a couple of caveats. First, it is unclear what Safari is using to make decisions about what information to autofill from my contact in the Mac's address book. My job title is filled in, but the company I work for isn't. Second, it doesn't give users the option to select which information they want to use. My contact contains both my home address and my work address. Safari only wants to autofill my home address. I'm out of luck if I want to ship something to the office. Payment fields are completely wonky When it comes to the payment fields, Safari's behavior changes completely. The autocomplete attribute is ignored. Instead, there is some sort of magical heuristic that Safari is using. And because I'm not an Apple magician, it has been difficult to discern what Safari is actually doing: In the video above, I edit the labels of two fields. Both have autocomplete set as well as name and id which should help Safari identify the field. And yet, Safari doesn't recognize the fields until the labels are changed to Name on Card and Credit Card Number . As mentioned earlier, Safari needs to see more than one field to trigger autofill. Then I try changing the label to CCNumber which continues to work. However, changing it to CC Number breaks the autofill. Basically, Safari has an unpublished list of terms that it is searching for. Fortunately, Jacques Caron was able to extract a list of strings from the iOS Simulator that Safari appears to be looking for: In my experiments, both: and: will trigger Safari's autofill and the scan credit card feature on iOS. However, putting the same values into the autocomplete attribute will not work. Building a cross browser autofill form Given what we've learned, is it truly possible to build a form that supports autofill across browsers? I think it is. Or at least, we can get very close by taking these four steps. 1. Add autocomplete attributes This is the future of autofill. Browsers that don't recognize the values will ignore them. This is progressive enhancement at its best. 2. Use common values for input name attributes To take advantage of autofill for Firefox and Edge users, you have to hope that the values you pick for the name attribute match what developers on other sites have used. One way to do this would be to survey popular sites and see what they use and then use the same values. Or perhaps use the same values as the autocomplete field in hopes that as more web developers become familiar with the standard that they will start using those names for their fields. Unfortunately, there is no way to guarantee that your Firefox and Edge users will have previously visited a form that uses the same name values as your form does. 3. Add name and/or label values that match the list Safari is looking for Using the list that Jacques Caron was able to extract, you can modify the values for the name attribute or the label to match what Safari expects. 4. Make autofill part of your test plans Lately, I've been asking audiences to raise their hands if autofill is part of their test plans. No one does. I've been working on the web since 1996, and I have yet to see autofill as part of the test plan. I suspect that autofill is a blindspot for web developers and designers. Therefore, it is critical we test our products to ensure that they work well with autofill. The final form Here is a sample form that supports autofill on Chrome, Safari, Opera, Firefox and Edge: See the Pen Cross Browser Autofill Form - Selected Fields by Jason Grigsby ( @grigs ) on CodePen . To see it in action, you'll need to view it on CodePen under https or the browser won't fill in the credit card information. I've also built a form with all 53 fields in the autocomplete specification . No browser currently supports all 53 fields. The future of autofill and forms There is a lot of movement from browser makers towards tackling the problem of web payments. The Payment Request API is being co-authored by Mozilla, Microsoft, Google and Facebook. Apple is participating in the Web Payments Working Group which is where the Payment Request API is being hashed out. So it appears that Apple is at least nominally onboard with the Payment Request API. Plus, rumor has it that Apple Pay will be available on mobile web for the holiday shopping season which makes it feel like this new momentum around web payments might be real this time. This renewed focus on streamling the web checkout process has me optimistic that support for autofill detail tokens will grow in the near term. And these tokens make creating forms that work with autofill so much simpler. And most importantly, supporting autofill will make filling out forms less tedious for our users and lead to increased sales for e-commerce sites.",en,70
187,1266,1465138844,CONTENT SHARED,-401664538366009049,-5868110530814399805,6837395310047141668,,,,HTML,http://www.adweek.com/news/technology/pinterests-real-world-pins-let-store-shoppers-save-real-items-virtual-boards-171815,pinterest's real-world pins let in-store shoppers save real items to virtual boards,"It's a real pin that looks like a virtual pin that looks like a real pin. In a bit of reverse engineered skeuomorphism, Pinterest has taken its virtual ""Pin"" button, which was always visually modeled after a physical pin, and actually made it a physical pin-for an intriguing campaign in Brazil that lets people pin items in stores and save them immediately to virtual inspiration boards on Pinterest itself. Agency DM9DDB created the technology and tested it in a campaign for Tok&Stok, Brazil's biggest design furniture store. The biggest challenge, of course, was how to get the pins to know who is pressing them, and thus to whose Pinterest boards the furniture items should be posted. To solve this, the pins use Bluetooth low energy (BLE) to connect with a ""PinList"" app that the shopper must download to his or her phone. This technology helps the pin locate the nearest person and connect with their app. It's explained in more detail in this video: ""The innovation is in the app's easy and intuitive use and all the technology backing it up, so that you don't have to leave the app open or pair your cellphone's Bluetooth with the physical button,"" says Igor Puga, vp for integration and innovation at DM9DDB. E-commerce has tried so hard to mimic the in-store experience; it's fun to see the reverse happening (even if these real-world Pinterest buttons are awfully big, and look kind of goofy when sitting on every item in the store). ""Pinterest has become a source of inspiration in the decor segment with 10 million ideas in Brazil every month. Tok&Stok has been so innovative in combining the process of discovery with saving what you find,"" says Mariana Sensini, managing partner of Pinterest in Brazil. CREDITS Client: Tok&Stok Title: PinList Agency: DM9DDB Chief Creative Officer: Aricio Fortes Executive Creative Director: Paulo Coelho Digital Interactive VP: Igor Puga Creative Director: Adriano Alarcon, Carlos Schleder e João Mostério Content Coordinator: Pedro Baptista Community Manager: Thiago Martinez Art Director: Daniel Lobo Designer: Daniel Matsumoto Illustrator: Big Studios - Rafael Nakahayashi and Rodrigo Alves. Project: Eduardo Martin, Fernando Tolusso, Rafael Gomes Account: Marcelo Passos, Claudia Almeida, Tania Pena, Beatriz Rodrigues, Thais Moura RTVC: Fabiano Beraldo, Juliana Henriques, Ana Lucia Marques Production: Clariana Regiani da Costa, Nereu Marinho Digital Production: Bizsys Approved by: Flavia Lucena",en,70
188,169,1459767022,CONTENT SHARED,-7660505434580831027,5746645399823844475,2679947461581233081,,,,HTML,http://www1.folha.uol.com.br/mercado/2016/04/1756923-empresa-britanica-adota-licenca-remunerada-no-periodo-menstrual.shtml,empresa britânica adota licença remunerada no período menstrual,"Divulgação/Coexist A diretora da empresa britânica Coexist, Bex Baxter (centro), e parte da sua equipe no escritório JULIANO MACHADO DE BERLIM Uma pequena empresa de Bristol (Reino Unido) chamada Coexist passa a adotar, a partir deste mês, uma licença-menstruação, pela qual funcionárias terão flexibilidade de ir para casa, se for necessário, e compensar depois as horas não trabalhadas -ou mesmo trabalhar de casa. ""Não fixaremos um número de dias remunerados ao mês porque não queremos associar isso a uma doença. As mulheres precisam de apoio para valorizar seu ciclo, e não se culpar por ele"", afirmou à Folha Bex Baxter, diretora da Coexist, que administra um espaço cultural na cidade. Baxter disse esperar que a iniciativa, considerada inédita no mercado britânico, chame a atenção de empresas maiores -a Coexist tem apenas 24 empregados, dos quais 15 são mulheres. ""Cada companhia tem de analisar o que é melhor, mas o importante é levantarmos a questão."" Ela mesma diz que sofria de dores terríveis, mas esse período diminuiu bastante desde que passou a flexibilizar sua jornada. ""Agora passo um dia trabalhando de casa em vez de três debilitada, sem produzir."" Um dos principais defensores da licenças, o ginecologista britânico Gedis Grudzinskas afirma que grandes corporações teriam até mais facilidade para aplicar a medida. ""Não existe sincronia menstrual, ou seja, ninguém ficaria sem todas as suas funcionárias por um período determinado."" A única grande empresa ocidental que se sabe que adota licença-menstruação remunerada é a Nike. A prática foi implementada em 2007 e se criou um memorando de entendimento para que seus parceiros comerciais também fizessem o mesmo em todos os países onde a empresa atua. preconceito O Brasil não possui legislação sobre a questão nem se sabe de companhias que adotem formalmente a prática por conta própria. Em janeiro, um boato circulou pela internet segundo o qual havia sido aprovada uma lei que permitia às mulheres três dias de descanso em casa por mês. O direito à licença-menstruação existe há décadas em vários países asiáticos, mas esbarra no medo que as mulheres têm de serem discriminadas caso o utilizem. O Japão foi o primeiro a introduzir a prática, em 1947, em meio à necessidade de atrair mão de obra feminina no combalido mercado pós-Segunda Guerra. A lei diz que cabe às empresas decidir o período da licença e se será remunerada. Na prática, porém, quase nenhuma mulher deixa de trabalhar por cólica menstrual. Na China, três províncias já adotam a licença -a última, Anhui, desde 1º de março. Nas redes sociais locais, a reação das mulheres foi de ceticismo sobre como isso será encarado por patrões, quase sempre homens. ""Parece que protege nossos direitos, mas no fim vai fazer as coisas piorarem"", escreveu @Woshiyamiedie.",pt,70
189,1394,1465953522,CONTENT SHARED,-3581194288660477595,-709287718034731589,8812735263251701985,,,,HTML,https://www.buzzfeed.com/katienotopoulos/the-end-of-apple-man,the end of apple man,"In the same way that Pinterest, with its made-from-scratch recipes and bespoke home decorating ideas, fills some people with a creeping sense of despair that they lead an inadequate life, Apple advertisements and keynote demos have always made me feel terrible about myself. The prototypical Apple demo person is someone I'll call Apple Man. Apple Man is a 40-something dad who just wants to FaceTime his adorable children while he's on a business trip, and also find a local pourover coffee shop while he's in town. Apple Man has an Apple Watch (obvious). He needs a way to manage his photos of his adorable children and hiking trips with friends. He loves jogging and mountain biking and wants to use his Apple Watch to monitor his workouts, because he LOVES working out. Apple Man is very fit for his age - you can just barely tell he's totally ripped through his light blue, off-the-rack, wrinkle-free button-down shirt. Apple Man has a great head of hair. Apple Man owns his home and wants to be able to open his garage door from his phone to park his family-sensible-yet-sporty-crossover. (He's on the Tesla Model 3 pre-order list.) He wants to make brunch plans, and it would be great if he could add a brunch plan to his calendar app directly from text messages. Apple Man wants to track his health, but of course he has no need for a period tracker. His calendar is full; his inbox is zero. If you're like me, somewhat disorganized and more likely to have a photo roll full of your drunk idiot friends than a white water rafting trip with your children, this feels not really relevant to your life. Or if you're basically anything OTHER than a rich white businessman who loves dim sum and jogging, then this might not feel relevant to you. When the Apple Watch came out it seemed like the ultimate hardware realization of Apple Man. A bulky fashion device with a huge price tag that seemed mostly useful for outdoor running or biking and getting alerted about business meetings. Indeed, some (frankly sketchy) sales analysis suggested that initial buyers of the watch were 80% male (the gender gap is closing, but still very real). But at yesterday's WWDC keynote, Apple announced new features for the Apple Watch that feel like they're designed with someone other than Apple Man in mind. The first one is the wheelchair activity monitor. While this of course, still appeals to people who love exercising (just like Apple Man), it's a feature launch devoted to people with disabilities, presented at a major event. The second feature is the emergency alert system. To me, this seemed so clearly designed for women - a safety alert system for walks home at night or through a deserted parking lot. Safety was one of the features women liked about the Apple Watch to begin with - like being able to call an Uber without taking their phone out of their purse. For women, safety while walking down the street is something we think about pretty much daily, most times we leave the house. Women have long adopted their own safety measures for walking in public: holding their keys a certain way to use as a weapon, carrying pepper spray, checking the backseat of a car before unlocking it, taking a longer route because the streets are brighter and more crowded. This isn't an afterthought or a minor convenience; it's a core user experience of being a woman or person vulnerable to violence. While certainly emergency calls are made by men and women, adding in an emergency alert feature to the wrist feels very obviously designed with women's safety in mind. The bonus feature that the Watch automatically knows what the country-specific version of 911 is if you're traveling addresses something women have been wary of for a long time: safety while traveling alone . Of course, plenty of the other demos for other products at this year's WWDC seemed to have Apple Man in mind - work productivity tools, messaging enhancements for wholesome activities like wishing your niece a happy graduation or planning to buy LCD Soundsystem tickets (Apple Man LOVES LCD Soundsystem). But the effervescent demo of Apple Music by the charismatic Bozoma Saint John - a black woman who looked and acted nothing like the typical Apple Men on stage before her and who in her opening remarks mentioned being a mother - felt like a breath of fresh air signaling that perhaps the winds are changing. There were other signals too. In the video segment cheering on developers using Apple's Swift programming language, the video ended with a black woman joyfully expounding how awesome coding was - certainly not the stereotype of a coder, and not totally reflective of the crowd there watching the video. At another Apple event in March, another female African American Apple executive, Lisa Jackson, took the stage to talk about Apple's environmental efforts. Breaking the Apple Man stereotype in the people who appear on stage as the Apple's evangelists is symbolic. Having a black woman present on stage might just mean the company is more aware of the optics of its events. But there is evidence that it's not just a hollow gesture - the actual features and hardware being announced on stage at Apple events are changing along with those presenters. When Health kit was announced last spring, the main complaint was that a period tracker wasn't included - for many women that is the only kind of health tracking they care about. By the next update that fall, they actually added the feature . The smaller iPhone SE seemed like an admission that not all men and women have giant Trump hands. People who use Apple products are sometimes young, they are women, they have disabilities, they don't work outside the home, they are single, they don't have kids, they are disorganized or just plain lazy; they don't exercise. It's nice to see features made for us . Apple Man isn't dead, not by a long shot. But maybe, hopefully, he's in retreat.",en,70
190,3092,1487154848,CONTENT SHARED,8729086959762650511,7645894863578715801,7395860765143290921,Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:51.0) Gecko/20100101 Firefox/51.0,SP,BR,HTML,https://martinfowler.com/articles/201701-event-driven.html,"what do you mean by ""event-driven""?","Towards the end of last year I attended a workshop with my colleagues in ThoughtWorks to discuss the nature of ""event-driven"" applications. Over the last few years we've been building lots of systems that make a lot of use of events, and they've been often praised, and often damned. Our North American office organized a summit, and ThoughtWorks senior developers from all over the world showed up to share ideas. The biggest outcome of the summit was recognizing that when people talk about ""events"", they actually mean some quite different things. So we spent a lot of time trying to tease out what some useful patterns might be. This note is a brief summary of the main ones we identified. Event Notification This happens when a system sends event messages to notify other systems of a change in its domain. A key element of event notification is that the source system doesn't really care much about the response. Often it doesn't expect any answer at all, or if there is a response that the source does care about, it's indirect. There would be a marked separation between the logic flow that sends the event and any logic flow that responds to some reaction to that event. Event notification is nice because it implies a low level of coupling, and is pretty simple to set up. It can become problematic, however, if there really is a logical flow that runs over various event notifications. The problem is that it can be hard to see such a flow as it's not explicit in any program text. Often the only way to figure out this flow is from monitoring a live system. This can make it hard to debug and modify such a flow. The danger is that it's very easy to make nicely decoupled systems with event notification, without realizing that you're losing sight of that larger-scale flow, and thus set yourself up for trouble in future years. The pattern is still very useful, but you have to be careful of the trap. A simple example of this trap is when an event is used as a passive-aggressive command. This happens when the source system expects the recipient to carry out an action, and ought to use a command message to show that intention, but styles the message as an event instead. An event need not carry much data on it, often just some id information and a link back to the sender that can be queried for more information. The receiver knows something has changed, may get some minimal information on the nature of the change, but then issues a request back to the sender to decide what to do next. Event-Carried State Transfer This pattern shows up when you want to update clients of a system in such a way that they don't need to contact the source system in order to do further work. A customer management system might fire off events whenever a customer changes their details (such as an address) with events that contain details of the data that changed. A recipient can then update it's own copy of customer data with the changes, so that it never needs to talk to the main customer system in order to do its work in the future. An obvious down-side of this pattern is that there's lots of data schlepped around and lots of copies. But that's less of a problem in an age of abundant storage. What we gain is greater resilience, since the recipient systems can function if the customer system is becomes unavailable. We reduce latency, as there's no remote call required to access customer information. We don't have to worry about load on the customer system to satisfy queries from all the consumer systems. But it does involve more complexity on the receiver, since it has to sort out maintaining all the state, when it's usually easier just to call the sender for more information when needed. Event-Sourcing The core idea of event sourcing is that whenever we make a change to the state of a system, we record that state change as an event, and we can confidently rebuild the system state by reprocessing the events at any time in the future. The event store becomes the principal source of truth, and the system state is purely derived from it. For programmers, the best example of this is a version-control system. The log of all the commits is the event store and the working copy of the source tree is the system state. Event-sourcing introduces a lot of issues, which I won't go into here, but I do want to highlight some common misconceptions. There's no need for event processing to be asynchronous, consider the case of updating a local git repository - that's entirely a synchronous operation, as is updating a centralized version-control system like subversion. Certainly having all these commits allows you to do all sorts of interesting behaviors, git is the great example, but the core commit is fundamentally a simple action. Another common mistake is to assume that everyone using an event-sourced system should understand and access the event log to determine useful data. But knowledge of the event log can be limited. I'm writing this in an editor that is ignorant of all the commits in my source tree, it just assumes there is a file on the disk. Much of the processing in an event-sourced system can be based on a useful working copy. Only elements that really need the information in the event log should have to manipulate it. We can have multiple working copies with different schema, if that helps; but usually there should be a clear separation between domain processing and deriving a working copy from the event log. When working with an event log, it is often useful to build snapshots of the working copy so that you don't have to process all the events from scratch every time you need a working copy. Indeed there is a duality here, we can look at the event log as either a list of changes, or as a list of states. We can derive one from the other. Version-control systems often mix snapshots and deltas in their event log in order to get the best performance. [1] Event-sourcing has many interesting benefits, which easily come to mind when thinking of the value of version-control systems. The event log provides a strong audit capability (accounting transactions are an event source for account balances). We can recreate historic states by replaying the event log up to a point. We can explore alternative histories by injecting hypothetical events when replaying. Event sourcing make it plausible to have non-durable working copies, such as a Memory Image . Event sourcing does have its problems. Replaying events becomes problematic when results depend on interactions with outside systems. We have to figure out how to deal with changes in the schema of events over time. Many people find the event processing adds a lot of complexity to an application (although I do wonder if that's more due to poor separation between components that derive a working copy and components that do the domain processing). CQRS Command Query Responsibility Segregation (CQRS ) is the notion of having separate data structures for reading and writing information. Strictly CQRS isn't really about events, since you can use CQRS without any events present in your design. But commonly people do combine CQRS with the earlier patterns here, hence their presence at the summit. The justification for CQRS is that in complex domains, a single model to handle both reads and writes gets too complicated, and we can simplify by separating the models. This is particularly appealing when you have a difference in access patterns, such as lots of reads and very few writes. But the gain for using CQRS has to be balanced against the additional complexity of having separate models. I find many of my colleagues are deeply wary of using CQRS, finding it often misused. Making sense of these patterns As a sort of software botanist, keen to collect samples, I find this a tricky terrain. The core problem is confusing the different patterns. On one project the capable and experienced project manager told me that event sourcing had been a disaster - any change took twice the work to update both the read and write models. Just in that phrase I can detect a potential confusion between event-sourcing and CQRS - so how can I figure out which was culprit? The tech lead on the project claimed the main problem was lots of asynchronous communications, certainly a known complexity-booster, but not one that's a necessary part of either event-sourcing or CQRS. Furthermore we have to beware that all these patterns are good in the right place and bad when put on the wrong terrain. But it's hard to figure out what the right terrain is when we conflate the patterns. I'd love to write some definitive treatise that sorts all this confusion out, and gives solid guidelines on how to do each pattern well, and when it should be used. Sadly I don't have the time to do it. I write this note in the hope it will be useful, but am quite aware that it falls well short of what is really needed.",en,70
191,2253,1472785321,CONTENT SHARED,-6523871595334455509,-534549863526737439,-6893819804532920307,,,,HTML,https://research.google.com/pubs/pub45542.html,"csp is dead, long live csp! on the insecurity of whitelists and the future of content security policy","Content Security Policy is a web platform mechanism designed to mitigate cross-site scripting (XSS), the top security vulnerability in modern web applications. In this paper, we take a closer look at the practical benefits of adopting CSP and identify significant flaws in real-world deployments that result in bypasses in 94.72% of all distinct policies.",en,69
192,2026,1470833640,CONTENT SHARED,1734266821019430183,2416280733544962613,-5892805645558108633,,,,HTML,http://www.economist.com/news/business/21703428-chinas-wechat-shows-way-social-medias-future-wechats-world,wechat's world,"YU HUI, a boisterous four-year-old living in Shanghai, is what marketing people call a digital native. Over a year ago, she started communicating with her parents using WeChat, a Chinese mobile-messaging service. She is too young to carry around a mobile phone. Instead she uses a Mon Mon, an internet-connected device that links through the cloud to the WeChat app. The cuddly critter's rotund belly disguises a microphone, which Yu Hui uses to send rambling updates and songs to her parents; it lights up when she gets an incoming message back. Like most professionals on the mainland, her mother uses WeChat rather than e-mail to conduct much of her business. The app offers everything from free video calls and instant group chats to news updates and easy sharing of large multimedia files. It has a business-oriented chat service akin to America's Slack. Yu Hui's mother also uses her smartphone camera to scan the WeChat QR (quick response) codes of people she meets far more often these days than she exchanges business cards. Yu Hui's father uses the app to shop online, to pay for goods at physical stores, settle utility bills and split dinner tabs with friends, just with a few taps. He can easily book and pay for taxis, dumpling deliveries, theatre tickets, hospital appointments and foreign holidays, all without ever leaving the WeChat universe. As one American venture capitalist puts it, WeChat is there ""at every point of your daily contact with the world, from morning until night"". It is this status as a hub for all internet activity, and as a platform through which users find their way to other services, that inspires Silicon Valley firms, including Facebook, to monitor WeChat closely. They are right to cast an envious eye. People who divide their time between China and the West complain that leaving WeChat behind is akin to stepping back in time. Among all its services, it is perhaps its promise of a cashless economy, a recurring dream of the internet age, that impresses onlookers the most. Thanks to WeChat, Chinese consumers can navigate their day without once spending banknotes or pulling out plastic. It is the best example yet of how China is shaping the future of the mobile internet for consumers everywhere. That is only fitting, for China makes and puts to good use more smartphones than any other country. More Chinese reach the internet via their mobiles than do so in America, Brazil and Indonesia combined. Many leapt from the pre-web era straight to the mobile internet, skipping the personal computer altogether. About half of all sales over the internet in China take place via mobile phones, against roughly a third of total sales in America. In other words, the conditions were all there for WeChat to take wing: new technologies, business models built around mobile phones, and above all, customers eager to experiment. The service, which is known on the mainland as Weixin, began five years ago as an innovation from Tencent, a Chinese online-gaming and social-media firm. By now over 700m people use it, and it is one of the world's most popular messaging apps (see chart). More than a third of all the time spent by mainlanders on the mobile internet is spent on WeChat. A typical user returns to it ten times a day or more. WeChat has worked hard to make sure that its product is enjoyable to use. Shaking the phone has proven a popular way to make new friends who are also users. Waving it at a television allows the app to recognise the current programme and viewers to interact. A successful stunt during last year's celebration of Chinese New Year's Eve saw CCTV, the official state broadcaster, offer millions of dollars in cash rewards to WeChat users who shook their phones on cue. Punters did so 11 billion times during the show, with 810m shakes a minute recorded at one point. Most importantly, over half of WeChat users have been persuaded to link their bank cards to the app. That is a notable achievement given that China's is a distrustful society and the internet is a free-for-all of cybercrime, malware and scams. Yet using its trusted brand, and putting to work robust identity and password authentication, Tencent was able to win over the public. In contrast, Western products such as Snapchat and WhatsApp have yet to persuade consumers to entrust them with their financial details. Japan's Line (which recently floated shares on the New York and Tokyo stock exchanges) and South Korea's KakaoTalk (in which Tencent is a big investor) have done better, but they cannot match the Chinese platform. One app to rule them all How did Tencent take WeChat so far ahead of its rivals? The answer lies partly in the peculiarities of the local market. Unlike most Westerners, many Chinese possessed multiple mobile devices, and they quickly took to an app that offered them an easy way to integrate them all into a single digital identity. In America messaging apps had a potent competitor in the form of basic mobile-phone plans, which bundled in SMS messaging. But text messages were costly in China, so consumers eagerly adopted the free messaging app. And e-mail never took off on the mainland the way it has around the world, mainly because the internet came late; that left an opening for messaging apps. But the bigger explanation for WeChat's rise is Tencent's ability to innovate. Many Chinese grew up using QQ, a PC-based messaging platform offered by Tencent that still has over 800m registered users. QQ was a copy of ICQ, a pioneering Israeli messaging service. But then the Chinese imitator learned to think for itself. Spotting the coming rise of the mobile internet, Tencent challenged several internal teams to design and develop a smartphone-only messaging app. The QQ insiders came up with something along the lines of their existing product for the PC, but another team of outsiders (from a just-acquired firm) came up with Weixin. When Tencent launched the new app, it made it easy for QQ's users to transfer their contacts over to the new app. Another stroke of brilliance came two years ago when the service launched a ""red packet"" campaign in which WeChat users were able to send digital money to friends and family to celebrate Chinese New Year rather than sending cash in a red envelope, as is customary. It was clever of the firm to turn dutiful gift-giving into an exciting game, notes Connie Chan of Andreessen Horowitz, a VC firm. It also encouraged users to bind together into groups to send money, often in randomised amounts (if you send 3,000 yuan to 30 friends, they may not get 100 yuan each; WeChat decides how much). That in turn led to explosive growth in group chats. This year, over 400m users (both as individuals and in groups) sent 32 billion packets of digital cash during the celebration. The enthusiasm with which WeChat users have adopted the platform makes them valuable to Tencent in ways that rivals can only dream of. After years of patient investment, its parent now earns a large and rising profit from WeChat. While other free messaging apps struggle to bring in much money, Duncan Clark of BDA, a technology consultancy in Beijing, estimates that WeChat earned about $1.8 billion in revenues last year. By the reckoning of HSBC, a bank, according to current valuations for tech firms, WeChat could be worth over $80 billion already. Over half of its revenues come from online games, where Tencent, the biggest gaming firm, is extremely strong. E-commerce is another driver of the business model. The firm earns fees when consumers shop at one of the more than 10m merchants (including some celebrities) that have official accounts on the app. Once users attach their bank cards to WeChat's wallet, they typically go on shopping sprees involving far more transactions per month than, for instance, Americans make on plastic. Three years ago, very few people bought things using WeChat but now roughly a third of its users are making regular e-commerce purchases directly though the app. A virtuous circle is operating: as more merchants and brands set up official accounts, it becomes a buzzier and more appealing bazaar. Users' dependence on the portal means a treasure-trove of insights into their preferences and peccadilloes. That, in turn, makes WeChat much more valuable to advertisers keen to target consumers as precisely as possible. There are few firms better placed to take advantage of the rise of social mobile advertising than WeChat, reckons Goldman Sachs, an investment bank. When BMW, a German carmaker, launched the first-ever ad to appear on the WeChat Moments page (which is akin to a Facebook feed) of selected users, there followed nothing like pique at the commercial intrusion, but rather an uproar from people demanding to know why they had not received the ad. Even though Tencent has deliberately trodden carefully in introducing targeted ads on users' Moments pages, its official corporate accounts enjoy billions of impressions each day. For Western firms, the most telling lesson from WeChat's success is that consumers and advertisers will handsomely reward companies that solve the myriad problems that bedevil the mobile internet. The smartphone is a marvellous invention, but it can be frustrating. In much of the world, there are too many annoying notifications and updates and the proliferation of apps is baffling. WeChat provides an answer to these problems. Better-known rivals in the West regard WeChat's rise with more than a tinge of jealousy. One executive, David Marcus, who runs Facebook Messenger, a popular messaging app run by the social network, is willing to talk about it openly. He calls WeChat, simply, ""inspiring"". His plan, to transform Messenger into a platform where people can communicate with businesses and buy things, sounds familiar. Even enthusiasts acknowledge that the mobile ecosystem is different in the West and that WeChat's reach and primacy in the eyes of consumers will not be easily replicated. It took off in China well before the app ecosystem had taken hold, as it has now in America and Europe. Western consumers are accustomed to using many different apps to access the internet, not just one. It would require a lot of nudging to encourage use of a single, central hub. Nor is there much chance that Facebook could make a significant dent in WeChat's dominance in China. The Silicon Valley darling enjoys incumbency and the network effect in many of its markets. That has sabotaged WeChat's own efforts to expand abroad (despite splashy ad campaigns featuring Lionel Messi, a footballer). But the same rule applies if Facebook enters China, which could happen this year or next. ""We have the huge advantage of incumbency and local knowledge,"" says an executive at Tencent. ""Weixin is quite simply more of a super-app than Facebook."" Indeed, WeChat has already proved itself in the teeth of competition. Many Chinese champions have succeeded only because the government has hobbled domestic rivals and blocked foreign entrants. Here, too, Tencent breaks the mould. It has withstood numerous attempts by Alibaba, a formidable local rival, to knock it and its creations off their perch. And it is Facebook's WhatsApp that is WeChat's most obvious rival. Unlike Facebook itself, and Twitter, both of which are blocked on the mainland, WhatsApp is free to operate. WeChat has flourished for simple, commercial reasons: it solves problems for its users, and it delights them with new and unexpected offerings. That will change the mobile internet for everyone-those outside China included, as Western firms do their all to emulate its success.",en,69
193,439,1460727723,CONTENT SHARED,6583734846225935852,-2623844842607044962,7622703213275261749,,,,HTML,https://macuserbrasil.wordpress.com/2016/04/01/no-ano-em-que-a-apple-completa-40-anos-confira-a-evolucao-do-iphone/,"no ano em que a apple completa 40 anos, confira a evolução do iphone","Nesse ano em que a firma de Cupertino completou 40 anos de existência, amada por muitos, odiada por alguns. Ninguém pode negar que ela mudou a história no ramo da tecnologia, uma empresa que começou em uma garagem de casa, por 2 gênios: Steve Jobs e Steve Wosniak. Ela foi fundada no dia 1 de abril de 1976, revolucionou o segmento de computador pessoal, tocadores de música, notebooks e eu acho que nem preciso citar as marcas que já estão eternizada na mente dos seus usuários. Porém, uma criação dela, merece um super infográfico, o blockbuster iPhone. Nunca um aparelho de telefone, ou melhor smartphone, mudou e influenciou tanto o ramo da telefonia móvel como o aparelho da Apple. Quando ele foi lançado em 2007 , não existia um aparelho com uma tela tão grande como o do iPhone 2G/Classic/Original com os seus 3,5 polegadas de tela , um dos primeiros a implementar a tela sensível ao toque, efetuar ligações telefônicas e o principal, navegar na internet , que convenhamos foi o grande diferencial, porque a partir daí, todos os outros fabricantes de smartphones praticamente seguiram a mesma trilha iniciada pelo iPhone, com telas maiores, acesso a internet, etc. O resto da história você já sabe, mas como recordar é viver, confira um infográfico bem interessante a seguir: Bacana né?!&#x1f609; via tecnologiauol",pt,69
194,2119,1471623409,CONTENT SHARED,-2440247087447106971,-7611460419696903236,8699291783730431926,,,,HTML,http://startupi.com.br/2016/08/itau-segue-tendencia-das-fintechs-e-se-torna-primeiro-banco-tradicional-permitir-abertura-de-contas-por-app/,itaú segue tendência das fintechs e se torna primeiro banco tradicional a permitir abertura de contas por app - startupi,"O banco Itaú lançou esta semana o Abreconta , aplicativo que permite abertura de contas de forma 100% digital. Assim como acontece com outras fintechs que não têm agências físicas, no app é possível enviar todos os dados para se tornar novo correntista, inclusive enviar os documentos necessários por foto, pelo próprio celular. A novidade foi anunciada por Marco Bonomi, diretor de varejo do Itaú Unibanco, em evento da Fenabrave. Por enquanto, o aplicativo está disponível apenas para smarphones iOS, mas em breve usuários de Android também poderão baixar. ""O Itaú já iniciou os testes com um grupo de clientes para oferecer a possibilidade de abertura de conta corrente de forma 100% online. O banco já disponibilizou o programa em uma loja de aplicativos e em breve o serviço estará disponível"", disse o diretor para o Startupi. Segundo informações do Estadão Conteúdo, com este lançamento o Itaú se torna o primeiro grande banco a seguir a resolução do Banco Central nº 4.480, que permite que contas-corrente passem a ser abertas por meio eletrônico, sem a necessidade de comparecer a uma agência bancária. Segundo Marco Bonomi, operações do banco junto a clientes digitais já somam cerca de 45% do resultado do varejo do Itaú, o que torna este segmento o principal gerador de lucro do banco.",pt,69
195,2245,1472750564,CONTENT SHARED,90383487344892230,7645894863578715801,535059460675595168,,,,HTML,http://www.kennybastani.com/2016/08/strangling-legacy-microservices-spring-cloud.html,building spring cloud microservices that strangle legacy systems,"The method that I explained above came to me about a year after completing a greenfield microservices project on a similar architecture. The project would be a pilot for building microservices that would extend legacy components of a retail banking platform-a system that was already serving millions of users in production. The result of the project was a success, as we realized the direct benefits of being agile with the microservices approach . While we were able to deliver business differentiating features quickly, our speed to market came at the cost of tightly coupling microservices to the existing components of the legacy system. There were a few factors that required us to create this tight coupling. We shackled ourselves into vertically scaled infrastructure provisioned in a private data center We didn't have a platform that supported cloud-native application development We didn't have a self-service tool in place to automate provisioning of databases for new microservices Because of these factors, we had to use the legacy system's large shared database for persistence by our new microservices. We would use database access control features to isolate our microservice's tables from being directly accessed by other applications. Even though these access features are for multitenancy , it would allow us to migrate the schema easily to a separate database at a later time. The fundamental issue with this approach was that it took us seven months to get the first microservice release into production. The early dependency on the shared database posed too much of a risk of impacting millions of production users. We realized that risk when we discovered a framework defect that caused our new microservices to be unable to release database cursors when undergoing stress testing in the performance environment. The lesson learned in this experience was an important one. A new microservice should encapsulate both the unit of service and the unit of failure -in production-on the very first day of development. When I say unit of service and unit of failure I am referring to a quote by storied computer scientist, Jim Gray. Gray wrote a technical report in 1985 titled Why Do Computers Stop and What Can Be Done About It? In the report, Gray talks about how to achieve fault-tolerance in software. As with hardware, the key to software fault-tolerance is to hierarchically decompose large systems into modules, each module being a unit of service and a unit of failure. A failure of a module does not propagate beyond the module. When I hear thought leaders talk about microservices and say that the ideas are not new, I always think back to this quote by Jim.",en,69
196,584,1461619724,CONTENT SHARED,7933360486658437274,-2626634673110551643,-5804099311266469071,,,,HTML,http://www.coindesk.com/brazils-bank-itau-blockchain-consortium-r3/,brazil's bank itaú joins r3 blockchain consortium - coindesk,"São Paulo-based Itaú Unibanco has become the first Latin America-based bank to join blockchain and distributed ledger consortium R3CEV. Launching last September with nine global banking partners, Itaú is the 45th global bank to join the R3 consortium, following South Korea's Hana Financial and Japan's SBI Holdings . Itaú saw R$21.9 billion ($6.1bn) in profits in 2014 as well as R$360 billion ($101bn) in assets under management, according to its latest annual report . In statements, Itaú general director for technology and operations Márcio Schettini said that Itaú joined to contribute to what he called the ""international drive"" toward building distributed ledger solutions for enterprise finance. Schettini said: ""We are convinced that these innovations will bring benefits to our customers and real gains in efficiency for the sector as a whole."" Notably, Itaú is not the only bank in R3 to offer services to Latin America. Group Santander and BBVA, for instance, serve Argentina, Brazil and Mexico, through franchise or subsidiary efforts, while HSBC had more than 60 branches in Latin America as of 2012. However, the addition comes amid a broader push by R3 to expand its membership beyond traditional banks, and as more regional banks join the growing global effort.",en,69
197,2436,1474946849,CONTENT SHARED,-2069509552243850466,3609194402293569455,-1454522461531275738,,,,HTML,https://msdn.microsoft.com/pt-br/virtualization/windowscontainers/quick_start/quick_start_windows_10,contêiner do windows no windows 10,"O exercício vai guiá-lo pela implantação e uso básicos do recurso de contêiner do Windows no Windows 10 Professional ou Enterprise (Anniversary Edition). Após a conclusão, você terá instalado a função de contêiner e implantado um contêiner do Hyper-V simples. Antes de começar esse início rápido, familiarize-se com a terminologia e os conceitos básicos do contêiner. Essas informações podem ser encontradas na Introdução rápida . Este início rápido é específico para contêineres do Hyper-V no Windows 10. É possível encontrar documentação adicional de início rápido no sumário à esquerda desta página. Pré-requisitos: Um sistema de computador físico executando o Windows 10 Anniversary Edition (Professional ou Enterprise). Esse início rápido pode ser executado em uma máquina virtual do Windows 10, porém a virtualização aninhada precisará ser habilitada. É possível encontrar mais informações no Guia de virtualização aninhada . 1. Instalar o recurso de contêiner O recurso de contêiner deve ser habilitado antes de trabalhar com contêineres do Windows. Para fazer isso, execute o seguinte comando em uma sessão do PowerShell com privilégios elevados. Como o Windows 10 dá suporte apenas a contêineres de Hyper-V, o recurso Hyper-V também deve ser habilitado. Para habilitar o recurso Hyper-V usando o PowerShell, execute o comando a seguir em uma sessão do PowerShell com privilégios elevados. Quando a instalação for concluída, reinicialize o computador. Após o backup ter sido realizado, execute o seguinte comando para corrigir um problema conhecido com os Contêineres do Windows no Windows 10. Nas versões atuais, você precisa desabilitar OpLocks para usar contêineres confiáveis do Hyper-V. Para reabilitar OpLocks, use o seguinte comando: Set-ItemProperty -Path 'HKLM:SOFTWARE\Microsoft\Windows NT\CurrentVersion\Virtualization\Containers' -Name VSmbDisableOplocks -Type DWord -Value 0 -Force 2. Instalar o Docker O Docker é necessário para trabalhar com contêineres do Windows. O Docker é composto pelo mecanismo do Docker e o cliente do Docker. Para este exercício, ambos serão instalados. Execute os comandos a seguir para fazer isso. Baixe o mecanismo e o cliente Docker como um arquivo zip. Expanda o arquivo zip em Arquivos de Programas, o conteúdo do arquivo já está no diretório do docker. Adicione o diretório do Docker ao caminho do sistema. Reinicie a sessão do PowerShell para que o caminho modificado seja reconhecido. Para instalar o Docker como um serviço Windows, execute o seguinte. Após ser instalado, o serviço pode ser iniciado. 3. Instalar imagens de contêiner base Os contêineres do Windows são implantados por meio de modelos ou imagens. Antes de ser possível implantar um contêiner, uma imagem do sistema operacional base do contêiner precisa ser baixada. Os comandos a seguir baixarão a imagem base do Nano Server. Faça o pull da imagem base do Nano Server. Assim que é feito o pull da imagem, executar docker images retornará uma lista de imagens instaladas, neste caso, a imagem do Nano Server. Para obter informações detalhadas sobre imagens de contêiner do Windows, consulte Gerenciar imagens de contêiner . 4. Implantar o primeiro contêiner Para este exemplo simples, uma imagem de contêiner ""Hello World"" será criada e implantada. Para obter a melhor experiência, execute estes comandos em um shell CMD do Windows elevado. Primeiro, inicie um contêiner com uma sessão interativa da imagem nanoserver . Depois que o contêiner for iniciado, você receberá um shell de comando de dentro do contêiner. Dentro do contêiner, criaremos um script 'Hello, World' simples. Quando tiver concluído, saia do contêiner. Agora, você criará uma nova imagem de contêiner do contêiner modificado. Para ver uma lista de contêineres, execute o seguinte e anote a ID do contêiner. Execute o seguinte comando para criar uma nova imagem 'HelloWorld'. Substitua com a ID do seu contêiner. Quando tiver concluído, você terá uma imagem personalizada que contém o script hello world. Isso pode ser visto com o seguinte comando. Finalmente, para executar o contêiner, use o comando docker run . O resultado desse comando docker run é que um contêiner do Hyper-V foi criado por meio da imagem 'HelloWorld', um script 'Hello World' de exemplo foi executado (saída ecoada para o shell) e, em seguida, o contêiner foi parado e removido. Os inícios rápidos de contêiner e Windows 10 subsequentes se aprofundarão sobre a criação e implantação de aplicativos em contêineres no Windows 10. Próximas etapas Contêineres do Windows no Windows Server",pt,69
198,2912,1482888269,CONTENT SHARED,-1572252285162838958,9210530975708218054,-7673376373070890422,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36",SP,BR,HTML,https://www.lpi.org/devops,lpic-ot devops engineer | linux professional institute,"As more and more companies introduce DevOps methodology to their workflows, skills in using tools supporting the collaboration model of DevOps become increasingly important. Focussing on the most relevant DevOps tools, LPIC-OT DevOps Engineers will be able to implement a DevOps workflow and to optimize their daily administration and development tasks.",en,68
199,2791,1480444518,CONTENT SHARED,1873952032843878490,1374824663945909617,6069689568663799842,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.100 Safari/537.36",SP,BR,HTML,https://blog.daftcode.pl/hype-driven-development-3469fc2e9b22?gi=e33d58da65a1,hype driven development,"Software development teams often make decisions about software architecture or technological stack based on inaccurate opinions, social media, and in general on what is considered to be ""hot"", rather than solid research and any serious consideration of expected impact on their projects. I call this trend Hype Driven Development, perceive it harmful and advocate for a more professional approach I call ""Solid Software Engineering"". Learn more about how it works and find out what you can do instead. New technology - new hope Have you seen it? A team picking newest, hottest technology to apply in the project. Someone reads a blog post, it's trending on Twitter and we just came back from a conference where there was a great talk about it. Soon after, the team starts using this new shiny technology (or software architecture design paradigm), but instead of going faster (as promised) and building a better product they get into trouble. They slow down, get demotivated, have problems delivering next working version to production. Some teams even keep fixing bugs instead of delivering new features. They need 'just a few more days' to sort it all out. Hype Driven Development Hype Driven Development (HDD) has many flavors and touches your project in many different ways: Reddit driven development  - when a team or individual decide on technology/architecture/design based on what popular blogger wrote or what is hot on reddit, hackernews, blogs twitter, facebook, GitHub or other social media. Conference driven development  - watch carefully what happens after people are back from conference. People get inspired. And that's a two-edged sword. Starting to use newest hottest lib/framework/architecture paradigm without enough research might be a highway to hell. Loudest guy driven decisions  - is when one guy is talking all the time about this new framework/lib/tech's that he has no experience with, but talks about it all the time and finally the team decides to use it. Gem/lib/plugin driven development  - a specially strong in Ruby On Rails community, where occasionally I can see a Gemfile so long that the only thing longer is the time it takes to load the app. It comes from the idea that every problem in rails should be solved with a gem. Sometimes it would take a couple of lines to build solution ourselves. But we're just solving problems by adding libs, plugins, gems or frameworks. I would also mention here behavior popular among hype driven developers -  Stack Overflow driven development  - when developers copy-paste solutions from Stackoverflow (or in general from the internet) without really understanding them. HDD is how teams bring doom on themselves The problem with hype is that it easily leads to bad decisions. Both bad architectural decisions and technological stack decisions often hunt a team months or even years later . In worst case they may lead to another very problematic situation in software engineering: The Big Rewrite . Which almost never works out. The root of all evil seems to be social media  - where new ideas spread much faster than they get tested.Muchfaster than people are able to understand their pros and cons. The Anatomy of Hype Most hypes have similar structure. Here it goes: Step 1: Real problem and solution They start in some company with a problem. A team within some company decides that solution to the problem is beyond the current technological stack, process or architecture. The company creates a new framework, library or paradigm and soon the problem is solved. Step 2: Announcement, buzz and keywords The team is excited to show their work to the rest of the world and soon they write blog posts and do talks on conferences. The problem oftentimes is non-trivial, so they are proud to present the impressive results of a non-trivial solution. People get excited about the new technology. The only problem is not everybody who gets excited is able to fully understand what the exact problem was and all the details of the solution. It was a non-trivial problem with a non-trivial solution after all. Takes more than a tweet, chit-chat or even blog post to explain. With communication tools like social media, blog posts and conference lightning talks the message gets blurred along the way. Step 3: Mania starts All shades of hype driven developers read blog posts and attend conferences. Soon the teams all over the world start using the new technology. Due to the blurred message - some of them make hasty decision to use framework even though it does not solve any of their actual problems. Yet the team does have expectation that this new technology will help. Step 4: Disappointment As the sprints go by, the technology does not improve the team's life as much as people hoped, but brings a lot of extra work. There's a lot of rewriting the code and extra learning for the team. Teams slow down, management gets pissed off. People feel cheated. Step 5: Realisation! Finally the team does retrospection and realizes what are the tradeoffs of the new technology and for what purpose it would be more relevant. They get wiser... till the next hype shows up. Examples of Hype: Let's examine some examples of hypes and see how those went through. Example 1: React.js Step 1: Facebook has a problem - advanced one page apps like Facebook itselves have, so many state changing events that it is hard to keep track what's going on and keep the application state consistent. Step 2: Facebook promotes new paradigm with buzzwords: functional, virtual DOM, components. Step 3: Mania: Facebook has created the front-end framework of the future! Let's write everything in react from now on! Step 4: Wait there is a lot of work, but no quick return on investment! Step 5: React is great for advanced one page app with lots of real-time notifications, but does not necessarily pay off for simpler applications. Example 2: TDD is dead by DHH Step 1: David Heinemeier Hansson (DHH, creator of Ruby on Rails framework) realises that it is hard to do TDD with Rails as this framework doesn't have architecture supporting good OOP. Makes a pragmatic choice - not to write tests upfront. Step 2: Hype starts with DHH blog post and conference talk . Hype keywords: TDD is DEAD. Step 3: Let's skip tests! Our Guru says so. We didn't write them anyway. Now we're at least not pretending. We're finally honest. Step 4: Wait! Even fewer things work now than before. We've built a buggy code. Step 5: ""TDD is not dead or alive. TDD is subject to tradeoffs, including risk of API changes, skill of practitioner and existing design"" - Kent Beck. Example 3: Microservices Step 1: Big monolith application scales hard. There is a point when we can break them down into services. It will be easier to scale in terms of req/sec and easier to scale across multiple teams. Step 2: Hype keywords: scalability, loose coupling, monolith. Step 3: Let's rewrite all to services! We have a 'spaghetti code' because we have a monolith architecture! We need to rewrite everything to microservices! Step 4: Shit! It is now way slower to develop the app, difficult to deploy and we spend a lot of time tracking bugs across multiple systems. Step 5: Microservices require a lot of devops skills in the team and with right investment might pay off as a way to scale the system and team. Before you reach serious scale issues it's an overinvestment. Microservices are extracted not written. You must be this tall to use microservices. Example 4: NoSQL Step 1: SQL databases have problems with high loads and unstructured data. Teams around the world start developing new generation of databases. Step 2: Hype keywords: Scalability, BigData, High Performance. Step 3: Our database is too slow and not big enough! We need NoSql! Step 4: We need to join tables? That is a no go. Simple SQL operations are becoming increasingly challenging. Development is slow and our core problems are not solved. Step 5: NoSql are tools to solve very specific problems (either extremely high volumes of data, unstructured data or very high load). SQL is actually a great tool and handles high load and huge data volumes well if used skillfully. The case for NoSql is still pretty rare in 2016. Example 5: Elixir and Phoenix (or put your favorite lang/framework pair here) Step 1: Web frameworks like Ruby On Rails don't deal well with with high performance applications, distributed applications and websockets. Step 2: Hype keywords: Scalability, High Performance, Distributed, Fault-tolerant. Step 3: Oh my good, our application is slow and our chat is not scalable! Step 4: Wow, learning functional programming and distributed approach is not that easy. We are now really slow. Step 5: Elixir and Phoenix is great framework, but takes significant effort to learn. It will pay back in a long run if you need specifically high performance app. The list goes on and on: In this crowded space of computer engineering we have a lot of areas where hypes are common. In JavaScript world new frameworks are born everyday. Node.js (keywords: event programming), reactive programming, Meteor.js (keywords: shared state), front-end MVC, React.js. You name it. In software engineering new architectures are born: Domain Driven Development, Hexagon, DCI. What is your favorite hype?",en,68
200,1542,1467035159,CONTENT SHARED,6426224004219108539,6735372008307093370,-6487028072647062097,,,,HTML,http://g1.globo.com/sp/campinas-regiao/noticia/2016/06/mesmo-sem-apoio-parada-lgbt-de-campinas-arrasta-multidao-pelas-ruas.html,"mesmo sem apoio, parada lgbt de campinas arrasta multidão pelas ruas","Multidão lota as ruas do centro de Campinas durante a realização da 16ª Parada do Orgulho LGBT (Foto: Renata Victal/G1) Apesar da falta de apoio financeiro da Prefeitura e de uma recomendação do Ministério Público para que a Polícia Militar não apoiasse o evento, a 16ª Parada do Orgulho LGBT (Lésbicas, Gays, Bissexuais, Travestis, Transexuais e Transgêneros) arrastou uma multidão pelas ruas do centro da cidade na tarde deste domingo (26). Confira as fotos da 16ª Parada do Orgulho LGBT de Campinas. ""Mesmo sem apoio, estamos aqui, lutando pelas nossas causas, contra qualquer tipo de preconceito. Muitos homosexuais são espancados e mortos a cada ano. Não podemos fazer de conta que o preconceito não existe"", destacoua drag queen Heloá Meireles. Look branco para celebrar a paz na 16ª edição da Parada LGBT (Foto: Renata Victal/G1) No começo do evento, os organizadores fizeram um minuto de silêncio em memória às vítimas do ataque à boate Pulse , em Orlando, no estado da Flórida (EUA) e também um protesto contra a falta de apoio da Prefeitura, da Polícia Militar e do Ministério Público. ""Entregamos toda a documentação em fevereiro, fizemos várias reuniões e faltando uma semana para o evento disseram que não dariam mais o apoio. Mas o povo veio mostrar que nada nos intimida"", ressaltou um dos organizadores da parada, Douglas Holanda. A Presidente da Comissão da Diversidade da Ordem dos Advogados do Brasil de Campinas , Ana Carolina Camargo de Oliveira, classificou a falta de apoio como um retrocesso. ""O Ministério Público fez um ofício pedindo que não apoiassem alegando falta de segurança, mas, como não houve uma ordem judicial, conversei com os organizadores e garanti a eles que a parada poderia acontecer. Esta é uma posição contrária ao que vem acontecendo em todo o mundo. Somos todos seres humanos e lutamos por uma causa. Essa falta de apoio foi um retrocesso. A homofobia ainda é tipificada como um crime simples"", lamentou Ana. Douglas Holanda, organizador Transfobia Nesta edição, a parada teve como tema ""Diga sim à educação e não à transfobia. Intolerância: o vírus mais assassino. Contra qualquer forma de opressão"", um alerta a qualquer tipo de intolerância, explica um dos organizadores da parada, Douglas Holanda. ""É preciso falar sobre o preconceito contra as transexual. Muita gente ainda tem certo receio de se aproximar de uma trans. Nossa luta é diária. As coisas mudaram muito, mas ainda temos alguns preconceituosos que nos atacam. Fizemos nosso percurso em paz, lembrando que nós representaremos as 49 vidas de Orlando, assim como todas as que se vão diariamente no Brasil e no mundo"", registrou Douglas Pastor Thiago Carlos dos Santos discursou pela igualdade. (Foto: Renata Victal/G1) Percurso e fé Este ano a concentração da 16ª edição da Parada foi ao lado do Fórum, na Avenida Dr. Campos Sales. De lá, a multidão subiu a Avenida Francisco Glicério até Dr. Moraes Sales, seguiu até o cruzamento com a Rua Irmã Serafina, continuando pela Avenida Anchieta até a Avenida Benjamin Constant. Ao retornarem à Avenida Francisco Glicério, o grupo seguiu até o Largo do Rosário. Como nos anos anteriores,o pastor Thiago Carlos dos Santos, da igreja Cidade do Refúgio, uma comunidade cristã inclusiva, usou o microfone para pregar contra a homofobia. Segundo ele, é importante que todos saibam que podem frequentar a igreja sem qualquer receio de ser vítima de preconceito. ""Na nossa igreja todos são bem-vindos. Pode ser homosexual, trans, homem, mulher, família. Não temos nenhum preconceito e estamos aqui para apoiar a causa da igualdade"", afirmou o pastor. Looks coloridos marcaram a 16ª Parada LGBT de Campinas (Foto: Renata Victal/G1)",pt,68
201,889,1462954505,CONTENT SHARED,-8085935119790093311,-1032019229384696495,7449982717083592275,,,,HTML,https://www.elastic.co/webinars/sneak-peek-of-graph-capabilities-with-elasticsearch,graph capabilities with the elastic stack,"Elasticsearch can power graph exploration at scale by collecting signals like clicks or purchases to identify meaningful connections between subjects on the fly. Watch this video for a preview and demo of Graph capabilities in the Elastic Stack, including: Detecting Sources of Risk - What are the shared behaviors of people trying to hack my website? Recommending Content - If users bought this type of gardening gloves, what other products might they be interested in? Identifying Relationships - Which people on Stack Overflow have expertise in both Hadoop-related technologies and python-related tech? Graph lets users leverage the relevance and distributed query execution capabilities of Elasticsearch - and we look forward to telling you how. Elasticsearch can power graph exploration at scale by collecting signals like clicks or purchases to identify meaningful connections between subjects on the fly. Watch this video for a preview and demo of Graph capabilities in the Elastic Stack, including: Detecting Sources of Risk - What are the shared behaviors of people trying to hack my website? Recommending Content - If users bought this type of gardening gloves, what other products might they be interested in? Identifying Relationships - Which people on Stack Overflow have expertise in both Hadoop-related technologies and python-related tech? Graph lets users leverage the relevance and distributed query execution capabilities of Elasticsearch - and we look forward to telling you how.",en,68
202,2472,1475174899,CONTENT SHARED,8779890754987103603,3609194402293569455,-1256266615792357100,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36",SP,BR,HTML,http://startupi.com.br/2016/09/mastercard-lanca-plataforma-de-apis-abertas-para-solucoes-de-pagamentos/,mastercard lança plataforma de apis abertas para soluções de pagamentos - startupi,"A Mastercard acaba de facilitar ainda mais a vida dos desenvolvedores que trabalham na criação da próxima geração de soluções para o comércio, com o lançamento do Mastercard Developers . Este portal único permite aos parceiros da Mastercard acessar uma diversidade de APIs (interfaces de programação de aplicativos) de pagamentos, dados e segurança. A plataforma também inclui a inédita categoria de APIs ""novas e experimentais"", permitindo que os parceiros testem as novas tecnologias e aplicativos. As APIs experimentais, desenvolvidas pelos oito laboratórios de P&D da empresa ao redor do mundo, ajudarão os parceiros a construir, dar escala e realizar pagamentos usando novas plataformas. Ao mesmo tempo, esses parceiros contarão com uma base para explorar novas modalidades de integração de pagamentos, como realidade aumentada e virtual, e a Internet das Coisas. ""Imaginamos o Mastercard Developers como algo que capacita os nossos clientes, parceiros e seus desenvolvedores para que eles possam inovar e fazer os seus negócios crescerem"", disse Oran Cummins, vice-presidente sênior de APIs da Mastercard. ""A nova plataforma será crucial para integrar as tecnologias e os serviços da Mastercard às soluções digitais desses parceiros de modo fácil e econômico. Com isso, os consumidores terão uma experiência mais simples, rápida e segura."" Os parceiros agora têm acesso a mais de 25 APIs da empresa - incluindo serviços essenciais como o MasterPass e os Mastercard Digital Enablement Services - em um formato que facilita a conexão ou integração com seus serviços. Dentre elas, há uma nova API desenvolvida para maximizar as ações de inclusão financeira. A Mastercard Aid Network (Rede de Ajuda Mastercard) ajuda a simplificar a distribuição de ajuda humanitária, mesmo em locais sem infraestrutura de telecomunicações, ao mesmo tempo em que torna a gestão de ONGs e organizações humanitárias mais transparente. Essa plataforma vai proporcionar uma experiência simplificada, documentação clara e intuitiva para os desenvolvedores com APIs suportadas por kits de desenvolvimento de software (SDKs), ferramentas de desenvolvimento e exemplos de código em seis linguagens de programação. A plataforma está crescendo e no ano passado seu uso já aumentou em 400%. Principais APIs no Mastercard Developers: Pagamento: MasterPass :Serviço de pagamento digital que simplifica a experiência de compras. Os consumidores podem fazer pagamentos com qualquer cartão, em qualquer lugar e com qualquer dispositivo. Serviços de Dados: Media Measurement : Mede o impacto que as campanhas de mídia digital têm nas vendas on-line e offline. Segurança: Experimental: Qkr! com MasterPass : Uma plataforma de pagamentos móveisque permite aos consumidores fazer pedidos e pagar produtos e serviços direto de seus dispositivos móveis. Mastercard Vending : Ajuda a conectar o seu aplicativo celular à plataforma de vendas - com isso os consumidores podem comprar diretamente pelo aplicativo. Inclusão Financeira: Outros serviços serão adicionados ao Mastercard Developers nos próximos meses. Para mais informações sobre a plataforma, acesse o site .",pt,68
203,1535,1466982935,CONTENT SHARED,-4027027091658759481,-1032019229384696495,-2318276786697086259,,,,HTML,https://medium.com/@marcushellberg/how-i-sped-up-the-initial-render-of-my-polymer-app-by-86-eeff648a3dc0,how i sped up the initial render of my polymer app by 86%,"How I sped up the initial render of my Polymer app by 86% I was trying to optimize the startup time of my Polymer application ( Expense Manager ) and realized that the main reason it took so long to render was that even though the user lands on the login page, the browser was busy setting up the main page that wasn't even visible yet. In addition, I had all my Polymer elements in one bundle file that needed to get loaded before anything could get shown. Here's what I did to speed up the first meaningful render from about 2,200ms to just over 300ms on localhost. Step 1: Split the bundle The first step was to ensure that I'm not loading more than needed for the initial login screen render. The way I decided to do that was to adapt my application to the new project structure that Polymer CLI uses. With this structure, instead of one bundle containing all the elements in the entire app, I got two bundles, one that just contained the components needed to show the login page, and one that contained the rest. Splitting up the bundle and deferring the load of the second bundle until navigation sped up the load time considerably. But there was one new issue that it brought with it. Now, after logging in, I had a brief flash of empty screen before the new bundle was actually downloaded and rendered. Step 2: Load the second page in the background While searching for ways to fix the flicker, I stumbled upon an undocumented feature in Polymer, a callback that gets called after a given component has finished rendering. With this callback, I was able to begin downloading and rendering the overview page as soon as the login page was rendered. Because users are slow compared to computers, this meant that by the time the user actually logs in, everything is already set up for them and the transition is instant. Step 3: Fetch data only once render is complete One final optimization I did using the same trick was to delay the Pouch DB setup until the second page was fully rendered. This way, the user gets quick visual results and the overall experience feels snappier . Conclusion Polymer.RenderStatus.afterNextRender gives you a simple way to defer things that need to get done soon, but that don't need to get done in the next render. You can find the demo online at: and source code at",en,68
204,2660,1477497746,CONTENT SHARED,5847042211895226591,-6786856227257648356,-9157637981906058578,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.101 Safari/537.36",MG,BR,HTML,https://www.elastic.co/blog/elastic-stack-5-0-0-released,elastic stack 5.0.0 released,"In February of 2016, following Elastic{ON} 16, I wrote a post titled Heya, Elastic Stack and X-Pack . Today, after almost a year of substantial effort, including 5 Alphas, 1 Beta, and 1 Release Candidate we are pleased to announce the GA release of the Elastic Stack. And, importantly, it is available - today - on Elastic Cloud . If you want hosted Elasticsearch and Kibana there is no other place to start with the most recent code. We are committed to making Elastic Cloud the best place to run hosted Elasticsearch. In fact, we even made the Release Candidate available on cloud for testing purposes. Our team is celebrating today. I hope you join us. The GA release is available today. Join the Elastic Team for a live virtual event on November 3 to learn more about the release and ask the creators questions (AMA style). Register now! Before exploring the release in detail, I want to take the opportunity to reflect on what has brought us to this point. Our Community During the recent Elastic{ON} Tour, I have begun each session discussing a brief history of the last several years. This session culminates in the announcement that we have reached a combined 75 Million downloads. When I first began the project, I hoped for widespread adoption. But the passion and fervor of our community continues to delight and amaze me. Pioneer Program With that in mind, I want to share the results of the Pioneer Program . The program began with a simple premise. Your usage of the Elastic Stack is of the utmost import in informing our development as well as ensuring we release the highest quality product available. I am pleased to say that the community has filed 146 issues since the first Alpha release in April. Our community is one of our most valued assets at Elastic. In fact, one of the most discussed changes in this release was the name ""Elastic Stack"". The Elastic Stack But Elastic Stack is more than just a name. When we began this release cycle we committed to developing, building, testing, and releasing the entirety of the Stack together. This is important, internally, to ensure compatibility. And, for you, it helps speed deployment, decrease version confusion, and make it easier for developers to add capabilities across the entirety of the Elastic Stack. A Feature Tour When I began this post, I intended to provide an overview of key features in each product. But, it was hard to know where to begin and where to stop. Each of our team and tech leads have created a post that discusses the features specific to their product. And there is no one better suited to tell the story than them. I am, particularly, excited about a few items but rather than enumerate in detail, I will provide a brief overview and encourage you to read the detail posts for each product. Ingest Node - Ingest Node is an Elasticsearch node type enabling some data enrichment capabilities like grok, geoip, date, and other basic event manipulation options at index (or re-index) time. Pipelines are constructed with processors, and accessed through the REST API by suffixing a query parameter ""?pipeline=x"" . The ability to add pre-processing to documents, natively in Elasticsearch, prior to indexing allows for a variety of creative ingest deployments. This doesn't replace Logstash. This doesn't remove the need for Beats, this just allows greater flexibility in designing your ingest architecture. Elasticsearch Performance - Benchmarks tend to have an agenda...especially competitive benchmarks. With that in mind, we have spent substantial effort comparing 5.0.0 to prior releases. This data is available to you. This data is what we inspect when we want to ensure that we are doing the right things with performance and we are doing so in public to work towards preventing the secrecy, and doubt, that are associated with benchmark numbers. In fact, not only are the results available but we also document our hardware configuration, we have open sourced the tooling (called Rally ) and the benchmarks themselves ( Rally-Tracks ). Metricbeat - Metricbeat replaces Topbeat as the primary tool for collecting metrics in the Elastic stack. Like Topbeat, Metricbeat collects ""top"" like statistics about host and per process resources (CPU, memory, disk, network). Unlike Topbeat, Metricbeat also collects metrics from systems such as Apache, HAProxy, MongoDB, MySQL, Nginx, PostgreSQL, Redis, or Zookeeper, with more to come in the near future. Logstash Monitoring APIs - A new monitoring feature provides runtime visibility into the Logstash pipeline and its plugins. This component collects various kinds of operational metrics while Logstash processes your data, and all of this information can be queried using simple APIs. Timelion - After being introduced as a {Re}search project, Timelion is now natively available in Kibana core. Timelion provides a query DSL and visualizations that let you explore your data over time. This is but a sample, I've left out BKD trees, scaled_float and half_float , the immense effort put into Elasticsearch Resiliency , the eye-meltingly beautiful redesign of Kibana (we never knew how much we hated borders until we removed them), Kafka output in Beats, and so much more. This is a massive release. Reading the individual posts is a must to begin to understand the scope of improvement. X-Pack At Elastic we loved extensions. So much so that we built them and gave them interesting names. Shield, Marvel, and Watcher all described individual closed source features that didn't take away for open source capability but were additive for our customers. Unfortunately, as the range of these features grew to include Graph and Reporting, the install process became difficult and, at times, quite confusing. Say Heya to X-Pack! One pack that adds security, alerting, monitoring & management, reporting, and graph capabilities to the Elastic Stack. Our engineering process for 5.0 wasn't limited to the Elastic Stack, but we've also extended X-Pack by adding: Management & Monitoring UIs to Kibana Security UIs to Kibana for creating both users and roles Greatly simplified the installation process X-Pack is available to trial and has both commercial and free (Basic) license options. We are particularly excited to make some X-Pack features available for free and details are available on our Subscriptions page. In Closing I am in awe of the effort that went into this release, the involvement from our community and customers, and the groundwork that this sets for future releases. As always, the best way to understand a release is to experience it.",en,67
205,1237,1464923401,CONTENT SHARED,-3605608255586388018,5127372011815639401,-2629452978222634122,,,,HTML,https://medium.freecodecamp.com/github-broke-my-1-000-day-streak-6ec0c4c3a7d9?gi=ec294df41928,"github broke my 1,000 day streak - free code camp","GitHub broke my 1,000 day streak ... and this is what I learned about myself, and open source, along the way. I was always planning to write a post when I broke my contribution streak. My first child (a beautiful baby girl we named Grace) was born five weeks ago, and I honestly didn't think my ""commit some code every day"" habit would last through that experience, or even leading up to it. Well, my habit did last. And this in no way reflects how present I was as a new parent, nor my ability to sort out my own priorities like a proper adult. Unlike some streak criticism, mine was personal and not for show or fame and wasn't a sign of a distressed work/life balance. Then again, when I started I didn't think it would last about a thousand days  - yet that's roughly how long it has been (how long for sure? GitHub won't tell you anymore, but based on my math it's about 980 days) Ultimately, all streaks will be broken, and like many long-lived ones, I guess I've been wondering why and how to end it. Out-lasting the feature on GitHub then - the one that inspired me to start it in the first place - seems like a fitting way to go out. This is my story, more because I want to tell it, than because I think you (or anyone) should read it. It's been a personal journey played out in a public space, and so this seems like a fitting way to wrap it up. Warning: this is a bit rambly. It's a snapshot in time of someone who's just worked at something for nearly three years and seen it disappear. The number isn't important, and the publicity less so. But it's been a constructive motivation in my life that I deliberately opted into, and I'm feeling pretty introspective and processing a lot now that the whole thing is done. My streak started in September 15th, 2013. I had returned from a five week honeymoon around Europe with my wife in July, and was ready to start a new life; by coincidence, that was also the day my company Thinkmill was officially registered. So, two birthdays in one. But it was probably two weeks later when I consciously decided to keep it up; like a personal challenge. I had also recently open sourced KeystoneJS which was one of the scariest moments in my life (what if people hate it! what if I get judged? what if I'm not good enough?) ... which lasted all of a few days until I realized nobody had noticed. Nobody cared, and nobody knew who I was. Heh. It's not actually that scary. Turns out if you want somebody to pay any attention to your open source stuff, you have to work for it . So the streak started as a public way to say ""Hey! I'm committed to this! You can trust my scrappy little open source project because I'm obviously paying attention to it and dedicated to building it!"" I went about two weeks before I knew I'd have to tell my wife; if I didn't have her buy-in, there was no way it was going to hold up. So I did, expecting her to dismiss it; instead, she got that I was doing something different and important to me, and encouraged me instead. I thought I'd go for a few months maybe, if I was strong. A few months in, my initial motivation of ""something to prove"" had disappeared and two patterns emerged instead: It got hard. There were days when I didn't want to keep it up. I wanted to tap out and let it go. Hardest of all was when I was under the gun at work and already pulling 12 hour days on things that didn't count. But I'd come so far; just do something. So I did, and invariably a day or two later I would be past it, and things were easy again. Lesson #1 I realised that when I didn't have time to fix something big, I'd pick off an easy issue from the pile (turned out people did like Keystone, and the natural result of this is GitHub Issues) and fix it. I was addressing things users cared about, and the quality of my project was better for it. This ""good maintainer"" thing led to more interest, more engaged users, and eventually some amazing contributors / collaborators. Lesson #2. Buoyed by this, I was feeling really good about the streak. I was also refining my art. I love coding, even though I often get caught up with business / management / architectural concerns, and no matter what else I do, coding is my art. Dedicating time to it every day (#1) and collaborating with an ever-expanding community (#2) are the best things I've ever done to develop my skill and experience. April the next year, my wife and I went on holiday to China for a couple of weeks. I was sure this was the End Of The Streak. Because I'm not going to let it be my life! It's not healthy . Plus, there's no way my wife would have patience for me coding on holiday :) ... but I was wrong. Initially, because I checked my notifications at the airport and merged a really good looking PR (being responsive to contributors is really important, more on this later!) Then I fixed a simple bug. It had momentum, and was so easy to keep up. About halfway through my wife asked ""have you done your streak yet?"" and I told her ""no, I'm not worrying about that"". She insisted. She'd seen the positive impact it had on me, and encouraged me to continue. For all the months I'd benefit from it after we'd come back, it was worth 15 minutes a day for the next week. I'd worked so hard for it, she said, don't throw it away now. ""Have you done your streak yet"" became a common refrain in our house over the next couple of years, always with encouragement and never resignation. If things are good for you, the people who love you will notice and encourage them. Lesson #3 Having an unbroken streak of nearly a year seemed a bit crazy. One of my friends went on his own self-driven coding adventure (Hi Tom ��) and made up a status dashboard on a big TV in our office, Panic-style, and one of the modules on it was my streak, in an industrial ""days since last accident"" design. It was fun, and egged me on a bit. Turns out being disciplined can be social and fun. Lesson #4 Worth noting, though, at this point my life was set up around what I now recognized as a discipline. It wasn't always easy, but no discipline is, and I worked at it. Having the support of my friends and family was crucial, and running a business where we were able to construct a healthy relationship between commercial and open source work was also extremely important. If you're going to do something hard, be smart about it, and set it up to be sustainable. Lesson #5 The next year was a bit of a blur. Keystone grew, but I was always humbled when people had heard of it, or liked it. Through this, what developed strongly was a sense of ""building something for good"". I'm not giving my time to work for free, I've just got these ideas to express (in the form of software) that I think will create benefit beyond what I could achieve on my own or for myself. I feel like I'm pretty blessed in life. Some would call it privilege, luck, fortune, whatever. I'm OK with that, because while I can't repay everyone who's ever helped me I can pay it forward and be as generous as possible with time, code or advice. But you know what? it comes back. At the start of 2015 I had the opportunity to go to the first React Conf where I made some excellent friends and started being part of the React Community. By this point, maintaining my streak was pretty much second nature. Almost forgotten, but still a discipline held, and foundational to the way I behave. One of the absolute highlights of 2015 was traveling to Paris for React Europe to talk about React and Mobile Apps. But not just for the conference or the talk. That year I'd become friends with Camille Reynders, a KeystoneJS team member, over many hours spent talking on Slack and working together. But we'd never met, because he lived in Belgium while I'm in Sydney. He and his wife drove down to hang out that weekend, just to spend some time in real life. We sat in cafes on the streets of Paris, watching the Pride Parade go past, eating cheese, drinking beer and hanging out. Starting an Open Source project and dedicating yourself to it can get you some great friends and unique experiences. Lesson #6 Coming into 2016, my wife and I were expecting our first child. I had a frantic start to the year, running a growing business and traveling overseas nearly monthly getting things done before I went seriously into family mode for a while. There was Nodevember - my first major conference talk on KeystoneJS, in Nashville. Then PhoneGap Day US in Utah where I pitched React to what felt like a room full of Angular developers. Then React Conf 2016 in San Francisco in February. At this point, each trip felt like catching up with friends, learning new things and developing ideas with developers I admire and am just flat-out stoked to spend time with in person. To contrast with my first experience open sourcing KeystoneJS to crickets, I've now got packages on npm that get over a million downloads a month, thousands of followers, and something like 15,000 stars across my various personal projects. But more importantly I've been told by several people that I've inspired them to get into open source as well. That's not a small thing, and something I'm really proud of. Because I believe contributing to open source is personal, powerful, and good. These days, it's not uncommon for me to hear feedback that the community around KeystoneJS is supportive, inclusive and inspiring. There are now other people inspiring yet more people around something that I created and imbued with my personal energy and philosophy, and that is just awesome . No other word for it. That is why I do this . Not to mention all the thanks, credit, and opportunities that have come flooding back in return. I do this for me, because I believe in this way of creating value, but having that realized with feedback is powerfully helpful. It's frankly been a lot of work, and sacrifice. But also? Totally worth it. Now, while I've taken personal and private pride in my streak and various other metrics, it's worth making some points. This is personal. It's been good for me. It's not for everyone. There is no judgement if you benefit from a more clear-cut work/life balance. I have no argument to make against those who say the GitHub streak guilts them into working on weekends and they want it gone. There are also downsides to open source that I didn't understand when I started. Guilt is massive and real. When someone submits a PR and I sit on it, I feel terrible for ignoring (or even just apparently ignoring) their work. Their time is valuable and they gave me some of it; in balance, they're implicitly asking for more of mine to review and maintain their suggestion. Sometimes I'm a great maintainer, and respond quickly with helpful feedback or just a ""Merge PR"" click. Other times, I'm not. I've been lucky to have people help me out as well, with vetting, triaging and taking on maintenance roles in various projects. I can't thank those people enough. This morning when I woke up and found out my streak had been removed, it just felt... weird . Not like I was robbed of something, not like I was liberated either. Just a bit empty, a bit numb. All those time I'd thought about it and been encouraged not to, and the choice was taken away from me. How anticlimactic. I don't know if the campaign to remove streaks from GitHub's UI was something I really agree with. I get the arguments, especially from a mental health perspective. Just because I can look back and feel good about my experience, maybe on balance I was an exception. Maybe for every developer who was constructively motivated by it, there was another who was unhealthily punished by it. Addressing health and balance in our (and every) industry is a positive thing, along with equality and inclusion. Clearly it's been good for some people. John Dalton's been on a streak longer than mine and is one of the most inspiring developers I know. John Resig, I think, motivated a great number of developers with his "" Write Code Every Day "" post. As he said: I consider this change in habit to be a massive success and hope to continue it for as long as I can. In the meantime I'll do all that I can to recommend this tactic to others who wish to get substantial side project work done. Having kept it up for nearly a thousand days, I'm OK to stop streak-counting. Maybe I'll keep that graph all green, or maybe some grey will sneak in. It doesn't really matter; after three years, the discipline has done its work, and I'll be the beneficiary of it for the rest of my life. And hey, I was planning to break it sometime anyway. I do wonder, when mechanisms for self discipline and motivation can cut both ways, and then be unilaterally removed (I assume in response to a vocal movement) what the next thing will be for a generation of developers who are just getting started? We humans respond well to pressure, competition and games. It's the very instinct that made the streak such a powerful metric in the first place. I hope something new emerges that doesn't have such a negative downside (the rock → hard place text was always a bit caustic) yet inspires developers to challenge themselves the way the GitHub Streak did.",en,67
206,1883,1469548372,CONTENT SHARED,-5848514031542611523,1908339160857512799,3464895365884046539,,,,HTML,https://googleblog.blogspot.com.br/2016/07/promoting-gender-equality-through-emoji.html,promoting gender equality through emoji �� ��,"More than 90 percent of the world's online population use emoji. But while there's a huge range of emoji, there aren't a lot that highlight the diversity of women's careers, or empower young girls. There are emoji like these for men: but with options like these for women: ... the emoji representing women aren't exactly, well, representative. So we've been working to make things better. In May, we proposed a set of new emoji to the Unicode Technical Committee that represent a wider range of professions for women (as well as men), and reflect the pivotal roles that women play in the world. Since then, we've worked closely with members of the Unicode Emoji Subcommittee to bring the proposal to life. Today, the Unicode Emoji Subcommittee has agreed to add 11 new professional emoji, in both male and female options and with all the skin tones. That's more than 100 new emoji to choose from! Unicode is also adding male and female versions to 33 existing emoji. For example, you'll be able to pick both a female runner emoji and a male runner emoji, or a man or woman getting a haircut: These additions can be included in future versions of Android and other platforms-because Unicode helps make sure that people with different phones can send and receive the same emoji. These new emoji are one of several efforts we're making to better represent women in technology, and to connect girls with the education and resources they need to pursue careers in STEM. One such effort is Made with Code, which helps girls pursue and express their passions using computer science. Ahead of World Emoji Day this weekend, Made with Code is releasing a new project that teaches coding skills through the creation of emoji-inspired stickers. We hope these updates help make emoji just a little more representative of the millions of people around the �� who use them.",en,66
207,2563,1476444319,CONTENT SHARED,-4487024160266973763,7308881151087125462,2824438335258380441,Android - Native Mobile App,SP,BR,HTML,https://www.linkedin.com/pulse/visa-inaugura-co-creation-center-em-s%C3%A3o-paulo-erico-fileno,visa inaugura co-creation center em são paulo,"Espaço será referência e protagonista na condução da revolução das novas tecnologias de pagamento no Brasil O Visa Brasil Co-Creation Center abre suas portas em São Paulo, como parte do plano global de inovação da Visa. A iniciativa tem como proposta trabalhar em conjunto com importantes players do mercado brasileiro para cocriar o futuro das soluções de pagamento, aproximar-se dos clientes e expor os ativos da Visa. O espaço atenderá aos emissores de cartão, credenciadoras, comércios, fintechs e startups e será um local de trabalho colaborativo, prático e dinâmico. O Centro expande o sucesso do Miami Innovation Center, instalação inaugurada em junho de 2016 e que funciona como um hub para a região da América Latina. O Visa Brasil Co-Creation Center será o primeiro ponto de contato da Visa com desenvolvedores e parceiros brasileiros. "" Estamos criando uma rede. Esse espaço representa uma continuação do esforço e jornada da Visa, e permitirá que nossos clientes locais participem de um grupo global de inovação para cocriar a próxima geração de aplicativos de pagamento e do comércio "", diz Eduardo Coello, Presidente da Visa Inc. para a América Latina e Caribe . A Visa está revolucionando a indústria global de meios eletrônicos de pagamento e esse espaço no Brasil vem para solidificar e fortalecer a liderança da empresa na região. "" Estamos no meio de uma grande mudança de paradigmas. Com a migração do cartão de plástico para a internet das coisas, o desenvolvimento dos pagamentos móveis, e a entrada de novas empresas no tradicional grupo de players do setor de pagamentos, a Visa assume o compromisso de protagonizar e potencializar essa revolução e de garantir que todas essas novidades sejam eficazes e seguras "", afirma Percival Jatobá, Vice-presidente de produtos da Visa do Brasil . No Brasil, esse espaço inovador está totalmente atrelado à abordagem de design thinking - conjunto de métodos e processos que busca solucionar desafios de forma colaborativa e mais humana, na qual todos os envolvidos são colocados no centro de desenvolvimento do produto. Os parceiros terão acesso a APIs (interfaces de programação de aplicativos) e SDKs (kits de desenvolvimento de software) da Visa por meio do Visa Developer Program - que transforma a maior rede de pagamento do mundo em uma plataforma aberta para desenvolvedores de softwares e aplicativos. O país foi escolhido para sediar um Co-Creation Center por seu potencial de crescimento e ambiente diverso e pujante de startups, e também porque os brasileiros são conhecidos como early adopters de tecnologias. Nos últimos meses, a Visa lançou algumas tecnologias pioneiras, como a tokenização e os vestíveis ( wearables) - o relógio Swatch Bellamy, a pulseira Bradesco Visa e o anel de pagamento Visa, além do cartão Riocard Duo, que traz dupla funcionalidade, para transporte e pré-pago. Para Percival, o Visa Brasil Co-Creation Center é um marco dentro da empresa por simbolizar o início de uma nova era. "" Temos convicção de que, ao aliar a criatividade dos empreendedores e desenvolvedores brasileiros com a expertise e a inovação gerados pela Visa, grandes negócios e novas parcerias surgirão no País. Vamos elevar o nível do setor de pagamentos no Brasil "", diz o executivo. Foco no Consumidor Fernando Teles , anunciado recentemente como o novo diretor geral da Visa do Brasil , acredita que o Co-Creation Center acelerará a criação de novas experiências comerciais, além de apoiar os consumidores que, cada vez mais, fazem uso de dispositivos conectados para comprar, pagar e receber pagamentos. "" Nosso Centro tem a missão clara de inspirar nossos clientes e parceiros com uma combinação de novas tecnologias que vão ao encontro das necessidades presentes e futuras do consumidor. Como líder em tecnologia de pagamentos, é imperativo que habilitemos nossos clientes a atender a demanda dos consumidores, e os ajudemos a acompanhar a velocidade da inovação no comércio digital "", diz o executivo.",pt,66
208,313,1460201474,CONTENT SHARED,-6840859460575237450,-6030696784871381528,3156557011070914976,,,,HTML,http://premierleaguebrasil.com.br/2016/04/06/esse-texto-do-ranieri-e-a-melhor-coisa-que-voce-lera-hoje/,este texto do ranieri é a melhor coisa que você lerá hoje,"*texto e imagens originalmente publicados no site . NÓS NÃO SONHAMOS por Claudio Ranieri Eu me lembro da primeira reunião que tive com o presidente quando cheguei ao Leicester City no verão. Ele sentou comigo e me disse: ""Claudio, esse é um ano muito importante para o clube. É muito importante que fiquemos na Premier League. Temos que nos livrar do rebaixamento"". Minha resposta foi: ""Ok, claro. Vamos trabalhar duro nos treinos e tentar alcançar esse objetivo"". Quarenta pontos. Esse era o plano. Esse era o total que precisávamos para ficar na primeira divisão, para dar aos nossos torcedores mais um ano de Premier League. Naquela época, eu nem imaginava que iria olhar aquele papel no dia 4 de abril e ver o Leicester no topo da tabela com 69 pontos. No ano passado, nesse mesmo dia, o clube era o último colocado na classificação. Inacreditável. Eu tenho 64 anos, e, por isso, não saio muito de casa. Minha esposa está comigo há 40 anos. Nos meus dias de folga, tento ficar perto dela. Nós vamos ao lago perto da nossa casa ou, se estivermos mais animados, assistimos a um filme. Porém, ultimamente, eu tenho ouvido o barulho que vem de todas as partes do mundo. É impossível ignorar. Tenho ouvido até que temos novos torcedores na América nos seguindo. Para vocês, eu digo: bem-vindos ao clube. Estamos felizes por termos vocês. Quero que vocês amem a maneira com a qual jogamos futebol, e quero que amem meus jogadores, porque a jornada deles é inacreditável . Vocês talvez já tenham ouvido os nomes deles. Jogadores que eram considerados muito pequenos ou muito lentos para outros grandes times. N'Golo Kanté. Jamie Vardy. Wes Morgan. Danny Drinkwater. Riyad Mahrez. Quando eu cheguei no meu primeiro dia de treinos e vi a qualidade desses jogadores, eu sabia quão bons eles poderiam ser. Bem, eu sabia que teríamos uma chance de sobreviver na Premier League. Esse tal de Kanté corria tanto que eu pensei que ele tinha uma caixa cheia de baterias sob os shorts. Ele nunca parava de correr no treino. Tive que dizer a ele: ""N'Golo, pega leve. Vá devagar. Não corra atrás da bola toda hora, ok?"" Ele me respondeu: ""Ok, chefe. Pode deixar"". Dez segundos depois, eu olho e ele está correndo de novo. Eu disse: ""Um dia, eu ainda vou ver você cruzar a bola e correr para cabeçear"". Ele é inacreditável, mas não é a única peça-chave. Existem muitas outras para citar nessa temporada incrível . Jamie Vardy, por exemplo. Ele não é um jogador de futebol. É um cavalo fantástico. Ele precisa estar livre no campo . Eu digo a ele: ""Você é livre para se movimentar da forma que quiser, mas você tem que nos ajudar quando perdermos a bola. Isso é tudo o que eu te peço. Se você começar a pressionar o adversário, todos os seus companheiros vão segui-lo"". Antes de jogarmos a primeira partida da temporada, eu disse aos jogadores: ""Eu quero que vocês joguem pelos seus companheiros. Somos uma equipe pequena, então temos que lutar com todo nosso coração, com toda nossa alma. Eu não quero nem saber o nome do oponente . Tudo o que eu quero é que vocês lutem. Se eles forem melhores que nós, ok. Parabéns. Mas eles têm que mostrar pra gente que são melhores "". Existia uma eletricidade fantástica em Leicester desde meu primeiro dia. Começa com o presidente e passa pelos jogadores, pela equipe de profissionais e pelos torcedores. Foi inacreditável o que eu senti. No King Power Stadium, existe uma energia incrível . Os torcedores só cantam quando temos a bola? Oh, não, não, não. Quando estamos sob pressão, eles entendem nosso sofrimento e cantam com o coração. Eles entendem a complexidade do jogo e sabem quando os jogadores estão sofrendo. Eles são muito, muito próximos a nós . Nós começamos muito bem a temporada. Mas nosso objetivo, eu repito, era salvar o time do rebaixamento. Nos primeiros nove jogos, nós estávamos ganhando, mas sofríamos muitos gols. Tínhamos que marcar dois ou três gols todo jogo para conseguirmos vencer. Isso me preocupava muito . Antes de cada partida, eu dizia: ""Vamos, pessoal! Quero uma clean sheet hoje"". Nenhuma clean sheet. Tentei todo tipo de motivação. Então, finalmente, antes da partida contra o Crystal Palace, eu disse: ""Vamos, galera! Eu ofereço uma pizza se vocês conseguirem uma clean sheet "". Claro que meus jogadores conseguiram a clean sheet contra o Crystal Palace . 1 a 0 pra gente. Então, eu cumpri a promessa e levei meus jogadores à Peter Pizzeria, em Leicester City Square. Mas eu tinha uma surpresa preparada lá. Eu disse: ""Vocês têm trabalhado por tudo. Vocês vão trabalhar pela sua pizza também . Nós mesmos as faremos"". Entramos na cozinha com a massa, o queijo e o molho. Fizemos tudo. Estava muito bom, também. Comi vários pedaços. O que eu posso dizer? Sou italiano. Adoro minha pizza e minha pasta. Agora, conseguimos várias clean sheets . Uma dúzia de jogos sem tomar gols depois da pizza, na verdade. Não acho que foi coincidência . Faltam seis jogos, e nós devemos continuar lutando com nosso coração e nossa alma. Esse é um clube pequeno que está mostrando ao mundo o que pode ser alcançado através de espírito e determinação. Vinte e seis jogadores. Vinte e seis cérebros diferentes. Mas um só coração . Há poucos anos, muitos dos meus jogadores estavam em ligas menores. Vardy trabalhava em uma fábrica . Kanté estava na terceira divisão na França. Mahrez, na quarta. Agora, estamos brigando por um título. Os torcedores do Leicester que encontro na rua me dizem que estão sonhando. Mas eu digo a eles: ""Ok, vocês sonham pela gente. Nós não sonhamos. Nós simplesmente trabalhamos duro "". Não importa o que aconteça ao fim dessa temporada. Eu acho que nossa história é importante para todos os torcedores ao redor do mundo. Isso dá esperança aos jovens jogadores que já ouviram um ""você não é bom o suficiente"". Eles podem dizer a eles mesmos: ""Como posso chegar ao topo? Se Vardy pode, se Kanté pode, eu também posso"". O que você precisa pra chegar lá? Um grande nome? Não . Um grande contrato? Não . Você só precisa manter a mente aberta, o coração aberto, e correr livre . Quem sabe? Talvez, ao fim da temporada, nós teremos duas festas da pizza. Comments comments About the Author Jornalista formado pela Unesp - Bauru. Assessor de Comunicação do CEPID-CeMEAI e apaixonado pelo futebol europeu.",pt,66
209,604,1461708262,CONTENT SHARED,4118743389464105405,-1032019229384696495,-940735523977516255,,,,HTML,https://cloudplatform.googleblog.com/2016/04/why-Google-App-Engine-rocks-a-Google-engineers-take.html,why google app engine rocks: a google engineer's take,"In December 2011, I had been working for Google for nine years and was leading a team of 10 software developers, supporting the AdSense business. Our portfolio consisted of over 30 software systems, mostly web apps for business intelligence that had been built over the past decade, each on a stack that seemed like a good idea at the time. Some were state-of-the-art custom servers built on the (then) latest Google web server libraries and running directly on Borg. Some were a LAMP stack on a managed hosting service. Some were running as a cron job on someone's workstation. Some were weird monsters, like a LAMP stack running on Borg with Apache customized to work with production load balancers and encryption. Things were breaking in new and wonderful ways every day. It was all we could do to keep the systems running - just barely. The team was stressed out. The Product Managers and engineers were frustrated. A typical conversation went like this: PM : ""You thought it would be easy to add the foobar feature, but it's been four days!"" Eng : ""I know, I know, but I had to upgrade the package manager version first, and then migrate off some deprecated APIs. I'm almost done with that stuff. I'm eager to start on the foobar, too."" PM : ""Well, now, that's disappointing."" I surveyed the team to find the root cause of our inefficiency: we were spending 60% of our time on maintenance. I asked how much time would be appropriate, and the answer was a grudging 25%. We made a goal to reduce our maintenance to that point, which would free up the time equivalent of three and a half of our 10 developers. Google App Engine had just come out of preview in September 2011. A friend recommended it heartily - he'd been using it for a personal site - and he raved that it was low-maintenance, auto-scaling and had built-in features like Google Cloud Datastore and user-management. Another friend, Alex Martelli, was using it for several personal projects. I myself had used it for a charity website since 2010. We decided to use it for all of our web serving. It was the team's first step into PaaS. Around the same time, we started using Dremel, Google's internal version of BigQuery . It was incredibly fast compared to MapReduce, and it scaled almost as well. We decided to re-write all of our data processing to use it, even though there were still a few functional gaps between it and App Engine at the time, for example visualization and data pipelines. We whipped up solutions that are still in use by hundreds of projects at Google. Now Google Cloud Platform users can access similar functionality using Google Cloud Datalab . What we saw next was an amazing transformation in the way that software developers worked. Yes, we had to re-write 30 systems, but they needed to be re-written anyway. WIth that finished, developing on the cloud was so much faster -- I recall being astonished at seeing the App Engine logs, that I had done 100 code, test, and deploy cycles in a single coding session. Once things were working, they kept working for a long time. We stopped debating what stack to choose for the next project. We just grabbed the most obvious one from Google Cloud Platform and started building. If we found a bug in the cloud infrastructure, it was promptly fixed by an expert. What a change from spending hours troubleshooting library compatibility! Best of all, we quickly got the time we spent on maintenance down to 25%, and it kept going down. At the end of two years I repeated the survey; the team reported that they now only spent 5% of their time on maintenance. We started having good and different problems. The business wasn't generating ideas fast enough to keep us busy, and we had no backlog. We started to take two weeks at the end of every quarter for a ""hackathon"" to see what we could dream up. We transferred half of the developers to another, busier team outside of Cloud. We tackled larger projects and started out-pacing much larger development teams. After seeing how using PaaS changed things for my team, I want everyone to experience it. Thankfully, these technologies are available not only to Google engineers, but to developers the world over. This is the most transformational technology I've seen since I first visited Google Search in 1999 - it lets developers stop doing dumb things and get on with developing the applications that add value to our lives.",en,66
210,644,1461851503,CONTENT SHARED,-1622037268576555626,-1032019229384696495,-1941773591979139720,,,,HTML,http://www.businessinsider.com/payments-ecosystem-research-and-business-opportunities-2016-4-27,you won't recognize the new world of digital payments without this report,"BI Intelligence The modern smartphone is a remarkable device. A single device that fits in your pocket can do all the tasks that once required cameras, camcorders, GPS devices, watches, alarm clocks, calculators, and even TVs. But the next change might be the most radical of all-it could eliminate the need to carry cash and credit cards. The growing importance of the smartphone as the go-to computing device for every digital activity is having a profound effect everywhere you look, but it's only the biggest story among many exciting developments in the world of payments: Apple Pay was first out of the gate, but now mobile wallets are everywhere you look-Android Pay, Google Pay, Chase Pay and even Walmart Pay are making smartphones a real alternative to carrying credit cards. And the potential for mobile wallets to limit a merchant's fraud liability could help them really take off in acceptance for small businesses. As consumers move more purchasing online, gateway vendors that can act as a front-end processor for online businesses are seeing explosive growth. PayPal-owned Braintree grew 111% YoY in the number of cards on file in Q4 2015, while Stripe and Klarna now have multi-billion dollar valuations. Mobile Point-Of-Sale (mPOS) startups like Square and ShopKeep have pioneered a whole new payments niche-accepting payments via tablets and smartphones. Coupling their transactions capabilities with new apps can revolutionize a small business' inventory management, marketing, loyalty and even payroll. Mobile Peer-to-Peer payments in the U.S. are forecast to grow from $5.6 billion in 2014 to nearly $175 billion by 2019 as consumers increasingly skip the hassle of writing a check or going to an ATM. But smartphone vendors like Apple could cripple the dominant player of 2016 (Venmo) if they make a serious push to own the space. If your job or your company is involved in payment processing in any way, you know how complex this industry is. And you know that you simply can't understand where the next big digital opportunities are unless you know the key players and roles in each step of the payments ""supply chain:"" Fortunately, managing analyst John Heggestuen and research analyst Evan Bakker of BI Intelligence , Business Insider's premium research service, have compiled a detailed report that breaks down everything you need to know-whether you're a payments industry veteran or a newcomer who is still getting a basic knowledge of this complex world. BI Intelligence Among the big picture insights you'll get from this new report, titled The Payments Ecosystem Report : Everything You Need to Know About The Next Era of Payment Processing : The 5 key events of 2015 that have set up 2016 as a watershed year for the entire payments ecosystem. The basics of traditional card processing from the start of the process through to the very end. Why new players and innovations like prepaid cards, store cards, and PIN debit transactions are gaining market share and creating new opportunities. The effects-good and bad-of the transition to new mobile payment methods. New players and old have surprising threats and opportunities in areas as varied as carrier billing, remittances, wearables, and more. This exclusive report takes you inside these big issues to explore: The critical steps in credit card transactions and how they are changing. The six major types of organizations involved in the payments ecosystem. The significant differences for industry players who operate closed-loop networks and offer prepaid cards. The challenges and opportunities facing hardware and software providers for the payments sector. The 8 reasons why mobile wallets are growing so fast and how they will disrupt all aspects of the mobile ecosystem. The exciting possibilities ahead in fast-growing payments subsectors like remittances, connected devices and mobile P2P payments. And much more. The Payments Ecosystem Report : Everything You Need to Know About The Next Era of Payment Processing is the only place you can get the full story on the rapidly-evolving world of payments. To get your copy of this invaluable guide, choose one of these options: Subscribe to an ALL-ACCESS Membership with BI Intelligence and gain immediate access to this report AND over 100 other expertly researched deep-dive reports, subscriptions to all of our daily newsletters, and much more. >> START A MEMBERSHIP Purchase the report and download it immediately from our research store. BUY THE REPORT >> The choice is yours. But however you decide to acquire this report, you've given yourself a powerful advantage in your understanding of the fast-moving world of payments.",en,65
211,1988,1470347942,CONTENT SHARED,-5065077552540450930,2542290381109225938,-1265449797350590617,,,,HTML,http://www.sonhoseguro.com.br/2016/08/ranking-das-maiores-seguradoras-da-europa-2015/,ranking das maiores seguradoras da europa - 2015 | sonho seguro,"A AXA se manteve na liderança do ranking de maiores seguradoras da Europa, segundo estudo divulgado pela Fundacion Mapfre nesta quinta-feira, com prêmios totais de 91,9 bilhões de euros em 2015, avanço de 6,6%. O ranking traz Allianz, Generali, Prudential, Zurich, Talanx, CNP, Credit Agricole, Aviva e Mapfre. As dez maiores segurdoras europeias registraram prêmios de 483,4 bilhões de euros em 2015, crescimento de 7,8% em relação ao ano anterior. O segmento Não Vida foi responsável por 217 bilhões de euros, liderado por Allianz, AXA e Zurich. No quesito margem de solvencia, a Mapfre é a líder do ranking, seguida por Prudential, AXA, Aviva e Talanx.",pt,65
212,2185,1472084340,CONTENT SHARED,3616722904601574426,-7496361692498935601,352146653115423838,,,,HTML,https://www.infoq.com/br/presentations/explorando-o-novo-dotnet-multiplataforma,"explorando o novo .net multiplataforma: asp.net core, .net core e ef core","Resumo Nesta palestra, exploraremos o estágio atual dos frameworks Core, vendo na prática uma aplicação ASP.NET e .NET Core 1.0 (MVC) com uso do Entity Framework Core 1.0 e baseada no .NET Core 1.0 sendo criada e executada em múltiplas plataformas. São discutidas as principais limitações existentes nos três frameworks em sua versão inicial - e como as mudanças podem afetar o desenvolvimento no dia a dia. Minibiografia Palestrante frequente em diversos eventos. Reconhecido pela Microsoft como MVP por quatro anos consecutivos. Foi responsável técnico pela implantação do projeto de adoção de livros digitais em tablets em uma das maiores escolas particulares de Brasília/DF, beneficiando inicialmente cerca de mil alunos, dentre diversas outras iniciativas em Tecnologia da Informação. Entre 28 de março e 1° de abril, São Paulo recebeu a nona edição brasileira do QCon. Organizado pelo InfoQ Brasil e com palestras selecionadas por um comitê independente, esta edição contou com 3 keynotes, 100 palestras e 13 workshops, totalizando 130 horas de conteúdo, o que levou o QCon São Paulo ao patamar dos maiores QCons mundiais.",pt,65
213,1809,1468852057,CONTENT SHARED,5533752987392101383,-2726721797588771398,-3534935468229000936,,,,HTML,https://hbr.org/2016/07/7-questions-to-ask-before-your-next-digital-transformation,7 questions to ask before your next digital transformation,"Although digital investment is almost unquestionably the right course of action for most firms, organizations still struggle to create the desired results. Estimates of digital transformation failures range from 66% to 84%. Such a high failure rate isn't surprising, as leaders are trying to create entirely new competencies and wedge them into an organization with strong legacy cultures and operating models. While most executives are pros at managing change, digital transformation is a much deeper change than the usual process or system update. Of course, digital technology can be used to improve or augment existing ways of operating, but it also opens entirely new ways of doing business based on digital networks like Uber, Airbnb, Yelp, and the Apple Developer Network - which is where a great deal of the digital value resides. So as you navigate your own digital transformation, we recommend beginning with a few questions that go deeper than ""what talent do you need"" or ""how much money will you spend"" and probe broader organizational readiness. Is this a digital upgrade or a digital transformation? Most companies target digital transformation and end up with digital upgrades, using digital technology to increase efficiency or effectiveness at something your firm is already doing. For example, increasing your marketing spend for digital channels or upgrading internal communication systems. On the other hand, a digital transformation occurs when you use digital technology to change the way you operate, particularly around customer interactions and the way that value is created-for example, Apple using its developer network to create software for its devices. If you discover that you are actually embarking on an upgrade instead of a transformation, ask yourself if that will be sufficient to maintain competitiveness when business models based on digital networks create market valuations four times higher than the rest. Are you really bought in, and is your team? Digital technology and business models are on the radar of every executive, and there is an expectation that most companies must change to keep up. However, a situation that we have seen over and over again is a leadership team trying to lead a digital transformation that they aren't particularly passionate about. We all have core beliefs about what creates value in the world, and these shape the way we allocate our time, attention, and capital. Most leaders have decades of experience focusing on assets like plants, real estate, inventory, and human capital. Shifting away from these habitual priorities takes self-reflection and openness, and often a concerted effort to build new patterns in thought and action. Are you prepared to share value creation with your customers? The latest technology-enabled business model, network orchestration, is premised on the fact that companies can allow customers and other networks to share in the process of value creation. Uber relies on a network of drivers; Airbnb relies on a network of property owners; Ebay relies on a network of sellers. These networks are essential to the organizations, and, by accessing external assets, these firms are able to achieve outstanding profitability. Sharing the workload seems like an obviously winning proposition, but many leaders are hesitant to relinquish control and rely on a network that lies outside of their chain of command. Working with these external groups requires new, co-creative leadership styles, but also can allow organizations to tap into enormous pools of capabilities and under-utilized resources. Have you put walls around your digital team? A digital upgrade requires a well-defined team with a narrow scope. A digital transformation requires a team with a cross-functional mandate and strong support. This becomes an important point because organizations usually do not change their internal structure as a part of digital transformation and so the teams working on these transformations get slotted into the existing structure. Where the team actually ""sits,"" both physically and in the org chart, can affect their ability to influence the cross-functional groups integral to real digital transformation. We have seen many companies limit the progress of digital by basing their team in marketing or IT. Do you know how to measure the value you intend to create? You manage what you measure. For most organizations, the focus is on physical capital (for making and selling goods) or human capital (for delivering services). These firms track inventory, productivity, utilization, and other traditional Key Performance Indicators (KPIs). Digital transformations don't always affect the KPIs a company is already measuring. Of course the end goal of a transformation is to affect revenue, profitability, and investor value. Along the way, however, it is useful to track intermediate indicators. For many digital network companies, this includes sentiment and engagement as well as network co-creation and value sharing. For example, when judging the success of the Developer Network, Apple can measure the number of developers creating apps for their app store, the amount of money generated by those apps that Apple shares with their community, and customer satisfaction with apps. Are you ready to make the tough calls about your team? There is an old saying: ""It is easier to change the people than to change the people."" Said another way, sometimes a new vision requires new people to create it. For many, the digital people you need on your team and on your board don't reside in your organization-at all, or at least not in the right quantity. Many of your current staff will be dedicated to doing what they have always done and will create resistance and roadblocks for change. To make room for your digital transformers, make the tough calls early on regarding your team and board. In our experience, nearly half of your team and board will need to turn over during the course of a successful digital transformation. Although painful, it's really a good thing for the organization-creating balance between the old and new. Will you be ready to spin off your digital business? Sometimes the upstart inside the organization becomes bigger and more valuable than the parent that gave birth to it-or risks not attracting the right talent or suffering turf wars between digital and legacy. (A great resource on this is The Second Curve by Ian Morrison.) Often, separation is required to enable both the parent and child to continue growth. Google is an expert at both creating new ventures and enabling them to grow; witness their recent reorganization into Alphabet to enable each of its major businesses to pursue their own potential (including Google and YouTube). In other organizations, the new, digital business will actually absorb and improve its parent. Transforming an organization is difficult, and the research proves it. But it is still worth doing. Forrester's assessment is that by 2020 every business will become either predator or prey . As a leader, you likely already know the basics of managing change, but a digital transformation goes deeper, and thus makes different demands on you, your team, and your organization. In return, however, you have the opportunity to invest in the most profitable and valuable business models the market has seen.",en,64
214,1616,1467381084,CONTENT SHARED,5445999759138709405,5660542693104786364,-4372460612946273438,,,,HTML,http://www.infomoney.com.br/conteudo-patrocinado/noticia/5233259/bradesco-lanca-servico-inedito-para-estimular-habito-poupar,bradesco lança serviço inédito para estimular o hábito de poupar,"SÃO PAULO - Uma das estratégias para manter suas finanças em dia, é ter sempre uma poupança, uma reserva de dinheiro que possa ser utilizada em casos de emergência, como a perda de emprego e outros imprevistos. Com objetivo de estimular o hábito de poupar e a formação da poupança no Brasil, o Bradesco lançou recentemente o Poupa Troco Bradesco . Trata-se de um serviço inédito no País, no qual os valores dos centavos dos débitos realizados em conta corrente serão arredondados para cima com a diferença sendo direcionada para uma conta de poupança. ""É um jeito simples de poupar sem perceber. Acreditamos que num curto espaço de tempo teremos a adesão de pelo menos 300 mil clientes ao Poupa Troco Bradesco, afirma Altair Antônio de Souza, diretor executivo do Bradesco. Como funciona Na ocorrência de um débito de uma conta de consumo no valor R$100,30, por exemplo, o Banco efetua a transferência de R$0,70 para uma conta de poupança de titularidade do cliente ou de uma outra pessoa que ele escolher, em qualquer agência Bradesco. O cliente escolhe ainda um valor inteiro, entre R$1 e R$5, que será adicionado à somatória dos centavos gerados pelos débitos ocorridos em sua conta corrente. Para ter acesso ao Poupa Troco, o cliente precisará cadastrar sua conta corrente ao novo serviço. Caso o cliente não tenha saldo disponível nenhum valor será transferido. A rentabilidade é a mesma da caderneta de poupança, que hoje está em 0,68% ao mês e com liquidez diária - o que permite o saque a qualquer momento, umas das principais vantagens da poupança. E este arredondamento vale não apenas para contas de consumo. Fazem parte do Poupa Troco as transações de débito automático de contas de consumo, boletos bancários, operações com cartão de débito, fatura de cartão de crédito, entre outras.",pt,64
215,1940,1469976528,CONTENT SHARED,1436883058900979473,-1616903969205976623,-5530575741825489868,,,,HTML,http://www.mckinseyonmarketingandsales.com/the-future-of-the-shopping-mall,the future of the shopping mall,"Officially shopping malls are defined as ""one or more buildings forming a complex of shops representing merchandisers, with interconnected walkways enabling visitors to walk from unit to unit."" 1 Unofficially, they are the heart and soul of communities, the foundation of retail economies, and a social sanctuary for teenagers everywhere. In recent decades, the concept of the shopping mall, which has its origins in the U.S. and became a full-blown modern retail trend there in the post-WWII years, has proliferated across the globe. The five largest malls in the world now reside in Asia. China's New South China Mall in Dongguan stands at the top of the heap with 2.9 million square meters of space. Despite its ubiquity, the mall as it's been conceived for the last half century is at a critical inflection point. A storm of global trends are coming together at the same time to cause malls to change the role they play in people's lives. No longer are they primarily about shopping. Now, when consumers visit malls, they are looking for experiences that go well beyond traditional shopping. When today's consumers visit malls, they are looking for experiences that go well beyond traditional shopping. The trends helping to create this change include changing demographics, such as an aging population and increased urbanization, which means more people living in smaller spaces and a greater need for public spaces in which to socialize and congregate. In this environment, malls offer a welcome watering hole, especially in cities where other public spaces are not safe. Sustainability concerns are causing some consumers to prefer mixed use developments where they can live, shop and work all within walking distance - instead of having to get into a car and drive to a crowded suburban mall. The growing middle classes in Latin America and Asia maintain a strong association between consumption and pleasure, driving the need for more engaging shopping experiences. And finally, the e-commerce revolution and the rise of digital technologies are fundamentally reshaping consumer expectations and shifting the function of stores toward useful and entertaining customer experiences. As these trends advance across the global stage, they are forcing mall operators to rethink how they conceive and operate their properties. This identity crisis is most intense in the U.S., the country that pioneered malls and has the most malls per inhabitant. Thanks to a continued economic slowdown and rapid advance of the digital revolution, the U.S. mall industry is retracting and facing high vacancy levels. Websites such as deadmalls.com collect pictures of weedy parking lots and barren food courts, and try to explain how once-thriving shopping centers began to spiral downward. In the face of these considerable challenges, malls are seeking to stay relevant, drive growth and boost efficiency. We see successful players investing along three key fronts. 1. Differentiating the consumer offering, with a focus on experience and convenience. Online shopping provides consumers with ultimate levels of convenience. Malls will never be able to compete with the endless product selection, price comparisons and always-on nature of online. Nor should they try. Instead, malls need to move in a different direction, away from commoditized shopping experiences and toward a broadened value proposition for consumers. In 2011, the Australian mall company Westfield launched an online mall (and later a mobile app) with 150 stores, 3,000 brands, and over 1 million products. Innovative malls are incorporating value-added elements that attempt to recast the mall as the new downtown, including concerts, arts centers, spas, fitness clubs, and farmer's markets. These services provide a level of leisure and entertainment that can never be satisfied online. Xanadu, a mall 30 km from Madrid, for instance, has gone out of its way to provide the means for parents to spend quality time with their children. The mall features a ski slope, go karts, balloon rides, bowling and billiards. Similarly, the Mall of America in Minnesota has an underwater aquarium, a theme park, and a dinosaur walk museum. In Brazil, for instance, a new focus on leisure and entertainment is already driving growth. Revenue coming into malls from these offerings grew 41 percent in 2013 compared to 2012. An emphasis on fine dining and events is also helping to make malls the hub of the local community - a place to share quality time with friends and family, not just wolf down a meal at the food court. The King of Prussia Mall, located 30 km from Philadelphia, has a Morton's Steakhouse and Capital Grille. The Crystal Cove shopping center in Newport Beach, CA has more than a dozen upscale restaurants, including Tamarind of London and Mastro's Ocean Club. On the tenant mix front, innovative malls are strategically rethinking the types of stores that consumers will respond to. Anchor tenants that drive traffic are still key, but we also see a new emphasis on a curated mix of smaller stores that add a sense of novelty to the mall offering. Additionally, some malls are making greater use of temporary, flexible spaces that can accommodate different stores over time. Pop up stores, showroom spaces and kiosks provide customers with a sense of the unexpected and give them a reason to treasure hunt. Finally, malls are overcoming the commoditization problem by focusing on specific consumer segments and/or creating specific zones within the mall that allow consumers to find an area that caters to them. In the Dubai Mall, for instance, ""Fashion Avenue"" is an area dedicated to luxury brands and services tailored to the upscale customer, including a separate outside entrance and parking area. In the 7-story CentralWord mall in Bangkok, home décor is on the 5th level, technology on the 4th, and fashion apparel on 1-3. This approach also represents a way for malls to ensure that customers don't get lost inside the ever increasing square footage of malls. 2. Transforming the mall experience by leveraging technology and multichannel strategies. The digital transformation of retail is not all bad news for malls. On the contrary, it presents new opportunities for malls to engage consumers throughout their decision journeys. There are three primary ways in which malls are leveraging technology: The most innovative malls today look nothing like their predecessors. First, they are extending their relationships with customers to before and after the mall visit. This is about engaging customers through compelling content and creating deeper bonds with them through social media and proprietary sites and apps, as well as loyalty programs. Social media can be used, for instance, to create buzz about new tenants or solicit ideas from consumers about ideas for new stores. One mall company has utilized segmented Facebook communication to speak to different communities, such as different geographies or interest groups or specific malls. Mall loyalty programs can provide the means for malls to establish a direct relationship with customers that goes beyond each visit to the mall, while allowing malls to collect precious information about customers. Just like retailers, malls should reach out to their customers with customized offers, gift ideas and other targeted advertisements based on real time intelligence and location-based marketing. While malls face the challenge of not having direct access to shopper purchase data, this can be overcome by inducing shoppers to use their smartphone to scan purchase receipts in exchange for points that can be redeemed for concerts tickets, books, discount vouchers for participating merchants, free parking or invitations to events (e.g., a fashion show). Alternatively, technologies such as face recognition, location-based mobile ads, and beacons are already being successfully applied in order to identify and establish targeted contact with repeat customers. Such technologies are also valuable for gathering consumer behavioral data from which malls can glean useful insights. Secondly, malls are using technology to transform mall usability as a means of improving customer satisfaction. There is ample opportunity for malls to decrease customer pain points, while simultaneously creating entirely new delight points. Technology, for instance, can be used to address one of the biggest challenges shoppers face at the mall - finding parking. Sensors located in parking lots detect how many spots are available on each level and give visual indicators to drivers. Once within the mall, mobile apps can offer quick, easy guides to help shoppers find what they're looking for at today's increasingly large and multi-level malls. Thirdly, malls are utilizing digital capabilities to take the shopping experience to the next level. It critical for malls to take a more active role in shaping the shopping experience, either by acting more like retailers or by partnering with them. Mall players are experimenting with a variety of different business models to make this happen, but there are no certain winners yet. To introduce elements of e-commerce into the mall, Taubman partnered with Twentieth Century Fox to put virtual storefronts - ""Fox Movie Mall"" - in at least 18 luxury malls. There, shoppers can purchase movie tickets by scanning a QR code with their smartphone. As the barriers between online and offline blur, some mall operators are venturing into online with a complete virtual mall offering. In 2011, the Australian mall company Westfield launched an online mall (and later a mobile app) with 150 stores, 3,000 brands, and over 1 million products. The company collects a small listing fee from merchants, as well as a commission of between 20-30 percent on every sale. Driven by the knowledge that 60 percent of the 1.1 billion annual shoppers in its malls use mobile devices, Westfield also created a research lab located in San Francisco, with the mission of finding technology applications and services that can further enhance the retail experience for both shoppers and retailers. 3. Exploration of new formats and commercial real estate opportunities. The most innovative malls today look nothing like their predecessors. Although location remains the key real estate consideration for malls, a differentiated design and structure is increasingly important. Open air malls go a long ways toward lending an atmosphere of a town center, especially when they incorporate mixed use real estate. Many of the malls being built in urban areas are open and fully integrated with the landscape. The Cabot Circus Shopping Centre in Bristol, England, for instance, has a unique shell-shaped glass roof that's the size of one and a half football fields. Incorporating environmental sustainability considerations, the mall is accessible by public transportation and features a rainwater harvesting system. Even malls that are enclosed are now incorporating more natural ambiance into their design, installing plants and trees, wood walls and floors, waterfalls, and lots of glass to let in natural lighting. Such elements help malls better blend in with their surroundings. It is critical that malls be about much more than stores. We see the mix of tenant/public space moving from the current 70/30 to 60/40, or even 50/50. When this happens, these expanded public spaces will need to be planned and programed over the year much like an exhibition. They will be managed more like content and media, instead of real estate. Mixed used developments offer consumers an attractive, integrated community in which to live, work and shop. They also serve to generate additional traffic for the malls while maximizing returns on invested capital. Other commercial real estate opportunities that can add alternative revenue streams are hotels, office buildings and airports. Lastly, outlets malls are an increasingly popular alternate format in more mature markets such as the U.S., particularly after the downturn of the economy, and they have been a key driver of growth for many players. In emerging economies like Brazil, outlets are also gaining attention and we see mall operators experimenting with this format as a means of attracting price conscious consumers and deal seekers. Implications for malls Although these trends are expressing themselves to varying degrees in different markets around the world, we believe they are relevant globally and should be taken to heart no matter where mall companies operate. There are three strategic considerations that players should understand when figuring out how to best react. 1) Evolve the offering by defining a clear value proposition for both consumers and retailers, anchoring it on deep consumer insights and bullet-proof economics. Among the large universe of options for enhancing the customer experience, it is possible to identify initiatives that will be both ROI-positive and substantially boost the satisfaction customers have toward malls. To do this, mall players must first isolate and quantify the consumer touch points that are most responsible for driving satisfaction. Use these touch points to prioritize areas of investment and to design a cohesive customer experience program that will yield higher visit and/or spend rates, and ultimately greater consumer loyalty. 2) Increase productivity and efficiency of the current mall base through a strategic review of the tenant mix, taking into account consumer needs and retailer economics. This analysis should guide the management of rent pricing and overall commercial planning. On the cost front, the focus should be on strict management of direct and indirect costs, combined with operational efficiency, which is critical for successful customer experience transformations. 3) Think surgically about where and how to grow in a way that won't jeopardize returns. Focus on city clusters and regions that have distinctive opportunities for growth. This includes thinking purposefully about disciplined capex management and which formats are going to create the biggest impact, whether that's traditional, multi-use, neighborhood or outlet. Executing against these considerations will often require that mall players develop new capabilities. Westfield, for example, has established a Digital Office group that reports to the CEO with the mission of spearheading digital initiatives across the organization. Other companies have created ""customer experience"" teams that are responsible for creating and integrating a unified vision of customer initiatives. Still others have created retail teams responsible for working on partnerships with retailers, or alternatively, operating retail operations themselves. The world of retail is changing dramatically, but the mall still can have a central role in urban and suburban societies. To avoid becoming what one chief executive calls a ""historical anachronism - a sixty-year aberration that no longer meets the public's needs,"" mall operators must expand their horizons of what a mall can be. They must envision themselves no longer as real estate brokers, but instead as customer-facing providers of shoppable entertainment.",en,64
216,1172,1464779739,CONTENT SHARED,-8370744479086515302,1895326251577378793,2217251992734926463,,,,HTML,http://computerworld.com.br/mobile-banking-conquista-o-coracao-dos-brasileiros,mobile banking conquista o coração dos brasileiros,"Depois das facilidades trazidas pelos serviços oferecidos via Internet, agora chegou a vez do mobile banking conquistar o coração dos brasileiros. Aos poucos, os smartphones revolucionam a forma como as pessoas se relacionam com o setor financeiro. Segundo dados da Febraban, o número de transações realizadas através de dispositivos móveis no Brasil saltou de 4,7 bilhões para 11,2 bilhões entre 2014 e 2015. O volume representa um aumento brutal da ordem de 138%. A escalada da mobilidade no setor financeiro foi intensa, ainda mais se considerarmos que esse canal registrava menos de 1% do total de operações realizadas em 2012. Atualmente, já responde por 21% dentre um total de 54 bilhões de transações realizadas nos 17 maiores bancos em atividade no País. Grande parte (95%) das operações realizadas através do mobile banking no ano passado são classificadas como movimentação não financeira, ou seja, referem-se a serviços de consulta de saldos e extratos, por exemplo. Apesar disso, o telefone celular se consolidou como ferramenta de relacionamento entre correntistas e seus bancos. ""O cliente pede e se mostra cada vez mais interessado na utilização desse canal"", enfatiza Gustavo Fosse, diretor setorial de tecnologia e automação bancária da Febraban, projetando que o uso acentuado da mobilidade se mantenha forte pelos próximos anos. O executivo baseia suas apostas nas estimativas do IBGE, que estima que 40% dos brasileiros possuem um smartphone atualmente. Esse percentual deve subir para 65% em 2020. ""Isso mostra espaço para o serviço avançar"", resume, citando que atualmente, 33 milhões de brasileiros usam mobile banking. Vetor digital Considerando todos os canais eletrônicos (web, mobile e POS), o digital respondeu por 69% das transações registradas nos bancos brasileiros em 2015. O internet banking registrou 17,7 bilhões de transações no último ano, pouco abaixo das 18 bilhões verificadas em 2014. A representatividade do canal caiu de 37% para 33%, talvez influenciada pelo avanço da mobilidade. Porém, segundo Fosse, mais pessoas têm utilizado a rede mundial de computadores para se relacionarem com instituições financeiras no Brasil. ""Tivemos um crescimento de 6 milhões de contas correntes que passaram a utilizar o canal entre 2014 e 2015"", cita o executivo, apontando que, atualmente, 62 milhões de brasileiros usam internet banking. ""A tecnologia se mostra um grande indutor da bancarização no Brasil"", avalia Fosse, comentando que 89,6% dos brasileiros já se relacionam com o sistema financeiro tradicional. Para suportar os avanços dos canais eletrônicos, a indústria investe pesado em TI. No último ano, o segmento aplicou nada menos que R$ 19 bilhões em projetos de tecnologia .",pt,64
217,2735,1478790716,CONTENT SHARED,-1425776303341065806,3609194402293569455,1020328625857133308,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36",SP,BR,HTML,http://startupi.com.br/2016/11/natura-inova-e-cria-bot-para-auxiliar-consumidores-a-escolherem-presentes-de-natal/,natura inova e cria bot para auxiliar consumidores a escolherem presentes de natal - startupi,"A Natura inaugura uma nova fronteira no atendimento ao cliente no Facebook por meio do Messenger, o aplicativo de mensagens instantâneas da plataforma digital. A empresa é uma das primeiras do setor de cosméticos a criar um ' bot ', aplicação de inteligência artificial que opera em aplicativos de mensagens para automatizar respostas, que irá auxiliar os consumidores a escolher o presente de Natal ideal para cada tipo de pessoa. Com a ajuda do 'bot', os clientes podem fazer escolhas mais adequadas ao responder a perguntas por meio de texto sobre, por exemplo, o estilo de vida da pessoa que vai ganhar o presente. Para acionar o bot, basta entrar na página da Natura no Facebook e mandar uma mensagem para a marca utilizando o Messenger. Depois, clicar em ""começar"" e em ""assistente de presentes"". ""Presentear nos aproxima ainda mais das pessoas queridas. É um momento em que nos dedicamos a esse relacionamento, ao fazer escolhas que correspondam às características de cada uma delas"", diz João Paulo Ferreira, presidente da Natura. ""Mas nem sempre é fácil acertar no presente. O 'bot' no Facebook simplifica e torna essa tarefa ainda mais agradável, oferecendo soluções surpreendentes para o consumidor"". O Facebook lançou os 'bots' para o Messenger em abril de 2016, durante a F8, a conferência para desenvolvedores, e já existem mais de 30 mil bots ativos no mundo. ""O Messenger do Facebook tem mais de 1 bilhão de pessoas ativas no mundo todo, o que representa um potencial de negócio incrível para as empresas"", afirma Marcos Angelini, diretor-geral do Facebook no Brasil. ""Nossas tecnologias aproximam as pessoas no Brasil inteiro, permitindo que elas se conectem e compartilhem o que mais importa para eles. Estamos felizes com esta parceria com a Natura, que vai usar nossa plataforma para revolucionar a interação com seus clientes"", completa o executivo. Para implementar o 'bot' na página da Natura, a empresa contou com a parceria das agências Salve e Insight, que foram as responsáveis pelo desenvolvimento da ferramenta. Além da assistência na escolha dos presentes, o 'bot' também está preparado para responder a diversas perguntas sobre a empresa e seus produtos.",pt,64
218,1543,1467046806,CONTENT SHARED,7270966256391553686,-4467650312287951120,6410806869226558749,,,,HTML,https://blogs.msdn.microsoft.com/dotnet/2016/06/27/announcing-net-core-1-0/,announcing .net core 1.0,"We are excited to announce the release of .NET Core 1.0, ASP.NET Core 1.0 and Entity Framework 1.0 , available on Windows, OS X and Linux! .NET Core is a cross-platform, open source, and modular .NET platform for creating modern web apps, microservices, libraries and console applications. This release includes the .NET Core runtime, libraries and tools and the ASP.NET Core libraries. We are also releasing Visual Studio and Visual Studio Code extensions that enable you to create .NET Core projects. You can get started at . Read the release notes for detailed release information. We are also releasing .NET documentation today at docs.microsoft.com , the new documentation service for Microsoft. The documentation you see there is just a start. You can follow our progress at core-docs on GitHub. ASP.NET Core documentation is also available and open source . Today we are at the Red Hat DevNation conference showing the release and our partnership with Red Hat. Watch the live stream via Channel 9 where Scott Hanselman will demonstrate .NET Core 1.0. .NET Core is now available on Red Hat Enterprise Linux and OpenShift via certified containers. In addition, .NET Core is fully supported by Red Hat and extended via the integrated hybrid support partnership between Microsoft and Red Hat. See the Red Hat Blog for more details. This is the biggest transformation of .NET since its inception and will define .NET for the next decade. We've rebuilt the foundation of .NET to be targeted at the needs of today's world: highly distributed cloud applications, micro services and containers. Moving forward .NET Framework and .NET Core and Xamarin are all important products that will continue to evolve, for Windows, cross-platform cloud and cross-platform mobile, respectively. The .NET Framework and traditional ASP.NET will continue to be relevant for your existing workloads. You can share code and reuse your skills across the entire .NET family so you can decide what to use and when, including mobile apps with Xamarin. And because we designed .NET to share a common library (the .NET standard library) .NET Framework, .NET Core and Xamarin apps will share new common capabilities in the future. It's really easy to try out .NET Core and ASP.NET Core on Windows, OS X or Linux. You can have an app up and running in a few minutes. You only need the .NET Core SDK to get started. The best place to start is the .NET Core home page. It will offer you the correct .NET Core SDK for the Operating System (OS) that you are using and the 3-4 steps you need to following to get started. It's pretty straightforward. To give you an idea, once you have the SDK installed, you can type these three simple commands for your first ""Hello World"" app. The first generates a template for you for a console app, the second restores package dependencies and the last builds and runs the app. You'll see (no surprise!): You'll likely get bored of ""Hello World"" quite quickly. You can read more in-depth tutorials at .NET Core Tutorials and ASP.NET Core Tutorials . Check out the Announcing EF Core 1.0 to find out how to get started with Entity Framework Core 1.0. About two years ago, we started receiving requests from some ASP.NET customers for "".NET on Linux"". Around the same time, we were talking to the Windows Server Team about Windows Nano, their future, much smaller server product. As a result, we started a new .NET project, which we codenamed ""Project K"", to target these new platforms. We changed the name, shape and experience of the product a few times along the way, at every turn trying to make it better and applicable to more scenarios and a broader base of developers. It's great to see this project finally available as .NET Core and ASP.NET Core 1.0. Open source is another important theme of this project. Over time, we noticed that all of the major web platforms were open source. ASP.NET MVC has been open source for a long time, but the platform underneath it, the .NET Framework, was not. We didn't have an answer for web developers who cared deeply about open source, and MVC being open wasn't enough. With today's releases, ASP.NET Core is now an open source web platform, top to bottom. Even the documentation is open source. ASP.NET Core is a great candidate for anyone who has open source as a requirement for their web stack. We'd like to express our gratitude for everyone that has tried .NET Core and ASP.NET Core and has given us feedback. We know that tens of thousands of you have been using the pre-1.0 product. Thanks! We've received a lot of feedback about design choices, user experience, performance, communication and other topics. We've tried our best to apply all of that feedback. The release is much better for it. We couldn't have done it without you. Thanks! If you are not a .NET developer or haven't used .NET in a while, now is a great moment to try it. You can enjoy the productivity and power of .NET with no constraints, on any OS, with any tool and for any application. All of that fully open source, developed with the community and with Microsoft's support. Check out dot.net to see the breadth of .NET options. This is a huge milestone and accomplishment for the entire .NET ecosystem. Nearly 10k developers contributed to .NET Core 1.0. We never imagined that many folks contributing to the product. We've also been impressed by the quality of the contributions. There are significant components that the community is driving forward. Nice work, folks! We also found that another 8k developers are watching these same repos, which effectively doubles the count. We believe that these developers watch these repos to either find that first opportunity to contribute or want to stay up-to-date on the project as part of their approach to .NET Core adoption. At this point, nearly half of all pull requests for .NET Core related projects (e.g. corefx, coreclr) come from the community. That's up from 20% one year ago. The momentum has been incredible. Check out the set of developers who contributed pull requests that were merged to the product. Thanks! Here's the breakdown of developers that created pull requests, created issues or made comments in any one of the .NET Core related repos, per organization, as determined by using the GitHub API: Total unique users: 9723 Note: The counts don't sum to the total because some users contribute to multiple organizations (thanks!) and we've tried to avoid double-counting. Note: The counts from the Microsoft org are specific to the few .NET Core-related repos that exist there, such as visualfsharp. Note: These numbers include Microsoft employees, which are (at most) 10% of the count. Samsung joins the .NET Foundation Increased interest in .NET Core has also driven deeper engagement in the .NET Foundation , which now manages more than 60 projects. Today we are announcing Samsung as the newest member . In April, Red Hat, Jet Brains and Unity were welcomed to the .NET Foundation Technical Steering Group. "".NET is a great technology that dramatically boosts developer productivity. Samsung has been contributing to .NET Core on GitHub - especially in the area of ARM support - and we are looking forward to contributing further to the .NET open source community. Samsung is glad to join the .NET Foundation's Technical Steering Group and help more developers enjoy the benefits of .NET."" Hong-Seok Kim, Vice President, Samsung Electronics. The contributions from Samsung have been impressive. They have a great team of developers that have taken an interest in .NET Core. We're glad to have them as part of the larger team. Some customers couldn't wait until the final 1.0 release and have been using preview versions of .NET Core in production, on Windows and Linux. These customers tell us that .NET Core has had a significant impact for their businesses. We look forward to seeing many of the applications that will get built over the next year. Please keep the feedback coming so that we can decide what to add next. Illyriad Games, the team behind Age of Ascent, reported a 10-fold increase in performance using ASP.NET Core with Azure Service Fabric. We are also extremely greatful for their code contributions to this performance. Thanks @benaadams ! NetEase, a leading IT company in China, provides online services for content, gaming, social media, communications and commerce, needed to stay on the leading edge of the ever-evolving mobile games space and chose .NET Core for their back end services. When compared to their previous Java back-end architecture: "".NET Core has reduced our release cycle by 20% and cost on engineering resources by 30%."" When speaking about the throughput improvements and cost savings: ""Additionally, it has made it possible to reduce the number of VMs needed in production by half."" We used industry benchmarks for web platforms on Linux as part of the release, including the TechEmpower Benchmarks . We've been sharing our findings as demonstrated in our own labs, starting several months ago. We're hoping to see official numbers from TechEmpower soon after our release. Our lab runs show that ASP.NET Core is faster than some of our industry peers. We see throughput that is 8x better than Node.js and almost 3x better than Go, on the same hardware. We're also not done! These improvements are from the changes that we were able to get into the 1.0 product. .NET developers know that the platform is a great choice for productivity. We want them to know that it's also a great choice for performance. We've been talking about .NET Core for about two years now, although it has changed significantly over that time. It's good to recap in this post what defines and is included in .NET Core 1.0. .NET Core is a new cross-platform .NET product. The primary selling points of .NET Core are: Cross-platform: Runs on Windows, macOS and Linux. Flexible deployment: Can be included in your app or installed side-by-side user- or machine-wide. Command-line tools: All product scenarios can be exercised at the command-line. Compatible: .NET Core is compatible with .NET Framework, Xamarin and Mono, via the .NET Standard Library . Open source: The .NET Core platform is open source, using MIT and Apache 2 licenses. Documentation is licensed under CC-BY . .NET Core is a .NET Foundation project. Supported by Microsoft: .NET Core is supported by Microsoft, per .NET Core Support Composition .NET Core is composed of the following parts: A .NET runtime , which provides a type system, assembly loading, a garbage collector, native interop and other basic services. A set of framework libraries , which provide primitive data types, app composition types and fundamental utilities. A set of SDK tools and language compilers that enable the base developer experience, available in the .NET Core SDK . The 'dotnet' app host , which is used to launch .NET Core apps. It selects the runtime and hosts the runtime, provides an assembly loading policy and launches the app. The same host is also used to launch SDK tools in the same way. Workloads By itself, .NET Core includes a single application model - console apps - which is useful for tools, local services and text-based games. Additional application models have been built on top of .NET Core to extend its functionality, such as: .NET Core Tools You typically start .NET Core development by installing the .NET Core SDK. The SDK includes enough software to build an app. The SDK gives you both the .NET Core Tools and a copy of .NET Core. As new versions of .NET Core are made available, you can download and install them without needing to get a new version of the tools. Apps specify their dependence on a particular .NET Core version via the project.json project file. The tools help you acquire and use that .NET Core version. You can switch between multiple apps on your machine in Visual Studio, Visual Studio Code or at a command prompt and the .NET Core tools will always pick the right version of .NET Core to use within the context of each app. You can also have multiple versions of the .NET Core tools on your machine, too, which can be important for continuous integration and other scenarios. Most of the time, you will just have one copy of the tools, since doing so provides a simpler experience. The dotnet Tool Your .NET Core experience will start with the dotnet tool . It exposes a set of commands for common operations, including restoring packages, building your project and unit testing. It also includes a command to create an empty new project to make it easy to get started. The following is a partial list of the commands. dotnet new - Initializes a sample console C# project. dotnet restore - Restores the dependencies for a given application. dotnet build - Builds a .NET Core application. dotnet publish - Publishes a .NET portable or self-contained application. dotnet run - Runs the application from source. dotnet test - Runs tests using a test runner specified in the project.json. dotnet pack - Creates a NuGet package of your code. dotnet works great with C# projects. F# and VB support is coming. .NET Standard Library The .NET Standard Library is a formal specification of .NET APIs that are intended to be available on all .NET runtimes. The motivation behind the Standard Library is establishing greater uniformity in the .NET ecosystem. ECMA 335 continues to establish uniformity for .NET runtime behavior, but there is no similar spec for the .NET Base Class Libraries (BCL) for .NET library implementations. The .NET Standard Library enables the following key scenarios: Defines uniform set of BCL APIs for all .NET platforms to implement, independent of workload. Enables developers to produce portable libraries that are usable across .NET runtimes, using this same set of APIs. Reduces and hopefully eliminates conditional compilation of shared source due to .NET APIs, only for OS APIs. .NET Core 1.0 implements the standard library, as does the .NET Framework and Xamarin. We see the standard library as a major focus of innovation and that benefits multiple .NET products. Support .NET Core is supported by Microsoft . You can use .NET Core in a development and deploy it in production and request support from Microsoft, as needed. Each release also has a defined lifecycle, where Microsoft will provides fixes, updates, or online technical assistance. The team adopted a new servicing model for .NET Core, with two different release types: Long Term Support (LTS) releases Typically a major release, such as ""1.0"" or ""2.0"" Supported for three years after the general availability date of a LTS release And one year after the general availability of a subsequent LTS release Fast Track Support (FTS) releases Typically a minor release, such as ""1.1"" or ""1.2"" Supported within the same three-year window as the parent LTS release And three months after the general availability of a subsequent FTS release And one year after the general availability of a subsequent LTS release Some customers want to deploy apps on very stable releases and do not want new features until the app is developed again. Those customers should consider LTS releases. Other customers want to take advantage of new features as soon as possible, particularly for apps that are almost always in development. Those customers should consider FTS releases. Note: We haven't released an FTS verion yet. .NET Core 1.0 is an LTS version. .NET Core Tools Telemetry The .NET Core tools include a telemetry feature so that we can collect usage information about the .NET Core Tools. It's important that we understand how the tools are being used so that we can improve them. Part of the reason the tools are in Preview is that we don't have enough information on the way that they will be used. The telemetry is only in the tools and does not affect your app. Behavior The telemetry feature is on by default. The data collected is anonymous in nature and will be published in an aggregated form for use by both Microsoft and community engineers under a Creative Commons license. You can opt-out of the telemetry feature by setting an environment variable DOTNET_CLI_TELEMETRY_OPTOUT (e.g. export on OS X/Linux, set on Windows) to true (e.g. ""true"", 1). Doing this will stop the collection process from running. Data Points The feature collects the following pieces of data: The command being used (e.g. ""build"", ""restore"") The ExitCode of the command For test projects, the test runner being used The timestamp of invocation The framework used Whether runtime IDs are present in the ""runtimes"" node The CLI version being used The feature will not collect any personal data, such as usernames or emails. It will not scan your code and not extract any project-level data that can be considered sensitive, such as name, repo or author (if you set those in your project.json). We want to know how the tools are used, not what you are using the tools to build. If you find sensitive data being collected, that's a bug. Please file an issue and it will be fixed. We use the MICROSOFT .NET LIBRARY EULA for the .NET Core Tools, which we also use for all .NET NuGet packages. We recently added a ""DATA"" section re-printed below, to enable telemetry from the tools. We want to stay with one EULA for .NET Core and only intend to collect data from the tools, not the runtime or libraries. You can build .NET Core apps with Visual Studio, Visual Studio Code or at the command-line. Visual Studio Code is the newest experience for building .NET apps. Let's take a look at building .NET Core apps with it. Using Visual Studio Code Show the experience using Visual Studio Code. To get started with .NET Core on Visual Studio Code, make sure you have downloaded and installed: You can verify that you have the latest version of .NET Core installed by opening a command prompt and typing dotnet --version . Your output should look like this: Next, you can create a new folder, scaffold a new ""Hello World"" C# application inside of it with the command line via the dotnet new command, then open Visual Studio Code in that directory with the code . command. If you don't have code on your PATH, you'll have to set it. If you don't have C# language plugin for Visual Studio Code it installed already, you'll want to do that. Next, you'll need to create and configure the launch.json and tasks.json files. Visual Studio Code will have asked if it can create these files for you. If you didn't allow it to do that, you will have to create these files yourself. Here's how: Create a new folder at the root level called .vscode and create the launch.json and tasks.json files inside of it. Open launch.json and configure it like this: Open the tasks.json file and configure it like this: Navigate to the Debug menu, click the Play icon, and now you can run your .NET Core applications! Note that if you open Visual Studio code from a different directory, you may need to change the values of cwd and program in launch.json and tasks.json to point to your application output folders. You can also debug your application by setting a breakpoint in the code and clicking the Play icon. The major differences between .NET Core and the .NET Framework: App-models - .NET Core does not support all the .NET Framework app-models, in part because many of them are built on Windows technologies, such as WPF (built on top of DirectX). The console and ASP.NET Core app-models are supported by both .NET Core and .NET Framework. APIs - .NET Core contains many of the same, but fewer, APIs as the .NET Framework, and with a different factoring (assembly names are different; type shape differs in key cases). These differences currently typically require changes to port source to .NET Core. .NET Core implements the .NET Standard Library API, which will grow to include more of the .NET Framework BCL APIs over time. Subsystems - .NET Core implements a subset of the subsystems in the .NET Framework, with the goal of a simpler implementation and programming model. For example, Code Access Security (CAS) is not supported, while reflection is supported. Platforms - The .NET Framework supports Windows and Windows Server while .NET Core also supports macOS and Linux. Open Source - .NET Core is open source, while a read-only subset of the .NET Framework is open source. While .NET Core is unique and has significant differences to the .NET Framework and other .NET platforms, it is straightforward to share code, using either source or binary sharing techniques. Thanks for all the feedback and usage. It's been a pleasure to build .NET Core and see so many people try it out. We really appreciate it. Please continue exploring the product and learning what it's capable of. We'll update the .NET Core Roadmap as we have clear plans for upcoming versions. Thanks for being part of the .NET community!",en,64
219,1256,1464982050,CONTENT SHARED,-5410531116380081703,2416280733544962613,3686082102675618374,,,,HTML,http://www.mckinsey.com/industries/consumer-packaged-goods/our-insights/meet-the-new-brazilian-consumer,meet the new brazilian consumer,"Amid one of the country's most severe recessions, how can consumer-goods companies and retailers succeed in Brazil? So much can change in just a few years. For most of the past decade, up until a couple of years ago, Brazil enjoyed a dramatic boom in consumption. Thirty-five million Brazilians-18 percent of the population-ascended to the middle class. As of 2012, Brazil's middle class encompassed 115 million people, or more than half of all Brazilians. But economic conditions in Brazil have taken a negative turn. Today, the country is facing one of the most severe recessions in its history. Will this recession be a short-term dip or a long slog? How are consumers changing their buying behavior? What can consumer-packaged-goods (CPG) and retail companies expect, and how should they adjust? What can Brazil learn from other countries' postrecession experiences-and are those lessons even applicable in Brazil? Our survey methodology Our recent survey of 1,000 Brazilian consumers confirms that most of them are indeed worried about their financial prospects and curtailing their spending. The Brazil survey was part of our broader global survey involving more than 22,000 respondents in 26 countries. Our aim in conducting the survey was to understand how consumers feel about their financial prospects and how these sentiments are affecting their buying behavior (see sidebar, ""Our survey methodology""). We found that even amid sharp declines in consumer confidence and private consumption, consumer companies can still find pockets of opportunity in Brazil. Cautious, concerned, and conservative Consumer confidence in Brazil was the lowest among 26 countries surveyed: only 8 percent of Brazilians were optimistic about the national economy (Exhibit 1). This bleak outlook is a departure from the positivity and optimism that Brazilians have historically displayed even in troubled times. It appears that the country's most recent economic woes have significantly shaken Brazilians' confidence. Fully 72 percent of Brazilians said they were worried that someone in their household would lose a job in the next year, and 49 percent said they were living paycheck to paycheck. A bump up in income wouldn't spur Brazilian consumers to spend much more. Even if their earnings were to rise by 10 percent, they said they'd spend only one-fourth of that extra money, much of it on everyday necessities such as food and drink. Almost half of the extra income would go toward savings and one-third toward paying off debt. Five attributes The survey responses brought to light a set of behavioral shifts among Brazilian consumers. Most of these behaviors are also evident among consumers elsewhere in the world, but they are amplified and intensified in Brazil. Consumer companies operating in the country would do well to view these shifts as a call to action. 1. They proactively search for savings. Consumers in Brazil are changing their buying behaviors in a variety of ways. Almost three out of every four respondents in Brazil agreed that they're ""increasingly looking for ways to save money"" (Exhibit 2). More than half said they are paying more attention to prices, actively looking for sales and promotions or delaying purchases. Many are also shopping at multiple stores to find the best deals or waiting for products to go on sale. In addition, Brazilian consumers are making thriftier food choices, with 42 percent saying they chose to eat at home more in the past year. 2. Some remain brand loyal-but only if the price is right. More than one-third of Brazilians claimed they haven't abandoned their preferred brands but are shopping around to find retailers that sell these brands at lower prices; 19 percent are purchasing in smaller quantities; and 14 percent are waiting until the brands are on sale or buying only with discount coupons (Exhibit 3). 3. Once they 'trade down,' they might not go back. Twenty-one percent of Brazilian consumers (versus 12 percent globally) said they traded down to less expensive brands. This number varied considerably by category; laundry supplies, household-cleaning products, and bottled water had the highest trade-down rates. Only 18 percent of consumers who traded down opted for private-label products. By contrast, in Latin America as a whole, 26 percent of down-traders chose private-label products; in more mature markets such as the United Kingdom and the United States, the figures were above 60 percent. Indeed, private-label or store brands in Brazil, although growing, still account for only a small fraction of total retail sales (5.1 percent, equivalent to approximately $970 million, according to data from Nielsen). Among Brazilian down-traders, 60 percent said they don't intend to go back to the more expensive brand (Exhibit 4). It's worth noting that US and European consumers who traded down during the global financial crisis have only recently begun to trade up again. If Brazil follows the same pattern, down-traders in Brazil will be trading down for at least the next few years. 4. There are splurgers in select categories. Even as most Brazilians sought to save money, some opted to trade up in certain categories. Although Brazil's trade-up rate of 5 percent is much lower than the global average of 11 percent, it's still meaningful in the handful of categories that clearly matter to consumers-in particular, alcoholic beverages and personal-care products. For example, 15 percent of consumers indicated they traded up in beer, 11 percent in wine, 10 percent in spirits, and 9 percent in cosmetics. Interestingly, in beer, a higher percentage of consumers stated having traded up versus traded down (12 percent). 5. They shop across channels. Brazilian consumers are shifting some of their spending to discount chains and the cash-and-carry format known locally as atacarejo , which combines both retail and wholesale. This trend isn't unique to Brazil: in more mature markets, such as the United Kingdom, consumers have already shifted much of their spending to discounters and are beginning to make distinctions between discount and ""premium"" discount. However, one channel that has made huge strides in some developed markets-the online channel-isn't yet a factor in Brazil's grocery categories. Imperatives for consumer companies In light of these new consumer behaviors, how can consumer-goods companies and retailers position themselves for success in a tough economic environment? In our view, they would be wise to act on the following imperatives. At every price point, think 'value for money.' With many consumers seeking savings opportunities, brands must give consumers solid reasons to choose their product over lower-priced alternatives. That means emphasizing not just the emotional attributes of a product but the functional benefits as well. They need to communicate a clear value proposition that resonates with consumers. Consider the success of salon-quality hair-care brands in several markets, including Brazil and the United States: these brands have been able to persuade consumers that they're getting good value for money, even though the salon-quality brand might be twice as expensive as their old shampoo brand. Urban world: The global consumers to watch Read the article Invest in advanced revenue-growth-management capabilities. By investing in cutting-edge revenue-growth-management solutions and analytical talent, leading companies arrive at data-driven answers to critical questions like these: Which promotions are most effective and why? How should prices and promotions be communicated? Based on these insights, companies determine optimal tactics and then devise granular strategies for their brands, portfolio, and promotions-and refine these strategies for each channel, customer segment, and region. Find granular opportunities for growth. To survive and thrive in Brazil's challenging economic environment, CPG companies and retailers need a granular, data-driven approach to identify pockets of growth. Consumption growth can vary considerably, both by region (for example, certain small and medium-size cities in Brazil's countryside are projected to grow at higher rates than state capitals) and by category (juices, for instance, are growing at two to three times the rate of carbonated drinks). Furthermore, a category's growth rates can vary significantly across cities and states. Review price architecture to capture both up-traders and down-traders. During downturns, Brazilians tend to gravitate to either high-end or low-end brands. CPG companies should have a clear and complete price architecture, with a premium offering to attract up-traders and a compelling low-priced offering aimed at down-traders and mass consumers. Higher-end products can justify their price points through a rich narrative (such as an artisanal process or a unique provenance) or new features. Value offerings, on the other hand, could emphasize bulk sizes, ""basic"" value cues in packaging or design, and no-frills but reliable quality and performance. Retailers, too, should consider offering a wider range of both high-end and low-end brands. Be thoughtful about channel changes. With Brazilian consumers doing more of their grocery shopping in discount and cash-and-carry formats, CPG manufacturers must launch initiatives to serve these fast-growing channels. Such initiatives might include the creation of exclusive SKUs, second-tier brands, or new pack sizes (especially as Brazilians have shown a willingness to buy in bulk and to engage in ""community shopping,"" or sharing their purchases with family and friends). Relentlessly optimize investments and operations. Efficiency is an imperative. Companies must untiringly pursue opportunities to optimize returns on their marketing investments, excel in store operations, and improve sales-force effectiveness. No drivers of value should remain unexplored; no excuses for delaying improvement initiatives should be made. The time is now to improve performance and efficiency and aim for excellence. In the early part of the past decade, for many consumer companies, building a strong presence in Brazil was synonymous with capturing rapid growth. We believe that, in the long run, the country will resume its growth trajectory-but in the current environment, the six imperatives discussed above must be at the top of the executive agenda. Efficiency, in particular, is crucial for any company that hopes to succeed in Brazil. About the author(s) Mariana Donatelli is an associate principal in McKinsey's São Paulo office, where Fernanda Hoefel is a partner, Suzana Resstom is an analyst, and Fábio Stul is a senior partner. Report - McKinsey Global Institute",en,63
220,1905,1469714751,CONTENT SHARED,-3780822597455574960,3609194402293569455,-3607853734254017965,,,,HTML,http://spectrum.ieee.org/computing/software/top-programming-languages-trends-the-rise-of-big-data,top programming languages trends: the rise of big data,"Illustration: Grzegorz Knec/Alamy Now that IEEE Spectrum is into the third year of annually ranking languages , we can start looking at some trends over time. What languages are ascendant? Which are losing traction? And which of the data sources that we use to create our rankings are contributing the most to these shifts? In this article I'm going to focus on so-called big-data languages, such as Julia , Python , R , and Scala . Most of these are purpose-built for handling large amounts of numeric data, with stables of packages that can be tapped for quick big-data analytic prototyping. These languages are increasingly important, as they facilitate the mining of the huge data sets that are now routinely collected across practically all sectors of government, science, and commerce. The biggest mover in this category was Go , an open source language created by Google to help solve the company's issues with scaling systems and concurrent programming back in 2007. In the default Spectrum ranking , it's moved up 10 positions since 2014 to settle into 10th place this year. Other big-data languages that saw moves since 2014 in the Spectrum ranking were R and Scala, with R ascending 4 spots and Scala moving up 2 (although down from 2015, when it was up 4 places from its 2014 position). Julia was added to the list of languages we track in 2015, and in the past year it's moved from rank 40 to 33, still a marginal player but clearly possessing some momentum in its growth. The chief reason for Go's quick rise in our ranking is the large increase in related activity on the GitHub source code archive. Since 2014, the total number of repositories on GitHub that list Go as the primary language went up by a factor of more than four. If we look at just active GitHub repositories, then there are almost five times as many. There's also a fair bit more chatter about the language on Reddit , with our data showing a threefold increase in the number of posts on that site mentioning the language. Explore the Interactive Rankings Another language that has continued to move up the rankings since 2014 is R, now in fifth place. R has been lifted in our rankings by racking up more questions on Stack Overflow -about 46 percent more since 2014. But even more important to R's rise is that it is increasingly mentioned in scholarly research papers. The Spectrum default ranking is heavily weighted toward data from IEEE Xplore , which indexes millions of scholarly articles, standards, and books in the IEEE database. In our 2015 ranking there were a mere 39 papers talking about the language, whereas this year we logged 244 papers. Contrary to the substantial gains in the rankings seen by open source languages such as Go, Julia, R, and Scala, proprietary data-analysis languages such as Matlab and SAS have seen a drop-off: Matlab fell four places in the rankings since 2014 and SAS has fallen seven. However, it's important to note that both of those languages are still growing; it's just that they're not growing as fast as some of the languages that are displacing them. When we weight the rankings toward jobs, we continue to see heavily used languages like Java and Python dominate. But recruiters are much more interested in R and Scala in 2016 then they were in 2014. When we collected data in 2014, there were only 136 jobs listed for Scala on CareerBuilder and Dice. But by 2016 there was more than a fourfold increase, to 631 jobs. This growth invites the question whether R can ever unseat Python or Java as the top languages for big data. But while R has seen huge gains over the last few years, Python and Java really are 800-pound gorillas. For instance, we found roughly 15 times as many job listings for pythonistas as for R developers. And while we measured about 63,000 new GitHub repositories in the last year for R, there were close to 458,000 for Python. Although R may be great for visualization and exploratory analysis and is clearly popular with academics writing research papers, Python has significant advantages for users in production environments: It's more easily integrated into production data pipelines, and as a general purpose language it simply has a broader array of uses. These data illustrate that despite the desire of some coders to evaluate languages on purely internal merits-the elegance of their syntax, or the degree and nature of the abstractions used-a big driver for a language's popularity will always be the domains that it targets, either by design or through the availability of supporting libraries.",en,63
221,2321,1473695293,CONTENT SHARED,4039031731864440026,881856221521045800,-2980396868411506251,,,,HTML,https://www.ibm.com/blogs/internet-of-things/olli-ai/,olli: artificial intelligence for the real world of connected commuting,"Automotive Moving fast with Olli. From concept, to prototype, through design, out into the market, and taking orders for sales of vehicles - all in less than three months. That's what disruptive technology is about. Olli is built by Local Motors , the start-up famous for creating the first 3D printed car. However, the brains for Olli were provided by IBM, which contributed its Watson artificial intelligence technology to the vehicle. Together, IBM and Local Motors have produced a vehicle that does far more than simply drive a predetermined route. Instead, Olli combines the capabilities of a chauffeur, a tour guide and a technology expert to communicate with passengers using spoken conversational language. Powered by IBM's Watson Internet of Things (IoT) for Automotive , Olli can take passengers to requested destinations, provide recommendations on where to go and answer questions about the vehicle, the journey and the surrounding environment. Learn more. Read the full story here . The adage 'garbage in, garbage out' may once have held true, particularly in the land of software programming where it originated, but in today's connected world, the Internet of Things is flipping it into 'one man's trash is another man's treasure' - literally. A team from the National Taipei University of Technology's Department of Electronic [...] For those wanting to dive in to IoT, our partnership with Cisco and SilverHook is the perfect example of why you should take the plunge, and get your feet wet... SilverHook powerboats These stunning vehicles reach speeds surpassing 100 MPH in the open ocean. SilverHook Powerboats pride themselves on their advanced monohull designs. These specialized [...] What's the future of mobility? How soon will we see self-driving cars? With ever increasing numbers of us living in cities, will we be able to get around or will we be trapped in perpetual gridlock? These are some of the thorny topics that AutoBlog wrestles with and on 6 October in Detroit, Michigan, they're [...]",en,63
222,2992,1485172712,CONTENT SHARED,5484061377044071389,102305705598210278,5527770709392883642,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36",SP,BR,HTML,https://www.narwhl.com/2015/03/the-ultimate-solution-to-versioning-rest-apis-content-negotiation/,the ultimate solution to versioning rest apis: content negotiation,"Versioning your API is terrifying. If you push out a ""breaking change"" - basically any change that runs counter to what client developers have planned for, such as renaming or deleting a parameter or changing the format of the response - you run the risk of bringing down many, if not all, of your customers' systems, leading to angry support calls or - worse - massive churn. For this reason, versioning is the number one concern among the development teams I work with when helping them design their APIs. The traditional way of handling versioning in REST is to apply some version number to the URL - either in the domain (i.e. apiv1.example.com/resource) or in the resource path (api.example.com/v1/resource). Non-breaking changes are frequently pushed out without much fanfare aside from a changelog posted to a blog or an update to the documentation. Breaking changes, on the other hand, require excessive testing, customer hand-holding, voluminous communication and a high level of planning, code maintenance and creative routing as you bend over backwards to ensure your customers can continue to work smoothly while you delicately push out a new release. With all that, everything can still go wrong. Most API producers handle these issues by placing tight restrictions on their development teams regarding when and how the API may be updated, leading to convoluted policies that often confuse client developers and confound innovation and iteration. For example, Facebook recently described their new versioning strategy for their Marketing API . In the old days, Facebook was rather cavalier about pushing out breaking changes. ""Move fast and break things"" worked fine for them, but annoyed the developers who relied on their API. Though they have apparently learned their lesson, their solution to versioning - which, to be fair, is common among RESTful API providers - prevents their API from taking advantage of continous releases and forces their client developers to assiduously watch for announcements about new releases. There's a better way. REST is closely tied to the HTTP specification, which has long had a way to communicate the format of the data exchanged between client and server - content negotiation . In fact, the ""Representational"" part of ""Representational State Transfer"" (for which REST is named) refers to this directly. Roy Fielding calls this out specifically in his 2000 thesis that defined REST (and which anyone talking about REST is obligated to reference). A resource may express one or more representations of its data based on the state of the resource. You typically see this manifest in APIs that support both JSON and XML responses - the client uses either a file extension in the URI or the ""Accept"" header to request their desired format. But that's just the tip of the iceberg. Resources should support more than simply the top-level format of their data - they should specify exactly what data that response contains. According to the Media Type specifications , you can define your own media types using either the ""vendor tree"" (i.e. ""application/vnd.example.resource+json"") or, to avoid registration requirements, the ""Unregistered x. Tree"" (i.e. ""application/x.example.resource+json""). In either case, you're clearly communicating to the client developer that the response they're receiving has a specified format beyond simply XML or JSON. You will need to provide documentation to your developers to describe the data each media type contains , but you should be doing that already. It seems odd to many to define new media types that effectively take advantage of existing media types. In this case, you can define the specific format of your response using the parameters afforded by the Media Type specification. For example, if I have a resource named ""product"", the media type for its basic JSON repesentation could be ""application/json; profile=vnd.example.product"". Defining the profile as a parameter indicates to the client that they should treat the JSON formatted data in a predetermined way. I have also seen - and advocated for - the looser format ""application/json;vnd.example.product"", but RFC wonks may shout you down on that. Regardless of how you present your media type, the version of your resource should also be reflected in it. For example, if you've made one or more breaking changes to the Product resource, you should add the version parameter to it (i.e. ""application/x.example.product+json; version=2"", ""application/json; profile=vnd.example.product version=2"" or ""application/json;vnd.example.product+v2""). Non-breaking changes should also be reflected using sub-versions (i.e. ""version=2.1""). Removing versioning from the URI in this way has a number of benefits: Rather than versioning the entire API when a single resource has a breaking change, you can version directly at the resource level. This allows teams responsible for only a handful of resources to operate independently and enforces the constraint that a resource is solely responsible for its state. Clients will only need to update their code when the resources they commonly use change rather than every time you upversion your entire API. Your team will no longer need to maintain various routing rules for URIs to redirect them to different back-end code bases, allowing for a cleaner and easier to use and cache set of URIs. Best of all, your API will no longer be tied to arcane release cycles separate from the rest of your application stack. So long as you maintain older versions of your representations for a reasonable period of time, you may release at will without breaking client code. You should also use content negotiation to define the different ""shapes"" of your resource - i.e. ""brief"" vs. ""full"", ""mobile"" vs. ""web"" and whatever else makes the most sense to provide for your customers. You can do this either through an additional custom media type or by adding a parameter (i.e. ""application/x.example.product+json; version=2 shape='full'""). It also introduces a few challenges: While URI routing is no longer an issue, routing of the correct response type gets moved up further in your API application stack. If you've stuck to an MVC pattern, your controller will be responsible for mapping the data from your model into the appropriate data structure while your views will be responsible for presenting it according to the requested media type. Clients will need to know the media type for each resource and request the same one throughout their use of your API to ensure their code continues to function normally as you push out new changes. You will still need to clearly communicate any changes to your API to allow your developers to determine how and when they will need to update their client code accordingly. Most of these challenges can be easily overcome. If you're not already taking advantage of the MVC pattern in your application stack, for example, you'd be wise to consider rearchitecting. Content negotiation doesn't necessarily eliminate the need for multiple code bases, but it does move it into a more manageble part of your code - specifically, the controllers and the views. As far as clients knowing the media type, I'd recommend they either store the media type they're using in a variable that gets passed in the Accept header or they do an ""Accept: / "" on the initial call and locally cache the media type that comes back for use with all subsequent calls. Content negotiation is not a common REST client consideration at this point, but that's no excuse for poor API design. Communicating API changes to developers has always been a challenge. You can tweet about them, update your blog, send an email and even call each developer directly and still have more than a few of them contacting your support team complaining about code broken during an update. At last year's API Craft conference in Detroit, some attendees recommended adding a link to resource responses that have been upversioned as an additional means of communication. I'm particularly fond of this as it gives the client developer the power to decide how to handle such updates directly from within their code. I recommend implementing this now, whether you're using content negotiation to handle your versioning or not. Essentially, if a developer is using an outdated version of the API, they should see an additional link in the response like this: The link relation follows the custom URN format (since this is not an IANA approved relation) with a relation of ""deprecated"". While this is not currently a standard, it should be. The link itself should go to some kind of message resource that contains the details of the deprecation - where you direct that is ultimately up to you. However, this resource should respond with at least two representations - one formatted appropriately for code (i.e. JSON or XML) and the other for human readability (i.e. an HTML page, blog post, changelog, etc.). The data found at this resource should show what the current version is and clearly explain the changes made to the resource that warranted a version change. The client developer should include a routine in their code to look for this link in all responses. If found, the client code should do whatever the developer deems necessary to alert the team responsible for maintaining this code about the change. This may include an email to an internal group, creating a Jira ticket using the details obtained by the deprecation link or any other preferred method guaranteed to get the attention of the team. Content negotiation is a clean, well-documented, standards-compliant way of handling a lot of the complexity found in managing and maintaining RESTful APIs. With everyone clamoring about hypermedia, the media type often gets overlooked. You've nothing to lose by beginning to implement it in your existing RESTful API, but so much to gain in added usability for your developer customers and flexibility and agility for you internal teams.",en,63
223,687,1461959979,CONTENT SHARED,-5756697018315640725,-1443636648652872475,-7777675406803857155,,,,HTML,https://medium.freecodecamp.com/being-a-developer-after-40-3c5dd112210c?gi=62f851a5d81a,being a developer after 40 - free code camp,"Being A Developer After 40 (This is the talk I have given at App Builders Switzerland on April 25th, 2016. The slides are available on SpeakerDeck and at the bottom of this article.) (A version in Russian is available in Habrahabr.ru .) Hi everyone, I am a forty-two years old self-taught developer, and this is my story. A couple of weeks ago I came by the tweet below, and it made me think about my career, and those thoughts brought me back to where it all began for me: I started my career as a software developer at precisely 10am, on Monday October 6th, 1997, somewhere in the city of Olivos , just north of Buenos Aires , Argentina . The moment was Unix Epoch 876142800 . I had recently celebrated my 24th birthday. The World In 1997 The world was a slightly different place back then. Websites did not have cookie warnings. The future of the web were portals like Excite.com . AltaVista was my preferred search engine. My e-mail was kosmacze@sc2a.unige.ch, which meant that my first personal website was located in We were still mourning Princess Lady Diana . Steve Jobs had taken the role of CEO and convinced Microsoft to inject 150 million dollars into Apple Computer. Digital Equipment Corporation was suing Dell. The remains of Che Guevara had just been brought back to Cuba. The fourth season of ""Friends"" had just started. Gianni Versace had just been murdered in front of his house. Mother Teresa , Roy Lichtenstein and Jeanne Calment (the world's oldest person ever) had just passed away. People were playing Final Fantasy 7 on their PlayStation like crazy. BBC 2 started broadcasting the Teletubbies . James Cameron was about to release Titanic . The Verve had just released their hit ""Bitter Sweet Symphony"" and then had to pay most royalties to the Rolling Stones. Smartphones looked like the Nokia 9000 Communicator ; they had 8 MB of memory, a 24 MHz i386 CPU and run the GEOS operating system. Smartwatches looked like the CASIO G-SHOCK DW-9100BJ . Not as many apps but the battery life was much longer. IBM Deep Blue had defeated for the first time Garry Kasparov in a game of chess. A hacker known as ""_eci"" published the C code for a Windows 3.1, 95 and NT exploit called ""WinNuke,"" a denial-of-service attack that on TCP port 139 (NetBIOS) causing a Blue Screen of Death. Incidentally, 1997 is also the year Malala Yousafzai , Chloë Grace Moretz and Kylie Jenner were born. Many film storylines take place in 1997, to name a few: Escape from New York , Predator 2 , The Curious Case of Benjamin Button , Harry Potter and the Half-Blood Prince , The Godfather III and according to Terminator 2: Judgement Day , Skynet became self-aware at 2:14 am on August 29, 1997. That did not happen; however, in an interesting turn of events, the domain google.com had been registered on September 15th that year. We were two years away from Y2K and the media were starting to get people nervous about it. My First Developer Job My first job consisted of writing ASP pages in various editors, ranging from Microsoft FrontPage , to HotMeTaL Pro to EditPlus , managing cross-browser compatibility between Netscape Navigator and Internet Explorer 4, and writing stored procedures in SQL Server 6.5 powering a commercial website published in Japanese, Russian, English and Spanish - without any consistent UTF-8 support across the software stack. The product of these efforts ran in a Pentium II server hosted somewhere in the USA, with a stunning 2 GB hard disk drive and a whooping 256 MB of RAM. It was a single server running Windows NT 4 , SQL Server 6.5 and IIS 2.0 , serving around ten thousand visitors per day. My first professional programming language was this mutant called VBScript , and of course a little bit of JavaScript on the client side, sprinkled with lots of ""if this is Netscape do this, else do that"" because back then I had no idea how to use JavaScript properly. Interestingly, it's 2016 and we are barely starting to understand how to do anything in JavaScript. Unit tests were unheard of. The Agile Manifesto had not been written yet. Continuous integration was a dream. XML was not even a buzzword. Our QA strategy consisted of restarting the server once a week, because otherwise it would crash randomly. We developed our own COM+ component in Visual J++ to parse JPEG files uploaded to the server. As soon as JPEG 2000 -encoded files started popping up, our component broke miserably. We did not use source control, not even CVS , RCS or, God forbid, SourceSafe . Subversion did not exist yet. Our Joel Test score was minus 25. 6776 Days For the past 6776 days I have had a cup of coffee in the morning and wrote code with things named VBScript, JavaScript, Linux, SQL, HTML, Makefiles, Node.js, CSS, XML, .NET, YAML, Podfiles, JSON, Markdown, PHP, Windows, Doxygen, C#, Visual Basic, Visual Basic.NET, Java, Socket.io, Ruby, unit tests, Python, shell scripts, C++, Objective-C, batch files, and lately Swift. In those 6776 days lots of things happened; most importantly, my wife and I got married. I quit 6 jobs and I was fired twice. I started and closed my own business. I finished my Master Degree. I published a few open source projects, and one of them landed me an article on Ars Technica by Erica Sadun herself . I was featured in Swiss and Bolivian TV shows. I watched live keynotes by Bill Gates and by Steve Jobs in Seattle and San Francisco. I spoke at and co-organised conferences in four continents. I wrote and published two books . I burned out twice (not the books, myself,) and lots of other things happened, both wonderful and horrible. I have often pondered about leaving the profession altogether. But somehow, code always calls me back after a while. I like to write apps, systems, software. To avoid burning out, I have had to develop strategies. In this talk I will give you my secrets, so that you too can reach the glorious age of 40 as an experienced developer, willing to continue in this profession. Advice For The Young At Heart Some simple tips to reach the glorious age of 40 as a happy software developer. 1. Forget The Hype The first advice I can give you all is, do not pay attention to hype. Every year there is a new programming language, framework, library, pattern, component architecture or paradigm that takes the blogosphere by storm. People get crazy about it. Conferences are given. Books are written. Gartner hype cycles rise and fall. Consultants charge insane amounts of money to teach, deploy or otherwise fuckup the lives of people in this industry. The press will support these horrors and will make you feel guilty if you do not pay attention to them. In 2000 it was SOAP & XML. In 2003 it was Model Driven Architecture and Software Factories . In 2006 it was Semantic Web and OLPC . In 2009 it was Augmented Reality . In 2012 it was Big Data . In 2015... Virtual Reality? Bots? Do not worry about hype. Keep doing your thing, keep learning what you were learning, and move on. Pay attention to it only if you have a genuine interest, or if you feel that it could bring you some benefit in the medium or long run. The reason for this lies in the fact that, as the Romans said in the past, Nil nove sul sole. Most of what you see and learn in computer science has been around for decades, and this fact is purposedly hidden beneath piles of marketing, books, blog posts and questions on Stack Overflow. Every new architecture is just a reimagination and a readaptation of an idea that was floating around for decades. 2. Choose Your Galaxy Wisely In our industry, every technology generates what I call a ""galaxy."" These galaxies feature stars but also black holes; meteoric changes that fade in the night, many planets, only a tiny fraction of which harbour some kind of life, and lots of cosmic dust and dark matter. Examples of galaxies are, for example, .NET, Cocoa, Node.js, PHP, Emacs, SAP, etc. Each of these features evangelists, developers, bloggers, podcasts, conferences, books, training courses, consulting services, and inclusion problems. Galaxies are built on the assumption that their underlying technology is the answer to all problems. Each galaxy, thus, is based in a wrong assumption. The developers from those different galaxies embody the prototypical attitudes that have brought that technology to life. They adhere to the ideas, and will enthusiatically wear the t-shirts and evangelize others about the merits of their choice. Actually, I use the term ""galaxy"" to avoid the slightly more appropriate if not less controversial term ""religion,"" which might describe this phenomenon better. In my personal case, I spent the first ten years of my career in the Microsoft galaxy, and the following nine in the Apple galaxy. I dare say, one of the biggest reasons why I changed galaxies was Steve Ballmer. I got tired of the general attitude of the Microsoft galaxy people against open source software. On the other hand, I also have to say that the Apple galaxy is a delightful place, full of artists and musicians and writers who, by chance or ill luck, happen to write software as well. I attended conferences in the Microsoft galaxy, like the Barcelona TechEd 2003, or various Tech Talks in Buenos Aires, Geneva or London. I even spoke at the Microsoft DevDays 2006 in Geneva. The general attitude of developers in the Microsoft galaxy is unfriendly, ""corporate"" and bound in secrecy, NDAs and cumbersome IT processes. The Apple galaxy was to me, back in 2006, exactly the opposite; it was full of people who were musicians, artists, painters; they would write software to support their passion, and they would write software with passion. It made all the difference, and to this day, I still enjoy tremendously this galaxy, the one we are in, right now, and that has brought us all together. And then the iPhone came out, and the rest is history. So my recommendation to you is: choose your galaxy wisely, enjoy it as much or as little as you want, but keep your telescope pointed towards the other galaxies, and prepare to make a hyperjump to other places if needed. 3. Learn About Software History This takes me to the next point: learn how your favorite technology came to be. Do you like C#? Do you know who created it? How did the .NET project came to be? Who was the lead architect? Which were the constraints of the project and why did the language turned out to be what it is now? Apply the same recipe to any language or CPU architecture that you enjoy or love: Python, Ruby, Java, whatever the programming language; learn their origins, how they came up to be. The same for operating systems, networking technologies, hardware, anything. Go and learn how people came up with those ideas, and how long they took to grow and mature. Because good software takes ten years , you know. The stories surrounding the genesis of our industry are fascinating, and will show you two things: first, that everything is a remix . Second, that you could be the one remixing the next big thing. No, scratch that: you are going to be the creators of the next big thing. And to help you get there, here is my (highly biased) selection of history books that I like and recommend: You will also learn to value those things that stood the test of time: Lisp , TeX , Unix , bash , C , Cocoa , Emacs , Vim , Python , ARM , GNU make , man pages . These are some examples of long-lasting useful things that are something to celebrate, cherish and learn from. 4. Keep on Learning Learn. Anything will do. Wanna learn Fortran? Go for it. Find Erlang interesting? Excellent. Think COBOL might be the next big thing in your career? Fantastic. Need to know more about Functional Reactive Programming ? Be my guest. Design? Of course. UX? You must. Poetry? You should . Many common concepts in Computer Science have been around for decades, which makes it worthwhile to learn old programming languages and frameworks; even ""arcane"" ones. First, it will make you appreciate the current state of the industry (or hate it, it depends,) and second, you will learn how to use the current tools more effectively - if anything, because you will understand its legacy and origins. Tip 1: learn at least one new programming language every year. I did not come up with this idea; The Pragmatic Programmer book did. And it works. One new programming language every year. Simple, huh? Go beyond the typical ""Hello, World"" stage, and build something useful with it. I usually build a simple calculator with whatever new technology I learn. It helps me figure out the syntax, it makes me familiar with the APIs or the IDE, etc. Tip 2: read at least 6 books per year. I have shown above a list of six must-read books; that should keep you busy for a year. Here goes the list for the second year: (OK, those are seven books.) Six books per year looks like a lot, but it only means one every 2 months. And most of the books I have mentioned in this presentation are not that long, and even better, they are outstandingly well written, they are fun and are full of insight. Look at it this way: if you are now 20 years old, by the age of 30 you will have read over 60 books, and over 120 when you reach my age. And you will have played with at least 20 different programming languages. Think about it for a second. Some of the twelve books I've selected for you have been written in the seventies, others in the eighties, some in the nineties and finally most of them are from the past decade. They represent the best writing I have come across in our industry. But do not just read them; take notes. Bookmark. Write on the pages of the books. Then re-read them every so often. Borges used to say that a bigger pleasure than reading a book is re-reading it. And also, please, buy those books you really like in paper format. Believe me. eBooks are overrated. Nothing beats the real thing. Of course, please know that as you will grow old, the number of things that qualify as new and/or important will drop dramatically. Prepare for this. It is OK to weep silently when you realise this. 5. Teach Once you have learnt, teach . This is very important. This does not mean that you should setup a classroom and invite people to hear your ramblings (although it would be awesome if you did!) It might mean that you give meaningful answers to questions in Stack Overflow; that you write a book; that you publish a podcast about your favorite technology; that you keep a blog; that you write on Medium; that you go to another continent and set up programming schools using Raspberry Pis; or that you help a younger developer by becoming their mentor (do not do this before the age of 30, though.) Teaching will make you more humble, because it will painfully show you how limited your knowledge is. Teaching is the best way to learn . Only by testing your knowledge against others are you going to learn properly. This will also make you more respectful regarding other developers and other technologies; every language, no matter how humble or arcane, has its place within the Tao of Programming , and only through teaching will you be able to feel it. And through teaching you can really, really make a difference in this world. Back in 2012 I received a mail from a person who had attended one of my trainings. She used to work as an Adobe Flash developer. Remember ActionScript and all that? Well, unsurprisingly after 12 years of working as a freelance Flash developer she suddenly found herself unemployed. Alone. With a baby to feed. She told me in her message that she had attended my training, that she had enjoyed it and also learnt something useful, and that after that she had found a job as a mobile web developer. She wrote to me to say thank you . I cannot claim that I changed the world, but I might have nudged it a little bit, into something (hopefully) better. This thought has made every lesson I gave since then much more worthwhile and meaningful. 6. Workplaces Suck Do not expect software corporations to offer any kind of career path. They might do this in the US, but I have never seen any of that in Europe. This means that you are solely responsible for the success of your career. Nobody will tell you ""oh, well, next year you can grow to be team leader, then manager, then CTO..."" Not. At. All. Quite the opposite, actually: you were, are and will be a software developer, that is, a relatively expensive factory worker, whose tasks your managers would be happy to offshore no matter what they tell you. Do not take a job just for the money. Software companies have become sweatshops where you are supposed to justify your absurdly high salary with insane hours and unreasonable expectations. And, at least in the case of Switzerland, there is no worker union to help you if things go bad. Actually there are worker unions in Switzerland, but they do not really care about situations that will not land them some kind of media exposure. Even worse; in most workplaces you will be harassed, particularly if you are a woman, a member of the LGBT community or from a non-caucasian ethnic group. I have seen developers threatened to have their work visas not renewed if they did not work faster. I have witnessed harassment of women and gay colleagues. Some parts of our industry are downright disgusting, and you do not need to be in Silicon Valley to live it. You do not need Medium to read it. You could experience that right here in Switzerland. Many banks have atrocious workplaces. Financial institutions want you to vomit code 15 hours a day, even if the Swiss working laws explicitly forbid such treatments. Pharmaceutical companies want you to write code to cheat test results and to help them bypass regulations. Startups want your skin, working for 18 hours for no compensation, telling you bullshit like ""because we give you stock options"" or ""because we are all team players."" It does not matter that you are Zach Holman and that you can claim in your CV that you literally wrote Github from scratch: you will be fired for the pettiest of reasons. It does not matter that the app brings more than half of your employer traffic and revenues; the API team will treat you and your ideas with contempt and sloppiness. I have been asked to work for free by very well known people in the industry, some of them even featured in Wikipedia, and it is simply appalling. I will not give out their names, but I will prevent any junior from getting close to them, because people working without ethics do not deserve anyone's brain. Whenever an HR manager tells you ""you must do this (whatever wrong thing in your frame of reference) because we pay you a salary,"" remember to answer the following: ""you pay me a salary, but I give you my brain in exchange, and I refuse to comply with this order."" And to top it all, they will put you in an open space, and for some reason they will be proud about it. Open spaces are a cancer. They are without a doubt the worst possible office layout ever invented, and the least appropriate for software development - or any type of brain work for that matter. Remember this: the fact that you understand something does not imply that you have to agree to it. Disobey authority. Say ""fuck you, I won't do what you tell me"" and change jobs. There are fantastic workplaces out there; not a lot, but they exist. I have been lucky enough to work in some of them. Do not let a bad job kill your enthusiasm. It is not worth it. Disobey and move on. Or, better yet, become independent. 7. Know Your Worth You have probably heard about the ""10x Software Engineer"" myth, right? Well here is the thing: it is not a myth, but it does not work they way you think it works. It works, however, from the point of view of the employer: a ""10x Software Engineer"" generates worth 10 times whatever the employer pays. That means that you she or he gets 100 KCHF per year, but she or he are actually creating a value worth over a million francs. And of course, they get the bonuses at the end of the fiscal year, because, you know, capitalism. Know your worth. Read Karl Marx and Thomas Piketty . Enough said. Keep moving; be like the shark that keeps on swimming, because your skills are extremely valuable. Speak out your salary, say it loud, blog about it, so that your peers know how much their work is worth. Companies want you to shut up about that, so that women are paid 70% of what men are paid. So speak up! Blog about it! Tweet it! I am making 135 KCHF per year. That was my current salary. How about you? And you? The more we speak out, the less inequality there will be. Any person doing my job with my experience should get the same money, regardless of race, sex, age or preferred football team. End of the story. But it is not like that. It is not. 8. Send The Elevator Down If you are a white male remember all the privilege you have enjoyed since birth just because you were born that way. It is your responsibility to change the industry and its bias towards more inclusion. It is your duty to send the elevator down. Take conscious decisions in your life. Be aware of your actions and their effect. Do not blush or be embarrased for changing your opinions. Say ""I'm sorry"" when required. Listen. Do not be a hotshot. Have integrity and self-respect. Do not critisize or make fun of the technology choices of your peers; for other people will have their own reasons to choose them, and they must be respected. Be prepared to change your mind at any time through learning. One day you might like Windows. One day you might like Android. I am actually liking some parts of Android lately. And that is OK. 9. LLVM Everybody is raving about Swift, but in reality what I pay more attention to these days is LLVM itself. I think LLVM is the most important software project today, as measured in its long-term impact. Objective-C blocks, Rust & Swift (the two most loved strongly typed and compiled programming languages in the 2016 StackOverflow developer survey ,) Dropbox Pyston , the Clang Static Analyser, ARC, Google Souper , Emscripten , LLVMSharp , Microsoft LLILC , Rubymotion , cheerp , watchOS apps, the Android NDK , Metal , all of these things were born out or powered by LLVM. There are compilers using LLVM as a backend for pretty much all the most important languages of today. The .NET CLR will eventually interoperate with it, and Mono already uses it. Facebook has tried to integrate LLVM with HHVM , and WebKit recently switched from LLVM to the new B3 JIT JavaScript compiler . LLVM is cross-platform , cross-CPU-architecture, cross-language, cross-compiler, cross-eyed-tested, free as in gratis and free as a bird. Learn all you can about LLVM. This is the galaxy where true innovation is happening now. This is the foundation for the next 20 years. 10. Follow Your Gut I had the gut feeling .NET was going to be big when I watched its introduction in June 2000 . I had the gut feeling the iPhone was going to be big when I watched its introduction in 2007 . In both cases people laughed at my face, literally. In both cases I followed my gut feeling and I guess things worked out well. Follow your gut. You might be lucky, too. 11. APIs Are King Great APIs enable great apps. If the API sucks, the app will suck, too, no matter how beautiful the design. Remember that chunky is better than chatty , and that clients should be dumb; push as much logic as you can down to the API. Do not invent your own security protocols. Learn a couple of server-side technologies, and make sure Node is one of those. Leave REST aside and embrace Socket.io, ZeroMQ, RabbitMQ, Erlang, XMPP; explore realtime as the next step in app development. Realtime is not only for chat apps. Remove polling from the equation forever. Oh, and start building bots around those APIs. Just saying. 12. Fight Complexity Simpler is better. Always. Remember the KISS principle. And I do not mean only at the UI level, but all the way until the deepest layers of your code. Refactoring, unit tests, code reviews, pull requests, all of these tools are at your disposal to make sure that the code you ship is the simplest possible architecture that works. This is how you build resilient systems for the long term. Conclusion The most important thing to remember is that your age does not matter. One of my sons said to me, ""Impossible, Dad. Mathematicians do all their best work by the time they're 40. And you're over 80. It's impossible for you to have a good idea now."" If you're still awake and alert mentally when you're over 80, you've got the advantage that you've lived a long time and you've seen many things, and you get perspective. I'm 86 now, and it's in the last few years that I've had these ideas. New ideas come along and you pick up bits here and there, and the time is ripe now, whereas it might not have been ripe five or 10 years ago. Michael Atiyah , Fields Medal and Abel Prize winner Mathematician, quoted in a Wired article. As long as your heart tells you to keep on coding and building new things, you will be young, forever. In 2035, exactly 19 years from now, somebody will give a talk at a software conference similar to this one, starting like this: ""Hi, I am 42 years old, and this is my story."" Hopefully one of you will be giving that presentation; otherwise, it will be an AI bot. You will provide some anecdotical facts about 2016, for example that it was the year when David Bowie , Umberto Eco , Gato Barbieri and Johan Cruyff passed away, or when SQL Server was made available in Linux , or when Google AlphaGo beat a Go champion, or when the Panama Papers and the Turkish Citizenship Database were leaked the same day, or when Google considered using Swift for Android for the first time , or as the last year in which people enjoyed this useless thing called privacy. We will be three years away from the Year 2038 Problem and people will be really nervous about it. Of course I do not know what will happen 19 years from now, but I can tell you three things that will happen for sure: Somebody will ask a question in Stack Overflow about how to filter email addresses using regular expressions. Somebody will release a new JavaScript framework. Somebody will build something cool on top of LLVM. And maybe you will remember this talk with a smile. Thank you so much for your attention.",en,63
224,1950,1470071010,CONTENT SHARED,5293701842202310496,-6067316262393890508,2245546651160341154,,,,HTML,https://contently.com/strategist/2016/07/29/can-accenture-take-over-advertising/,can accenture take over advertising?,"Last month, in New York City's Soho neighborhood, global strategic consulting firm Accenture did something that might surprise you: It opened a 10,000-square-foot content studio. The goal? To develop advertising and marketing creative for its clients. Accenture's content studio includes a 3,500-square-foot post-production facility where the firm has the ability to cut and edit anything from a 30-second Facebook video to a Super Bowl spot. And there are six more studios in the works. The move is the latest in a string of developments that are pitting consultancies against agencies. Both are battling for content marketing dollars, relationships with C-suite executives, and above all, longterm partnerships with brands than transcend individual campaigns-and the conflict threatens to change the landscape of the advertising world. An opening In the past, consultancies focused on providing industry-specific business and management advice to brand clients. Now, thanks to an opening created by the digital disruption of advertising, they've created studios and digital agency arms meant to attract business that traditionally went to advertising agencies. How big are digital agencies born of strategic consultancy firms? Research firm and online resource Econsultancy ranked digital agencies based on their earnings for its "" Top 100 Digital Agencies "" report. Of the top five agencies, three-IBM iX, Accenture Interactive, and Deloitte Digital UK-are subsidiaries of consultancies. Plenty of the services offered by consultancy agencies are also offered by traditional ad agencies: marketing management, branding strategy, ad and content development, media planning, and so on. What consultancy agencies like Accenture Interactive and Deloitte Digital claim is that they have advantages ad agencies simply can't match, like vertical experts, global consumer insight, the manpower to produce thousands of pieces of content, and-because of their existing consulting relationships-a better understanding of how digital marketing can fit into an overall business strategy. In other words, consultancy agencies are leveraging their relationship with their larger consultancy parent companies for an edge. Meanwhile, ad agencies find themselves under assault by publisher agencies and major tech platforms , and the agency-of-record (AOR) model on which agency-marketer relationships have long been built-whereby companies assign their creative work to a single agency-appears to be waning. In recent years, Mondelez , Best Buy , and Frito-Lay have all moved away from AORs to instead work with multiple specialized shops. At last year's Association of National Advertisers' Masters of Marketing event, the president of PepsiCo's global beverage group warned that ""the agency model is not going to bend-it's going to break."" Are consultancy agencies, with their cross-business acumen and resources, poised to disrupt an industry that dates back centuries ? ""There is no doubt that traditional agencies are feeling the heat,"" said Susan Borst, senior director of industry initiatives with the Interactive Advertising Bureau (IAB). ""This is definitely an interesting time in ad land."" How media's transformation transformed consultancies Accenture's shift toward digital content marketing began several years ago with the launch of Accenture Digital in 2013. The company acquired London-based design firm Fjord in 2013, followed by Austin-based creative studio Chaotic Moon last year. What was starting to happen is that Accenture was being asked to ""step into the creative space,"" said Donna Tuths, Accenture Interactive's managing director and global head of digital content. The thinking was that if Accenture already handled companies' business strategy and intelligence, technology, infrastructure, and cloud computing needs, why not take on digital marketing, mobility, and analytics, too? At the same time, clients were increasingly ""decoupling"" their advertising production, meaning that they transferred video duties away from their agencies to work with production and post-production companies directly. That created an opportunity for Accenture to swoop in with a full-service digital offering. Accenture Interactive (part of Accenture Digital) categorizes its services as Experience, Marketing, Commerce, and Content, with Content being the largest. ""I see many of these things as no longer being the domain of agencies,"" said Tuths, who worked with such agencies as Ogilvy and Organic prior to joining Accenture. ""In a virtual world, you are your content. ... Consumers have begun to understand that, while many companies are just beginning to."" What's more, she said, ""They're starting to see the folly in trusting their brands to short-term, campaign-driven partners."" The move from a campaign mindset to an always-on mentality factored into the growth and success of strategic consultancy agency Deloitte Digital, which launched in 2012, as well. Alan Schulman, national director of brand creative and content marketing for Deloitte Digital, spent much of the last two decades making TV campaigns and accompanying digital content for major agencies. But he believes that approach is on its way out. ""You can't just go from campaign to campaign anymore,"" Schulman said, referring to the way many brands work with agencies on a project by project basis. ""We're now living in a time when consumers can turn your brand on or off 24/7, 365 days, so you better have stuff out there."" But Denise Blasevick, CEO of advertising and public relations agency The S3 Agency in New Jersey, says there's still a place for campaigns in the modern marketing world. ""For brand-driven organizations, I see the right mix as combining carefully pulsed content with appropriate punches of campaign advertising,"" she explained. ""That mix of strategic planning and creative execution remains the stronghold of traditional agencies."" Some in the agency world also maintain consultancies simply can't match the creative power and experience of ad agencies. ""Our business is-at the core- about ideas,"" said Laurent Ezekiel, managing director of the New York region and global client services director with DigitasLBi . ""And whilst our business has evolved and we live in an era of fragmentation, the alchemy of creativity and technology is where the magic happens for brands."" Ezekiel adds that DigitasLBi, which is owned by multinational advertising company Publicis Groupe and ranked seventh on Econsultancy's top agency list , has the benefit of experience. ""Since we, the agencies, have been in the creative business for several generations, clients will continue to engage with us to help evolve their brands."" The C-suite access advantage Schulman says he's been ""mightily impressed"" with what he's seen at Deloitte Digital since joining just over a year ago. That includes having direct access to decision makers on the brand client side, an advantage ad agencies have a difficult time matching. ""Consultancies are already pretty embedded in the C-suite, so it makes sense to start from strategy first and go from there to content,"" Schulman said, noting that all strategic directives come from Deloitte Consulting's Strategy & Operations group, while Deloitte Digital provides the copywriting, art direction, user experience, experiential design, and customer experience know-how. Bridging the gap between content and business operations, he notes, helps Deloitte Digital ensure that it's not just making buzz-worthy creative, but meeting its clients' overall and long-term goals. But for agencies of all kinds, a challenge remains: convincing CFOs of the critical importance of content. Digital content can be slow to deliver a return on investment. According to the Content Marketing Institute, it can take upwards of 15 months to monetize a content marketing program-which can make it a tough sell. The Content Marketing Institute also reports that only 30 percent of B2B marketers believe their organizations are effective at content marketing. Just 37 percent of B2C marketers say their content marketing is ""sophisticated or mature."" ""There's the tech, plumbing layer of marketing, and there's the poetry, creative layer,"" Schulman explained. ""Getting big organizations to think of these together and switch from a campaign structure to content marketing is not easy."" Deloitte Digital offers workshops and education tools around ideation, workflow process, and implementing creative to its C-Suite customers with the aim of getting them on board for the long haul. Schulman adds that CFOs ""glaze over pretty quickly"" when agencies talk about likes and shares. ""They want to know you were able to move the brand, advocacy, commercial results, or engagement from X to Y,"" he said. When a firm already has a CFO's ear-as is the case with Deloitte's clients, who already use the company for strategy and business operations advice-it stands to reason that it has an edge over traditional agencies, which tend to work primarily with heads of marketing. Cross-industry capability, global strength Most of today's digital agencies are either boutique shops or offshoots of multinational advertising networks. Deloitte and Accenture believe that what they can offer in terms of consumer insight and industry expertise outstrips both of these models. ""In my agency career I never encountered such a depth of vertical industry expertise and research as what we have at Deloitte,"" said Schulman. ""As a former agency person, it's refreshing to be getting the kind of access that I have now."" Deloitte Digital, which has six offices in the U.S. and 33 abroad, boasts experts in consumer and industrial products, energy and resources, federal, financial services, life sciences and healthcare, public sector, technology, media, and telecom. ""That enables me as head of the content group to reach out and staff someone from a vertical industry who's really steeped in what's going on,"" Schulman said. That expert then works in tandem with content strategy and content creation to form a ""very purpose-built team."" Accenture, meanwhile, has about 5,000 employees working in the content space, spanning about 20 locations around the globe. ""We operate some of the largest content operations in the world,"" Tuths said. ""Even big agencies just are not able to house the same range of technical skills or attract the same level of talent."" As an example, Tuths points to one of her retail clients, whom she opted not to name in order to protect its marketing strategy. The company offers monthly subscriptions to a box of curated products. For every box, the brand has been creating four how-to videos, but it will soon be adding both more boxes and more products, so its needs for instructional videos are becoming increasingly complex. ""Honestly, these types of challenges are not really for your creative agency,"" she said, pointing out that many agencies outsource their video work. ""We take a much more strategic and long-term approach to the content space. It's about today, but more about tomorrow and helping our clients navigate the space and grow."" DigitasLBi's Ezekiel counters that agencies have evolved more than some think. ""Honestly, six or seven years ago we were not set up in a way [that] our markets could benefit from each other's knowledge, and this is because there was no pressure on the demand side,"" he said. ""Essentially, we didn't have enough global clients 'stitching' the network together."" That has since changed, largely through the growth of the agency's global client base. ""It's through these global clients, who represent over 35 percent of our revenues today, that we have developed a clear process and toolkit for multi-market engagements."" And the winner is? Last year, media research firm PQ Media reported that by the year 2019, U.S. brands will be spending upwards of $300 billion per year on content marketing. Will they opt to stick with the agencies they know, or will they jump ship to new competitors like consultancies? Or will they opt for a third route and increasingly bring content marketing in-house with the help of freelancers and content marketing technology solutions, as companies like Coca-Cola and Cisco have done? We know where consultancies are placing their bets-so how do agencies view these new competitors? ""I think both types of companies have their unique skills, experience, and heritage,"" Ezekiel said. ""The digital creative agencies will always put ideas first and therefore are set apart from the consultancies. The consultancies have a strong respected heritage in business consulting. The convergence of these two is where the future lies, but we believe that the brands that focus on ideas and their reasons to exist will prevail."" ""I would argue that brand-driven companies will win with traditional agencies,"" said Blasevick. ""The content [we] create is driven by the brand and not the channel. I have never heard a single person complain that there is a shortage of content out there. The opposite is true: people are feeling overwhelmed by content-and because of that, they are tuning out if it isn't on brand."" According to the IAB's Susan Borst, the spoils will belong to the firms that best grasp advertising's technological transformation and the ""radically new way of thinking about creative and messaging"" that it requires. ""Those who crack the digital and mobile nut,"" she said, ""with clients that understand and embrace the need for fundamental change, will win in the long-term.""",en,63
225,2384,1474400010,CONTENT SHARED,-7623502978685822577,-709287718034731589,-7053709891702726147,,,,HTML,https://blog.nugget.one/2016/09/04/dont-start-big-start-a-little-snowball/,"don't start big, start a little snowball","Little Snowballs When myself and my co-host interviewed Travis Kalanick on our podcast, he had recently co-founded a little snowball called UberCab. It was so early in Uber's existence he didn't even mention it. I notice Uber falls into a category of companies I call little snowballs. There are some fundamental features these companies have in common. I thoguht it might be helpful to list a few little snowballs and then talk about how you can go about starting your own. Uber Travis Kalanick and Garrett Camp commissioned the creation of a simple app that enabled users to hail a black car on demand. To validate they asked friends and colleagues to install the app. They hired a driver & watched what happened. After a few months of hustle the app hit 10 rides in one day. Airbnb Brian Chesky and Joe Gebbia bought a few airbeds and put up a static site called ""Air Bed and Breakfast"". They expanded the concept so that other people could offer airbeds in their homes and make money too. Then they expanded the idea to rooms, then to entire apartments. Their journey was a hustle from hell, but they made it happen. Google Sergey Brin and Larry Page built a web search engine. It was one page with a search box and submit button. The backend was a simple database search returning ranked results based on the number of backlinks. It did a really good job at providing search results, and took off. Slack Stewart Butterfield worked with his Tiny Speck group to build a better team messaging app. Slack makes it really easy for people to signup and start chatting as a team. At it's core Slack is a simple IM client. Other examples AT&T, Dollar Shave Club, Buffer, Shazam, DropBox What is a Little Snowball? ""A startup that is founded to do-one-simple-thing-well, and that can be massively grown outward from that starting point."" Typical markers: Very simple idea with a single facet Extremely low product surface area (frontend and backend) Can generally be prototyped and brought to market in months Is instantly useful and quickly gains early-adopter traction Each of these markers makes it considerably easier for you as a founder to start and grow the first version of your business. A Simple Single Facet Just about every part of growing your business becomes easier if the foundational idea is do-one-thing-well. I've been part of a number of startups where it was almost impossible to explain what we do in a single sentence. This wreaks havok during sales meetings and investor pitches. However, when your startup is a single cell organism, customers and investors are not confused about what you do. Word of mouth and viral loop growth becomes significantly easier to achieve. You can focus on growing the business, rather than trying to understand what the business is. A Low Product Surface Area This is a huge, almost unfair, advantage. Dollar Shave Club is the ultimate example. An idea you can build and launch in a weekend. Then, from Monday forward, spend all your time, effort and money on marketing and learning about your customers. I learnt this lesson the hard way. I lunched my tool Pluggio, a tool that buffers social content, at the same time as Buffer. Buffer focused on doing one-thing-well while I built Pluggio out to become a social media dashboard. Ironically, no matter how many features I added, Pluggio's main selling point continued to be social buffering. So, while I was adding 100k lines of code to make a really slick single page web app, buffer was marketing and integrating their very simple app. Due to my high product surface area my time was sucked up by support requests. Sales were harder because the product was harder to explain. Integrations were a non starter because the product was to complex. After 3 years Pluggio's yearly revenue was $50k and Buffer's was $1m. Fast to Prototype and Launch The sooner you can get to market the sooner you can validate and start learning about how your customers make purchase or adoption decisions. Often times you can quickly validate a business by taking very manual steps that don't scale. For example, if you wanted to validate the business hypothesis - People will order coffee from a delivery app - you could get 250-500 business cards printed out saying - We deliver coffee. Call us on 123-123-1234. - then hand them out at the mall at breakfast time. The very next day you will have some great information about your basic hypothesis. Build Something People Want The final little snowball component is to make sure your core idea is something people actually want. Without that je ne sais quoi your rolling stone will gather no moss. How Can I Find My Own Little Snowball? Full disclosure, I am founder of Nugget (startup incubator & community). Our primary purpose is to help you find, and grow, your own little snowball. That said, here are some tips to help you come up with off the cuff ideas: Think Small This sounds easier than it is. Many founders think at large scale because they want to build a Google or Uber. They feel it's too much of a gamble to think about smaller things. Another common trap is to get caught in the mindset of thinking more features equals more sales. To think small, you should notice details of everyday problems going on around you and try to find do-one-thing-well opportunities. Notice Problems Problems are everywhere. The trick is to open your consciousness to the stream of problems that life presents us with. You would be surprised at how capable we are of drowning out regularly annoying tasks that become part of our daily life. Now it's time to start to notice those things we've trained ourself to ignore. One note about this, it's very important you solve a problem that other people have. That sounds like obvious advice, but if no one else has the problem, you're dead in the water. Look For Optimizations Optimization essentially means - to make something better - and is really just another way to solve a problem. However, thinking through the mental model of optimization, can lead you to come up with some interesting product ideas. For example the core essence of Buffer is an optimization of Twitter. As a side note. Question: Why didn't Twitter simply add that feature when they saw it take off in Buffer, Hootsuite and other apps? Answer: They were too busy staying on task and building their own little snowball. Explore Old Ideas You probably already have a number of ideas you've been thinking about and possibly even products you've launched. Have another look at them through the lens of what we've discussed here. With any luck you might find a single facet that can be extracted from that old work and turned into a do-one-thing-well little snowball. . . . I'm building Nugget (startup incubator & community). We send a new SaaS startup idea to your inbox every single day. Signup for free .",en,63
226,1227,1464890552,CONTENT SHARED,-8771338872124599367,3609194402293569455,6147896666617046192,,,,HTML,https://medium.com/coolhow-creative-lab/funcion%C3%A1rios-do-m%C3%AAs-no-coolhow-os-slackbots-32bbcc322575,funcionários do mês no coolhow: os slackbots - coolhow creative lab,"Funcionários do mês no CoolHow: os Slackbots Vamos ser sinceros, nem sempre é fácil mudar os costumes e adotar novas técnicas ou ferramentas. O Slack também não foi amor à primeira vista, mas hoje em dia é o queridinho do CoolHow - ainda mais depois que a Íris descobriu os bots maravilhosos da plataforma. Para os que veem o Slack ""apenas"" como uma forma de comunicação e organização, hora do mindblow: dá pra pedir Uber , falar com psicólogos de verdade, jogar Pokémon e fazer basicamente qualquer coisa nele. E hoje vamos apresentar os bots que andam nos ajudando demais. Se você precisa analisar métricas do Google Analytics , New Relic ou Mixpanel , conheça o seu novo melhor amigo: Statsbot. Com ele, você pode conectar os dados das plataformas diretamente ao Slack e pedir que o bot produza relatórios diários, semanais ou mensais. Caso você tenha várias contas, você ainda pode dividi-las em canais e ajustar para relatar apenas os dados necessários. Obrigado por existir, Statsbot! ❤ Quase uma mãe para os que vivem esquecendo as coisas, o Luno responde dúvidas de forma automática. Por exemplo: se você ou sua equipe querem ter o e-mail do cliente sempre à mão, basta inserir a pergunta ""qual é o e-mail do cliente?"" e a resposta está em sua base de dados. Da próxima vez que bater a dúvida, basta perguntar. O fofoqueiro, mas do bem. Sempre que sua empresa ou outra palavra-chave for mencionada, ele te conta na hora. Ele pesquisa, gratuitamente, menções no Twitter, Tumblr, Hacker News, Produc Hunt e Reddit, mas a versão beta está disponibilizando Google +, Instagram e YouTube, que futuramente serão pagos. Fontes como Facebook, Blogs, portais de notícia, Medium e outras plataformas (igualmente pagas) ainda serão adicionadas. Agora que você é o mestre dos bots da empresa, seja também o mais legal. Diariamente, o Humblebot dá dicas para que você se torne uma pessoa melhor. O nosso de hoje foi reconhecer alguém por algo que ele tenha feito recentemente. Bônus: Facebook Bot O And Chill não é do Slack, mas merece estar aqui. Disponível para SMS ou Facebook Messenger, o bot te ajuda a escolher o próximo filme que você assistirá. Como? Primeiro, ele pede que você cite um filme que gostou e porquê. A partir de um algorítimo secreto que seus criadores não revelaram como funciona, ele sugere alguns outros filmes. Achou bom demais para ser verdade? A galera da Tech Crunch também, e eles contam como foi colocar o bot à prova.",pt,63
227,441,1460735810,CONTENT SHARED,3689128258624052102,-5803328010978180572,166134078931949885,,,,HTML,http://www.jornaldacidadeonline.com.br/noticias/2501/deputada-mais-jovem-da-camara-silencia-o-plenario-com-lucidez-do-discurso-veja-o-video,deputada mais jovem da câmara silencia o plenário com lucidez do discurso (veja o vídeo),"Deputada mais jovem da Câmara silencia o plenário com lucidez do discurso (veja o vídeo) Mariana Carvalho, deputada de Rondônia, 29 anos, médica e advogada, surpreendeu o plenário da Câmara Federal com o discurso mais incisivo e lúcido até o momento, a favor do impeachment da presidente Dilma Rousseff. A paulistana, radicada em Rondônia, onde iniciou suas atividades políticas com apenas 16 anos de idade, conseguiu em apenas 15 minutos, sintetizar todos os motivos e motivações que frustraram a população brasileira, após a reeleição da presidente Dilma Rousseff. O plenário da Câmara silenciou para ouvir a jovem parlamentar. Mariana Carvalho, não obstante a pouca idade, tem uma imensa bagagem política. Foi presidente do PSDB jovem em Rondônia, vereadora em Porto Velho, candidata a prefeita em 2012 e deputada federal em 2014, a terceira mais votada do estado, alcançando o percentual de 7,55% dos votos. Vale a pena ver o discurso da Redação Veja o vídeo: Se você é a favor de uma imprensa totalmente livre e imparcial, colabore curtindo a nossa página no Facebook e visitando com frequência o site do Jornal da Cidade Online",pt,63
228,51,1459338001,CONTENT SHARED,4774970687540378081,1895326251577378793,-167447336240321466,,,,HTML,http://www.mckinsey.com/business-functions/strategy-and-corporate-finance/our-insights/the-economic-essentials-of-digital-strategy,the economic essentials of digital strategy,"A supply-and-demand guide to digital disruption. In July 2015, during the championship round of the World Surf League's J-Bay Open, in South Africa, a great white shark attacked Australian surfing star Mick Fanning. Right before the attack, Fanning said later, he had the eerie feeling that ""something was behind me."" Then he turned and saw the fin. A digital-strategy framework Thankfully, Fanning was unharmed. But the incident reverberated in the surfing world, whose denizens face not only the danger of loss of limb or life from sharks-surfers account for nearly half of all shark victims-but also the uncomfortable, even terrifying feeling that can accompany unseen perils. Just two years earlier, off the coast of Nazaré, Portugal, Brazilian surfer Carlos Burle rode what, unofficially, at least, ranks as the largest wave in history. He is a member of a small group of people who, backed by board shapers and other support personnel, tackle the planet's biggest, most fearsome, and most impressive waves. Working in small teams, they are totally committed to riding them, testing the limits of human performance that extreme conditions offer. Instead of a threat of peril, they turn stormy seas into an opportunity for amazing human accomplishment. Digital Disruption These days, something of a mix of the fear of sharks and the thrill of big-wave surfing pervades the executive suites we visit, when the conversation turns to the threats and opportunities arising from digitization. The digitization of processes and interfaces is itself a source of worry. But the feeling of not knowing when, or from which direction, an effective attack on a business might come creates a whole different level of concern. News-making digital attackers now successfully disrupt existing business models-often far beyond the attackers' national boundaries: Simple (later bought by BBVA) took on big-cap banks without opening a single branch. A DIY investment tool from Acorns shook up the financial-advisory business. Snapchat got a jump on mainstream media by distributing content on a platform-as-a-service infrastructure. Web and mobile-based map applications broke GPS companies' hold on the personal navigation market. No wonder many business leaders live in a heightened state of alert. Thanks to outsourced cloud infrastructure, mix-and-match technology components, and a steady flood of venture money, start-ups and established attackers can bite before their victims even see the fin. At the same time, the opportunities presented by digital disruption excite and allure. Forward-leaning companies are immersing themselves deeply in the world of the attackers, seeking to harness new technologies, and rethinking their business models-the better to catch and ride a disruptive wave of their own. But they are increasingly concerned that dealing with the shark they can see is not enough-others may lurk below the surface. Deeper forces Consider an insurance company in which the CEO and her top team have reconvened following a recent trip to Silicon Valley, where they went to observe the forces reshaping, and potentially upending, their business. The team has seen how technology companies are exploiting data, virtualizing infrastructure, reimagining customer experiences, and seemingly injecting social features into everything. Now it is buzzing with new insights, new possibilities, and new threats. The team's members take stock of what they've seen and who might disrupt their business. They make a list including not only many insurance start-ups but also, ominously, tech giants such as Google and Uber-companies whose driverless cars, command of data, and reimagined transportation alternatives could change the fundamentals of insurance. Soon the team has charted who needs to be monitored, what partnerships need to be pursued, and which digital initiatives need to be launched. Just as the team's members begin to feel satisfied with their efforts, the CEO brings the proceedings to a halt. ""Hang on,"" she says. ""Are we sure we really understand the nature of the disruption we face? What about the next 50 start-ups and the next wave of innovations? How can we monitor them all? Don't we need to focus more on the nature of the disruption we expect to occur in our industry rather than on who the disruptors are today? I'm pretty sure most of those on our list won't be around in a decade, yet by then we will have been fundamentally disrupted. And how do we get ahead of these trends so we can be the disruptors, too?"" This discussion resembles many we hear from management teams thoughtful about digital disruption, which is pushing them to develop a view of the deeper forces behind it. An understanding of those forces, combined with solid analysis, can help explain not so much which companies will disrupt a business as why -the nature of the transformation and disruption they face rather than just the specific parties that might initiate them. In helping executives to answer this question, we have-paradoxically, perhaps, since digital ""makes everything new""-returned to the fundamentals of supply, demand, and market dynamics to clarify the sources of digital disruption and the conditions in which it occurs. We explore supply and demand across a continuum: the extent to which their underlying elements change. This approach helps reveal the two primary sources of digital transformation and disruption. The first is the making of new markets, where supply and demand change less. But in the second, the dynamics of hyperscaling platforms, the shifts are more profound (exhibit). Of course, these opportunities and threats aren't mutually exclusive; new entrants, disruptive attackers, and aggressive incumbents typically exploit digital dislocations in combination. We have been working with executives to sort through their companies' situations in the digital space, separating realities from fads and identifying the threats and opportunities and the biggest digital priorities. Think of our approach as a barometer to provide an early measure of your exposure to a threat or to a window of opportunity-a way of revealing the mechanisms of digital disruption at their most fundamental. It's designed to enable leaders to structure and focus their discussions by peeling back hard-to-understand effects into a series of discrete drivers or indicators they can track and to help indicate the level of urgency they should feel about the opportunities and threats. We've written this article from the perspective of large, established companies worried about being attacked. But those same companies can use this framework to spot opportunities to disrupt competitors-or themselves. Strategy in the digital age is often asymmetrical, but it isn't just newcomers that can tilt the playing field to their advantage. Realigning markets We usually start the discussion at the top of the framework. In the zone to the upper right, digital technology makes accessible, or ""exposes,"" sources of supply that were previously impossible (or at least uneconomic) to provide. In the zone to the upper left, digitization removes distortions in demand, giving customers more complete information and unbundling (or, in some cases, rebundling) aspects of products and services formerly combined (or kept separate) by necessity or convenience or to increase profits. The newly exposed supply, combined with newly undistorted demand, gives new market makers an opportunity to connect consumers and customers by lowering transaction costs while reducing information asymmetry. Airbnb has not constructed new buildings; it has brought people's spare bedrooms into the market. In the process, it uncovered consumer demand-which, as it turns out, always existed-for more variety in accommodation choices, prices, and lengths of stay. Uber, similarly, hasn't placed orders for new cars; it has brought onto the roads (and repurposed) cars that were underutilized previously, while increasing the ease of getting a ride. In both cases, though little has changed in the underlying supply-and-demand forces, equity-market value has shifted massively: At the time of their 2015 financing rounds, Airbnb was reported to be worth about $25 billion and Uber more than $60 billion. Airbnb and Uber may be headline-making examples, but established organizations are also unlocking markets by reducing transaction costs and connecting supply with demand. Major League Baseball has deployed the dynamic pricing of tickets to better reflect (and connect) supply and demand in the primary market for tickets to individual games. StubHub and SeatGeek do the same thing in the secondary market for tickets to baseball games and other events. Let's take a closer look at how this occurs. Unmet demand and escalating expectations Today's consumers are widely celebrated for their newly empowered behaviors. By embracing technology and connectivity, they use apps and information to find exactly what they want, as well as where and when they want it-often for the lowest price available. As they do, they start to fulfill their own previously unmet needs and wants. Music lovers might always have preferred to buy individual songs, but until the digital age they had to buy whole albums because that was the most valuable and cost-effective way for providers to distribute music. Now, of course, listeners pay Spotify a single subscription fee to listen to individual tracks to their hearts' content. Similarly, with photos and images, consumers no longer have to get them developed and can instead process, print, and share their images instantly. They can book trips instantaneously online, thereby avoiding travel agents, and binge-watch television shows on Netflix or Amazon rather than wait a week for the next installment. In category after category, consumers are using digital technology to have their own way. In each of these examples, that technology alters not only the products and services themselves but also the way customers prefer to use them. A ""purification"" of demand occurs as customers address their previously unmet needs and desires-and companies uncover underserved consumers. Customers don't have to buy the whole thing for the one bit they want or to cross-subsidize other customers who are less profitable to companies. Skyrocketing customer expectations amplify the effect. Consumers have grown to expect best-in-class user experiences from all their online and mobile interactions, as well as many offline ones. Consumer experiences with any product or service-anywhere-now shape demand in the digital world. Customers no longer compare your offerings only with those of your direct rivals; their experiences with Apple or Amazon or ESPN are the new standard. These escalating expectations, which spill over from one product or service category to another, get paired with a related mind-set: amid a growing abundance of free offerings, customers are increasingly unwilling to pay, particularly for information-intensive propositions. (This dynamic is as visible in business-to-business markets as it is in consumer ones.) In short, people are growing accustomed to having their needs fulfilled at places of their own choosing, on their own schedules, and often gratis. Can't match that? There's a good chance another company will figure out how. What, then, are the indicators of potential disruption in this upper-left zone, as demand becomes less distorted? Your business model may be vulnerable if any of these things are true: Your customers have to cross-subsidize other customers. Your customers have to buy the whole thing for the one bit they want. Your customers can't get what they want where and when they want it. Your customers get a user experience that doesn't match global best practice. When these indicators are present, so are opportunities for digital transformation and disruption. The mechanisms include improved search and filter tools, streamlined and user-friendly order processes, smart recommendation engines, the custom bundling of products, digitally enhanced product offerings, and new business models that transfer economic value to consumers in exchange for a bigger piece of the remaining pie. (An example of the latter is TransferWise, a London-based unicorn using peer-to-peer technology to undercut the fees banks charge to exchange money from one currency into another.) Exposing new supply On the supply side, digitization allows new sources to enter product and labor markets in ways that were previously harder to make available. As ""software eats the world""-even in industrial markets-companies can liberate supply anywhere underutilized assets exist. Airbnb unlocked the supply of lodging. P&G uses crowdsourcing to connect with formerly unreachable sources of innovation. Amazon Web Services provides on-the-fly scalable infrastructure that reduces the need for peak capacity resources. Number26, a digital bank, replaces human labor with digital processes. In these examples and others like them, new supply becomes accessible and gets utilized closer to its maximum rate. What are the indicators of potential disruption in this upper-right zone as companies expose previously inaccessible sources of supply? You may be vulnerable if any of the following things are true: Customers use the product only partially. Production is inelastic to price. Supply is utilized in a variable or unpredictable way. Fixed or step costs are high. These indicators let attackers disrupt by pooling redundant capacity virtually, by digitizing physical resources or labor, and by tapping into the sharing economy. Making a market between them Any time previously unused supply can be connected with latent demand, market makers have an opportunity to come in and make a match, cutting into the market share of incumbents-or taking them entirely out of the equation. In fact, without the market makers, unused supply and latent demand will stay outside of the market. Wikipedia famously unleashed latent supply that was willing and elastic, even if unorganized, and unbundled the product so that you no longer had to buy 24 volumes of an encyclopedia when all you were interested in was, say, the entry on poodles. Google's AdWords lowers search costs for customers and companies by providing free search for information seekers and keyword targeting for paying advertisers. And iFixit makes providers' costs more transparent by showing teardowns of popular electronics items. To assess the vulnerability of a given market to new kinds of market makers, you must (among other things) analyze how difficult transactions are for customers. You may be vulnerable if you have any of these: high information asymmetries between customers and suppliers high search costs fees and layers from intermediaries long lead times to complete transactions Attackers can address these indicators through the real-time and transparent exchange of information, disintermediation, and automated transaction processing, as well as new transparency through search and comparison tools, among other approaches. Extreme shifts The top half of our matrix portrays the market realignment that occurs as matchmakers connect sources of new supply with newly purified demand. The lower half of the matrix explains more extreme shifts-sometimes through new or significantly enhanced value propositions for customers, sometimes through reimagined business systems, and sometimes through hyperscale platforms at the center of entirely new value chains and ecosystems. Attacks may emerge from adjacent markets or from companies with business objectives completely different from your own, so that you become ""collateral damage."" The result can be not only the destruction of sizable profit pools but also the emergence of new control points for value. Established companies relying on existing barriers to entry-such as high physical-infrastructure costs or regulatory protection-will find themselves vulnerable. User demand will change regulations, companies will find collaborative uses for expensive infrastructure, or other mechanisms of disruption will come into play. Companies must understand a number of radical underlying shifts in the forces of supply and demand specific to each industry or ecosystem. The power of branding, for example, is being eroded by the social validation of a new entrant or by consumer scorn for an incumbent. Physical assets can be virtualized, driving the marginal cost of production toward zero. And information is being embedded in products and services, so that they themselves can be redefined. Taken as a whole, these forces blur the boundaries and definitions of industries and make more extreme outcomes a part of the strategic calculus. New and enhanced value propositions As we saw in the top half of our framework, purifying supply and demand means giving customers what they always wanted but in new, more efficient ways. This isn't where the disruptive sequence ends, however. First, as markets evolve, the customers' expectations escalate. Second, companies meet those heightened expectations with new value propositions that give people what they didn't realize they wanted, and do so in ways that defy conventional wisdom about how industries make money. Few people, for example, could have explicitly wished to have the Internet in their pockets-until advanced smartphones presented that possibility. In similar ways, many digital companies have gone beyond improving existing offerings, to provide unprecedented functionality and experiences that customers soon wanted to have. Giving consumers the ability to choose their own songs and bundle their own music had the effect of undistorting demand; enabling people to share that music with everyone via social media was an enhanced proposition consumers never asked for but quickly grew to love once they had it. Many of these new propositions, linking the digital and physical worlds, exploit ubiquitous connectivity and the abundance of data. In fact, many advances in B2B business models rely on things like remote monitoring and machine-to-machine communication to create new ways of delivering value. Philips gives consumers apps as a digital enrichment of its physical-world lighting solutions. Google's Nest improves home thermostats. FedEx gives real-time insights on the progress of deliveries. In this lower-left zone, customers get entirely new value propositions that augment the ones they already had. What are the indicators of potential disruption in this position on the matrix, as companies offer enhanced value propositions to deepen and advance their customers' expectations? You may be vulnerable if any of the following is true: Information or social media could greatly enrich your product or service. You offer a physical product, such as thermostats, that's not yet ""connected."" There's significant lag time between the point when customers purchase your product or service and when they receive it. The customer has to go and get the product-for instance, rental cars and groceries. These factors indicate opportunities for improving the connectivity of physical devices, layering social media on top of products and services, and extending those products and services through digital features, digital or automated distribution approaches, and new delivery and distribution models. Reimagined business systems Delivering these new value propositions in turn requires rethinking, or reimagining, the business systems underlying them. Incumbents that have long focused on perfecting their industry value chains are often stunned to find new entrants introducing completely different ways to make money. Over the decades, for example, hard-drive makers have labored to develop ever more efficient ways to build and sell storage. Then Amazon (among others) came along and transformed storage from a product into a service, Dropbox upped the ante by offering free online storage, and suddenly an entire industry is on shaky ground, with its value structure in upheaval. The forces present in this zone of the framework change how value chains work, enable step-change reductions in both fixed and variable costs, and help turn products into services. These approaches often transform the scalability of cost structures-driving marginal costs toward zero and, in economic terms, flattening the supply curve and shifting it downward. Some incumbents have kept pace effectively. Liberty Mutual developed a self-service mobile app that speeds transactions for customers while lowering its own service and support costs. The New York Times virtualized newspapers to monetize the demand curve for consumers, provide a compelling new user experience, and reduce distribution and production costs. And Walmart and Zara have digitally integrated supply chains that create cheaper but more effective operations. Indicators of disruption in this zone include these: redundant value-chain activities, such as a high number of handovers or repetitive manual work well-entrenched physical distribution or retail networks overall industry margins that are higher than those of other industries High margins invite entry by new participants, while value-chain redundancies set the stage for removing intermediaries and going direct to customers. Digital channels and virtualized services can substitute for or reshape physical and retail networks. Hyperscaling platforms Companies like Apple, Tencent, and Google are blurring traditional industry definitions by spanning product categories and customer segments. Owners of such hyperscale platforms enjoy massive operating leverage from process automation, algorithms, and network effects created by the interactions of hundreds of millions, billions, or more users, customers, and devices. In specific product or service markets, platform owners often have goals that are distinct from those of traditional industry players. Moreover, their operating leverage provides an opportunity to upsell and cross-sell products and services without human intervention, and that in turn provides considerable financial advantages. Amazon's objective in introducing the Kindle was primarily to sell books and Amazon Prime subscriptions, making it much more flexible in pricing than a rival like Sony, whose focus was e-reader revenues. When incumbents fail to plan for potential moves by players outside their own ecosystems, they open themselves up to the fate of camera makers, which became collateral damage in the smartphone revolution. Hyperscale platforms also create new barriers to entry, such as the information barrier created by GE Healthcare's platform, Centricity 360, which allows patients and third parties to collaborate in the cloud. Like Zipcar's auto-sharing service, these platforms harness first-mover and network effects. And by redefining standards, as John Deere has done with agricultural data, a platform forces the rest of an industry to integrate into a new ecosystem built around the platform itself. What are the indicators that hyperscale platforms, and the dynamics they create, could bring disruption to your door? Look for these situations: Existing business models charge customers for information. No single, unified, and integrated set of tools governs interactions between users and suppliers in an industry. The potential for network effects is high. These factors invite platform providers to lock in users and suppliers, in part by offering free access to information. Finding vulnerabilities and opportunities in your business All of these forces and factors come together to provide a comprehensive road map for potential digital disruptions. Executives can use it to take into account everything at once-their own business, supply chain, subindustry, and broader industry, as well as the entire ecosystem and how it interacts with other ecosystems. They can then identify the full spectrum of opportunities and threats, both easily visible and more hidden. Digital's impact on strategy By starting with the supply-and-demand fundamentals, the insurance executives mentioned earlier ended up with a more profound understanding of the nature and magnitude of the digital opportunities and threats that faced them. Since they had recognized some time ago that the cross-subsidies their business depended on would erode as aggregators made prices more and more transparent, they had invested in direct, lower-cost distribution. Beyond those initial moves, the lower half of the framework had them thinking more fundamentally about how car ownership, driving, and customer expectations for insurance would evolve, as well as the types of competitors that would be relevant. It seems natural that customers will expect to buy insurance only for the precise use and location of a car and no longer be content with just a discount for having it garaged. They'll expect a different rate depending on whether they're parking the car in a garage, in a secured parking station, or on a dimly lit street in an unsavory neighborhood. Rather than relying on crude demographics and a driver's history of accidents or offenses, companies will get instant feedback, through telematics, on the quality of driving. In this world, which company has the best access to information about where a car is and how well it is driven, which could help underwrite insurance? An insurance company? A car company? Or is it consumer device makers that might know the driver's heart rate, how much sleep the driver had the previous night, and whether the driver is continually distracted by talking or texting while driving? If value accrues to superior information, car insurers will need to understand who, within and beyond the traditional insurance ecosystem, can gather and profit from the most relevant information. It's a point that can be generalized, of course. All companies, no matter in what industry, will need to look for threats-and opportunities-well beyond boundaries that once seemed secure. Digital disruption can be a frightening game, especially when some of the players are as yet out of view. By subjecting the sources of disruption to systematic analysis solidly based on the fundamentals of supply and demand, executives can better understand the threats they confront in the digital space-and search more proactively for their own opportunities. About the Authors Angus Dawson is a director in McKinsey's Sydney office, Martin Hirt is a director in the Taipei office, and Jay Scanlan is a principal in the London office. The authors would like to thank Chris Bradley, Jacques Bughin, Dilip Wagle, and Chris Wigley for their valuable contributions to this article.",en,62
229,2591,1476888660,CONTENT SHARED,2725842723638001955,-3595444231792050977,798149484404780068,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.59 Safari/537.36",SP,BR,HTML,http://blog.caelum.com.br/micro-profile-javaee-com-wildfly-swarm/,micro profile javaee com wildfly swarm,"Foi-se o tempo em que desenvolver uma aplicação JavaEE era algo extremamente burocrático e que tínhamos um servidor de aplicação inchado e com um consumo excessivo de recursos. Hoje temos servidores que foram pensado na reutilização dos recursos e ainda temos a possibilidade de escolher quais módulos vamos inicializar junto com o servidor de aplicação, os chamados profiles . Mas e quando pensamos em micro-serviço? Como podemos trabalhar com micro-serviços no JavaEE? A ideia do micro-serviço é de ter serviços isolados e independentes. Até aí tudo bem, podemos criar aplicações JavaEE separadas(isto é cada uma com seu .war ). Mas como podemos fazer o deploy dessas aplicações? Podemos ter um único contêiner (servidor de aplicação) para onde efetuamos o deploy de todas as aplicações. Mas com isso estamos indo de frente com a ideia de micro-serviço. Pois estamos gerando um ponto único de falha ( SPOF ). Em outras palavras se precisarmos por alguma razão interromper o contêiner, todas as aplicações penduradas nele cairam também. Outra alternativa seria cada serviço ter seu próprio contêiner, mas dessa forma estaríamos subutilizando o próprio contêiner. Pois nele temos diversos serviços rodando e que nossas aplicações não iriam precisar. Se não tivessemos usando servidor de aplicação poderiamos usar um contêiner embutido na aplicação (Tomcat, Jetty, Undertown e etc..). Porém dessa forma perderiamos as facilidades que o servidor de aplicação nos proporcionam. E aí que entra uma ferramenta nova para nos ajudar, Wildfly-Swarm . Com ele podemos declarar quais ""módulos"" da plataforma JavaEE queremos utilizar. A partir daí é desenvolver sua aplicação JavaEE normalmente e ao empacotar nossa aplicação, o wildfly-swarm pegará nosso pacote .war e irá embrulhar em um pacote dele com um contêiner micro para rodar nossa aplicação. Daí o nome de Micro-Profile . Esse artefato final (o pacote que foi gerado a partir do nosso .war ) é um arquivo .jar e podemos executar com um simples comando java -jar no nosso terminal. Wildfly-Swarm usa o conceito de UberJar para gerar o artefato final. Que nada mais é do que um arquivo .jar que contém além do seu .war todas as dependências necessárias para que o wildfly-swarm consiga rodar. Logo um UberJar é um arquivo .jar um pouco mais inchado mas somente com o necessário para rodar a aplicação. O mínimo necessário para rodar o wildfly-swarm é JDK8 e Maven ou Gradle . No wildfly-swarm temos o conceito de fraction , que nada mais é do que uma funcionalidade/configuração do servidor de aplicação. Na maioria das vezes um fraction pode ser comparado (mapeado) com um subsystem do servidor de aplicação (ex.: Datasource, Driver, Pool, socket-binding e etc...), temos outros casos em que um fraction é uma funcionalidade que antes não tínhamos (nativamente) no servidor de aplicação (Ex.: Jolokia, Spring, NetflixOSS ). Para usar o wildfly precisamos importar um pom.xml do wildfly-swarm no nosso projeto, e adicionar um plugin que fará a geração do UberJar . Nesse post vamos construir uma simples aplicação rest usando o wildfly-swarm. Vamos lá! Com o projeto (web) maven criado precisamos importar o pom.xml do wildfly-swarm ao nosso projeto. Esse pom.xml é importado declarando uma denpendencia gerenciada e declarando que o escopo da mesma será import . Para não ficar espalhando a versão do wildfly-swarm estamos utilizando por todo nosso pom.xml vamos declarar uma propriedade com ela. Além disso já vamos definir a versão do Java que iremos utilizar, e como se trata de um projeto web teoricamente precisariamos de um arquivo web.xml porém não é necessário para nosso caso. Então vamos definir que não deve ser gerado um erro caso não exista o arquivo web.xml . Agora vamos importar o arquivo pom.xml do wildfly-swarm que tem o nome de BOM (Bill of Materials). Além disso precisamos adicionar o plugin que irá gerar o UberJar baseado no nosso war . Precisamos também adicionar a dependência referente à api do JavaEE e esta será provida pelo wildfly-swarm. E esse é o setup para utilizar o wildfly-swarm, ao final teriamos o nosso arquivo pom.xml mais ou menos da seguinte maneira: Agora vamos adicionar as dependências que queremos do servidor de aplicação, para o nosso caso vamos adicionar, jax-rs , cdi , ejb , jpa , datasources . Vamos começar a ""codar"" nosso projeto de livraria. Vamos iniciar criando nossas classes de domínio Livro e Autor e seus respectivos DAOs Vamos criar uma classe para configurar o JAX-RS para receber requisições a partir da raiz: Agora que já temos como receber requisições vamos criar nossos recursos que serão expostos pela nossa api LivroResource e AutorResource além disso já vamos marcar-los como EJB Stateless para que já tenhamos transação entre outras coisas disponíveis. Nesse recurso temos as seguintes URIs /livros - GET (Retorna uma lista com todos os livros) /livros/id - GET (Retorna o livro com ID especificado) /livros - POST (Cria um novo livro) Nesse recurso temos as seguintes URIs /livros/idDoLivro/autores - GET (retorna todos os autores de um livro especifico) /livros/idDoLivro/autores/idDoAutor - GET (retorna um autor especifico de um livro especifico) /livros/idDoLivro/autores - POST (cria um novo autor associado ao livro) Agora que temos nosso projeto pronto temos que registrar um datasource e associa-lo no nosso arquivo persistence.xml . Para o nosso caso, esta configuração será feita em uma classe com um método main . Porém para configurar o datasource precisamos dizer qual o driver, e no nosso caso será um driver para mysql. para não precisarmos registrar um driver, podemos adicionar a dependência para esse driver diretamente no pom.xml Para que seja adicionado corretamente o driver para mysql precisamos excluir o driver default que vem quando adicionamos a dependência para JPA que é o H2 . Para isso vamos alterar a dependência de JPA e remover o H2 . (Se não fizermos essa configuração ao subirmos o wildfly-swarm iremos receber um erro, informando que esta rolando um conflito entre essas dependências H2 , MYSQL ). Pronto agora vamos registrar nosso datasource dentro de um método main: Como estamos sobrescrevendo o comportamento default do wildfly-swarm precisamos ensinar ele como ele deve fazer o deploy da nossa aplicação (ou seja dentro do nosso .war quais classes devem estar disponíveis, quais arquivos e etc...). Para fazer isso iremos usar uma biblioteca chamada ShrinkWrap da própria JBoss que serve para criar um pacote programaticamente, ela é muito utilizada quando estamos usando o Arquillian para testes de aceitação/integração. Com ele podemos criar JARArchive ( .jar ) e WARArchive ( .war ). Além desses temos outros tipos mais especificos por exemplo RibbonArchive um archive especifico que pode ser registrar em aplicações baseadas em Ribbon, Secured que já injeta o arquivo keycloak.json e já configura a parte de seguraça. Temos também um outro tipo especifico que é JAXRSArchive que é um archive que já configura o jax-rs e faz o binding para classe de configuração Application/@ApplicationPath . Vamos utilizar esse archive. Além disso precisamos dizer que ao fazer o deploy da aplicação seja levado os arquivos persistence.xml e beans.xml necessários para o funcionamento da JPA e CDI . O arquivo beans.xml deve estar dentro do diretório WEB-INF a partir do classpath e o arquivo persistence.xml , deve estar em META-INF a partir do classpath também. E precisaremos configurar isso também. Como ultima configuração precisamos indicar ao plugin do wildfly-swarm que ele não deve usar sua classe padrão para iniciar. Ele deve usar nossa classe Boot e com isso alterar o arquivo de manifesto para usar essa classe como ponto inical da nossa aplicação. Vamos alterar a declaração do plugin no arquivo pom.xml e adicionar a configuração "". Para que seja gerado o UberJar devemos executar o goal package do maven (Ex.: mvn package ). Ao termino dessa execução no diretório target teremos um arquivo livraria.war e um livraria-swarm.jar (entre outros arquivos). O arquivo livraria-swarm.jar é o nosso UberJar . Para executarmos podemos faze-lo pelo plugin do maven através de mvn wildfly-swarm:run ou executando o comando java -jar livraria-swarm.jar . Dessa forma temos uma aplicação JavaEE somente com o mínimo necessário e um contêiner bem mais leve para rodar a mesma. Aprendendo mais Quer aprender mais sobre como utilizar a especificação JavaEE ? Não deixe de conferir nosso curso Plataforma JavaEE , nele abordamos jax-rs , jpa , cdi , ejb entre outros recursos da especificação JavaEE . E aí o que você achou do wildfly-swarm ?",pt,62
230,2746,1479178420,CONTENT SHARED,6750029764020453263,-1578287561410088674,2000151301182442550,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) SmartCanvas/0.1.0 Chrome/49.0.2623.75 Electron/0.37.2 Safari/537.36",SP,BR,HTML,https://macmagazine.com.br/2016/11/14/microsoft-anuncia-visual-studio-para-mac-disponivel-ainda-nesta-semana/,"microsoft anuncia visual studio para mac, disponível ainda nesta semana | macmagazine.com.br","Acredito que, a este ponto, dizer que a Microsoft deu uma reviravolta de 180º (para melhor!) nos últimos anos sob o comando de Satya Nadella é chover no molhado. A empresa está mais amigável, mais aberta, mais criativa e acessível que nunca - e, se você quer mais uma prova disso, basta saber que, pela primeira vez nos seus 20 anos de história, o Visual Studio está ganhando uma versão para Mac. A novidade foi anunciada por meio de um post no blog oficial da Microsoft - talvez um pouco mais cedo do que o planejado, já que ele foi apagado um pouco depois ( aqui pode-se acessar uma versão em cachê), mas enfim: o fato é que uma das ferramentas de desenvolvimento mais populares do mundo está encontrando o seu caminho para o mundo Apple e agora desenvolvedores que queiram utilizar o Visual Studio num Mac não mais precisarão recorrer a alternativas como o Boot Camp, o Paralells ou o VMware Fusion. Obviamente, a intenção aqui não é que os usuários escrevam aplicações inteiras para Windows num Mac - afinal de contas, para rodar tais criações ainda seria necessária uma máquina virtual -, mas para edições rápidas ou criação de softwares para outras plataformas, como iOS, Android ou o próprio Mac, o Visual Studio cumprirá muito bem a sua função. Como dizia um trecho do post agora deletado: Se você gosta da experiência de desenvolvimento do Visual Studio, mas precisa ou quer utilizar o macOS, você certamente se sentirá em casa. A interface de usuário [da versão para Mac] é inspirada pela do Visual Studio, mas desenhada para que o programa transpareça como um cidadão nativo do macOS. Ainda que traga boa parte das capacidades da sua versão original do Windows, o Visual Studio para Mac terá algumas limitações, como falta de suporte a alguns tipos específicos de projeto. A grande vantagem do novo programa, entretanto, é a possibilidade de compartilhar projetos entre Macs e PCs instantaneamente, sem necessidade de conversão. Segundo o anúncio, o Visual Studio para Mac será apresentado em maiores detalhes amanhã no evento para desenvolvedores Connect(), da Microsoft, com uma versão de testes disponibilizada já nesta semana. [via The Verge ] Se houver algum erro no post acima, selecione-o e pressione Shift + Enter ou clique aqui para nos notificar. Obrigado! aplicativo Boot Camp desenvolvedor desenvolvimento developer Mac macOS Microsoft programa Software Visual Studio Windows Sobre o Autor Aviso: nossos editores/colunistas estão expressando suas opiniões sobre o tema proposto e esperamos que as conversas nos comentários sejam respeitosas e construtivas. O espaço acima é destinado a discussões, debates sobre o tema e críticas de ideias, não às pessoas por trás delas. Ataques pessoais não serão tolerados de maneira nenhuma e nos damos ao direito de ocultar/excluir qualquer comentário ofensivo, difamatório, preconceituoso, calunioso ou de alguma forma prejudicial a terceiros, assim como textos de caráter promocional e comentários anônimos (sem nome completo e/ou email válido). Em caso de insistência, o usuário poderá ser banido.",pt,61
231,2357,1474028337,CONTENT SHARED,-6545872007932025533,-1402490765047599382,-9043465215175509636,,,,HTML,https://medium.com/chris-messina/silicon-valley-is-all-wrong-about-the-airpods-8204ede08f0f,why silicon valley is all wrong about apple's airpods - chris messina,"So you think Apple is a tech company? No, you're wrong. In July of 1997, right before his return to Apple, Steve Jobs told BusinessWeek: ""The products suck! There's no sex in them anymore! Start over"". Ten years later, building on the dripping sex and rock and roll of the iPod (touched with a Bono �� no less!), Jobs revealed the iPhone and changed computing forever. Last week, Apple did it again, but for some reason, nearly everyone in Silicon Valley is confused about what just happened. I mean, I understand the confusion, but do people really think that the most significant announcement was the removal of the 3.5mm analog headphone jack ? I mean, it was, but not for the reasons everyone's panties seems to be bunched up about. Apple doesn't give a shit about neckbeard hipsters who spent thousands of dollars on expensive audiophile gear that rely on 100-year-old technology to transmit audio signals . They'll readily drop them faster than Trump drops facts to make an argument in a televised debate . Apple is securing its future , and to do that, it must continue to shrink the physical distance between its products and its customers' conceptions of self. The ᴡᴀᴛᴄʜ came first, busting our sidekick supercomputer out of our pockets and onto our skin. Apple's next move will put its products literally within earshot of our minds. This is no accident. How quickly we forget the past In 2007, not only did Apple launch the iPhone, but they also changed their name from Apple Computer, Inc., to Apple Inc. This change was perhaps as big a deal as the iPhone itself, but it's taken another decade for its implications to become clear. Oops, did you blink and miss it? No problem. Apple made you a movie: Maybe it's still not clear to you. That's ok, I'll spell it out. Repeat after me: Apple is not a technology company The problem with Silicon Valley is Benedict Evans . I mean, not Benedict specifically, because he's actually incredibly smart and holds sophisticated perspectives on the tech industry and adoption cycles, and also, he gives good tweets , but he's not a product designer. And yet, we look to him and other folks of his ilk to understand Apple's moves. But there are no users like Benedict Evans in the world, except in Silicon Valley, and as much as we like to think of Silicon Valley as the center of the universe, it's not ( aside to my SV friends: I know, I know, deep breaths ). While we live and breathe tech products, and love to play armchair product quarterbacks (side note: Product Hunt is the NFL of product design ), we don't represent the masses of normals. He and I like to indulge the fantasy that Apple makes things exclusively for us, but they obviously care way less about two Twitter-loving technophiles in Silicon Valley than they do about the rest of the world. Thus when I consider who influences my thoughts on Apple's moves, I need to be mindful of the Kool-aid I'm drinking, who's making it, and what their lived context is. Do they represent the broader whole of humanity, or a narrow sliver of land on the West Coast of the United States of America? So, then, why should you listen to me? Who died and crowned me an expert? No one. I just kind of became an expert by virtue of the sheer number of hours I've spent on this stuff. Kind of like Benedict, but it's also his job. But, I'm also kind of a fraud, like the rest of you. I grew up never fitting in with any crowd and never being popular, but I learned to observe people, and then chameleon myself into their cliques so I could feign belonging . It makes me a good faker and it makes me pretty good at listening to the words people use, but better at paying attention to what their behavior actually says . I've learned to differentiate how experts think about things from the way laypersons do, and how to discount each respective perspective accordingly (including my own, definitely including my own ). Most Silicon Valley pundits that we enjoy listening to or reading only reinforce our own over-developed, over-informed (and thus, unrepresentative ) viewpoints. They say things that validate our shallow egos and make us feel less alone, like when they decry the death of the 3.5 mm analog jack as anathema. We tweet our adolescent angst in solidarity because it feels good to belong and to rage in unison, and because we recoil from physical affection from each other, we seek likes and retweets to soothe our wounded inner children because that kind of validation is the closest human connection to getting a hug that we're willing to tolerate. And fuck yeah, Techmeme, thank you for showing me that I'm not alone ! But, I digress. What was I talking about? Oh, right. Apple is a fashion brand that makes jewelry that connects to the internet The thing that makes me crazy about Apple (and not in the fanboi sense) is that they both give a shit and don't give a shit about what anybody else thinks, and what everyone else is doing. Like, under Tim Cook, they're a lot more ""out there"" and verbally responsive to customer complaints , but in a totally controlled and measured way. Not like Jobs didn't write emails to customers , but Cook is a little faster and looser. A little. And from an industry perspective, Apple doesn't seem to want to keep up with the Jones's (Google, Facebook, Amazon, Tencent, et al), except when they do. For example, Samsung showcased a waterproof S7 back in February and then Apple followed suit in the iPhone 7. In other areas, however, Apple is out on their own. That's where it's worth paying attention, and that's what brings us back to nixing the headphone jack with the one-two punch of a Lightning port coupled with Bluetooth audio. Yes, others, from Slate to Chris Saad ( 1 ), have pointed out that this change is not about music, but about how Apple's new AirPods will usher in the wonderful (and yet unproven) world of voice computing. And, I agree, but that perspective is insufficient to understand why Apple doing this is significant. It's not like they're the first. This image, though, helps: ""You've got to start with the customer experience and work backwards to the technology ."" - Steve Jobs The thing is, we've had wireless headsets for a while, but they've always made people (mostly men) look like dickwads. They're confusing to pair, and frustrating to use. And so if you're willing to put up with them, you're letting technology ruin your life. And so, you're a dickwad. Don't take my word for it, look at this unlicensed "" confident businessman with wireless headset "" stock photo: Pure dickwad. Poor guy. Heck, even if you kind of look like Chris Hemsworth, you can't really make a once state of the art wireless headset look like something you'd choose to adorn your pretty little head with: The reality is, the "" Bionic Man "" look isn't really in, no matter how much utility these devices provide (I say this even as Bluetooth headphones sales eclipse the wired sort ) or people attempt to get the design right . Yves Behar couldn't make it happen when he partnered with Jawbone. I mean, would you choose to wear something like this on a hot date? Probably not if you wanted a second date, amirite? But the EarPods, and now AirPods, for some reason* (* no, it's a very specific reason ), defeat this crisis of user acceptance. What Apple has done is produce something that isn't a technology product, but is, rather, a fashion object -a piece of jewelry, an entertainment product, a status symbol, a genie in a bottle-that drips with sex appeal. I mean, that iPhone 7 launch video probably was directed by The Weeknd , because I want to watch it, Often, followed by a cold shower. �� So, don't confuse AirPods with just another Bluetooth headset; that's not what they're replacing. AirPods offer a new relationship because they're alluring, sensuous, and sultry: AirPods are sex sticks that fuck your ears . (Hmm. Or maybe your ears spoon them? I can't decide.) Regardless, the fucking or the cuddling goes both ways, and if I'm saying anything, it's that AirPods aren't a technology device, but instead a way to get Her's Scarlett Johansson character into your bed... errr... I mean, head because whatever is going on in this image, it's the equivalent of what we all know actually takes place on Snapchat (or used to), except it's happening between you and a bot named Siri: And this is what Apple can do that no one else can: make the behavior of talking to a disembodied entity on your face so socially acceptable that the voice computer revolution can finally get underway. Nor are they starting from square one. They've already taught us to behave this way, even if we don't realize it. How many times have you walked down the street talking to a colleague or family member on your EarPods? It's normal. It's not weird. Who cares if you're talking instead to your robot overlord? What sets AirPods apart is that they build on existing habits, require only slightly modified expectations on behalf of the user, and benefit from the wisdom of the phalanx of fashion luminaries that Apple has brought in-house over the past decade. In contrast, here's a weird product with no sex appeal which had no prior user adoption to build on and that was doomed to fail from the outset, no matter how many models showed up for the fashion walks : You can buy fashionable friends, and you can pay them to wear your stuff to make some photos, but you can't get them to choose to wear what you're offering in their real lives unless there's a bridge to the familiar, essential, and down-to-earth. AirPods build on the success of the iPod, which is related to the story of Napster and taking on the record industry, saying "" Fuck you! "" to Metallica ( especially to Lars ), putting 1,000 songs in your pocket, the clickwheel, trading 128kbps MP3s in internet forums, suffering through dial-up download speeds, Firewire, USB, and basically punk rock. AirPods are legit like Richard Branson because they've been around forever and yet they're still new and cool as fuck. Patience is a virtue lost on Silicon Valley Here in Silicon Valley, we're a bunch of inchoate Peter Pans , which affects how we approach relationships, how we design, build, and grow apps, and it affects our ability to relate to the people that use the things we make (because everything we make is soooo important, magical, revolutionary, changing the world, solving world hunger, making life less demanding by making everything available on-demand). Somehow ( maybe it was the acid trip Jobs went on ), Apple learned to take their time with products, and to pace their product evolution. They seem slow at times, but maybe it's just because they resist the short-sighted approach that most tech companies feel forced to take to try to get ahead. That means most tech companies struggle to fully understand the problems they're solving, and don't stop to saddle up alongside their users to develop empathy-to really understand what their users are willing to put up with and what they never will. Apple began the journey of promoting user acceptance of technology apparatuses as fashion accessories with the introduction of the iPod in 2001, fifteen years ago . You can hear it when Jobs explains why he decided to pursue music in the first place: he knew it was universal and represented a huge addressable market in which there was no market leader . He also knew that everyone loved music, and that their personal, emotional relationships with music would give him the opening he needed to send in the  ᴛʀᴏᴊᴀɴ ʜᴏʀsᴇ to permeate their lives for a generation. And now, by exploiting that same relationship, Apple is doing it again: offering a sexy fashion statement, an expensive luxury item, an entertainment accessory, which will usher in the era of voice-controlled intimate computing. Apple won't sell the AirPods by enumerating their tech specs but by evoking an emotional, aspirational response-which is an approach vividly different from nearly anything else that comes out Silicon Valley's burgeoning nerdtopia. When we decry the lack of diversity in Silicon Valley (and yes, Apple absolutely should examine its own house ), we should remember that true diversity is complex with many dimensions. The broad, eventual appeal of AirPods come from the diversity of talent working behind the scenes to bring this product to life-beyond the engineering and industrial design-which includes disciplines from marketing to retailing to storytelling to fashion, as well as the disciplinary will to resist shipping shit products. A diversity of perspectives had to be brought together to make this product happen in this moment, with this narrative, with the relatively reserved emphasis on Siri. No, people aren't quite ready for the conversational software world of the future  - but that's okay, because, guys, Apple's on it, and they've got plenty of time to get it right. And I hope you understand what Apple's up to a little bit better now.",en,61
232,2047,1471002666,CONTENT SHARED,6540624159201421051,-1602833675167376798,-4293036500619421365,,,,HTML,https://pagamento.me/bradesco-vai-lancar-novo-banco-digital-o-next/,bradesco vai lançar novo banco digital: o next.,"Bradesco vai lançar novo banco digital. Com o nome de Next , o banco pretende lançar o novo produto em breve. Já não bastasse as iniciativas do grupo Elopar, o banco Bradesco vai tentar inovar e atingir o público jovem. O banco já usava o termo ""Next"" em alguns espaços das agências e agora assume o nome para o novo negócio. O banco já está em desenvolvimento dentro da Cidade de Deus, em São Paulo. O nome ""Next"" foi escolhido para falar com o potencial público alvo: millenials . A diretoria do banco nome nomeou Maurício Minas, VP do grupo, para tocar a operação. Tamanho do investimento: R$120 milhões (fontes do Estadão). A notícia vem logo após anúncio da chegada do Digio (da Elopar) , que também é controlado pelo Bradesco. O domínio Banco Next ( www.banconext.com.br ) foi registrado em 15/07/2015, o que demonstra que o banco está planejando o lançamento desde o ano passado. Parece que a ""nova onda"" do setor financeiro são os bancos digitais. Resta saber se o Next vai conseguir manter a ""pegada"" de inovação de Nubank , Banco Neon e Original . Vale frisar que esse novo ""business"" é um projeto essencialmente de inovação. Apesar da base do Bradesco e da rede de atendimento valerem o investimento, vão ter que aprender ""na marra"" como um negócio digital pode escalar. Vamos acompanhar, bem de perto.",pt,61
233,1443,1466287803,CONTENT SHARED,6829640091575814990,-1130272294246983140,-782450246769532337,,,,HTML,http://www.updateordie.com/2016/06/09/branding-e-problema-seu-e-meu/,branding é problema seu. e meu.,"B randing é um assunto injustiçado, coitado. Tachado de complexo além do que é, de fato. Que ironia: o Branding tem um problema de imagem. E, como consequência, duas coisas. Primeiro, um abismo enorme entre os que se acham ""branders"" e os que se acham leigos. Depois, a inevitável consequência da distância: o efeito não-é-problema-meu. Voluntária ou involuntariamente, na mente dos menos envolvidos com o tema, o trabalho dos ""branders"" se torna algo muito específico, restrito a marqueteiros, publicitários, comunicólogos e seres de espécies similares. Restringir a função de gestão da marca a um setor específico da empresa, que geralmente é a comunicação, é um erro. Temos aí a origem de um dos principais - senão o próprio - problemas das empresas: a fragmentação. Diferente de outras disciplinas presentes no dia-a-dia das empresas, não é possível fazer Branding eficiente se apenas uma área olhar para isso. Não dá! Sabe por que? Porque a marca é como o ar. Ela vive em todo lugar. Dentro da empresa, está em todas as áreas. Fora dela, perambula no seu ecossistema. Onde houver experiências de marca, haverá marca. E o que são experiências de marca? Toda e qualquer interação que se tem com ela. Vamos considerar o atendimento, que é uma interação típica entre marca e cliente. Se eu fui bem atendido, inevitavelmente foi criada uma pastinha da marca na minha mente. Dentro dessa pasta, o atributo ""atendimento de qualidade"" foi gravado. E lá, dependendo da próxima experiência, ele pode aumentar seu peso, diminuir ou ser excluído e substituído pelo atributo ""atendimento zoado"". Mas não estamos falando apenas de interações ou experiências diretas com a marca, mas também as indiretas. O ecossistema de uma marca é composto por todos os públicos que tem algum contato com ela. Esses públicos interagem com a marca, claro, mas também interagem entre si. Em todas essas interações, experiências de marca podem ser criadas, mesmo que ela não participe ""oficialmente"" dessa interação. É por isso que dizemos que a marca vive dentro, sim, mas principalmente fora da empresa. É como um filho, sabe? A gente cria pro mundo. :P É por isso também que fazer a gestão de uma marca é muito mais difícil que tirar doce da mão de criança - mas muito menos cruel. Marca é um nome ao qual relacionamos atributos, sentimentos e uma estética. Ou, para simplificar, um nome com associações. Essas associações são feitas a partir de toda e qualquer experiência que se tem com ela, como o exemplo típico do parágrafo acima. Ou o exemplo do parágrafo abaixo, não tão típico assim. Se um caminhão da Coca-Cola tombar em cima de mim - o exagero ajuda nesse caso - posso nunca mais querer sentir o cheiro nem o gosto daquela bebida escrota que quase me afogou depois do acidente. A culpa pode ter sido toda do motorista terceiro bêbado, mas a Coca-Cola estará relacionada a isso eternamente em minha mente. RIP Coke. Pronto. Isso é marca. Um nome ao qual fazemos associações de todo tipo, partindo das experiências mais diversas que se pode imaginar. Agora, pense você mesmo: existe uma área da empresa responsável por criar e gerir todas as experiências de marca possíveis? Eu respondo: não existe nem nunca existirá - frase de efeito irresponsável. Nem mesmo uma empresa inteira focada em fazer a gestão das experiências de marca daria conta, porque elas são só parcialmente controláveis. Existe todo um ecossistema criando experiências o tempo todo, das mais diversas formas, que não podemos gerir completamente. A essa altura, você, que não é ""brander"", deve ter percebido que você cria experiências de marca o tempo todo. Seja no papel de representante oficial da marca, quando você atende mal um telefonema de um fornecedor, ou no papel de cliente reclamão (público externo à empresa, mas essencial no ecossistema de qualquer marca), quando você xinga muito no Twitter aquela marca de tênis que usa mão de obra infantil e escrava. Ou no papel de membro fofoqueiro, quando você fala mal do padre. Como o padre é representante da Igreja, você está, por tabela, falando mal dela - Deus tá vendo! Caso mais grave do que apenas uns bilhões de dólares perdidos por uma imagem de marca manchada. Nesse cenário, o que as empresas precisam fazer? Gerir com a maior excelência possível todas as experiências que ela cria com seus públicos - atendimento, comunicação, redes sociais, embalagens, força de vendas etc. Assim, as chances de as demais experiências serem positivas será grande. Fora isso, vale rezar - pense bem antes de falar mal do padre - pra que nenhum caminhão tombe, nenhum rato invada suas garrafas, nenhuma barragem se rompa liberando 40 bilhões de litros de lama. E pra você, amigo, resta assumir esses vários chapeus na sua cabeça. Você tem a marca pela qual trabalha, a marca da qual é cliente, a marca da qual é fornecedor. Tem até sua marca pessoal, cujas experiências geram uma boa ou uma má reputação. Ou seja, sua reputação depende de um bom trabalho de Branding de você mesmo. Se a primeira impressão é a que fica, sua missão começa cedo. Seja no trabalho ou na vida social, um ""bom dia"" dá um belo pontapé. É por isso que Branding é problema seu. E meu.",pt,61
234,1297,1465334139,CONTENT SHARED,-5997769775112032630,-1032019229384696495,-7696592431575292648,,,,HTML,http://venturebeat.com/2016/06/05/how-bot-to-bot-could-soon-replace-apis/,how bot-to-bot could soon replace apis,"By now it's clear that bots will cause a major paradigm shift in customer service, e-commerce, and, quite frankly, all aspects of software-to-human interaction. For the moment, the state of the art of bots is bot-to-consumer, meaning bots communicating with humans. But at some point soon, bots will start talking to other bots. Enter the bot-to-bot era. Imagine that a bot - let's call her Annie - needs to answer a question from a customer but lacks information from her own backend systems. Annie is powered with artificial intelligence and spontaneously decides to reach out to another bot to get the information she needs. Annie aggregates the information and delivers it back to the customer. Death of the API ? Today when two software systems have to talk to each other, software developers need to implement an integration using APIs (application programming interfaces). This integration process is time consuming. That's why, over the last couple of years, services such as Zapier , Scribe , and IFTT have become popular. They provide out-of-the-box interfaces to hundreds of software applications, allowing you to connect, for example, your CRM system with a mailing tool or analytics platform. In the bot-to-bot era, however, each software application can talk to each other system, regardless of whether they have an actual API integration in place. Granted, bot-to-bot communication will not be used to exchange large amounts of data, but it will allow for ad-hoc communication between, for example, my banking software and a web shop. My banking software could talk to the webshop bot and ask for that missing invoice: ""Niko needs an invoice for order 45678, can you provide that?"" The big finale: bot-to-bot-to-consumer The beauty of bot-to-bot communication will be that it is in plain English, it will be conversations that every human can understand. Assuming that all conversations between my bot Annie and other bots are archived, I will be able to go back and see how my two little bots came to a certain conclusion. In my banking example, when an invoice is missing after all, I could click on a ""details"" button, which would show me the conversation Annie had with the webshop. The archived bot-to-bot conversation would show me the webshop bot response, that the invoice will not be available for another couple of weeks. But it gets better. If my bot is stuck in a conversation with another bot, she can call me in for help: ""Niko, it's Annie here, your finance bot. I'm talking to a supplier, but I'm having some trouble understanding what they are saying."" I could chime in - when I have time of course, a couple of hours later, since bots have unlimited patience - and I would rephrase the question of Annie and get the answer from the other bot. Next, Annie could continue the conversation and handle my business. Semantic web Didn't we talk about connecting every online service with every other online service 10 years ago? What was it called again? The Semantic web? Every website was going to be annotated using standard data formats, allowing other services to crawl that data and use it in their own business logic. I believe that bots will deliver on that promise in the next 3 to 5 years, and this will not mean that all data will have to be uniformly formatted. Instead, bots will expose online services and data in plain English, allowing both humans and other bots to interact, even if they have never communicated before. Calling all software developers So, software developers, when you develop your platform for e-commerce, online marketing, finance, ERP or any other software solution, please think about the implementation of a smart bot, besides your traditional APIs, so next time when I buy a new BBQ online, my bot will alert me that it's going to rain for the next two weeks. Niko Nelissen is VP of Mobile, Data and Engagement at Etouches , an event management solution. Follow Niko on Twitter. AI. Messaging. Bots. Arm yourself for the next paradigm shift at MobileBeat 2016. July 12-13 at The Village in San Francisco. Reserve your place here.",en,61
235,1670,1467813602,CONTENT SHARED,3180828616327439381,1895326251577378793,6337372998984359835,,,,HTML,http://computerworld.com.br/governo-define-cronograma-para-plano-nacional-para-internet-das-coisas,governo define cronograma para plano nacional de internet das coisas,"Ricardo Rivera, gerente setorial das indústrias de tecnologia da informação do BNDES, anunciou durante a Rio Info 2016 que 29 propostas foram entregues para a contratação da consultoria que vai desenhar o plano de ação nacional para Internet das Coisas, a ser conduzido pelo banco estatal e pelo MCTIC. Segundo ele, na próxima semana serão conhecidas as cinco finalistas e até o final de julho será revelada a empresa vencedora. A expectativa é que a pesquisa sobre o tema aconteça de outubro até o final de dezembro. A meta, explicou o executivo, é que no segundo semestre de 2017, o plano nacional de IoT comece a ser implantado.""O levantamento vai fazer um estudo completo do mercado, mas terá recursos para uma implantação imediata. A intenção é remover as principais barreiras para Internet das Coisas"", sustentou. O plano terá com validade de 2017 a 2022. ""O modelo terá recurso para garantir a implantação. Ele não será um estudo sem medidas práticas"", detalhou Rivera, ao afirmar que um ponto central do estudo BNDES/MCTIC é obrigar o monitoramento das ações. ""Queremos que a política seja implantada. Isso é crucial para o Brasil ter um lugar nesse mercado"", afirmou. O levantamento terá pontos voltados às questões regulatórias, as melhores práticas de financiamento e o compartilhamento de informações. ""Há ações de IoT acontecendo no mundo. Precisamos dividir conhecimento. Há muitas oportunidades de negócios, mas temos que estruturar quais são as competências do Brasil"", completou. A Internet das Coisas desperta muito interesse das empresas. Em contrapartida, muitas dúvidas rondam os executivos. ""Precisamos entender se o que estamos fazendo é realmente Internet das Coisas. Se não corremos o risco de fazer investimentos que ser perderão"", observou Gabriel Antônio Mourão, consultor de novas tecnologias da Perception. Ele afirmou que este é o momento ideal para investir em IoT. ""Temos que aproveitar ao máximo esse momento"", comentou. ""A IoT já agrega diferentes tecnologias, como cloud computing e big data, e precisa se preparar para as novas que vão surgir. É preciso ainda encontrar novas soluções sobre questões importantes como segurança, privacidade ou volume de dados nas comunicações. A aplicabilidade e compatibilidade entre diferentes plataformas também precisa ser pensada"", ponderou o executivo. Para Fabio Porto, professor e pesquisados do Laboratório Nacional de Computação Cientifica (LNCC), que tem desenvolvido vários projetos em parceria com instituições e empresas para aplicação de IoT com uso de dados captados por sensores, para desenvolver sistemas é preciso entender bem a estrutura e característica dos dados a serem avaliados e da aplicação que se pretende construir. ""Cada tipo de dado, sejam eles de séries temporais, comprimidos, criptografados, preciso ou impreciso, tem uma interpretação diferente"", indicou. Um dos projetos desenvolvidos pelo laboratório de pesquisa é o SAHA, de acompanhamento do desempenho de atletas de alto rendimento a partir do monitoramento de dados em diferentes áreas, como nutrição e bioquímica. ""A IoT é a próxima grande onda. E três pontos chave são mais relevantes para se pensar neste momento: variedade, velocidade e veracidade"", definiu.",pt,60
236,605,1461715957,CONTENT SHARED,-8618420761918493321,-1032019229384696495,-2767767474126851917,,,,HTML,http://www.greenbot.com/article/3060757/android/what-the-google-i-o-schedule-tells-us-about-the-future-of-android.html,what the google i/o schedule tells us about the future of android,"Google I/O may be a conference for developers, but what happens there will have a major impact on how you interact with Android and Google's other services in the near future. The conference covers all of Google's projects, but Android is definitely the star of this year's show, which will be held in Mountain View from May 18 to 20. Some of the focus will be on better app performance and design, which is always in demand. Google will also have plenty to say about how Android will usher in a future of virtual reality and connectedness across your phone, car, television, and other devices that may not even be here yet. After studying the schedule , there are four key themes that emerge, each illustrating how Android will move forward in the next year and what it will mean for putting your digital life in Google's hands. Virtual Reality is big Google has big ambitions in virtual reality. Cardboard is just the start, as there have been rumors of the company building its own VR headset and indications from Android N about how the operating system will give more native support to VR . So set your eyes on the VR at Google session on May 19, which is hosted by Clay Bavor, Google's vice president of virtual reality (who also has a fascinating photography blog ). Right now Facebook-owned Oculus is leading the VR game and Google's frenemy Samsung makes the most popular consumer device in the Gear VR. So expect Google to invest heavily to ensure the company's services are where the Internet is going. YouTube, as an example, recently added support for VR and 360-degree video. And the outdoor venue for Google I/O may not be a coincidence; it could be a showcase for all Google plans to do with digitizing the outside world. YouTube YouTube now supports videos shot specifically for VR. We have more tangible evidence of the company's plans for augmented reality with Project Tango . Two sessions are devoted to the technology, which empowers phones and tablets to see and sense the environment around them: one focused on gaming with Project Tango and another titled, "" Introducing Project Tango Area Learning ."" Lenovo announced at CES that it would have a first phone with Project Tango technology by the end of this summer, so perhaps we'll get to see a near-final version of this. Either way, Project Tango is inching out of the lab and into actual consumer products soon. And let's not forget about Android Auto. There's a session titled "" Android Auto for everyone ,"" which is all about helping developers extend their apps to the car dashboard. Android Auto has been gaining steam this year with expansions into new vehicles, which makes it a key piece of Google's strategy to have Android and Google's contextual information follow you wherever you are. Going global Another session that caught my eye was, "" Building for billions on Android ."" Led by a team of Android developer advocates, it's likely to include strategies for making apps work across the wide swath of Android hardware. Google Android One is part of Google's effort to make Android the OS of choice in the developing world. Google's OS is growing strongest in what's called emerging markets, places like India, China, and South American nations where many people are just now getting their first smartphone. Usually such phones are lower cost than a flagship sold in the U.S., and they're powered by Android. That's why Google sought to get directly involved with phone sales with Android One , though the program has had a mixed record. In order for Google to keep people hooked into Android, the ecosystem needs good apps that will work well on devices that have lower computing power and Internet speed access. It's the reality for developers who want to build on the platform, and it'll be interesting to see what solutions Google offers. Material Improvements Material Design is nearing its second birthday, as Google first announced the design language and guidelines at the 2014 I/O conference. However, even now some apps still haven't fully embraced the look, which is probably due in part to how many mobile app developers put their initial emphasis on iOS. It also doesn't help that, as a recent App Annie report revealed, the App Store earns almost double the revenue as does Google Play despite half the number of app downloads. The Plaid app is all material, all the time. Day one has two different sessions focused on design issues: "" Material improvements "" and "" Discover the expanded material design motion guidelines ."" The former is to highlight all the tweaks to Material Design during the past couple of years, while the latter will give developers details on how the language has evolved. Nick Butcher, a design and developer advocate at Google, will lead the first session. He's one of the major evangelists for Material Design, which is highlighted notably in his Android app Plaid . These sessions should also serve as a reminder that Material Design is an evolving design language, not a static textbook. Speeding up Android apps No one likes a slow app. Google doesn't either, and there are multiple sessions geared towards strategies for faster performance and less memory usage. For example, a workshop entitled "" Android battery and memory optimizations "" is led by two Googlers who should offer good insight into this area: Ashish Sharma heads Android's Volta Team that focuses on lengthening battery life. Meghan Desai works on Android framework battery and memory management features for Google. The less battery Android apps use, the longer your phone will last. Then there's "" Lean and fast: putting your app on a diet ."" The goal is to reduce the size of an app's APK, which will certainly be welcome in an era where many phones don't offer expandable storage. Hopefully there will be some game developers in this session, as they tend to be some of the heftiest apps around. There's going to be a lot to devour during the conference's three days, and the entire Greenbot team will be there for all the action. Be sure to check our live coverage of the keynote on the morning of May 18 and follow along as we provide analysis of all the major reveals.",en,60
237,1142,1464620108,CONTENT SHARED,1929674614667189969,-1032019229384696495,-2446252650393115078,,,,HTML,http://techcrunch.com/2016/05/30/diane-greene-wants-to-put-the-enterprise-front-and-center-of-google-cloud-strategy/,diane greene wants to put the enterprise front and center of google cloud strategy,"When Google bought bebop Technologies last fall for $348 million , it got more than a stealthy startup. It also landed Diane Greene as executive vice president of Google Cloud Enterprise and that perhaps was the bigger prize. Greene brought with her years of industry experience having co-founded and been CEO at VMware for a decade, building it into a virtualization powerhouse. In fact, under Greene's watch, EMC bought VMware in 2003 for $635 million . She stuck around for 5 years seeing the company spun off in an IPO in 2007 , before departing in 2008 a wealthy woman. She spent the next several years helping other companies as a board member. One of those companies was Alphabet, and it was through this relationship that she was lured back to the big corporation where she was charged with taking Google's struggling cloud business and turning it into an enterprise powerhouse, many suspected it always could be. Way back in the pack In a world with three or four big players, by just about any measure AWS is light years ahead of everyone with a market share lead that, as of last year, was 10 x bigger than its closest 14 competitors combined. Google is fourth in that mix behind Microsoft in second and IBM in third, according to numbers from Synergy Research . The bad news for Google is that it has under five percent marketshare as of Q4 2015. The good news is that it grew at a brisk 108 percent for the quarter, second only behind Microsoft's 124 percent growth rate. That means Greene has her work cut out for her, but she doesn't seem all that worried. She says AWS has a big lead simply because it got a head start on everyone else including Google. ""They were there in the public cloud long before Google. We didn't decide to do public cloud for about four years after AWS,"" Greene told TechCrunch. She says that AWS has a big chunk of what essentially is a very small piece of the potential market, and she believes her company has plenty of time to catch up and grab a substantial share of the remainder. While that's all true, it's worth pointing out that Google has had 6 years to work at this and in spite of all its resources, has managed to garner less than five percent of market share. The right woman for the job After having so much success, why did Greene want to go back to high-level executive job at a big company and take on this challenge to improve Google's cloud position? She says she just sort of fell into it, but given her background and experience in the enterprise, she certainly appears to be the right person for the job. One thing led to another and we were having trouble finding someone, and I eventually said I would do it. As she tells it, she had a lot of conversations with the folks at Google as part of her job as an Alphabet board member, and she began to see a role for herself there. It all started when she became friendly with Urs Hölzle, senior VP of technical infrastructure at Google Cloud while walking their dogs together. She knew they were ramping Google Cloud pretty aggressively and as their friendship grew, they were discussing possible candidates for the role to run the overall cloud business. ""He is a totally brilliant and wonderful person. We started taking our dog for walks and became pretty good friends,"" she explained. Eventually they focused on her. ""One thing led to another and we were having trouble finding someone, and I eventually said OK I would do it - and here I am,"" she said. With Greene, Google scored someone with a tremendous enterprise pedigree. At the time of her hiring in November, Steve Herrod, who was CTO at VMware under Greene, and who is currently managing partner at venture capital firm General Catalyst spoke of her in glowing terms . ""She is awesome and immediately changes the game for Google's cloud efforts. The engineering team at bebop was outstanding as well and they'll bring a ton of enterprise DNA to Google,"" he told TechCrunch at the time. Taking care of business When Greene came on board, the cloud business was fragmented and one of the first things she did was unify all of the pieces under a single umbrella with her at the top of the unit. Sundar Pichai outlined the new organization in a company blog post when he announced Greene's hiring. As he stated at the time , Greene would be in charge of a newly integrated enterprise cloud businesses, that brought together Google for Work, Google Cloud Platform, and Google Apps under a consolidated product, engineering, marketing and sales team. She said one of the reasons for this single Google Cloud view was her experience at VMware where they valued integrated execution across the company. She felt that, in order for this to work well, all of these cloud pieces had to be working in sync with a consistent message up and down the entire cloud stack. The other thing she emphasized at VMware that she brought to Google was the importance of building a broad partnership network. ""We were super friendly to partners at VMware and I brought that in here. There are huge opportunities to partner with companies and we've been accelerating that very quickly. Google is committed to open source and open APIs and part of that is creating a partner-friendly place,"" she explained. While she wasn't ready to name names yet because the ink was still drying on some of the agreements, Google did let me know that they have 13,000 partners in the network, so it's not something they just started after Greene came to the company, but she is attempting to build on that existing effort. The enterprise heats up As Greene applies her enterprise chops to the Google Cloud platform, she rightly sees a market that's heating up and one that Google should by all rights be well-positioned to grab a big piece of. ""The enterprise has become a super interesting and exciting place,"" she said. ""Everybody is realizing they have to digitally transform themselves ."" She sees that cloud having a big role in that as it allows companies to communicate better, flatten hierarchies and move much more quickly than they could running equipment inside a data center. The enterprise has become a super interesting and exciting place. - Diane Greene, Google One of the big difference makers Greene is seeing is the power of data and the role the cloud plays in that. ""It's all about data and getting insights out of data. This is all fairly recent, that everybody is like 'whoa, if I don't do this and my competitor does, I'm going to be left behind,'"" she said. As you might expect, she sees Google as a strong contender for these transforming businesses because it has had to do this itself as a company, building data centers, processing huge amounts of data and applying analytics to it to understand what it has. She sees that experience as a big differentiator for her company. ""It is Google's time for enterprise. Google has a lot to offer the world and it all came together in a nice way. There was just a lot of effort going on around the enterprise that has set things up really well,"" she explained. That could be true, but Google needs to win the hearts and minds of enterprise IT staff, who might mistakenly see Google as merely Google Apps and not the entire cloud stack it has become. Having a spokesperson like Greene should help in that regard and she says a big part of her job is talking to customers and finding out what they need and how Google can help. There's plenty of time AWS is clearly the big dawg at the moment, but Greene doesn't seem all that worried. In spite of AWS existing for a decade, she sees Google as a worthy competitor in a market that's just getting started . ""I don't feel like we both started off building cloud at the same time. We came from different places. The place where we are behind and that's changing quickly is in workloads. Every customer wants to run on all the clouds. Pilots have been deployed and are up and running and growing incredibly fast right now,"" she said. She gets the current landscape, but with her in charge, she sees a market that's wide open for competition and a company that's ready to seize that opportunity. ""They have more workloads and features and more partners, but those are very straight forward things to change."" she said. Featured Image: Courtesy of Google",en,60
238,188,1459792892,CONTENT SHARED,4814419120794996930,268671367195911338,-3964029441019551093,,,,HTML,http://fortune.com/2016/03/27/netflix-predicts-taste/,"netflix says geography, age, and gender are ""garbage"" for predicting taste","Netflix rolled out to 130 new countries earlier this year, and you might expect that it began carefully tailoring its offerings for each of them, or at least for various regions. But as a new Wired feature reveals, that couldn't be further from the truth-Netflix uses one predictive algorithm worldwide, and it treats demographic data as almost irrelevant. Get Data Sheet , Fortune 's technology newsletter. ""Geography, age, and gender? We put that in the garbage heap,"" VP of product Todd Yellin said. Instead, viewers are grouped into ""clusters"" almost exclusively by common taste, and their Netflix homepages highlight the relatively small slice of content that matches their taste profile. Those profiles could be the same for someone in New Orleans as someone in New Delhi (though they would likely have access to very different libraries ). Netflix seems to have discovered (or built on) a powerful insight from sociology and psychology: That in general, the variation within any population group is much wider than the collective difference between any two groups. So if you want to, say, get someone to stream more of your content, you're better off leveraging what you know about similar individuals in completely different demographic groups, than trying to cater to broad generalizations. For more on Netflix, watch our video: As an example, Wired shares that 90% of Netflix's total anime streaming volume comes from outside Japan. That's because how much you like anime is determined less by your nationality than by (pardon me) how big of a nerd you are. There's a huge, crucial lesson here for other businesses-and perhaps a slightly scary reality for consumers. In the era of big data, consumer profiling can't rely on broad categories like race or location. To target the customers who want what you're offering, you have to get past the surface and see what really makes them tick.",en,60
239,599,1461695700,CONTENT SHARED,-5571606607344218289,6003902177042843076,119592526124196527,,,,HTML,http://www.infoq.com/br/news/2015/06/falta-compentencias-testes,agile: falta competência nos testes,"Fran O'Hara, diretor e principal consultor da Inspire Quality Services, recentemente compartilhou suas lições aprendidas integrando teste s no ciclo de vida do desenvolvimento ágil . A principal mensagem de O'Hara é que competências de teste são necessárias mesmo quando os papéis tradicionais de testes não existem. Quando equipes ágeis focam somente em automatização de testes funcionais, falhas aparecem em termos de testes exploratórios e outras áreas de risco tais como: testes de integração de sistemas e testes não funcionais (desempenho e usabilidade, por exemplo). Embora o objetivo seja ter equipes multi-funcionais, estas não devem ser compostas somente de especialistas (como um testador, um desenvolvedor, um designer, entre outros) ou por profissionais com formação generalista. Neste último caso a equipe pode não ter as habilidades maduras o suficiente para entregar efetivamente um software. Competências de testes em particular (como levantar casos de teste, esclarecimento de requisitos de negócio ou testes automáticos limpos) tendem a se perder quando organizações interpretam o Scrum ao pé da letra, assim, montando equipes compostas apenas por desenvolvedores (além do Scrum Master e do Product Owner). De acordo com O'Hara, equipes com profissionais com habilidades de testes regulares tem desempenho melhor do que equipes que não tem, provando a necessidade de misturar as habilidades e até mesmo os traços de personalidades (testadores tendem a ter uma pedante atenção aos detalhes para encontrar equívocos e brechas nos requisitos antes que eles sejam inseridos no produto) da equipe. Outras competências tipicamente atribuídas para gerente de testes ou qualidade no desenvolvimento tradicional como definição do processo e estratégia de testes e planejamento de testes podem ser necessárias no desenvolvimento ágil, adiciona O'Hara. Por um lado, muitas organizações ainda agrupam novas funcionalidades em grandes versões, que exigem algum nível de integração de sistemas para entregar. Por outro lado, quando equipes de desenvolvimento ágil que fazem testes durante o desenvolvimento (usando TDD e BDD por exemplo) ainda focam tipicamente em aspectos funcionais das estórias de usuário, deixando de fora o desempenho, usabilidade e outros requisitos não funcionais. O último ainda precisa ser executado em algum momento antes da entrega, necessitando assim de coordenação e planejamento. O'Hara recomenda o apontamento de campeões ou consultores de testes que podem trabalhar aconselhando múltiplas equipes sobre como preencher competências de testes faltantes. O objetivo final é alcançar uma integração completa dos testes nas equipes multi-funcionais, como no cenário C da imagem a seguir: O'Hara alerta que múltiplas práticas de trabalho são necessárias para alcançar este nível de integração de testes, a partir de uma abordagem de testes de aceitação, incluindo tarefas de testes (como testes de ambiente / configuração inicial ou testes exploratórios) no planejamento da sprint, limitando o trabalho em progresso em algumas histórias de cada vez, evitando que os testes e validações sejam o último passo (por exemplo, ""validação"" ser a última coluna do quadro de tarefas é um mau sinal), efetivo e frequente refinamento do backlog, foco na qualidade do código (padrões de código, revisões, TDD, BDD) e aderente a uma rigorosa definição de pronto (mesmo em tempos de estresse). Discutindo os requisitos de uma perspectiva de negócio durante o refinamento do backlog (diminuindo a extensão do planejamento da sprint) é também uma atividade crucial, mas de acordo com O'Hara, para que esta atividade seja efetiva o Product Owner deve trazer para a mesa não apenas funcionalidades de alto nível, mas também um conjunto inicial de critérios de aceitação que podem em seguida ser discutido e refinado pela equipe em pequenas estórias dentro do escopo. Finalmente, retrospectivas devem focar em melhorar a definição de pronto para ajudar a gerenciar débito técnico e garantir a qualidade interna (código) e externa (funcional e não funcional), diz O'Hara.",pt,60
240,2115,1471607487,CONTENT SHARED,2417383163258637695,7645894863578715801,5690372062060589522,,,,HTML,https://m.signalvnoise.com/eat-sleep-code-repeat-is-such-bullshit-c2a4d9beaaf5?gi=699b51bde52c,"""eat, sleep, code, repeat"" is such bullshit - signal v. noise","I'm on my way back home from Google I/O 2016. It was a fantastic conference - I met some great people and learned a lot. But while I was there, I saw something horrifying, something I couldn't shake from the moment I saw it... ""Eat. Sleep. Code. Repeat."" was printed on everything. I'd seen the phrase before, but this time it burned into my brain, probably because it was being so actively marketed at a large conference. I literally let out an ""ugh"" when I saw it. What's the big deal? It's just a shirt. Look, I get it - Google I/O is a developer conference, and the ""eat, sleep, code, repeat"" phrase is intended to be a clever way (albeit a completely unoriginal one) of saying ""coding is awesome and we want to do it all the time!"" I appreciate the enthusiasm, I do. But there's a damaging subtext, and that's what bothers me. The phrase promotes an unhealthy perspective that programming is an all or nothing endeavor - that to excel at it, you have to go all in. It must be all consuming and the focus of your life. Such bullshit. In fact it's the exact opposite. At Basecamp I work with some of the best programmers in the world. It's no coincidence that they all have numerous interests and talents far outside of their programming capabilities. Whether it's racing cars, loving art, reading, hiking, spending time in nature, playing with their dog, running, gardening, or just hanging out with their family, these top-notch programmers love life outside of code. That's because they know that a truly balanced lifestyle - one that gives your brain and your soul some space to breathe non-programming air - actually makes you a better programmer. Life outside of code helps nurture important qualities: inspiration, creative thinking, patience, flexibility, empathy, and many more. All of these skills make you a better programmer, and you can't fully realize them by just coding. Don't believe the hype It's no secret that the tech industry loves hyperbole. How will you ever reach the coveted title of ninja, rock star, or wizard if you don't spend all your waking, non-eating hours programming?! I'll give my standard advice: ignore the hype. It's wonderful to be so dedicated to your craft that programming is all you ever want to do. I love that enthusiasm. It can carry you to great heights. But if you want to become the very best programmer you can be, make space for some non-programming activities. Let your brain stretch its legs and you might find a whole new level of flow. ��",en,60
241,1969,1470224517,CONTENT SHARED,7087600436874507849,3609194402293569455,-2345143130174579669,,,,HTML,http://www.inova.unicamp.br/inovajovem/workshop-de-design-thinking/,workshop de design thinking - unicamp,"O Workshop de Design Thinking é a primeira capacitação do Programa Inova Jovem 2016. Sob o comando de Luiz Borges, engenheiro de produto e desenvolvimento da 3M, a atividade pretende introduzir a metodologia Design Thinking e apresentar as etapas de montagem de um projeto, a fim de qualificar os alunos para a entrega das propostas durante a fase de inscrição de projetos no Inova Jovem. O evento contará com uma dinâmica. É indicado que os alunos inscritos levem materiais para auxiliar na atividade, como: papéis coloridos, tesoura, lápis, giz de cera, canetinha, fitas adesivas, tampinhas de garra, canudo, palito de sorvete, entre outros. As vagas são limitadas e a capacitação não é obrigatória para a inscrição de projetos na competição. Inscrições abertas: de 1º a 23 de agosto. Data: 27 de agosto, das 9h às 13h Local: Auditório 5 da FCM (Faculdade de Ciências Médicas) da Unicamp",pt,60
242,2266,1473162790,CONTENT SHARED,-6772940823843058601,-1402490765047599382,6366377273742956136,,,,HTML,http://exame.abril.com.br/tecnologia/noticias/visa-inaugura-centro-para-inovacao-em-pagamento-digital,visa inaugura centro para inovação em pagamento digital | exame.com,"São Paulo - A Visa deu mais um passo para acabar com o cartão de crédito físico. A empresa inaugura nesta quinta-feira seu primeiro centro de inovação no Brasil, localizado em sua sede na capital paulista. O foco é o desenvolvimento de novos métodos de pagamento digital a partir de parcerias com startups , empresas e desenvolvedores. ""O centro de inovação muda a forma como dialogamos com nossos desenvolvedores e clientes"", falou a EXAME.com Érico Fileno, diretor executivo de produtos da Visa no Brasil. ""Ele amplia a atuação da empresa e desenvolve inovações de uma forma mais rápida."" A ideia é que a parceria com startups traga inovação a pagamentos digitais. ""Queremos mudar a imagem de que um produto foi 'feito pela Visa' para 'habilitado pela Visa'"", falou a EXAME.com Percival Jatobá, vice-presidente de produtos da Visa no Brasil. Neste momento, o foco da Visa é em duas tecnologias que podem mudar o mercado de pagamentos digitais: biometria e internet das coisas. ""A internet das coisas vai transformar a maneira como a sociedade faz e recebe pagamentos"", diz Jatobá. ""Não teremos conexão com uma máquina, mas com várias máquinas ao mesmo tempo."" Segundo uma pesquisa da empresa, 50 bilhões de máquinas estarão interconectadas em 2020. Para o executivo, essas mudanças não assustam. Ele explica que a Visa passou por um momento de educar o mercado na transição do cartão de tarja para o chip (com o qual se usa uma senha eletrônica). ""Naquela época, fizemos um projeto de aculturamento do comércio. Com pagamentos móveis, teremos que fazer esse processo novamente."" Para que tudo isso fosse possível, no entanto, uma mudança iniciada no começo deste ano foi crucial: a permissão para que desenvolvedores externos acessem os sistemas da Visa . ""Apenas com a abertura dos nossos sistemas conseguiremos criar novas tecnologias"", falou a EXAME.com José María Ayuso, vice-presidente de produtos da Visa na América Latina. Uber dos pagamentos Pelo programa Visa Developer , a empresa já concede acessos a 11 APIs (sigla para interfaces de programa de aplicação), que permitem a comunicação entre sistemas da Visa e externos. Kits de desenvolvimento de software também foram disponibilizados. Essa estratégia, diz Ayuso, faz parte da ""uberização"" do mercado. O termo é uma alusão ao Uber , que não criou sistemas de mapeamento, pagamento ou comunicação. Em vez disso, usou APIs abertas para viabilizar o serviço. O espaço em São Paulo não é o primeiro do tipo. Outros estão em cidades como Dubai, São Francisco e Londres. Aqui, fica localizado dentro da própria sede da empresa. O espaço tem clima mais descontraído do que o restante da empresa. Produtos como relógios , pulseiras e anéis de pagamentos ficam disponíveis para testes. Ayuso diz que o Brasil foi uma escolha fácil. ""O país comporta mais da metade do volume de negócios da Visa na América Latina. De todas as transações de e-commerce feitas na América Latina, 60% são feitas aqui."" Outro fator que pesou foi o fato de que os bancos brasileiros inovam rápido. Neste início, a Visa fechou uma parceria com a Farm, aceleradora de startups. ""Afinal, a Visa também já foi uma startup e se tornou uma grande empresa devido ao pensamento disruptivo de um indivíduo, o nosso fundador"", diz Jatobá. Centros como esse podem ajudar a acabar com os cartões físicos. Mas é um plano para longo prazo. ""Talvez, no futuro, as pessoas não utilizem mais o cartão de crédito físico. Mas não acredito que a minha geração ou ainda a próxima irá descarta-lo"", diz Ayuso.",pt,60
243,1449,1466398173,CONTENT SHARED,-4338308747999225618,-1032019229384696495,2118450654323055722,,,,HTML,http://www.businessinsider.com/how-diane-greene-transformed-googles-cloud-2016-6,how the queen of silicon valley is helping google go after amazon's most profitable business,"Google The first thing to understand about Diane Greene, the woman Google acqui-hired in November to transform its fragmented cloud business, is that she has the mind of an engineer. Cool technology, elegantly designed and built, lights her up.Even her jokes tend to be geek oriented. (A lifelong competitive sailor, she was a mechanical engineer who built boats and windsurfers before she became an iconic Silicon Valley computer scientist.) The second thing to understand about her is that she hates the limelight. While she's fine with standing on stage talking about all the cool things Google is building for their new target customer, big companies, she prefers not to talk about herself. In fact, she's so ego-free, her office at Google's Mountain View, California, headquarters is just a tiny windowless room, big enough to hold an ordinary desk and two chairs. Business Insider Before she took the job, Google had been building products and pursuing business customers in a sort of hodgepodge way. Its Google for Work unit had Google Apps, Chromebooks, and an assortment of other products like videoconferencing. It had poached Amit Singh from Oracle a few years back to help turn Google Apps into a more professional business unit, capable of taking on Microsoft Office. He had hired salespeople and created a support organization. (He's since moved on to work for Google's young virtual reality unit .) But Google for Work wasn't working very closely with Google's nascent cloud computing business, running under Urs Hölzle. That unit included a huge cadre of people running Google's data centers (600 computer security experts alone, for instance), but only a small separate sales force. In the seven months since Greene came in that's changed. She: hired experienced enterprise sales and support personnel. created the office of the CTO which handles the technical questions, design or customization a large customer needs. created units that focus on specific industries, because an agriculture firm has different needs than a retailer. created programs for getting more ""reseller"" partners on board, the small consultants who will sell and support Google's cloud to smaller customers, offering niche services. created a Global Alliance program for working with big global partners. ""So these are all new,"" Greene tells us. Now, all the teams are working together. ""We all get together once a week, we share and discuss and debate,"" she says. ""It wasn't possible before I came because sales and marketing were in a different division than cloud. And cloud was in a different division than Apps. I feel like the structure is in place now and we're hiring very aggressively."" Hölzle wooed her to the job Greene made her name as cofounder of VMware, with her famous Stanford professor husband. Vmware has gone on to become a giant tech company. She left the VMware CEO role about eight years ago, after EMC bought it. Google+ Until taking this Google job, she was quietly doing her own thing, raising her kids, advising and angel investing in startups (many of which did spectacularly well ), and being on a few boards, including Google's board since 2012. She was under the radar but still highly and widely respected, the queen of enterprise computing. She was also working on a new startup, Bebop Technologies, until Google bought it for $380 million when it hired her. Greene's take was $149 million, and she and her husband dedicated that money to charity. Hölzle, the engineer who famously built Google's data centers and runs the technical side of the cloud business is Greene's partner. He believes that within a few years, Google's cloud business can be bigger than its ad business. That's a big goal: Google currently makes the vast majority of its $75 billion in annual revenue from ads. Hölzle is the one who talked Greene into taking this job as they hung out walking their dogs together. ""Through being on the board, I got to know Urs and started working with him informally,"" Greene says. ""We knew we needed an overall business leader. He's a brilliant person and fun to work with. He really wanted to me to do it. I just realized, wow, partnering with Urs, we can really do this, with the backdrop of Google which is just this amazing company,"" she says. A new phenom Google has placed itself at the center of one of the biggest, newest trends happening in the enterprise market. Some people call this trend digital transformation. But it's more than just automating manual processes or turning paper forms into iPad apps. Flickr/Amanda Parsons More and more, the IT departments at large companies have started treating their tech vendors as partners that help them co-create the tech they need. ""This is new for me. I've never been in the enterprise where your customers are your partners. It was always, you had customers and you had partners. But almost every customer of a certain size is a partner. It's going both ways now,"" Greene says. She points to one customer, Land O'Lakes, as an example. Land O'Lakes is probably best known for its butter and dairy products. It took crop and weather data from Google and worked with Google to build an app hosted on Google's cloud. The app helps its farm and dairy co-op members improve their crop yields. ""It's fun for us to help them do that,"" she says. Unlike the old days, where an IT company would be the one to build the app and sell it to agriculture companies, ""we don't have to do it ourselves."" 'More and more' This idea of partnering with customers is the key to her strategy. Tim Stenovec/Business Insider ""For me, this is such a revolution,"" she says. ""Everything is changing now that we are in the cloud in terms of sharing our data, understanding our data using new techniques like machine learning."" Google's competitive strength, Greene believes, is the breadth of the tech it can offer an enterprise. Enterprise app developers can tap into things like Maps, Google's computer vision engine (the tech that powers Google Photos), weather data, language/translation/speech recognition. They can build apps on top of Google's Calendar, documents, spreadsheet and presentation apps. And, under Greene's new integrated organization, they can even tap into the tech that powers Google's ads or YouTube, search, or its many other services. ""And we're going to have more and more,"" she says. When a company can take its own data and combine it with all of Google's technology and Google's data, ""there's just huge possibilities,"" she says. Google Greene will tell you, ""We're the only public cloud company with all of that."" When pointing out that Microsoft also offers a computer vision API, translation services and APIs for Office 365, IBM also offers weather data and language services, and so on, Greene's got a come-back ready. ""We have Chromebooks."" Well, Microsoft has Surface. ""But Chromebooks can run all the Android apps, are totally secure, they have administration ... and they have a nice keyboard,"" she laughs. In fact, Greene says, ""I only use a Chromebook now. I never thought I could do that but I love it."" She's watching Amazon In truth, she's not laser focused on overtaking Microsoft, widely considered the No. 2 cloud player, with Google trailing behind. Google She, like all the cloud vendors, are looking at market leader Amazon Web Services, which is raking in the enterprise cloud customers. AWS is even convincing a growing number of them to shut down all of their data centers and just rent everything from AWS. This includes Intuit , the other company where Greene is a board member. AWS is so successful it's currently on track to do $10 billion in revenue this fiscal year and it's also Amazon's most profitable business unit . And it blows all the competition out of the water in the sheer number of features on its cloud , as well as its partner ecosystem. So how is she going to beat Amazon? By offering better tech, she says. ""I'm a little biased but I really do think, on the hard stuff, we're the world's best cloud,"" she says. Google ""I agree we have more features to do, although we have the basics for enterprise that you need. We have more partners to bring on, but we're doing that very quickly. But the hard stuff, I do think we're the world's best."" While Greene would not share the the cloud unit's growth numbers, she says that ""growth is really good and we're doing great stuff with some really big customers."" She adds: ""We've been moving customers to our cloud both from Amazon and on-prem."" 'On-prem' means getting companies to move the apps they have running in their own computers on their own premises into Google's cloud. Google has even been engaging Amazon with its price cuts war, she says. ""They've been following our price cuts. We've been initiating them,"" she says. She jokes, ""We should make a T-shirt 'The highest quality, lowest-cost cloud.'"" Disclosure: Jeff Bezos is an investor in Business Insider through his personal investment company Bezos Expeditions. SEE ALSO: Netflix, Juniper, and Intuit explain how Amazon is eating the $3.5 trillion IT industry SEE ALSO: The inside story of why Cisco's 4 most powerful engineers really quit - and why many employees are relieved NOW WATCH: NASA released the sharpest photos of Pluto in history - and they're spectacular",en,60
244,2222,1472499986,CONTENT SHARED,2103268612948910635,1908339160857512799,-8312304323942241499,,,,HTML,http://revistamarieclaire.globo.com/Noticias/noticia/2016/08/nao-ao-preconceito-onu-celebra-dia-da-visibilidade-de-mulheres-lesbicas-e-bissexuais.html,não ao preconceito! onu celebra dia da visibilidade de mulheres lésbicas e bissexuais,"ONU celebra o Dia da Visibilidade de Mulheres Lésbicas e Bissexuais (Foto: Reprodução / Faceboook / Carolina Rosseti) Com o propósito de dar visbilidade às contribuições de lésbicas e bissexuais para a sociedade, a ONU lança nesta segunda-feira (29) a campanha ""Livres & Iguais"". A data foi escolhida por ser o Dia da Visibilidade de Mulheres Lésbicas e Bissexuais. As Nações Unidas veicularão, nas redes sociais, três vídeos de conscientização e ilustrações da designer Carolina Rosseti com histórias de algumas de mulheres. A morte de Luana Barbosa dos Reis em abril deste ano vítima de espancamentos por oficiais da Polícia Militar foi um caso destacado pelo organismo internacional. Ela era negra, lésbica e moradora da periferia de Ribeirão Preto, em São Paulo. A campanha ressalta também a importância do apoio familiar. ""Quando os direitos LGBTI são afetados, todos nós somos afetados"", afirmou o coordenador residente do Sistema ONU no Brasil, Niky Fabiancic. Segundo ele, a luta por uma sociedade mais justa para gays, lésbicas, bissexuais, pessoas trans e intersex é um compromisso da ONU. Jaime Nadal, representante nacional do Fundo de População das Nações Unidas (UNFPA), lembrou que, em outubro do ano passado, a ONU expressou preocupação com a tramitação do Estatuto da Família (PL 6583/2013) no Congresso. O organismo internacional fez uma pelo para que o governo reconhecesse todos os arranjos familiares existentes no Brasil, não apenas aqueles formados por casais heterossexuais. ""É muito importante que uma nova legislação garanta os direitos dos casais LGBT que a jurisprudência hoje já permite"", reiterou Nadal.",pt,59
245,524,1461211879,CONTENT SHARED,7229629480273331039,-1032019229384696495,-6158072573466935774,,,,HTML,http://www.businessinsider.com/organization-ranks-google-as-best-cloud-2016-4,"an independent organization just ranked google as the best cloud, beating amazon","Fortune Global Forum Amazon may be the 800-pound gorilla of the cloud computing market, but some new research indicates that Google offers the best cloud in the for the money. So says a cloud benchmark of top US clouds developed by French IT business news site JDN, based on a similar benchmark of clouds in France . JDN banded together with two companies that test cloud performance, CloudScreener and Cedexis, to determine the best clouds. They looked at Amazon Web Services, Google Compute Engine, IBM SoftLayer, Microsoft Azure, and Rackspace. Cloudscreener tested the clouds for performance, prices and level of service. Cedexis tested for network performance. All told, they came up with four rankings: a price ranking, a performance ranking, a service level ranking (a way to measure reliability), and an overall ranking. Google won the overall ranking with a score of 85 out of a 100. Amazon came in second with 75 and IBM was third at 72. (They're not listed on the chart, but Microsoft Azure landed in No. 4, with 63 and Rackspace in last with 62.) JDN/CloudScreener/Cedexis U.S. Cloud Benchmark Google's victory is largely because it won the price war, coming in as less expensive than the others for the same type of cloud configuration. The index tested three common types of cloud services companies buy: a large Windows machine, a large Linux machine, and a combination of machines (a ""cluster""). JDN/CloudScreener/Cedexis U.S. Cloud Benchmark Rackspace won as the best performing cloud. And Amazon won the ""Service Level"" rankings which looked at things like having data centers in different regions and certifications that prove the cloud is secure. While this is just one attempt to compare and rank clouds, it's a good sign for Google, because most enterprises will do their own kinds of testing before picking them. Right now, Amazon is the big cloud leader. But the market is young and Google intends to be a player. ""Every business in the world is going to run on cloud eventually,"" Google CEO Sundar Pichai said a few months ago on a post earnings call . ""There's great buzz at Google around this area, and we continue to heavily ramp up investment here."" Disclosure: Jeff Bezos is an investor in Business Insider through his personal investment company Bezos Expeditions.",en,59
246,2160,1471966811,CONTENT SHARED,2468005329717107277,-709287718034731589,-8543976727396364315,,,,HTML,https://uxdesign.cc/how-netflix-does-a-b-testing-87df9f9bf57c?gi=27177b9af62,how netflix does a/b testing - uxdesign.cc - user experience design,"How Netflix does A/B Testing Have you ever wondered why Netflix has such a great streaming experience? Do you want to learn how they completed their homepage plus other UI layout redesigns through A/B testing? If so, then this article is for you! I'll start with sharing my takeaways from a Designers+Geeks event I attended last week at Yelp . The two great speakers Anna Blaylock and Navin Iyengar, both product designers at Netflix , walked through insights gleaned from their years of A/B testing on tens of millions of Netflix members, and showed some relevant examples from the product to help attendees think about their own designs. Experimentation I really liked this first slide of the presentation and think it's smart to use an image from the TV show "" Breaking Bad "" to explain the concept of experimentation! The Scientific Method Hypothesis In science, a hypothesis is an idea or explanation that you then test through study and experimentation. In design, a theory or guess can also be called a hypothesis . The basic idea of a hypothesis is that there is no pre-determined outcome. It is something that can be tested and that those tests can be replicated. ""The general concept behind A/B testing is to create an experiment with a control group and one or more experimental groups (called ""cells"" within Netflix) which receive alternative treatments. Each member belongs exclusively to one cell within a given experiment, with one of the cells always designated the ""default cell"". This cell represents the control group, which receives the same experience as all Netflix members not in the test."" - Netflix blog Here's how A/B testing is done at Netflix: as soon as the test is live, they track specific metrics of importance. For example, it could be elements like streaming hours and retention. Once the participants have provided enough meaningful conclusions, they move onto the efficacy of each test and define a winner out of the different variations. Experiment Experimentation is the act of experimenting .Many companies like Netflix run experiments to generate user data. It is also important to take time and effort to organize the experiment properly to ensure that both the type and amount of data is sufficient and available to clarify the questions of interest as efficiently as possible. You probably have noticed that the featured show on the Netflix homepage seems to change whenever you log in. They're all part of Netflix's complex experiments to get you to watch their shows. The idea of A/B testing is to present different content to different user groups, gather their reactions and use the results to build strategies in the future. According to this blog post written by Netflix engineer Gopal Krishnan : If you don't capture a member's attention within 90 seconds, that member will likely lose interest and move onto another activity. Such failed sessions could at times be because we did not show the right content or because we did show the right content but did not provide sufficient evidence as to why our member should watch it. Netflix did an experiment back in 2013 to see if they can create a few artwork variants that increase the audience for a title. Here is the result: It was an early signal that members are sensitive to artwork changes. It was also a signal that there were better ways they could help Netflix members find the types of stories they were looking for within the Netflix experience. Netflix later created a system that automatically grouped artwork that had different aspect ratios, crops, touch ups, localized title treatments but had the same background image. They replicated experiment on their other TV shows to track relative artwork performance. Here are some examples: How Netflix selects the best artwork for videos through A/B testing The Netflix experimentation platform , a service which makes it possible for every Netflix engineering team to implement their A/B tests with the support of a specialized engineering team. Check out these two blog posts to learn more about Netflix A/B testing: What I learned When and why A/B testing Once you have a design in production, use A/B testing to tweak the design and target two key metrics: retention and revenue. By A/B testing changes throughout the product and tracking users over time, you can see whether your change improves retention or increases revenue. If it does, make it the default. In this way A/B testing can be used to continuously improve business metrics. Are your users finding or doing one thing you want them to find or to do? My experience is that often times users cannot always complete a task as fast as you expect, and sometimes they can't even find a certain button you put on a page. The reasons can vary: it might because the design is not intuitive enough; the color is not vibrant enough; the user is not tech savvy; they don't know how to make a decision because there are too many options on one page, and so on. Are your intuitions correct? Sadly, when it comes to user behavior, our intuitions could be wrong, and the only way to prove it is through A/B testing. It is the best way to validate whether one UX design is more effective than another. At work, our consumer product team have proved that through A/B testing on our real estate website. For example, they wanted to figure out whether they can make a design change to improve the registration rate for users who clicked on a Google Ad. They created a few different experimental designs and tested them. They thought the design that only hides the property image would win, but found that the design that hides both the property image and the price got the highest conversation rate. Explore the boundaries The best ideas come from many idea explorations. At work, our product team works collaboratively across many different projects. With so many parties involved (from designers to product managers to developers), we get to explore the boundaries together. Some of the best ideas are sometime from the developers or the product managers after testing out our prototypes. Observe what people do, not what they say When talking to users, it's important to keep this in mind: they always say one thing but do it differently. I conducted a few user testing sessions this week and have one perfect example to show you why. I had this one user testing out a Contacts list view prototype and asked him if he usually sorts/filters his Contacts. He said no because he wouldn't need do so. However, when he discovered the new filters dropdown menu, he was amazed by how convenient it is to sort and filter multiple options at a time and immediately asked when that can roll out in production. Use data to estimate size of opportunity * It's always about the whys * Data can help shape ideas * Check if any A/B testing are in conflict A/B testing is the most reliable way to learn user behaviors. As designers, we should think about our work through the lens of experimentation. Isn't it so fun to be a UI and UX designer? :) Knowing your user is the most exciting part of design process! There is no finished design, but many chances for iteration to improve the design and give our users the best experience possible! I enjoy the opportunity to make subtle tweaks for our users, measure their reactions and work with the product team to figure out the next steps. If you like this article, please kindly tap the ❤ button below! :)",en,59
247,1244,1464962240,CONTENT SHARED,-541666025693385823,-1032019229384696495,7836530673378398837,,,,HTML,http://thenextweb.com/apple/2016/05/29/google-material-desing-ios-complaints/,calm down: google's use of material design on ios is fine,"A recent bit of commentary from Macworld posits that Google's use of Material Design on iOS is a mistake. Another site piled on , going so far as to call it ""dumb"" and ""rude."" The main takeaway is that design can be divisive, and is always subjective. Material Design is no different, but there needs to be a lot of chill on this one. I don't need to like it So, here's some personal insight: I fucking hate Material Design. I think it's too reliant on dumbed-down, wannabe 'clever' animations and feigned shadowing. It tries way too hard to look like it's not digital (Google's design lead Matias Duarte notes Material Design's creation had a lot of paper cut-outs at inception to help designers understand its 'real world' feel), but it is. Material Design wants to make you feel as though you're touching something tangible, but you're not. You. Are. Tapping. A. Screen. I don't care to have a button spin and slide away when I press it, and I don't need a clever spring-to-life page pop-up. So, no, I don't like Material Design. I hope that's very clear. Don't drink the 'haterade' I will, however, defend Google's right to use it as they please. Macworld points out that Google's widespread use of Material Design across platforms is akin to Microsoft in the 90's, and that's fair, but it didn't end there. Microsoft is still committed to its square design (which I'm also not fond of, and have now realized I'm some sort of design-snob wolf in developer's sheepish wool), but nobody seems concerned about that. Instead, we laud it for making its apps and services available across platforms. The convenient trope is 'if you don't like it, don't use it,' but Google is hard to avoid. I do for the most part, though; I use DuckDuckGo for search , and have eschewed just about all Google-y apps for iOS. There are many reasons for that, and design is definitely part of it. But many can't - or won't - move away from Google. That should tell us that while design is fundamentally important, it takes a backseat to experience, and Google does a great job there. I do think we should be celebrating diversity in design, even if we don't like it. The rare times I have to dig into my Google Drive folder (it's literally my lone holdout; damn you, cloud storage!), I'm reminded just how much i dislike Material Design. As for the narrative that Google is trying to give iOS users a 'sneak peak' of Android - that's just laughable. Design isn't likely to make anyone switch platforms. Gotta love the sensationalism, though. Be fair The opposite point can be made for Apple. Material Design was built with the Web in mind, so should the iCloud Web client use Material Design? Hell no. It's a slippery slope. If we're asking Google to adhere to Apple's design philosophy for native apps, are we also going to ask Fantastical to look and act more like Apple's (terrible) stock calendar? Should PCalc be more like Apple's (boring) calculator, or your favorite note-taking app more like - well, Notes? ( which is awesome , so yes?) The orginal argument makes Material Design about platforms, but it's not. Google designed it to be used anywhere and everywhere because it's 2016 and platforms aren't as important. The Android vs. iOS debate is tired, and we should stop entertaining it. I doubt Apple cares how many Apple Music users are on Android versus iOS; Google is also a company of services, not a single platform. the same is now true for Microsoft. There's something to be said for using platform-specific tools, but even that's a grey area. Unless Google did something like completely disable the share sheet on iOS, I don't see much harm. Material Design is a shitshow, but it's Google's shitshow. I don't have to stop and look (unless it's Drive; stupid easy-to-use cloud storage). I think a lot of what Apple does with design sucks too, and developers like to bitch and moan about how Apple makes it hard to paint outside the lines in Xcode. And shouldn't we be celebrating diversity on iOS, which has suffered through that type assimilation for a long time? We say 'damn Google for Material Design,' then complain about Apple making it difficult to stray too far from center when it comes to making apps. Maybe we should stop complaining so much on the internet (now that's laughable). Be fair, be objective, and chill out. Also, try Pages or iCloud Drive on iOS. Complaints about Material Design seems downright trite when you attempt to use Apple's services on mobile. At least Google nailed UX, even if its UI is terrible. Read next: Bring your surroundings to life with the futuristic Touch Board DIY Starter Kit",en,59
248,1017,1463617819,CONTENT SHARED,-1995591062742965408,-9016528795238256703,5698124852693708605,,,,HTML,https://firebase.google.com/docs/test-lab/,firebase test lab for android,"Test your app on devices hosted in a Google datacenter. Firebase Test Lab for Android provides cloud-based infrastructure for testing Android apps. With one operation, you can initiate testing of your app across a wide variety of devices and device configurations. Test results-including logs, videos, and screenshots-are made available in your project in the Firebase console. Even if you haven't written any test code for your app, Test Lab can exercise your app automatically, looking for crashes. Key functions Test on real devices Use Test Lab to exercise your app on physical devices installed and running in a Google data center. Test Lab helps you to find issues that only occur on specific device configurations (for example, a Nexus 5 running a specific Android API level and specific locale settings). Run app tests, even if you haven't written any You can use Robo test to find issues with your app so you can test your app even if you haven't written app tests. Robo test analyzes the structure of your app's user interface and then explores it, automatically simulating user activities. If you have written instrumentation tests for your app, Test Lab can also run those tests. Workflow integration Test Lab is integrated with Android Studio, the Firebase console, and the gcloud command line. You can also use Test Lab with Continuous Integration (CI) systems. How does it work? Test Lab uses devices running in a Google data center to test your app. The devices used for testing are real production devices that are flashed with updated Android API levels or locale settings that you specify so that you can road-test your app against a real-world collection of real devices and device configurations. Devices in a data center Test Lab lets you run Espresso , Robotium , or UI Automator 2.0 instrumentation tests written to exercise your app from the Firebase console, Android Studio , or the gcloud command line interface . You can also use Robo test to automatically exercise your app from the Firebase console or the gcloud command line. Robo test captures logs, creates an ""activity map"" that shows a related set of annotated screenshots, and creates a video from a sequence of screenshots to show you the simulated user operations that it performed. Learn more about Robo test . Implementation path If you are running instrumentation tests, write your app-specific test. When developing instrumentation tests for your app, remember to add the Test Lab screenshot library to your app test project so that you can more easily interpret test results. Choose a test environment and a test matrix. Using a test environment of your choice (the Firebase console, Android Studio, or the gcloud command line interface), define a test matrix by selecting a set of devices, API levels, locales, and screen orientations. Run your tests and review test results. Depending on the size of your test matrix, it may take several minutes for Test Lab to complete your test run. After your test run is complete, you can review test results in the Firebase console. Next steps",en,59
249,2318,1473688554,CONTENT SHARED,5008913705695203000,3302556033962996625,-7667010802419955831,,,,HTML,http://jakewharton.com/just-say-no-to-hungarian-notation/,post: just say mno to hungarian notation,"Just Say mNo to Hungarian Notation Every day new Java code is written for Android apps and libraries which is plagued with an infectious disease: Hungarian notation. The proliferation of Hungarian notation on Android is an accident and its continued justification erroneous. Let's dispel its common means of advocacy: "" The Android Java style guide recommends its use "" There is no such thing as an Android Java style guide that provides any guidance on how you should write Java code. Most people referencing this non-existent style guide are referring to the style guide for contributions to the Android Open Source Project (AOSP) . You are not writing code for AOSP so you do not need to follow their style guide. If you're working on code that might someday live in AOSP you don't even need to follow this style guide. Almost all of the Java libraries imported by AOSP do not follow it, and even some of the ones developed inside of AOSP don't either. "" The Android samples use it "" These samples started life in the platform inside of AOSP so they adhere to the AOSP style. For those which did not come from AOSP, the author either incorrectly believes the other points of advocation in this post or simply forget to correct their style when writing the sample. "" The extra information helps in code review "" The 'm' or 's' prefix on name indicates a private/package instance field or private/package static field, respectively, where this would otherwise not be known in code review. This assumes the field isn't visible in the change, since then its visibility would obviously be known regardless. Before I attempt to refute this, let's define Hungarian notation. According to Wikipedia , there are two types of Hungarian notations: System notation encoded the data type of the variable in its name. A user ID that was a long represented in Java would name a variable lUserId to indicate both usage and type information. Apps notation encoded the semantic use of the variable rather than it's logical use or purpose. A variable for storing private information had a prefix (like mUserId ) whereas a variable for storing public information had another prefix, or none whatsoever. So when you see the usage of a field, which piece of information is more important for the review: the visibility of that field or the type of that field? The visibility is a useless attribute to care about in a code review. The field is already present and available for use, and presumably its visibility was code-reviewed in a previous change. The type of a field, however, has a direct impact on how that field can being used in the change. The correct methods to call, the position in arguments, and the methods which can be called all are directly related to its type. Not only is advocating for 'apps' Hungarian wrong because it's not useful, but it's doubly wrong since 'system' Hungarian would provide more relevant info. That's not to say you should use 'system', both the type and visibility of a field changes and you will forget to update the name. It's not hard to find static mContext fields , after all. "" The extra information helps in development "" Android Studio and IntelliJ IDEA visually distinguish field names based on membership (instance or static): IDEs will enforce correct membership, visibility, and types by default so a naming convention isn't going to add anything here. A popup showing all three properties (and more) of a field is also just a keypress away. "" I want to write Java code like Google does "" While Android and AOSP are part of the company, Google explicitly and actively forbids Hungarian notation in their Java style guide . This public Java style guideline is the formalization of long-standing internal conventions. Android had originated outside of Google and the team early on chose to host the Hungarian disease. Changing it at this point would be needless churn and cause many conflicts across branches and third-party partners. With your continued support and activism on this topic, this disease can be eradicated in our lifetime. mFriends don't let sFriends use Hungarian notation! - Jake Wharton",en,59
250,1936,1469819995,CONTENT SHARED,6519443272707698315,8766802480854827422,-8534301860414756085,,,,HTML,https://www.linkedin.com/pulse/two-different-sales-motions-geoffrey-moore,two different sales motions . . . .,"I have been spending a fair amount of time recently with sales organizations selling SaaS applications to enterprises of all sizes around the globe, and the most successful ones are bumping up against a familiar challenge in sales management. To put it simply, what looks like one sales cycle that can be managed by one end-to-end pipeline model is diverging into two separate ones. Here's a snapshot of how it is playing out: Sales motion 1 optimizes around deals less than $500K, often significantly so. They are normally funded out of an existing IT budget, typically where a SaaS application is directly displacing a legacy on-premise predecessor. This is frequently the ""land"" move in a land-and-expand sales strategy, and the key is to find the openings, qualify them clearly and crisply, and then win the deals. These deals are typically quite competitive, both with the legacy vendor who is about to get displaced as well as with other SaaS vendors pursuing the same opportunity. Deal size is measured in terms of ACV, the annual contract value for the first year. Lead generation focuses primarily on top-of-the-funnel pipeline development based on connecting on-line or in person at an event, the goal being to schedule an on-site meeting with a key influencer. Middle of the funnel progress is a function of strong product and solution marketing and good systems engineers who can help navigate the specifics of the prospective customer's IT environment. Bottom of the funnel is all about closing, and the recurrent winners tend to be the people best at that art. If this all sounds familiar, it should. It has generated the bulk of SaaS enterprise sales bookings for the past decade. That is, unlike the prior era of client-server systems where big global deals dominated the landscape and later on trickled down to the mid-market, in SaaS it was the mid-market that took off first, with global deals coming in much more slowly. That's because the value proposition for a mid-market company was incredibly compelling-Global 500 quality infrastructure with very low cap ex, pay-as-you-go op ex, and low overhead for in-house IT. Interestingly, none of these value propositions are particularly compelling to G500 CIOs who have already invested most of the necessary cap ex, do not need to slow roll their op ex, and are deeply committed to an in-house IT organization regardless. For them the issues that dominate Sales Motion #1 are at best a second tier concern. Hence the need for Sales Motion #2. Sales Motion #2 focuses on modernizing an enterprise's current operating model by leveraging next-generation technology, whether to get ahead of the competition or to catch up to it. With all the process reengineering and change management entailed, the overall budget is typically multiple millions of dollars, even tens of millions, within which the IT portion is likely to be well north of $1 million. So the good news is this is going to be a big deal. The bad news is that there is no line item in the IT budget that corresponds to this work. As a result, the project has to be sponsored by the line-of-business executive who oversees the operations to be modernized. That person needs to redirect part of his or her budget to fund the IT portion of work. Until this commitment is secured, the opportunity is not fully qualified. The silver lining here is that the vendor who helps secure this commitment typically has the inside track to winning the deal, often without having it put out for competitive bid. That's because they have become such a trusted advisor and have made themselves so knowledgeable about the prospect's operations that it becomes hard for the customer to imagine working with anyone else. This sort of sales motion does not begin with conventional bottom-up lead generation. The key introduction to the line of business executive most often comes as a peer-to-peer referral from someone in your (or your company's) network who is happy to broker the relationship. At that first meeting the vendor must demonstrate sufficient thought leadership in the issues facing the executive that they commit to exploring the opportunity more deeply. Once serious interest has been confirmed, the middle of the sales funnel is taken up with solution architects conducting the consultative interactions needed to complete that exploration-often via one or more workshops followed up with deeper dives as needed-all leading up to a draft proposal. That proposal is couched first in business language that resonates with the line of business sponsor's vision and then translated into IT language that maps to the products and services needed to implement that vision. Closing is a matter of getting everyone's fingerprints on that proposal, both on the line of business and the IT side, including whatever partners and allies are needed to deliver on the total promise. This is an act of orchestration that requires open communication, collaborative spirit, and trust-hence the claim that collaborators win. Everyone gets that both sales motions are needed, and that most account executives are likely to be considerably better at one of them than the other. What sales executive do not acknowledge is that there are two distinct sales pipelines at work here requiring two distinct Lead-to-Close stage templates. Instead, everyone is expected to pour their opportunities into a single, standard template, which typically defaults to Sales Motion #1. And then they wonder why the organization struggles with Sales Motion #2. Enough is enough. Two playbooks require two templates, with AEs making the call as to which play they are running in each of their sales opportunities. Yes, this is more complicated, and no, it is not easy to do this in most CRM systems, but frankly, that has to change. And now would not be too soon. That's what I think. What do you think? _________________________________________________________________________ Geoffrey Moore | Zone to Win | Geoffrey Moore Twitter | Geoffrey Moore YouTube",en,58
251,1280,1465267165,CONTENT SHARED,5964888566269323942,-6316613156648676087,-378770909400450623,,,,HTML,http://itforum365.com.br/noticias/detalhe/119152/pwc-faz-virada-digital-e-ja-fatura-us-1-bilhao-no-mundo,pwc faz virada digital e já fatura us$ 1 bilhão no mundo,"Transformação Quando se imagina o dia a dia de uma das consultorias conhecidas como as Big Four logo vem à cabeça o ambiente austero herdado dos serviços de auditoria, centenas de engravatados discutindo estratégias e planos de longo prazo nas práticas de advisory, etc. Isso não morreu e nem é a ideia. Mas começa a ganhar força, ao menos na PwC, um ambiente novo, chamado internamente de Plataforma Digital. Mas por que novo? Porque muda o modelo de pensar, agir, negociar, vender e criar. Essa transformação interna para construção de uma área para competir na seara digital - até porque ninguém quer ficar de fora - começou na consultoria há algum tempo por meio de aquisições importantes e se consolida com a criação de centros de excelência e de experiência do cliente, onde se discute, cocria, aprende, experimenta, monta protótipos e se define passos importantes para o futuro de um negócio, seja ele de qual indústria for. A Booz & Company e a BGT talvez sejam as aquisições mais marcantes nesse processo e que ajudaram a fazer com que a área - caçula nessa gigante de 100 anos - já tenha um faturamento de US$ 1 bilhão no mundo. Mas a ideia é crescer e de acordo com um dos sócios e responsável por tecnologias emergentes na Plataforma Digital da PwC no Brasil, Norberto Tomasini. A aposta está na metodologia criada que antecipa, por exemplo, a experiência. Durante uma longa conversa na sede da consultoria na capital paulista, ele passou por diversos detalhes, exemplos de projetos, ainda mantidos em sigilo a pedido dos próprios clientes, e pelas mudanças de forma de pensar até internamente. É possível dizer que nasce da iniciativa uma nova PwC, onde o jeans e a camisa de mangas arregaçada são permitidos, a linguagem utilizada é mais informal e antenada com o que acontece não apenas no universo corporativo e uma equipe com talentos diversificados e que não para de buscar conhecimento. O próprio Tomasini passou no final de 2015 uma temporada nos Estados Unidos para entender o que estava se discutindo de vanguarda nos principais centros de excelência, como o MIT. ""Lá eles já discutem roupas que vão carregar baterias e os desafios futuros com carros elétricos pelo consumo excessivo de energia."" Voltando à metodologia de atuação da PwC na Plataforma Digital, ela passa por estratégia, inovação, experiência, tecnologia, analytics e termina em ativação/otimização. E a grande sacada, como frisou o sócio, está em poder experimentar o ""produto""ou a ""ideia"" muito antes. Ainda hoje, mesmo quando vai se implantar um software ou criar algum tipo de aplicativo, a fase de testes sempre está no final e muito próxima ao lançamento, o que compromete a qualidade de entrega e, consequentemente, a satisfação do usuário. Quem é o dono? Nesse mundo que se cria em torno do digital, que traz uma forma de agir e pensar diferente, não pode ser assim. O usuário penaliza e sem pena. Além da metodologia, algo que tem funcionado e contribuído para a geração de resultados do digital na PwC são os Centros de Experiência. Hoje o principal hub para a América Latina está em Miami, mas a partir de maio São Paulo ganhará uma versão, sendo o primeiro instalado na AL. A área ocupará um andar inteiro no prédio onde está a sede da consultoria, tendo 1 mil metros quadrados dedicados a buscar a melhor forma de o cliente encarar a jornada digital. ""Tenho um problema. A primeira pergunta é: pode resolver com digital? Quando a resposta é sim, hoje levamos alguns clientes para o centro de Miami para um workshop de três dias e ele sai de lá com um vídeo da solução proposta"", explicou Tomasini. Em meio a palestras, interações e discussões, tudo feito com ferramentas de última geração, mesas e lousas digitais, holografias, entre outros, uma equipe da PwC faz uma varredura em tudo o que foi discutido e começa a bolar uma espécie de protótipo, de maneira que o cliente tenha uma ideia mais clara do que está sendo proposto. ""Nós aceleramos a execução digital. Nosso forte está em efetivar propostas, porque o cliente sai com visão de como será o dia a dia após a implantação, isso é mais que um plano de mobilidade, por exemplo."" No Brasil, a expectativa é que a Plataforma Digital tenha oportunidades a explorar equivalentes a quase um terço do faturamento da prática de consultoria de TI. Se pensarmos que é um negócio recente, o volume é bastante interessante. Mas, mais interessante que o dinheiro na mesa, é como e com quem tem acontecido as negociações desse tipo de projeto. Diferentemente do que muitos imaginam, a TI neste caso não tem sido a grande porta de entrada, pelo menos quando se avalia os projetos fechados pela PwC. De acordo com Tomasini e com o sócio líder da área para América Latina, Sérgio Alexandre, as discussões normalmente acontecem com o CEO e, muitas vezes, com o CMO. Para Alexandre, tem sido muito comum o marketing abrir as portas para esse tipo de transformação por já vivenciá-la de certa maneira. Isso não significa, no entanto, que eles dão as costas para a TI. Todo Centro de Experiência tem um líder com DNA de tecnologia até para fazer a ponte entre o CIO e quem estiver liderando o projeto de digital na corporação.",pt,58
252,2788,1480423373,CONTENT SHARED,-7646922141533719881,1374824663945909617,6069689568663799842,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.100 Safari/537.36",SP,BR,HTML,https://www.linkedin.com/pulse/using-microservices-architecture-api-enablement-strategy-rafael-rocha,using microservices architecture as api enablement strategy,"Today Microservices architecture has become hype topic in software development industry. It is an approach to modularity which functionally decomposes an application into a set of services. The development teams can adopt the most appropriate technology stack in order to resolve specific issues. The microservices architecture also improves services scalability by enabling features such auto-scaling and micro-container approach. API's and Microservices are so related once the most common facade are API interface uses fully compliant RESTful services. But nowaday we are still facing issues when needs to integrate with legacy systems to externalize its functionalities as services once the most systems does not expose standards protocols such as Web Services or REST Interfaces. Let's explore this issue. The Problem and Microservice Approach The most companies desire to expose API's internally or externally but their systems or application were not built for this purpose. The most applications are based on the following architecture: Custom web monolithic applications using a single database. Eg: Java (JSF) with Oracle Database Product or platform based applications such as SAP application High-level applications such as cobol-cics programs Client-Server applications, for example VB6 with SQL Server Database When facing this kind of scenario, it is a common approach build an adapter component to expose the service with standard protocol. This component should look like the diagram below: The service adapter is the key component of solution since it will enable legacy service externalization. To provide this expected standardization, the following capabilities should be implemented: RESTful compliant Organized around common business domains Easily scalable Lightweight packages As service adapters are considered kind of integration applications, they should follow an architectural style in order to be compliant with common standards. The architectural style that best suite the above capabilities and others commons requirements is the Microservice approach. Read more about [1] Microservices . Microservice Implementation Strategy Once decided that service adapter implementation are based on Microservice Architecture style some capabilities are required such as: Small package and low memory consumption Application startup should be fast to load new container instances. Fast and simple development based on common standards such as Swagger specification. Easy security integration to provide features such as Basic Auth or OAuth2. Some frameworks has the most of capabilities listed above. The recommend are: Another crucial capability when enabling API endpoints through Microservice implementation is fully integration capability with legacy systems. This kind of feature requires specific frameworks which implements enterprise integration patterns. The recommendation here is use the most famous Java framework called Apache Camel. Read more about [2] Apache Camel . Microservice Deployment Strategy Once the package were built, it need to be deployed. The recommended strategy is deploy it into a PaaS (Platform as a Service) because it offers some built-in features such as: Containerization Container Orchestration Storage Monitoring Logging Also, another two crucial capabilities should be provided: Being scalable in order to support traffic spikes Automation API's to create autonomous deployment pipeline The main market PaaS offering should be considered to be used as deployment strategy are: Pivotal Cloud Foundry: is good choice when using Spring stack technology because it has a native integration Red Hat OpenShift: could be an alternative when using some Red Hat technology. It also uses docker and kubernetes to containerization SalesForce Heroku: It abstracts the features implementations such as containers or logging. It's a good choice when building applications using the Twelve Factor App methodology Other choices to be considered are Amazon Elastic Beanstalk and Google App Engine . Both of them are interesting because have native integration with cloud services and infrastructure once they are strong IaaS providers. For more details about PaaS features, use the [3] PaaS comparator . Read more about [4] Twelve Factor App. But the best alternative to deploy and run microservices are solutions which provide the runtime platform (PaaS) and fully integration with API Manager Platform. In this case, the Sensedia API Management Suite offers a built-in feature called BaaS (Backend as a Service) which is compliant with PaaS features and capability. The BaaS feature should be used to deploy and run those microservices which exposes API from legacy systems or also when creating new application or services using this architecture style. The Sensedia BaaS platform supports natively the following technologies: Read more about [5] Sensedia API Manager Suite . Microservices and API Management Platforms Once microservices are deployed and running, its interface should be exposed as an API and must be managed using an API Management Suite because this kind of solution offers a lot of advantages. The most of API Manager solutions have the features below: Security and Resilience : to protect backend microservices from non-managed consumers. When the API are open to partners or community, those microservices should be protected from being overloaded with spikes of traffic using capabilities such as rate limit, payload size limit or spike arrest. Access Control: the consumers should use the API under access policies. The API Manager must provide standard protocol such as OAuth 2.0 or JSON Web Token for consumer authentication and generate those tokens. Also, some policies may be configured such as expiration or rate limit. Monitoring and tracing: the operation teams use this capability for platform health check and debugging. All capabilities listed above are common in API Gateway solutions, but other features are crucial for API Manager solutions such as: Caching: should be use to avoid unnecessary microservice use or latency once some read functions may be cached. Note that some cases, the backend services are rated and this feature could minimize some cost. Analytics: the API usage could be monitored in real time. This kind of feature should provide dashboard for metric extractions. As mentioned above, some API Manager platforms offers full integration and management for microservices deployment and runtime capabilities. This kind of feature offers end-to-end management platform, it is not necessary provide aparted infrastructure to run microservices. The Sensedia API Manager Suite provides a solution look like the diagram below: Summary Using microservices architecture style is an development approach to enable RESTful interfaces from legacy systems which not expose these kind of endpoints natively, but the first challenge is choose the right implementation tools. There are a lot of frameworks and languages flavors which could help on microservices implementation. The decision of which one is chosen depends of scenario are being faced but there are some recommendations for help as mentioned above. After development toolkits were chosen, the next decision is establish the microservice runtime and deployment platform. Once more, the decision depends of the scenarios are being faced. But in this case, the main goal is expose a legacy functionality as RESTful API and because this reason, it makes sense deploy the microservices in the same platform. The Sensedia API Management Suite is a API Management Platform which provide the backend as a service (BaaS) feature which performs as microservices runtime having full platform as a services (PaaS) capabilities. Furthermore, the platform offers standard features as API Gateway, Caching and Analytics. In short, the recommendation is use this kind of platform which provide full API management and also microservice runtime in all-in-one solution. References [1] Microservices - [2] Apache Camel - [3] PaaS Comparator - [4] Twelve Factor App - [5] Sensedia API Manager Suite -",en,58
253,1561,1467136120,CONTENT SHARED,-2549933363319068481,801895594717772308,8570706322370780593,,,,HTML,https://www.washingtonpost.com/news/wonk/wp/2016/04/27/why-you-cant-help-read-this-article-about-procrastination-instead-of-doing-your-job/,the real reasons you procrastinate - and how to stop,"Have you ever sat down to complete an important task - and then suddenly discovered you were up loading the dishwasher or engrossed in the Wikipedia entry about Chernobyl? Or perhaps you suddenly realize that the dog needs to be fed, emails need to be answered, your ceiling fan needs dusting - or maybe you should go ahead and have lunch, even though it's only 11 a.m.? Next thing you know, it's the end of the day and your important task remains unfinished. For many people, procrastination is a strong and mysterious force that keeps them from completing the most urgent and important tasks in their lives with the same strength as when you try to bring like poles of a magnet together. It's also a potentially dangerous force, causing victims to fail out of school, perform poorly at work, put off medical treatment or delay saving for retirement. A Case Western Reserve University study from 1997 found that college-age procrastinators ended up with higher stress, more illness and lower grades by the end of the semester. But the reasons people procrastinate are not understood that well. Some researchers have viewed procrastination largely as a failure of self-regulation - like other bad behaviors that have to do with a lack of self-control, such as overeating, a gambling problem or overspending. Others say it's not a matter of being lazy or poor time management, as many smart overachievers who procrastinate often can attest. They say it may actually be linked to how our brain works and to deeper perceptions of time and the self. How exactly does procrastination work, and how do you stop it? Psychological research, comics and ""The Simpsons"" will explain. The real origins of procrastination Most psychologists see procrastination as a kind of avoidance behavior, a coping mechanism gone awry in which people ""give in to feel good,"" says Timothy Pychyl, a professor who studies procrastination at Carleton University, in Ottawa. It usually happens when people fear or dread, or have anxiety about, the important task awaiting them. To get rid of this negative feeling, people procrastinate - they open up a video game or Pinterest instead. That makes them feel better temporarily, but unfortunately, reality comes back to bite them in the end. Once the reality of a deadline sets in again, procrastinators feel more extreme shame and guilt. But for an extreme procrastinator, those negative feelings can be just another reason to put the task off, with the behavior turning into a vicious, self-defeating cycle. [ Why it feels so good to read about this Princeton professor's failures ] Tim Urban, who runs the blog Wait But Why, created an amazing and funny (if layman's) explanation of what may happen inside the brain of a procrastinator. Urban calls himself a master procrastinator - he didn't begin writing a 90-page senior thesis until 72 hours before it was due. Urban recently gave a TED Talk about his own extreme procrastination tendencies, in which he used some of his own cartoons to explain how life is different for an extreme procrastinator. First, he describes the brain of a non-procrastinator, in which a ""rational decision-maker"" has a firm grip on the wheel: The brain of a procrastinator looks similar, except for the presence of a little friend, which Urban labels the ""instant gratification monkey."" The monkey seems as though he will be fun, but in fact he is a lot of trouble, as Urban's comics illustrate. This continues until things get really bad - the prospect of the end of your career or your schooling looms. Then something that Urban calls the ""panic monster"" kicks in and finally spurs you into action. People can be various kinds of procrastinators, Urban says. Some procrastinate by doing useless things, such as searching for cat GIFs. Others actually accomplish things - cleaning their homes, working their boring jobs - but never quite getting to the things they really want to accomplish in life, their most important, long-term goals. To illustrate this, Urban uses a concept that is known as an Eisenhower Matrix, a graphic that was included in ""The Seven Habits of Highly Effective People."" It's named after Dwight D. Eisenhower, the famously productive president. Eisenhower thought that people should spend their time on what was truly important to them - the tasks in Quadrants 1 and 2 below. Unfortunately, most procrastinators spend little time in those quadrants, Urban says. Instead, they mostly hang out in Quadrants 3 and 4, doing things that may be urgent, but are not important. Occasionally, when the panic monster takes over, they take a very brief detour to Quadrant 1. Urban says this habit is disastrous because ""the road to the procrastinator's dreams - the road to expanding his horizons, exploring his true potential and achieving work he's truly proud of - runs directly through Quadrant 2. Q1 and Q3 may be where people survive, but Q2 is where people thrive, grow and blossom."" This is Urban's own personal explanation of how and why he procrastinates - but his account actually corresponds with psychological research on the topic. Pychyl discusses the idea of the ""monkey mind"" - that our thoughts are constantly darting all over the place, preventing us from concentrating. And psychologists agree that the problem with procrastinators is that they are tempted to give in to instant gratification, which brings people the kind of instant relief psychologists call ""hedonic pleasure,"" rather than staying focused on the long-term goal. Important goals (the kind that occupy the first and second quadrants above) are more challenging but in the long run bring longer lasting feelings of well-being and self-satisfaction that psychologists call ""eudaimonic pleasure."" Present Homer vs. Future Homer Psychologists have some other fascinating models to understand the forces behind procrastination. Some believe that procrastination is so intractable because it's linked to deeper perceptions of time and the difference between what they call ""the present and future self."" The idea is that, even though we know that the person we will be in a month is theoretically the same person that we are today, we have little concern, understanding or empathy for that future self. People are far more focused on how they feel today. Pychyl points to a clip from ""The Simpsons"" as a pretty good illustration of the different ways we think about our present and future selves. In one episode, Marge scolds her husband for not spending enough time with the kids. ""Some day, these kids will be out of the house, and you'll regret not spending more time with them,"" she says. ""That's a problem for future Homer. Man, I don't envy that guy,"" Homer says, while pouring vodka into a mayonnaise jar and then downing the concoction before collapsing on the floor. ""When making long-term decisions, [people] tend to fundamentally feel a lack of emotional connection to their future selves,"" says Hal Hershfield, a psychologist at UCLA Anderson School of Management who studies the present and future self.""So even though I know on some fundamental level in a year's time, I'll still be me, in some ways I treat that future self as if he's a fundamentally different person, and as if he's not going to benefit or suffer from the consequences of my actions today."" Hershfield's research supports this idea. Hershfield has taken fMRI scans of people's brains as they thought about themselves in the present, a celebrity like Natalie Portman or Matt Damon, and then themselves in the future. He found that people process information about their present and future selves with different parts of the brain. Their brain activity when describing their self in a decade was similar to when they were describing Natalie Portman. Emily Pronin of Princeton University led a study with somewhat similar findings in 2008. She presented people with a nasty concoction of soy sauce and ketchup and had them decide how much they or another person would have to drink. Some people choose for themselves, others chose for other people and a third group chose for themselves in two weeks. The study showed that people were willing to commit to drinking a half-cup of the nasty concoction in the future but committed to only two tablespoons that day. Pychyl's latest research suggests that those who were more in touch with their future selves -- both two months and 10 years down the line -- reported fewer procrastination behaviors. However, research also suggests that procrastinators might be able to get more in touch with their future selves - a change that could help make them happier in the long term. In one study by Hershfield , some subjects used virtual reality to look at digitally aged photographs of themselves. Then all of the test subjects were asked how they would spend $1,000. Those who saw the aged photo chose to invest twice as much in a retirement account as those who did not. Interestingly, insurance companies have latched onto these findings to try to drum up more business. Bank of America Merrill Lynch launched a service called Face Retirement , in which you can upload a photograph of yourself and see it digitally aged. Allianz also created a similar tool with the help of its own team of behavioral scientists . How to return to the land of the productive Beyond trying to be kinder to our future selves, what else can people do about procrastination? Tim Urban points out that the typical advice for procrastinators - essentially, to stop what they're doing and get down to work, is ridiculous, because procrastination isn't something that extreme procrastinators feel as though they can control. ""While we're here, let's make sure obese people avoid overeating, depressed people avoid apathy, and someone please tell beached whales that they should avoid being out of the ocean,"" Urban writes. But there are some simple tips, those who study the subject say, that can help procrastinators get down to business. Interestingly, research suggests that one of the most effective things that procrastinators can do is to forgive themselves for procrastinating. In a study by Pychyl and others, students who reported forgiving themselves for procrastinating on studying for a first exam ended up procrastinating less for a second exam. This works because procrastination is linked to negative feelings, the researchers say. Forgiving yourself can reduce the guilt you feel about procrastinating, which is one of the main triggers for procrastinating in the first place. But the best thing that Pychyl recommends is to recognize that you don't have to be in the mood to do a certain task - just ignore how you feel and get started. ""Most of us seem to tacitly believe that our emotional state has to match the task at hand,"" says Pychyl. But that's just not true. ""I have to recognize that I'm rarely going to feel like it, and it doesn't matter if I don't feel like it."" Instead of focusing on feelings, we have to think about what the next action is, Pychyl says. He counsels people to break down their tasks into very small steps that can actually be accomplished. So if it's something like writing a letter of reference, the first step is just opening the letterhead and writing the date. Even if it's an extremely small action, a little progress will typically make you feel better about the task and increase your self-esteem, which in turn reduces the desire to procrastinate to make yourself feel better, he says. Pychyl believes that teachers and parents should teach kids to deal with the temptations of procrastination from a young age. ""A lot of teachers think that kids have time-management problems, when they procrastinate. And they don't have a time-management problem. ... What they have is an emotion-management problem. They have to learn that you don't feel good all the time, and you've got to get on with it."" ""Mark Twain is quoted as saying, 'If your job is to eat a frog, eat it first thing in the morning, and if your job is to eat two frogs, eat the big one first,'"" Pychyl says. Urban basically says the same thing in different language. ""No one 'builds a house,'"" he writes . ""They lay one brick again and again and the end result is a house. Procrastinators are great visionaries - they love to fantasize about the beautiful mansion they will one day have built - but what they need to be are gritty construction workers, who methodically lay one brick after the other, day after day, without giving up, until a house is built."" You might also like: What really drives you crazy about waiting in line (it actually isn't the wait at all) Your reaction to this confusing headline reveals more about you than you know How to tell if other people think you're hot, according to science",en,58
254,1548,1467083975,CONTENT SHARED,5795281877636738272,-1032019229384696495,-1382304469206973597,,,,HTML,http://www.wired.com/2016/06/microsofts-open-source-love-affair-reaches-new-heights/,microsoft's open source love affair reaches new heights,"It's official: Microsoft code isn't just for Windows anymore. Today, the company released .NET Core 1.0, a version of its popular software development platform that will run not just on its own Windows operating systems, but on the Linux and Mac OS X operating systems as well. What's more, .NET Core is open source, meaning that any developer can not only use it for free to build their own applications, but also modify and improve the platform to suit their needs and the needs of others. Microsoft first released .NET Core and its source code back in 2014 , but previous versions of the software were only intended for testing purposes. Today marks the first time that Microsoft has officially supported using the platform for real-world applications on Linux and OS X, the two primary competitors to Windows. Red Hat, one of the world's primary Linux vendors, also announced that it will officially support .NET on its popular Red Hat Enterprise Linux operating system. All this highlights an enormous change not only in Microsoft, but in the software industry as a whole. Over the last decade, the world's tech businesses, from Google and Facebook and Twitter on down, have increasingly used Linux and other open source software to build their online services and other technologies, and as a result, IT vendors-the companies that help businesses build their online services-have moved closer and closer to the open source way. This includes Microsoft, one of the largest IT vendors. In order to compete, Microsoft must ensure not only that .NET is open source, but that other important Microsoft IT tools run on all operating systems, including, most notably, Linux As Microsoft put the finishing touches on .NET, it also released a preview version of its SQL Server database software that runs on Linux. The database itself is not open source, and it's not yet ready for the real-world, but this is the first time Microsoft has offered the product for Linux. Traditionally, Microsoft only supported running software on its own operating systems, perhaps out of fear of cannibalizing the sales of Windows licenses. But the world has changed, and Microsoft is changing with it. A Long Time Coming Microsoft has been open sourcing parts of the .NET environment for years, starting with a programming framework called ASP.NET MVC back in 2009. The company also helped port several important pieces of open source software-including the data crunching platform Hadoop and the coding tool Node.js-to Windows. But even after it announced that it would support Linux on its Azure cloud service, Microsoft still didn't write much software for the operating system. Microsoft would help you run Linux, but you were on your own when it came to software. That changed in 2014, when the company announced that not only would it open source the heart of .NET, but port it to Linux and OS X. Then, earlier this year, Microsoft acquired a company called Xamarin, which has long made software that helps developers use .NET technologies to build software for a wide variety of platforms, including Linux (through its open source Mono project) and mobile operating systems like Android and iOS (through its flagship product). Microsoft's motivation for supporting Linux and releasing open source software isn't altruistic. It's necessary for the company's survival. Over the years, Linux has edged out Windows Server in the web server market, and coders have flocked to open source programming languages and frameworks like Ruby on Rails, Python, and Google's Go language to build the next generation of applications. Julia Liuson, corporate vice president of Microsoft's developer division, says her team feared that Microsoft's once mighty brand was losing its cachet. ""If you talk to any developer, they hold Visual Studio in high regard,"" she says. ""Despite that, we were not as relevant to developers as we would have liked."" The answer was obvious: the way to reach developers was through open source. So Liuson and company endeavored to make .NET more open than ever before, and that meant making it run not just on Windows, but wherever developers might want to use it. A Threat to Windows? Liuson says there's little concern that making .NET Core available on Linux and OS X will reduce sales of Windows licenses-those developers were probably never going to use Windows in the first place. But now they might consider buying licenses for Microsoft's Visual Studio and Xamarin products, or use its Azure cloud services instead of competing services from Amazon and Google. But open source is about more than just selling more software and services. About 18,000 developers from more than 1,300 different companies outside of Microsoft have contributed to .NET Core 1.0, according to the company. Why work on Microsoft's products for free? For James Niesewand and his team at Illyriad Games, it allows them to fix their own problems .NET without having to wait for Microsoft to do it, or writing their own programming platform from scratch. ""Three years ago if we had a problem with .NET, we'd write-up a bug report, submit it,"" he says. ""After a few weeks you might get a response acknowledging it, and maybe a year later you'd get a release that fixes it."" Now, he says, the company can write their own fixes and have them approved by Microsoft in hours. Microsoft reaps huge benefits from this. The company uses .NET for its own cloud-based services, so, in theory, the improvements made to the platform by Illyriad and other outside developers could have ripple effects throughout Microsoft's empire, from Outlook.com to Cortana. That's how Facebook and Google develop software, too. If an outside developer figures out how to speed up Facebook's development framework React, then everyone-including Facebook's users-benefit from faster, more responsive applications. If an academic studying artificial intelligence finds a way to make Google's AI framework TensorFlow better, then that researcher will get a better tool and Google will get improvements that could trickle out to every part of its business that depends on TensorFlow. Microsoft is finally catching on to this new way of thinking and we're only just beginning to see the results.",en,58
255,2523,1475772354,CONTENT SHARED,660451400045043370,3609194402293569455,5681278989312189918,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36",SP,BR,HTML,https://techcrunch.com/2016/10/06/duolingos-chatbots-help-you-learn-a-new-language/,duolingo's chatbots help you learn a new language,"Today's chatbots, for the most part, aren't all that useful, but what if you could use them to learn a new language? When it comes to learning languages, using what you've learned in the context of a conversation is extremely useful. If you are learning online, though, you often don't have anybody to talk to. That's why Duolingo today introduced chatbots to its app that allow you to have AI-powered conversations. These Duolingo Bots currently work for users who want to learn French, Spanish and German. The company promises it'll add other languages soon. Sadly, this feature also currently only works in the Duolingo iPhone app . Given that the bots' intelligence resides in the cloud, you'll have to be connected to the Internet to use this feature. To make talking to the bots a bit more compelling, the company tried to give its different bots a bit of personality. There's Chef Robert, Renée the Driver and Officer Ada, for example. They will react differently to your answers (and correct you as necessary), but for the most part, the idea here is to mimic a real conversation. These bots also allow for a degree of flexibility in your answers that most language-learning software simply isn't designed for. There are plenty of ways to greet somebody, for example, but most services will often only accept a single answer. When you're totally stumped for words, though, Duolingo offers a ""help my reply"" button with a few suggested answers. ""One of the main reasons people learn languages is to have conversations,"" writes Duolingo CEO and co-founder Luis von Ahn in today's announcement. ""Students master vocabulary and comprehension skills with Duolingo, but coming up with things to say in real­-life situations remains daunting. Bots offer a sophisticated and effective answer to that need."" For now, you can only use text chat to talk to the bots. Over time, Duolingo plans to allow for spoken conversations as well, though.",en,58
256,613,1461758485,CONTENT SHARED,8651615486877503655,4209517478660372522,3925037686854874534,,,,HTML,http://zeroturnaround.com/rebellabs/java-8-best-practices-cheat-sheet/,java 8 best practices cheat sheet,"Hi there, do you like trains? Here at RebelLabs, we love trains! Not just a regular love, but an unconditional love as well as immense respect for trains of all kinds. Let me give you an example, a Java train that is regularly delayed till a later date. Long story short, Java 9 should have been feature complete by now, but it isn't. There's no need to assign blame or to point fingers and complain how bad the red shepherd of Java is. Jigsaw is a hard problem and takes time to get right and... we're not going to talk about that right now. Java 9 is far away from being just around the corner, so we'll have be friends with Java 8 for a little while longer. Java 8 has already been with us for quite some time. Let's reflect on it and talk about the best practices that have naturally grown in its typical usage. In this post we'll look at the hot topics of the Java 8 language: default methods, lambdas and streams and using Optional to represent absent values. SHOW ME A PRINTABLE JAVA 8 CHEAT SHEET! Default methods in interfaces The ability to specify default method implementations in interfaces was added into JDK 8 so that collections could evolve without breaking backward compatibility. Previously, we couldn't just add a method to an interface without requiring all the implementing subclasses to specify an implementation of the new method. Breaking backward compatibility is a deal-breaker for Java. So since version 1.8 we can mark a method with the default keyword and provide the body of the method right in the interface. This, as any powerful feature does, opens all sorts of doors for unmaintainable, confusing code, if abused. However in small doses one can enhance existing interfaces and make them more useful in the codebase. The main rule of thumb for using default methods is not to abuse them and not to make the code messier than it would be without it. For example, if you want to add some sort of functionality to Java classes without polluting their hierarchy with a common superclass, consider creating a separate interface just for this one utility method. Here's an example of an interface called Debuggable that uses the reflection API to get the access to the object's fields and provides a decent t oString() implementation for the object that prints the fields values. The important part here is the default keyword on the method signature. One could now use this for any class that you need to peek at, like this: Which prints the expected line: "" Main [ a = 100 b = Home ] "". Functional interfaces of Java 8 deserve a special mention here. A functional interface is one that declares a single abstract method. This method will be called if we use it with the lambda syntax later. Note that default methods don't break single abstract method contract. You can have a functional interface bearing many default methods if you choose. Don't overuse it though. Code conciseness is important, but code readability trumps it by far. Lambdas in Streams Lambdas, oh sweet lambdas! Java developers have been eagerly waiting for you. For years Java has received the label of not being an appropriate programming language for functional programming techniques, because functions were not the first class citizens in the language. Indeed, there wasn't a neat and accepted way to refer to a code block by a name and pass it around. Lambdas in JDK 8 changed that. Now we can use method references, for better or worse, to refer to a specific method, assign the functions into variables, pass them around, compose them and enjoy all the perks the functional programming paradigm offers. The basics are simple enough, you can define functions using the arrow (->) notation and assign them to fields. To make things simpler when passing functions as parameters, we can use a functional interface, with only one abstract method. There are a bunch of interfaces in the JDK that are created for almost any case: void functions, no parameters functions, normal functions that have both parameters and the return values. Here's a taste of how your code might look using the lambda syntax. The caveat here is that the code is tough to manage if you let your anonymous functions grow over a certain threshold. Think about the fattest lambda you've seen? Right. That should have never existed. The most natural place for a lambda to exist is code which processes data. The code specifies the data flow and you just plug in the specific functionality that you want to run into the framework. The Stream API immediately comes to mind. Here's an example: That's pretty self-explanatory, right? In general, when working with streams, you transform the values contained in the stream with the functions you provide for example using the lambda syntax. However, for a better explanation, check out the Java 8 Streams cheat sheet , it has a short, clear explanation when and why you want to use certain stream methods and what pitfalls might await you. Lambda Takeaways If the code doesn't specify the framework for the data flow into which you plug your functions, consider avoiding multiplying lambdas. A proper class might be more readable. If your lambda grows above 3 lines of code - split it: either into several map() invocations that process the data in steps or extract a method and use the method reference syntax to refer to it. Don't assign lambdas and functions to the fields of the objects. Lambdas represent functions and those are best served pure. I'VE SEEN ENOUGH, I NEED TO PRINT MY FREE JAVA 8 CHEAT SHEET! java.util.Optional Optional is a new type in Java 8 that wraps either a value or null, to represent the absence of a value. The main benefit is that it your code can now gracefully handle the existence of null values, as you don't have to explicitly check for nulls anymore. Optional is a monadic type you can map functions into, that will transform the value inside the Optional. Here's a simple example, imagine you have an API call that might return a value or null, which you need to then process with the transform() method call. We'll compare code with and without using Optional types. vs. It isn't nice to have null checks polluting your code, is it? The best part is that we can now live inside this Optional world and never leave it, since all functions can be mapped into it. What about the functions that already return an Optional? Have no fear with the flatMap method, you won't end up double wrapping Optionals. Check out flatMap's signature: It takes care of the required unwrapping so you have just one level of Optionals! Now, before you rewrite all your code to have Optionals all over the place. Hold on for a minute longer. Here's a rule of thumb for where you want to use Optional types: Instance fields - use plain values. Optional wasn't created for usage in fields. It's not serializable and adds a wrapping overhead that you don't need. Instead use them on the method when you process the data from the fields. Method parameters - use plain values. Dealing with tons of Optionals pollutes the method signatures and makes the code harder to read and maintain. All functions can be lifted to operate on optional types where needed. Keep the code simple! Method return values - consider using Optional. Instead of returning nulls, the Optional type might be better. Whoever uses the code will be forced to handle the null case and it makes for cleaner code. All in all, using the Optional type correctly helps you keep your codebase clean and readable. And that's very important! Disregarding that, both in life and in your code, is a recipe for disaster! Conclusion I hope this post gives you an idea of some of the best practices on using Java 8 features and still have readable and maintainable code. If you liked it, don't forget to print the handy 1 page cheat sheet we prepared with the takeaways from the post so you can print it out and put it under your less experienced colleague's coffee cup. AGREED, I NEED THIS JAVA 8 CHEAT SHEET! What are your rules of thumb and best practices with dealing with Java 8 features? Hit the comment section below or find us on Twitter: @ZeroTurnaround . This is our first one page cheat sheet. Are there any other areas you'd like a cheat sheet on? Would you like to read a post summarizing best practices of Java Date & Time API, more on stream processing or how to use Collectors to the fullest? Make your selection on this single question survey and we'll try our best to share our experience with you. Authors Oleg Shelajev Simon Maple",en,58
257,1205,1464869091,CONTENT SHARED,5787934590133549357,-1387464358334758758,7170482234777071965,,,,HTML,http://googlediscovery.com/2016/06/01/ouca-musica-criada-pela-inteligencia-artificial-do-google/,ouça a música criada pela inteligência artificial do google | google discovery,"A inteligência artificial do Google mostrou ter uma certa habilidade artística com a revelação de que a tecnologia foi capaz de desenvolver uma música de 90 segundos. Este é o primeiro resultado do programa Magenta do Google, anunciado na semana passada, que vem sendo projetado para colocar os sistemas de aprendizado de máquina do Google para trabalhar na criação de arte e música. A equipe do buscador afirma que o desafio não está em somente em desenvolver artes, mas que sejam capazes de contar histórias a partir de suas criações. ""Acreditamos que esta capacidade irá semear uma nova cultura emocionante de ferramentas para a criação de arte e música"", escreveu Douglas Eck, um dos cientistas do Google. é bacharel em administração de empresas e fundador da FragaNet Networks - empresa especializada em comunicação digital e mídias sociais. Em seu portfólio estão projetos como: Google Discovery, TechCult, AutoBlog e Arquivo UFO. Também foi colunista de tecnologia no TechTudo, da Globo.com.",pt,58
258,1234,1464909124,CONTENT SHARED,4277778562135834208,-8020832670974472349,2348433893919184737,,,,HTML,http://hood.ie/blog/beyond-progressive-web-apps-part-1.html,beyond progressive web apps part 1,"Introduction I am very excited about a new initiative to make web sites and web apps more compelling for end users. And in this series of posts, I intend to provide a tutorial to get you up and running with it. The initiative is called Progressive Web Apps . And while it has ""Apps"" in the title, the principles and technology behind them are applicable across apps and sites alike. The best introduction into the topic is Jake Archibald's Google I/O 2016 talk: This three part tutorial is meant to be read as a follow-up piece to Jake's that talk, so I recommend you watch it. Especially if you are new to the topic, but also if you already know what a ServiceWorker is, there are a number of new things in there. I'll wait here sipping my tea while you watch. So no rush! Watched it? Okay. Welcome back! Let's go. There's an old saying I paraphrased in this tweet : ""Friends don't let friends build their own {CRYPTO, SYNC, DATABASE}."" - @janl on September 24th, 2014 What do I mean by that? Well, it's very hard to get these things right. Additionally, not getting them right will mean a lot of unhappy developers and end-users. In other words, these things are best left to the experts. However. Brought to its logical conclusion though, we'll end up with a situation where nobody is doing crypto, sync or databases anymore, because no new people learn these things., So there are exceptions. You can be an exception . Let's become experts in one of these things! Specifically, for the purposes of this series of posts: sync . Or, to put it another way: let's look at a bunch of different problems and solutions that relate to making data available offline. The Future After watching Jake's talk you might be convinced Progressive Web Apps are the future. Not just for single-page apps, but also for content heavy sites - like the Guardian . But even if you're not convinced, you might be interested in where this road leads to, and that's what this tutorial is about. You already know how to store your website's assets in a ServiceWorker cache. You already have some idea about how to store server-delivered content in IndexedDB . And you might know how to locally store user-entered data (think an address book, note taking, Foursquare favourites, and so on) in IndexedDB or localStorage so you can push it to the server when a network connection becomes available. We can already see a few different scenarios: server always pushes (news) client always pushes (notes) both push (Foursquare, email, or multi-device access of individual notes, and all forms of group data sharing.) Let's have a look at the different techniques required to make this all work. We'll see which applies to which use-case while we go through everything, step-by-step. The Scene: Background Sync In his talk, Jake shows an example of a chat application. Towards the end , he uses the Background Sync API to send new messages ""later"", whenever the browser thinks it has an actual internet connection (as opposed to having a network but not thinking there's an internet connection). This is a great UX experience: the interaction is done as far as the user is concerned. There is still an indicator that the message hasn't reached the recipient yet, but there is no need to keep the user waiting for any network operations. In the same talk, Jake explained that the browser (and sometimes mobile operating system) mechanics of determining whether a device is online are not very useful. That's because they only cover the connection from the device to the wifi router or cell tower. But there are a lot of steps between there and the final web server. For example: ISP routers, transparent proxies, satellite uplinks, just to name a few. Now imagine we are sending a message within a Background Sync event as shown in Jake's talk, but we're on a fast train that goes through a bunch of tunnels in fast succession. Or we are on a conference or hotel wifi. Or we're a large event with thousands of other people. Our phone might get a request/response going, and as a result, wakes up Background Sync and tries to send our message. But by the time the message gets sent, we don't get any requests going because the network became unavailable again.. Background Sync will then re-try at a later time. Which is great, and exactly what we want. We don't have to worry about whether the message is sent, because we know it will be, eventually. Now. Let's look behind the scenes. The Cycle of (the) Web Even though there are many hops at which things can go wrong in a HTTP request/response cycle (see part one and two of this Hoodie series on offline first), there are generally two parts: the request and the response. Each part can fail, so we have to account for the following scenarios: the request fails the request makes it, but the response fails Scenario one is neatly covered by Background Sync, but what about scenario two? Imagine we are sending a message. The server accepts it and sends it onwards to the final recipient. Then, the server responds to the device, saying it handled the message correctly. If that response fails, Background Sync will consider the message sending request as failed and will re-try it later. At that time, the server accepts the message and sends it to the recipient again. And now we've created a mess, because the recipient is left wondering why there are two messages that are exactly the same. And if we are really unlucky, our recipient will get an infinite stream of the same message because the same problem is happening over and over and over again. Let's see how we can avoid this. There are solutions for this exact problem that we can employ in the server portion of our app.. But we'll get to that later. For now, let's look at how to solve this more generically , so that we can solve the same problem when it comes up in very different contexts, not just when sending messages. Identity In programming, we generally work with bits of data wrapped up in objects. If we want to be able to refer to an object at a later time, we must be able to reference it unambiguously. Sometimes that means giving something a name (like an auto incrementing number) and sometimes we can derive an identity from a number of uniquely identifying properties of the object. For example: in an address book, assuming no two people have the same name (a bad assumption, but we'll get to this shortly), the identity (or ID) of an object could be derived from the combination of the first name and the last name. That would be an example of what is known in database circles as a natural key . The advantage of a natural key is that you don't need to store any extra data on the object to be able to refer to it later. It also means your data is easier to de-duplicate. Natural Key Disadvantages There are two disadvantages to using natural keys: changes in the natural key and the need for uniqueness. Key Changes Say your natural key is somebody's email address. Email addresses change, and you need to be able to deal with this change. For example, if you update someone's email address, but another object was using that as the natural key to refer to your object, that second object will need to update its reference.. While this type of change might be infrequent, other natural keys can change more frequently. This is opposed to an opaque key , or surrogate key , that is a number (like an account number, a social security number, a phone number) or string of random characters (e.g. a UUID ). Every time you've seen an auto incrementing ID column in an SQL database, that is a surrogate key. Uniqueness In our address book example, when your natural key is derived from last name and the first name, we may hit a problem. What if you know two people named Jane Smith? Our natural key is not unique, so if we try to look up ""Jane Smith"", we can't be sure which object to return. Surrogate Key Advantages Surrogate keys have the advantage of being agnostic to any changes in our objects' data. The disadvantage is that they are opaque, so there's no natural relationship between an object's ID and its data. The ID 43135 doesn't tell you a whole lot about a user record for Jane Smith. While these IDs are usually only used by computers, sometimes they leak out into the real world. Sometimes we even expect people to remember them and manipulate them. Not ideal.. In addition, they make debugging and logging harder, as devs are now forced to map IDs to objects' natural data to see anything useful. Moving Forwards Since we are looking at how to make data available offline , and since that means storing data on a end-user device and a server, we have multiple copies of all our data. So we need to be sure that any operations can unambiguously reference that data. So, when creating a new object that we give it a unique ID. And we can't guarantee this with natural keys. A new Jane Smith could be added on the server and the user's device while they can't talk to each other because one is offline. That would spell all sorts of trouble for us. Unless any of our data lends itself to be a natural key without the disadvantages, we use surrogate keys. Specifically we use UUIDs, because they have the convenient property that even though you still can't guarantee the uniqueness of natural data on disconnected devices, the chance of assigning the same UUID to different objects on two or more devices is so unlikely, we practically don't even have to consider it a possibility. With that sorted, we have a new problem. We have multiple sets of data, which may or may not diverge from each other in various ways. In my next post, I'll show you how to figure out what's changed, what needs syncing, and in what direction. This concludes part 1 of our series ""Beyond Progressive Web Apps"". Stay tuned for part 2: ""We need to know what's new"". Thanks to Noah Slater , Katharina Hößel and Jake Archibald for their reviews ❤",en,58
259,1342,1465581994,CONTENT SHARED,-6074646493203713780,-8020832670974472349,-6121624872835463728,,,,HTML,http://blog.cleancoder.com/uncle-bob/2016/06/10/MutationTesting.html,clean coder blog,"At XP2016 I attended an open-space demonstration of mutation testing. In particular, an open source tool for the Java space named pitest . I came away pretty impressed. I had heard of mutation testing before. A decade and a half ago there was an open source tool named jester . Nothing much came of Jester back then. Perhaps we were too focussed upon TDD to think beyond it. After all, the very notion of rigorous unit testing was very controversial -- at the time. But before I get philosophical, perhaps I should describe what mutation testing is. The Problem. What's the problem with unit tests? Dijkstra said it long, long ago. ""Testing shows the presence, not the absence of bugs."" This sentiment has long been used as a complaint against the discipline of TDD. ""Write all the tests you like"", the detractors would say, ""it still doesn't prove your code actually works."" Dijkstra was right, of course. However, his complaint could also be raised about the scientific method. To paraphrase: Experiments can only disprove, never prove, a theory."" And yet every day we are willing to bet our lives on those unproven theories of Newton, Einstein, Maxwell, and Boltzmann. If experiments are good enough for science, why aren't unit tests good enough for software? There are many answers to that, which I will phrase as questions[1]: Have you written enough tests? Have you covered every line, every branch, every path? If a semantic change is made to the code, will some test fail? Sufficiency. The first question is obvious. If you missed a test, you may have a bug. There are two kinds of missing tests. The first is some statements in the code that are not tested by the tests. The second is some requirements that the developers missed. There's not much we can do about that later case other than to carefully review the requirements and to expose the software to customers and users early and often to get their feedback. The former case is a symptom of test-after. If you write the code first, and then write the tests for the code, you are very likely to miss some statements or branches. This is one of the biggest arguments in favor of the test-first strategy of TDD. If you write your tests first, and if you refuse to write a line of production code unless it is to get a failing test to pass, then you are not likely to leave any code uncovered by the tests. Coverage. And that brings us to the second question. Have you covered all the lines, branches, and paths? Covering every path is impractical. The simple explosion in the number of paths makes the testing burden enormous. We may solve this problem one day; but today is not that day. However, we have tools that will tell us what lines and branches are covered. These coverage tools are readily available, easy to run, and generally quite fast. They aren't perfect, but overall they're pretty good. So what should your goal be? The question is absurd. There is no justifiable goal other than 100%. Every single line, and every single branch, should be tested by your unit tests. I realize that this goal is not practicably achievable. So I think of it as an asymptotic goal -- one that we are always pushing towards, but never quite achieving. Of course the problem with coverage is that it doesn't prove what you might think it proves. It does not prove that you have tested every line and every branch. All it proves is that you have executed every line and every branch. Pull out all the asserts from your tests, and your coverage remains unchanged! Semantic Stability. And that leads us to the third, and most important question. If you make a semantic change to the code -- a change that alters the meaning of the code -- will one of your tests detect it? That's a very high bar for a test suite. But, again, it is the only justifiable bar to set. Of course your test suite should fail if you make a semantic change to your production code. Does anybody realistically doubt that? And think about what such semantic stability means. It means that your test suite tests every line and every branch. It means that your test suite verifies every behavior written into your system. Well, perhaps not quite. Remember we aren't ensuring that all pathways are covered. Still, if we change the sense of an if statement, and some test doesn't fail, that's a problem. If, on the other hand, we walk through the code and, one-by-one, change the sense of every if statement to see if a test fails, then we can be pretty sure that all our if statements are covered and tested. If we also change, one-by-one, the sense of every while statement; and if, one-by-one, we remove every function call; and we ensure that each of those changes causes our test-suite to fail, then we can be pretty sure that those while statements and function calls are covered and tested. Mutation Testing This is what mutation testing does. The pitest tool first runs a coverage analysis by executing your test suite and measuring which lines are covered by the tests. Then, one-by-one, it makes semantic changes to the Java byte code, such as inverting the sense of if and while statements, and removing function calls. Each one of those changes is called a mutant . For each mutant the tool runs the unit test suite; and if that suite fails , the mutant is said to have been killed . That's a good thing. If, on the other hand, a mutant passes the test suite, it is said to have survived . This is a bad thing. It means that the tests do not check for that semantic change. Strangely, the sense for mutant tests is inverted ; we expect them to fail. A passing mutant test is bad. Mutants should all be red! A surviving (green) mutant might be the result of tests that have been @ignore d, or commented out, or when asserts have been removed, or never added. It can also happen if TDD discipline got a little lax at some point, and some code got added without a corresponding test. As a side note, I have found pitest to be pretty easy to operate, and relatively fast. It apparently does some smart dependency detection to help it determine what tests need to be run for particular mutants. Your mileage may vary; and I did have to break the mutation tests up into small parts for one larger project I am working on. Still, I have found the tool to be quite useful at identifying semantic instabilities in my test suites. Implications A fundamental goal of TDD is to create a test suite that you can trust, so that you can effectively refactor. We need to be able to refactor in order to keep the code clean enough to modify and enhance without paying huge costs and taking huge risks. The cleaner the code the longer it's useful lifetime. For years the argument has been that test-after simply cannot create such a high reliability test suite. Only diligent application of the TDD discipline has a chance of creating a test suite that you implicitly trust. However, with a mutation testing tool like pitest I have successfully augmented a test-suite created with lax TDD discipline into one that I can implicitly trust. The implication of that is significant. If a development team dedicates itself to creating a test suite that is semantically stable, and verifies that stability by using a mutation tester like pitest , then does it really matter if the tests were written first? Oh there are other arguments for test-first, of course. There's the design argument, and the cycle time argument, and the fun factor argument among many others. Valid as those arguments may be, they pale in comparison to creating a test suite that guarantees semantic stability. As hard-nosed as I am about TDD as a necessary discipline; if I saw a team using mutation testing to guarantee the semantic stability of a test-after suite; I would smile, and nod, and consider them to be highly professional. (I would also suggest that they work test-first in order to streamline their effort.) [1] If you think about it, each of those questions has an analog for science. We don't trust our lives to theories that have not been subjected to experimental testing that is complete, covered, and controlled (semantically stable).",en,58
260,603,1461702759,CONTENT SHARED,-4875060556471576861,7392990465409599343,1162110890105301178,,,,HTML,https://jenkins.io/blog/2016/04/26/jenkins-20-is-here/,jenkins 2.0 is here!,"Published on 2016-04-26 by kohsuke Over the past 10 years, Jenkins has really grown to a de-facto standard tool that millions of people use to handle automation in software development and beyond. It is quite remarkable for a project that originally started as a hobby project under a different name. I'm very proud. Around this time last year, we've celebrated 10 years, 1000 plugins, and 100K installations. That was a good time to retrospect, and we started thinking about the next 10 years of Jenkins and what's necessary to meet that challenge. This project has long been on a weekly ""train"" release model, so it was useful to step back and think about a big picture. That is where three pillars of Jenkins 2.0 have emerged from. First, one of the challenges our users are facing today is that the automation that happens between a commit and a production has significantly grown in its scope. Because of this, the clothing that used to fit (aka ""freestyle project"", which was the workhorse of Jenkins) no longer fits. We now need something that better fits today's use cases like ""continuous delivery pipeline."" This is why in 2.0 we've added the pipeline capability . This 2 year old effort allows you to describe your chain of automation in a textual form. This allows you to version control it, put it alongside your source tree, etc. It is also actually a domain specific language (DSL) of Groovy , so when your pipeline grows in complexity/sophistication, you can manage its complexity and keep it understandable far easily. Second, over time, Jenkins has developed the ""assembly required before initial use"" feeling. As the project has grown, the frontier of interesting development has shifted to plugins, which is how it should be, but we have left it up to users to discover & use them. As a result, the default installation became very thin and minimal, and every user has to find several plugins before Jenkins becomes really functional. This created a paradox of choice and unnecessarily hurt the user experience. In 2.0, we reset this thinking and tries to create more sensible out of the box experience that solves 80% use cases for 80% of people. You get something useful out of the box, and you can get some considerable mileage out of it before you start feeling the need of plugins. This allows us to focus our development & QA effort around this base functionality, too. By the way, the focus on the out of the box experience doesn't stop at functionality, either. The initial security setup of Jenkins is improved, too, to prevent unprotected Jenkins instances from getting abused by botnets and attacks. Third, we were fortunate to have a number of developers with UX background spend some quality time on Jenkins, and they have made a big dent in improving various parts of Jenkins web UI. The setup wizard that implements the out of the box experience improvement is one of them, and it also includes other parts of Jenkins that you use all the time, such as job configuration pages and new item pages. This brings much needed attention to the web UI. As you can see, 2.0 brings a lot of exciting features on the table, but this is an evolutionary release, built on top of the same foundation, so that your existing installations can upgrade smoothly. After this initial release, we'll get back to our usual weekly release march, and more improvements will be made to those pillars and others in coming months and years continuously. If you'd like to get more in-depth look at Jenkins 2.0, please join us in our virtual Jenkins meetup 2.0 launch event . Thank you very much for everyone who made Jenkins 2.0 possible. There are too many of you to thank individually, but you know who you are. I wanted to thank CloudBees in particular for sponsoring the time of many of those people. 10 years ago, all I could utilize was my own night & weekend time. Now I've got a team of smart people working with me to carry this torch forward, and a big effort like 2.0 wouldn't have been possible without such organized effort.",en,57
261,2178,1472056923,CONTENT SHARED,-3959242148361340089,9109075639526981934,-3684143564258997387,,,,HTML,https://www.gartner.com/doc/reprints?id=1-2PMFPEN&ct=151013,gartner reprint,"The operational database management system (DBMS) market is concerned with relational and nonrelational database management products suitable for a broad range of enterprise-level transactional applications. These include purchased business applications, such as those for ERP, CRM, catalog management and security event management, and custom transactional systems built by organizations' own development teams. Also included in Gartner's definition of this market are DBMS products that support interactions and observations as new types of transaction.",en,57
262,1328,1465518989,CONTENT SHARED,-307396946333196366,3829784524040647339,-3033571558709312013,,,,HTML,http://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/?imm_mid=0e491e&cmp=em-data-na-na-newsltr_20160608,cracking the job interview with tensorflow,"interviewer: Welcome, can I get you coffee or anything? Do you need a break? me: No, I've probably had too much coffee already! interviewer: Great, great. And are you OK with writing code on the whiteboard? me: It's the only way I code! interviewer: ... me: That was a joke. interviewer: OK, so are you familiar with ""fizz buzz""? me: ... interviewer: Is that a yes or a no? me: It's more of a ""I can't believe you're asking me that."" interviewer: OK, so I need you to print the numbers from 1 to 100, except that if the number is divisible by 3 print ""fizz"", if it's divisible by 5 print ""buzz"", and if it's divisible by 15 print ""fizzbuzz"". me: I'm familiar with it. interviewer: Great, we find that candidates who can't get this right don't do well here. me: ... interviewer: Here's a marker and an eraser. me: [thinks for a couple of minutes] interviewer: Do you need help getting started? me: No, no, I'm good. So let's start with some standard imports: interviewer: Um, you understand the problem is fizzbuzz , right? me: Do I ever. So, now let's talk models. I'm thinking a simple multi-layer-perceptron with one hidden layer. interviewer: Perceptron? me: Or neural network, whatever you want to call it. We want the input to be a number, and the output to be the correct ""fizzbuzz"" representation of that number. In particular, we need to turn each input into a vector of ""activations"". One simple way would be to convert it to binary. interviewer: Binary? me: Yeah, you know, 0's and 1's? Something like: interviewer: [stares at whiteboard for a minute] me: And our output will be a one-hot encoding of the fizzbuzz representation of the number, where the first position indicates ""print as-is"", the second indicates ""fizz"", and so on: interviewer: OK, that's probably enough. me: That's enough setup, you're exactly right. Now we need to generate some training data. It would be cheating to use the numbers 1 to 100 in our training data, so let's train it on all the remaining numbers up to 1024: interviewer: ... me: Now we need to set up our model in tensorflow. Off the top of my head I'm not sure how many hidden units to use, maybe 10? interviewer: ... me: Yeah, possibly 100 is better. We can always change it later. We'll need an input variable with width NUM_DIGITS, and an output variable with width 4: interviewer: How far are you intending to take this? me: Oh, just two layers deep -- one hidden layer and one output layer. Let's use randomly-initialized weights for our neurons: And we're ready to define the model. As I said before, one hidden layer, and let's use, I don't know, ReLU activation: We can use softmax cross-entropy as our cost function and try to minimize it: interviewer: ... me: And, of course, the prediction will just be the largest output: interviewer: Before you get too far astray, the problem you're supposed to be solving is to generate fizz buzz for the numbers from 1 to 100. me: Oh, great point, the predict_op function will output a number from 0 to 3, but we want a ""fizz buzz"" output: interviewer: ... me: So now we're ready to train the model. Let's grab a tensorflow session and initialize the variables: Now let's run, say, 1000 epochs of training? interviewer: ... me: Yeah, maybe that's not enough -- so let's do 10000 just to be safe. And our training data are sequential, which I don't like, so let's shuffle them each iteration: And each epoch we'll train in batches of, I don't know, 128 inputs? So each training pass looks like and then we can print the accuracy on the training data, since why not? interviewer: Are you serious? me: Yeah, I find it helpful to see how the training accuracy evolves. interviewer: ... me: So, once the model has been trained, it's fizz buzz time. Our input should just be the binary encoding of the numbers 1 to 100: And then our output is just our fizz_buzz function applied to the model output: interviewer: ... me: And that should be your fizz buzz! interviewer: Really, that's enough. We'll be in touch. me: In touch, that sounds promising. interviewer: ... I didn't get the job. So I tried actually running this ( code on GitHub ), and it turned out it got some of the outputs wrong! Thanks a lot, machine learning! I guess maybe I should have used a deeper network.",en,57
263,986,1463442549,CONTENT SHARED,2259988589066296676,-1799631734242668035,1342884206924700315,,,,HTML,http://www.proxxima.com.br/home/proxxima/2016/05/16/agencias-de-propaganda-o-que-sera-delas.html,meio&mensagem,"Pyr Marcondes16 de maio de 2016 - 7h30 Por Pyr Marcondes As agências de propaganda dificilmente serão o que foram até hoje, em futuro não muito distante. Um inexorável processo de transformação da indústria da comunicação passou a impor novos modelos de negócio e os formatos anteriores começam a deixar de ter a eficácia que tinham, enquanto os novos ainda não se consolidaram. O setor, assim como inúmeros outros, enfrenta hoje, por um lado, a disrupção trazida por novas tecnologias vindas de indústrias conexas, por outro, a incerteza dos anunciantes, que declaradamente enxergam agora menor valor no que as agências oferecem hoje, da forma como oferecem hoje. A rentabilidade nunca esteve tão baixa. Demissões se sucedem. Consolidações buscam salvar a parte possível dos legados. O setor sofre hoje o maior abalo da sua história desde que surgiu, no final do século XIX. Difícil encontrar um líder do setor que discorde desse quadro, ainda que nunca publicamente. Mais difícil ainda encontrar um líder do setor que saiba como fazer para transformar sua agência para os novos tempos. Mudar com a profundidade que exige o cenário é desafio considerável. Como âncora a retardar as transformações necessárias estão as fontes de receita, que no Brasil se reduzem a poucas, sendo o BV a mais relevante dentre elas. É o BV que ainda paga a conta, notadamente dos grandes players, mas o horizonte aponta para um futuro em que ele perderá a força que tem. Fundamentalmente, porque os anunciantes tenderão a apertar esse cinto, exigindo mais transparência nas negociações com suas verbas. Mas também porque os grandes grupos de mídia que bancam esse jogo estão amargando transformações igualmente impactantes. Os veículos cortarão ou reduzirão consideravelmente o BV. As agências sentirão fortemente esse impacto. Entre manter-se falsamente equilibrados com o que ainda têm e ir em busca de novos modelos de negócio e de receita, os líderes do setor oscilam, uns mais que outros. Há os que buscam saídas, ainda que incertos do que devem encontrar. E há os que não conseguem se mover, por paralisia funcional, fruto da inércia de anos caminhando sempre na mesma direção. O cachimbo entortou a boca. O estudo da FENAPRO: um espelho difícil de olhar. Circula no mercado, já desde o ano passado, estudo promovido pela FENAPRO, que tenta endereçar ao menos parte desses problemas. Foram ouvidos 160 empresários e altos executivos de agências de propaganda, em grupos de trabalho que se multiplicaram em todo o País, onde se discutiu, corajosamente, é importante que se diga, o setor e seus desafios. E buscou-se criar saídas conceituais e de posicionamento para a crise. Textualmente, trechos do levantamento, concluem que vivemos ""um mundo de inovações constantes e complexidade crescente, onde o crescimento dos meios digitais fez os antigos monólogos entre marcas e consumidores virarem diálogos. E onde surgem sem parar novas e inúmeras formas de interação, desafiando as agências a descobrir todos os dias de que forma engajar consumidores às marcas que estão sob seus cuidados"". A pergunta a ser respondida foi: como podemos garantir a sustentabilidade da agência de propaganda hoje e nos próximos anos? Analisando mais profundamente a sua própria situação, as agências listaram seus problemas: - Pressão por novos modelos de remuneração - Distanciamento do C-Level dos clientes - Qualificação de profissionais inadequada para as demandas atuais - Queda da atratividade da atividade junto a estudantes e novos talentos - Baixa percepção de valor dos produtos mais preciosos da agência: inteligência, estratégia, criação e planejamento - Agenciamento de mídia x geração de ideias - Juniorização (tanto no cliente, como na agência) - Baixa coesão do setor - Imagem deteriorada - Modelo de atuação desgastado - Perda de protagonismo e relevância - Crise de identidade Para fazer face a essa lista difícil de ler e de aceitar para si mesmas, as agências se auto-propuseram a: desenvolver uma atuação mais sintonizada com o negócio dos clientes; para conquistar o respeito, a confiança e a interlocução dos principais executivos, é preciso falar a linguagem deles, conhecer seu negócio com profundidade; ir além do que simplesmente elaborar campanhas publicitárias; estar apta a fazer um diagnóstico preciso do problema ou oportunidade existente; performance é o nome do jogo atual; ser proativa e audaciosa e estar disposta a correr riscos com o cliente; estar apta a mensurar o ROI das ações criadas e praticadas; saber cobrar pela real entrega da agência; cobrar pela criação, planejamento, estratégia e também pelo sucesso alcançado pelas suas ações; não necessariamente romper com o formato existente, mas estar pronta para criar novas formas de remuneração: fee, time sheet, success fee, etc. O documento ainda aponta, adicionalmente, que as agências devem ter células de inteligência e laboratórios de inovação, mesclar publicitários com membros de outas disciplinas, agregar mais marketing à atividade, não focando apenas em comunicação. Este é, muito certamente, o mais corajoso e crítico estudo que já se fez sobre agências de propaganda no País, a partir da ótica das próprias agências. As respostas estão soprando com o vento Há no mercado, brasileiro e internacional, indícios, pistas do que as agências deveriam fazer para mudar e se adaptar. Elas estão por todas as partes. No estudo da FENAPRO, nas manchetes da mídia do trade, em eventos e seminários mundo afora. As respostas estão, como diria Bob Dylan, soprando com o vento. Crítico é admitir que não basta conhecer e decifrar o que sopra o vento. É preciso rever profundamente a estrutura interna e, ainda mais importante, a mente corporativa das agências de propaganda. E isso não só não é nada fácil, como para muitas será, infelizmente, impossível. E algum preço será cobrado por isso. Algumas já estão pagando. Do vento, reuni algumas considerações e modelos. Não existe aqui aquele ditado dos norte-americano, one size fits all. Cada agência desenvolverá sua própria solução. O que deverá resultar num mercado mais diversificado em termos de posicionamento estratégico, aumentando a oferta e as alternativas aos anunciantes. Uma variedade certamente maior do que a de hoje, em que muito do que muitas agências oferecem é pura commodity indiferenciada, com glacê de chocolate ou morango. Entre as áreas de negócio e os serviços que as agências deverão passar a oferecer estão: - Consultoria de Negócios e de Posicionamento Estratégico - as grandes consultorias do mundo estão entrando fortemente no mercado de agências porque as agências deixaram durante anos esse flanco aberto. Agora é a hora de fortalecer a entrada do fundo do castelo, antes que o estrago seja irremediável. Oferecendo consultoria de negócios (entender o negócio do cliente, como aponta a FENAPRO) e aprofundar serviços de posicionamento estratégico, que hoje até são prestados, mas estão longe de atingir alto grau de excelência. Notadamente se comparados aos projetos das big 5. - Consultoria de Brand e de Branding - esse segmento se separou do corpo da agência há anos e escritórios especializados ocuparam esse mercado, mas se a agência não cuidar da marca e do branding de seus clientes, vai cuidar do que? - Tech - as agências terão que se aliar a parceiros ou desenvolver skills de tecnologia, que envolvam disciplinas como as hoje em expansão Internet das Coisas, Realidade Aumentada, Realidade Virtual, robótica, etc. Não é rocket science, são essas disciplinas aplicadas ao marketing. O modelo para isso é criar laboratórios específicos para esse fim. - Data - desnecessário dizer que data é o centro de toda a nova forma de abordagem do consumo e do consumidor. As agências terão que trazer para si esse conhecimento e a gestão desse universo. Não apenas contratando parceiros terceirizados para isso, como fazem hoje, mas incorporando talentos e estruturas que possam minimante, de dentro da agência, criar e coordenar as ações nessa área. - Prototipagem de Produtos - muitas agências nos EUA estão se especializando nisso. Não mais apenas desenvolver campanhas para produtos dos clientes, mas propriamente criar os novos produtos para os seus clientes. - e-commerce - não dá mais para deixar a mais importante atividade online de vendas dos anunciantes na mão deles mesmos ou de terceiros especialistas, sem que as agências se responsabilizem não só pela gestão abrangente do e-commerce, como também por aumentar as vendas nesse canal. - Conteúdo - uma vez mais aqui, escritórios de todos os portes e empresas conexas ao setor de publicidade ocuparam esse setor e não há qualquer razão para que as agências não atuem diretamente aí. Os clientes serão cada vez mais publishers e deixar que outros controlem essa produção é abrir mão de uma área estrategicamente valiosíssima para o marketing dos anunciantes. - Inovação e Startups - inovação é um método e um conhecimento, que gera soluções fora da caixa para problemas já existentes ou novos problemas. Ela é uma prima mais inteligente e esperta do planejamento estratégico, porque enxerga mais longe e se adapta melhor a mudanças rápidas, como as que vivemos hoje. É preciso criar núcleos de inovação para assessorar o cliente em seus desafios de mudança. Nesse contexto, a aliança com startups vai passar a ser não apenas uma jogadinha de marketing para dizer que sua agência é muderna, mas uma atividade core na geração de novas soluções. - Realtime - incorporar ao cotidiano da agência as tarefas de tempo real que todos os clientes têm hoje e que terão cada vez mais. Gestão de lançamentos, administração de redes sociais, acompanhamento de performance de mídia e de vendas, etc. Essas novas disciplinas abrem o leque de receitas, como já apontado pela própria FENAPRO. Passam a fazer parte dessa nova lista a remuneração não só por atividades que as agências nunca cobraram, como criação e planejamento, como também novidades como a remuneração por time sheet no caso de desenvolvimento de soluções, success fee no caso da geração de receita de e-commerce ou de superação de metas de performance. Há ainda a remuneração pela concepção de novos produtos e seu design, e pela criação, produção e gestão de conteúdos. Atividades mais conceituais e estratégicas como as de consultorias e as que agregam inovação e novas tecnologias podem também ser cobradas através de modelos específicos. São muitas as novas possibilidades. E para os que acreditam que tudo isso é algo inatingível, ou invencionice, vale dar uma olhada no site da R/GA ( www.rga.com ). Ela oferece todas essas atividades. E algumas mais. Nenhuma de graça. Não se trata aqui de copiar modelos, mas de adaptá-los. Ou ainda, inová-los. Mais que tudo, trata-se de olhar no difícil espelho da realidade, como fizeram as agências do estudo da FENAPRO, e partir para a ação. A inadiável ação de mudar. O Google joga fora todo o seu legado tecnológico e desenvolve outro completamente novo a cada 18 meses. A mudança lá não é algo fortuito, é parte do modelo de gestão e de atualização do negócio. Foi pensando e agindo assim que o Google criou o Google Ads, um business de bilhões, bem nas barbas do mercado publicitário mundial, que nunca pensaria algo assim, porque é adverso a mudanças (mantém o mesmo modelo de negócios desde o século XIX) e, em muitos casos, parece mais um gato gordo e largadão, um Garfield reclamando da sua própria sorte. E maldizendo o Google.",pt,57
264,2917,1483017574,CONTENT SHARED,2930191803881821526,2754566407772265068,-7945576473420652773,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36",MG,BR,HTML,https://blog.risingstack.com/node-js-best-practices-2017/,node.js best practices - how to become a better developer in 2017 | @risingstack,"A year ago we wrote a post on How to Become a Better Node.js Developer in 2016 which was a huge success - so we thought now it is time to revisit the topics and prepare for 2017! In this article, we will go through the most important Node.js best practices for 2017, topics that you should care about and educate yourself in. Let's start! Node.js Best Practices for 2017 Use ES2015 Last year we advised you to use ES2015 - however, a lot has changed since. Back then, Node.js v4 was the LTS version, and it had support for 57% of the ES2015 functionality. A year passed and ES2015 support grew to 99% with Node v6 . If you are on the latest Node.js LTS version you don't need babel anymore to use the whole feature set of ES2015. But even with this said, on the client side you'll probably still need it! For more information on which Node.js version supports which ES2015 features, I'd recommend checking out node.green . Use Promises Promises are a concurrency primitive, first described in the 80s. Now they are part of most modern programming languages to make your life easier. Imagine the following example code that reads a file, parses it, and prints the name of the package. Using callbacks, it would look something like this: Wouldn't it be nice to rewrite the snippet into something more readable? Promises help you with that: Of course, for now, the fs API does not have an readFileAsync that returns a Promise. To make it work, you have to wrap it with a module like promisifyAll . Use the JavaScript Standard Style When it comes to code style, it is crucial to have a company-wide standard, so when you have to change projects, you can be productive starting from day zero, without having to worry about building the build because of different presets. At RisingStack we have incorporated the JavaScript Standard Style in all of our projects. With Standard, there is no decisions to make, no .eslintrc , .jshintrc , or .jscsrc files to manage. It just works. You can find the Standard rules here . Use Docker - Containers are Production Ready in 2017! You can think of Docker images as deployment artifacts - Docker containers wrap up a piece of software in a complete filesystem that contains everything it needs to run: code, runtime, system tools, system libraries - anything you can install on a server. But why should you start using Docker? it enables you to run your applications in isolation, as a conscience, it makes your deployments more secure, Docker images are lightweight, they enable immutable deployments, and with them, you can mirror production environments locally. To get started with Docker, head over to the official getting started tutorial . Also, for orchestration we recommend checking out our Kubernetes best practices article. Monitor your Applications If something breaks in your Node.js application, you should be the first one to know about it, not your customers. One of the newer open-source solutions is Prometheus that can help you achieve this. Prometheus is an open-source systems monitoring and alerting toolkit originally built at SoundCloud. The only downside of Prometheus is that you have to set it up for you and host it for yourself. If you are looking for on out-of-the-box solution with support, Trace by RisingStack is a great solution developed by us. Trace will help you with alerting, memory and CPU profiling in production systems, distributed tracing and error searching, performance monitoring, and keeping your npm packages secure! Use Messaging for Background Processes If you are using HTTP for sending messages, then whenever the receiving party is down, all your messages are lost. However, if you pick a persistent transport layer, like a message queue to send messages, you won't have this problem. If the receiving service is down, the messages will be kept, and can be processed later. If the service is not down, but there is an issue, processing can be retried, so no data gets lost. An example: you'd like to send out thousands of emails. In this case, you would just have to put some basic information like the target email address and the first name, and a background worker could easily put together the email's content and send them out. What's really great about this approach is that you can scale it whenever you want, and no traffic will be lost. If you see that there are millions of emails to be sent out, you can add extra workers, and they can consume the very same queue. You have lots of options for messaging queues: Use the Latest LTS Node.js version To get the best of the two worlds (stability and new features) we recommend using the latest LTS (long-term support) version of Node.js. As of writing this article, it is version 6.9.2 . To easily switch Node.js version, you can use nvm . Once you installed it, switching to LTS takes only two commands: Use Semantic Versioning We conducted a Node.js Developer Survey a few months ago, which allowed us to get some insights on how people use semantic versioning. Unfortunately, we found out that only 71% of our respondents uses semantic versioning when publishing/consuming modules. This number should be higher in our opinion - everyone should use it! Why? Because updating packages without semver can easily break Node.js apps. Versioning your application / modules is critical - your consumers must know if a new version of a module is published and what needs to be done on their side to get the new version. This is where semantic versioning comes into the picture. Given a version number MAJOR.MINOR.PATCH , increment the: MAJOR version when you make incompatible API changes, MINOR version when you add functionality (without breaking the API), and PATCH version when you make backwards-compatible bug fixes. npm also uses SemVer when installing your dependencies, so when you publish modules, always make sure to respect it. Otherwise, you can break others applications! Secure Your Applications Securing your users and customers data should be one of your top priorities in 2017. In 2016 alone, hundreds of millions of user accounts were compromised as a result of low security. To get started with Node.js Security, read our Node.js Security Checklist , which covers topics like: Security HTTP Headers, Brute Force Protection, Session Management, Insecure Dependencies, or Data Validation. After you've embraced the basics, check out my Node Interactive talk on Surviving Web Security with Node.js ! Learn Serverless Serverless started with the introduction of AWS Lambda. Since then it is growing fast, with a blooming open-source community. In the next years, serverless will become a major factor for building new applications. If you'd like to stay on the edge, you should start learning it today. One of the most popular solutions is the Serverless Framework , which helps in deploying AWS Lambda functions. Attend and Speak at Conferences and Meetups Attending conferences and meetups are great ways to learn about new trends, use-cases or best practices. Also, it is a great forum to meet new people. To take it one step forward, I'd like to encourage you to speak at one of these events as well! As public speaking is tough, and ""imagine everyone's naked"" is the worst advice, I'd recommend checking out speaking.io for tips on public speaking! Become a better Node.js developer in 2017 As 2017 will be the year of Node.js, we'd like to help you getting the most out of it! We just launched a new study program called ""Owning Node.js"" which helps you to become confident in: Async Programming with Node.js Creating servers with Express Using Databases with Node Project Structuring and building scalable apps If you have any questions about the article, find me in the comments section!",en,57
265,1235,1464914016,CONTENT SHARED,3269302169678465882,-1443636648652872475,-6631146006024713322,,,,HTML,http://techcrunch.com/2016/06/02/the-barbell-effect-of-machine-learning/,the barbell effect of machine learning.,"If there's one technology that promises to change the world more than any other over the next several decades, it's (arguably) machine learning. By enabling computers to learn certain things more efficiently than humans, and discover certain things that humans cannot, machine learning promises to bring increasing intelligence to software everywhere and enable computers to develop new capabilities -- from driving cars to diagnosing disease -- that were previously thought to be impossible. While most of the core algorithms that drive machine learning have been around for decades, what has magnified its promise so dramatically in recent years is the extraordinary growth of the two fuels that power these algorithms - data and computing power. Both continue to grow at exponential rates, suggesting that machine learning is at the beginning of a very long and productive run. As revolutionary as machine learning will be, its impact will be highly asymmetric. While most machine learning algorithms, libraries and tools are in the public domain and computing power is a widely available commodity, data ownership is highly concentrated. This means that machine learning will likely have a barbell effect on the technology landscape. On one hand, it will democratize basic intelligence through the commoditization and diffusion of services such as image recognition and translation into software broadly. On the other, it will concentrate higher-order intelligence in the hands of a relatively small number of incumbents that control the lion's share of their industry's data. For startups seeking to take advantage of the machine learning revolution, this barbell effect is a helpful lens to look for the biggest business opportunities. While there will be many new kinds of startups that machine learning will enable, the most promising will likely cluster around the incumbent end of the barbell. Democratization of Basic Intelligence One of machine learning's most lasting areas of impact will be to democratize basic intelligence through the commoditization of an increasingly sophisticated set of semantic and analytic services, most of which will be offered for free, enabling step-function changes in software capabilities. These services today include image recognition, translation and natural language processing and will ultimately include more advanced forms of interpretation and reasoning. Software will become smarter, more anticipatory and more personalized, and we will increasingly be able to access it through whatever interface we prefer - chat, voice, mobile application, web, or others yet to be developed. Beneficiaries will include technology developers and users of all kinds. This burst of new intelligent services will give rise to a boom in new startups that use them to create new products and services that weren't previously cost effective or possible. Image recognition, for example, will enable new kinds of visual shopping applications. Facial recognition will enable new kinds of authentication and security applications. Analytic applications will grow ever more sophisticated in their ability to identify meaningful patterns and predict outcomes. Startups that end up competing directly with this new set of intelligent services will be in a difficult spot. Competition in machine learning can be close to perfect, wiping out any potential margin, and it is unlikely many startups will be able to acquire data sets to match Google or other consumer platforms for the services they offer. Some of these startups may be bought for the asset values of their teams and technologies (which at the moment are quite high), but most will have to change tack in order to survive. This end of the barbell effect is being accelerated by open source efforts such as OpenAI as well as by the decision of large consumer platforms, led by Google with TensorFlow , to open source their artificial intelligence software and offer machine learning-driven services for free, as a means of both selling additional products and acquiring additional data. Concentration of Higher-Order Intelligence At the other end of the barbell, machine learning will have a deeply monopoly-inducing or monopoly-enhancing effect, enabling companies that have or have access to highly differentiated data sets to develop capabilities that are difficult or impossible for others to develop. The primary beneficiaries at this end of the spectrum will be the same large consumer platforms offering free services such as Google, as well as other enterprises in concentrated industries that have highly differentiated data sets. Large consumer platforms already use machine learning to take advantage of their immense proprietary data to power core competencies in ways that others cannot replicate - Google with search, Facebook with its newsfeed, Netflix with recommendations and Amazon with pricing. Incumbents with large proprietary data sets in more traditional industries are beginning to follow suit. Financial services firms, for example, are beginning to use machine learning to take advantage of their data to deepen core competencies in areas such as fraud detection, and ultimately they will seek to do so in underwriting as well. Retail companies will seek to use machine learning in areas such as segmentation, pricing and recommendations and healthcare providers in diagnosis. Most large enterprises, however, will not be able to develop these machine learning-driven competencies on their own. This opens an interesting third set of beneficiaries at the incumbent end of the barbell: startups that develop machine learning-driven services in partnership with large incumbents based on these incumbents' data. Where the Biggest Startup Opportunities Are The most successful machine learning startups will likely result from creative partnerships and customer relationships at this end of the barbell. The magic ingredient for creating revolutionary new machine learning services is extraordinarily large and rich data sets. Proprietary algorithms can help, but they are secondary in importance to the data sets themselves. What's critical to making these services highly defensible is privileged access to these data sets. If possession is nine tenths of the law, privileged access to dominant industry data sets is at least half the ballgame in developing the most valuable machine learning services. The dramatic rise of Google provides a glimpse into what this kind of privileged access can enable. What allowed Google to rapidly take over the search market was not primarily its PageRank algorithm or clean interface, but these factors in combination with its early access to the data sets of AOL and Yahoo, which enabled it to train PageRank on the best available data on the planet and become substantially better at determining search relevance than any other product. Google ultimately chose to use this capability to compete directly with its partners, a playbook that is unlikely to be possible today since most consumer platforms have learned from this example and put legal barriers in place to prevent it from happening to them. There are, however, a number of successful playbooks to create more durable data partnerships with incumbents. In consumer industries dominated by large platform players, the winning playbook in recent years has been to partner with one or ideally multiple platforms to provide solutions for enterprise customers that the platforms were not planning (or, due to the cross-platform nature of the solutions, were not able) to provide on their own, as companies such as Sprinklr , Hootsuite and Dataminr have done. The benefits to platforms in these partnerships include new revenue streams, new learning about their data capabilities and broader enterprise dependency on their data sets. In concentrated industries dominated not by platforms but by a cluster of more traditional enterprises, the most successful playbook has been to offer data-intensive software or advertising solutions that provide access to incumbents' customer data, as Palantir , IBM Watson , Fair Isaac , AppNexus and Intent Media have done. If a company gets access to the data of a significant share of incumbents, it will be able to create products and services that will be difficult for others to replicate. New Playbooks New playbooks are continuing to emerge, including creating strategic products for incumbents or using exclusive data leases in exchange for the right to use incumbents' data to develop non-competitive offerings. Of course the best playbook of all - where possible - is for startups to grow fast enough and generate sufficiently large data sets in new markets to become incumbents themselves and forego dependencies on others (as, for example, Tesla has done for the emerging field of autonomous driving). This tends to be the exception rather than the rule, however, which means most machine learning startups need to look to partnerships or large customers to achieve defensibility and scale. Machine learning startups should be particularly creative when it comes to exploring partnership structures as well as financial arrangements to govern them - including discounts, revenue shares, performance-based warrants and strategic investments. In a world where large data sets are becoming increasingly valuable to outside parties, it is likely that such structures and arrangements will continue to evolve rapidly. Perhaps most importantly, startups seeking to take advantage of the machine learning revolution should move quickly, because many top technology entrepreneurs have woken up to the scale of the business opportunities this revolution creates, and there is a significant first-mover advantage to get access to the most attractive data sets. Featured Image: a-image / Shutterstock",en,57
266,1240,1464953944,CONTENT SHARED,-3723217532224917485,-8132559109129514792,-5261597004215022082,,,,HTML,http://www.rs-online.com/designspark/electronics/knowledge-item/eleven-internet-of-things-iot-protocols-you-need-to-know-about,11 internet of things (iot) protocols you need to know about,"There exists an almost bewildering choice of connectivity options for electronics engineers and application developers working on products and systems for the Internet of Things (IoT). Many communication technologies are well known such as WiFi, Bluetooth, ZigBee and 2G/3G/4G cellular, but there are also several new emerging networking options such as Thread as an alternative for home automation applications, and Whitespace TV technologies being implemented in major cities for wider area IoT-based use cases. Depending on the application, factors such as range, data requirements, security and power demands and battery life will dictate the choice of one or some form of combination of technologies. These are some of the major communication technologies on offer to developers. Bluetooth An important short-range communications technology is of course Bluetooth, which has become very important in computing and many consumer product markets. It is expected to be key for wearable products in particular, again connecting to the IoT albeit probably via a smartphone in many cases. The new Bluetooth Low-Energy (BLE) - or Bluetooth Smart, as it is now branded - is a significant protocol for IoT applications. Importantly, while it offers similar range to Bluetooth it has been designed to offer significantly reduced power consumption. However, Smart/BLE is not really designed for file transfer and is more suitable for small chunks of data. It has a major advantage certainly in a more personal device context over many competing technologies given its widespread integration in smartphones and many other mobile devices. According to the Bluetooth SIG, more than 90 percent of Bluetooth-enabled smartphones, including iOS, Android and Windows based models, are expected to be 'Smart Ready' by 2018. Devices that employ Bluetooth Smart features incorporate the Bluetooth Core Specification Version 4.0 (or higher - the latest is version 4.2 announced in late 2014) with a combined basic-data-rate and low-energy core configuration for a RF transceiver, baseband and protocol stack. Importantly, version 4.2 via its Internet Protocol Support Profile will allow Bluetooth Smart sensors to access the Internet directly via 6LoWPAN connectivity (more on this below). This IP connectivity makes it possible to use existing IP infrastructure to manage Bluetooth Smart 'edge' devices. More information on Bluetooth 4.2 is available here and a wide range of Bluetooth modules are available from RS. Zigbee ZigBee, like Bluetooth, has a large installed base of operation, although perhaps traditionally more in industrial settings. ZigBee PRO and ZigBee Remote Control (RF4CE), among other available ZigBee profiles, are based on the IEEE802.15.4 protocol, which is an industry-standard wireless networking technology operating at 2.4GHz targeting applications that require relatively infrequent data exchanges at low data-rates over a restricted area and within a 100m range such as in a home or building. ZigBee/RF4CE has some significant advantages in complex systems offering low-power operation, high security, robustness and high scalability with high node counts and is well positioned to take advantage of wireless control and sensor networks in M2M and IoT applications. The latest version of ZigBee is the recently launched 3.0, which is essentially the unification of the various ZigBee wireless standards into a single standard. An example product and kit for ZigBee development are TI's CC2538SF53RTQT ZigBee System-On-Chip IC and CC2538 ZigBee Development Kit . Z-Wave Z-Wave is a low-power RF communications technology that is primarily designed for home automation for products such as lamp controllers and sensors among many others. Optimized for reliable and low-latency communication of small data packets with data rates up to 100kbit/s, it operates in the sub-1GHz band and is impervious to interference from WiFi and other wireless technologies in the 2.4-GHz range such as Bluetooth or ZigBee. It supports full mesh networks without the need for a coordinator node and is very scalable, enabling control of up to 232 devices. Z-Wave uses a simpler protocol than some others, which can enable faster and simpler development, but the only maker of chips is Sigma Designs compared to multiple sources for other wireless technologies such as ZigBee and others. Standard: Z-Wave Alliance ZAD12837 / ITU-T G.9959 Frequency: 900MHz (ISM) Range: 30m Data Rates: 9.6/40/100kbit/s 6LowPAN A key IP (Internet Protocol)-based technology is 6LowPAN (IPv6 Low-power wireless Personal Area Network). Rather than being an IoT application protocols technology like Bluetooth or ZigBee, 6LowPAN is a network protocol that defines encapsulation and header compression mechanisms. The standard has the freedom of frequency band and physical layer and can also be used across multiple communications platforms, including Ethernet, Wi-Fi, 802.15.4 and sub-1GHz ISM. A key attribute is the IPv6 (Internet Protocol version 6) stack, which has been a very important introduction in recent years to enable the IoT. IPv6 is the successor to IPv4 and offers approximately 5 x 10 28 addresses for every person in the world, enabling any embedded object or device in the world to have its own unique IP address and connect to the Internet. Especially designed for home or building automation, for example, IPv6 provides a basic transport mechanism to produce complex control systems and to communicate with devices in a cost-effective manner via a low-power wireless network. Designed to send IPv6 packets over IEEE802.15.4-based networks and implementing open IP standards including TCP, UDP, HTTP, COAP, MQTT, and websockets, the standard offers end-to-end addressable nodes, allowing a router to connect the network to IP. 6LowPAN is a mesh network that is robust, scalable and self-healing. Mesh router devices can route data destined for other devices, while hosts are able to sleep for long periods of time. An explanation of 6LowPAN is available here , courtesy of TI. Thread A very new IP-based IPv6 networking protocol aimed at the home automation environment is Thread. Based on 6LowPAN, and also like it, it is not an IoT applications protocol like Bluetooth or ZigBee. However, from an application point of view, it is primarily designed as a complement to WiFi as it recognises that while WiFi is good for many consumer devices that it has limitations for use in a home automation setup. Launched in mid-2014 by the Thread Group , the royalty-free protocol is based on various standards including IEEE802.15.4 (as the wireless air-interface protocol), IPv6 and 6LoWPAN, and offers a resilient IP-based solution for the IoT. Designed to work on existing IEEE802.15.4 wireless silicon from chip vendors such as Freescale and Silicon Labs, Thread supports a mesh network using IEEE802.15.4 radio transceivers and is capable of handling up to 250 nodes with high levels of authentication and encryption. A relatively simple software upgrade should allow users to run thread on existing IEEE802.15.4-enabled devices. WiFi WiFi connectivity is often an obvious choice for many developers, especially given the pervasiveness of WiFi within the home environment within LANs. It requires little further explanation except to state the obvious that clearly there is a wide existing infrastructure as well as offering fast data transfer and the ability to handle high quantities of data. Currently, the most common WiFi standard used in homes and many businesses is 802.11n, which offers serious throughput in the range of hundreds of megabit per second, which is fine for file transfers, but may be too power-consuming for many IoT applications. A series of RF development kits designed for building WiFi-based applications are available from RS. Standard: Based on 802.11n (most common usage in homes today) Frequencies: 2.4GHz and 5GHz bands Range: Approximately 50m Data Rates: 600 Mbps maximum, but 150-200Mbps is more typical, depending on channel frequency used and number of antennas (latest 802.11-ac standard should offer 500Mbps to 1Gbps) Cellular < Any IoT application that requires operation over longer distances can take advantage of GSM/3G/4G cellular communication capabilities. While cellular is clearly capable of sending high quantities of data, especially for 4G, the expense and also power consumption will be too high for many applications, but it can be ideal for sensor-based low-bandwidth-data projects that will send very low amounts of data over the Internet. A key product in this area is the SparqEE range of products, including the original tiny CELLv1.0 low-cost development board and a series of shield connecting boards for use with the Raspberry Pi and Arduino platforms. Standard: GSM/GPRS/EDGE (2G), UMTS/HSPA (3G), LTE (4G) Frequencies: 900/1800/1900/2100MHz Range: 35km max for GSM; 200km max for HSPA Data Rates (typical download): 35-170kps (GPRS), 120-384kbps (EDGE), 384Kbps-2Mbps (UMTS), 600kbps-10Mbps (HSPA), 3-10Mbps (LTE) NFC NFC (Near Field Communication) is a technology that enables simple and safe two-way interactions between electronic devices, and especially applicable for smartphones, allowing consumers to perform contactless payment transactions, access digital content and connect electronic devices. Essentially it extends the capability of contactless card technology and enables devices to share information at a distance that is less than 4cm. Further information is available here . Sigfox An alternative wide-range technology is Sigfox , which in terms of range comes between WiFi and cellular. It uses the ISM bands, which are free to use without the need to acquire licenses, to transmit data over a very narrow spectrum to and from connected objects. The idea for Sigfox is that for many M2M applications that run on a small battery and only require low levels of data transfer, then WiFi's range is too short while cellular is too expensive and also consumes too much power. Sigfox uses a technology called Ultra Narrow Band (UNB) and is only designed to handle low data-transfer speeds of 10 to 1,000 bits per second. It consumes only 50 microwatts compared to 5000 microwatts for cellular communication, or can deliver a typical stand-by time 20 years with a 2.5Ah battery while it is only 0.2 years for cellular. Already deployed in tens of thousands of connected objects, the network is currently being rolled out in major cities across Europe, including ten cities in the UK for example. The network offers a robust, power-efficient and scalable network that can communicate with millions of battery-operated devices across areas of several square kilometres, making it suitable for various M2M applications that are expected to include smart meters, patient monitors, security devices, street lighting and environmental sensors. The Sigfox system uses silicon such as the EZRadioPro wireless transceivers from Silicon Labs, which deliver industry-leading wireless performance, extended range and ultra-low power consumption for wireless networking applications operating in the sub-1GHz band. Standard: Sigfox Frequency: 900MHz Range: 30-50km (rural environments), 3-10km (urban environments) Data Rates: 10-1000bps Neul Similar in concept to Sigfox and operating in the sub-1GHz band, Neul leverages very small slices of the TV White Space spectrum to deliver high scalability, high coverage, low power and low-cost wireless networks. Systems are based on the Iceni chip, which communicates using the white space radio to access the high-quality UHF spectrum, now available due to the analogue to digital TV transition. The communications technology is called Weightless, which is a new wide-area wireless networking technology designed for the IoT that largely competes against existing GPRS, 3G, CDMA and LTE WAN solutions. Data rates can be anything from a few bits per second up to 100kbps over the same single link; and devices can consume as little as 20 to 30mA from 2xAA batteries, meaning 10 to 15 years in the field. LoRaWAN Again, similar in some respects to Sigfox and Neul, LoRaWAN targets wide-area network (WAN) applications and is designed to provide low-power WANs with features specifically needed to support low-cost mobile secure bi-directional communication in IoT, M2M and smart city and industrial applications. Optimized for low-power consumption and supporting large networks with millions and millions of devices, data rates range from 0.3 kbps to 50 kbps. More about the Internet of Things in our IOT Design Centre Follow Like this Leave a comment",en,57
267,28,1459268727,CONTENT SHARED,-3173020603774823976,-1032019229384696495,3042342415047984532,,,,HTML,https://nodejs.org/en/blog/announcements/welcome-google/,welcome google cloud platform!,"Google Cloud Platform joined the Node.js Foundation today. This news comes on the heels of the Node.js runtime going into beta on Google App Engine , a platform that makes it easy to build scalable web applications and mobile backends across a variety of programming languages. In the industry, there's been a lot of conversations around a third wave of cloud computing that focuses less on infrastructure and more on microservices and container architectures. Node.js, which is a cross-platform runtime environment that consists of open source modules, is a perfect platform for these types of environments. It's incredibly resource-efficient, high performing and well-suited to scalability. This is one of the main reasons why Node.js is heavily used by IoT developers who are working with microservices environments. ""Node.js is emerging as the platform in the center of a broad full stack, consisting of front end, back end, devices and the cloud,"" said Mikeal Rogers, community manager of the Node.js Foundation. ""By joining the Node.js Foundation, Google is increasing its investment in Node.js and deepening its involvement in a vibrant community. Having more companies join the Node.js Foundation helps solidify Node.js as a leading universal development environment."" Along with joining the Node.js Foundation, Google develops the V8 JavaScript engine which powers Chrome and Node.js. The V8 team is working on infrastructural changes to improve the Node.js development workflow, including making it easier to build and test Node.js on V8's continuous integration system. Google V8 contributors are also involved in the Core Technical Committee. The Node.js Foundation is very excited to have Google Cloud Platform join our community and look forward to helping developers continue to use Node.js everywhere.",en,57
268,2930,1483615325,CONTENT SHARED,-8314629309720421219,3609194402293569455,2836247268716655754,"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36",SP,BR,HTML,https://blog.sucuri.net/2017/01/hacked-website-report-2016q3.html,hacked website report - 2016/q3,"Today we are proud to release our quarterly Hacked Website Report for 2016/Q3. This report is based on data collected and analyzed by the Sucuri Remediation Group (RG), which includes the Incident Response Team (IRT) and the Malware Research Team (MRT). The data presented is based on the analysis of over 8,000 infected websites. This report compares data from our 2016/Q1 and 2016/Q2 reports. We enjoy sharing these insights with the hope that they will continue to help website owners strengthen their security posture. CMS Analysis Similar to what we saw in 2016 - Q1 / Q2, the three leading affected CMS platforms were WordPress (74%) , Joomla! (17%) , and Magento (6%) . As we've mentioned before, this does not imply these platforms are more or less secure than others. Often the compromises analyzed had little, if anything, to do with the core of the CMS application itself, but more with improper deployment, configuration, and overall maintenance by the webmasters. Out-of-Date CMS at Point of Infection In this section we assessed the percentage of CMS installations that were out-of-date at the point of infection. A CMS was considered out-of-date if it was not on the latest recommended security version or had not patched the environment with available security updates at the time Sucuri was engaged to perform incident response services . This quarter we noticed a 6% increase in out-of-date, vulnerable versions of WordPress installations at the point of infection. Drupal also experienced a 2% increase from Q2 to Q3. Similar to prior quarters, Magento (94%) and Joomla! (84%) websites were mostly out-of-date, and vulnerable, at the point of infection. Both Joomla! and Magento though saw a 1% and 2% decrease compared to prior quarters. We share these as general observations, they are not meant to dictate the state of a CMS being more or less secure than others as there are a number of contributing factors. Further details are explained in the report. WordPress Analysis The WordPress platform contributed to 74% of our sampling. With this in mind, we dive deeper into our data related to WordPress. Unfortunately, due to data corruption we were unable to include the plugin distribution, but hope to reintroduce it next quarter. The top three WordPress plugins involved in website hacks continue to be TimThumb, Revslider, and Gravity Forms . This does not mean there are not others; it does mean that in the aggregate, no one plugin, beyond these three, has a footprint greater than 1% of the total infections. Although the top three plugins remained the same, we saw an improvement in Revslider, dropping 1.5% to 8.5% , and in GravityForms, dropping 2% to 4% (a total of 4.5% year to date). The total number of infected WordPress installations as a result of these three platforms has dropped significantly this year, from 25% in Q1 to 18% in Q3 . The continued decrease is expected as more website owners and hosts continue to proactively patch out of date environments. Perhaps the most disturbing dataset is the lack of change in TimThumb - likely due to the fact that many websites are still unaware that the script is on their site. In many ways, Revslider suffers from the same issue, where many website owners are unaware they have it installed as part of a theme. The TimThumb performance to date might be a roadmap of what we can expect from RevSlider in the coming years. Blacklist Analysis We continued our analysis of blacklists, which have the ability to adversely affect website owners. If you have been affected by a blacklist, learn how to remove a blacklist warning. Having a warning on your site by any one of the many authorities can adversely affect your brand and business economics. Approximately 15% of the infected websites were blacklisted (a 3% decrease from Q2). This indicates that approximately 85% of the thousands of infected websites we analyzed this quarter were freely distributing malware (without being blacklisted). These findings highlight the importance of continuous monitoring of your web property beyond traditional means like Google and Bing webmaster tools. Malware Families Understanding malware families is a crucial aspect of our research; it provides us the necessary insights to better understand the Tactics, Techniques and Procedures (TTP) cyber criminals are employing. Our research over the past quarter includes analyzing the various infection trends , specifically how they correlate to our malware families. We found that 72% of all compromises had a PHP based backdoor hidden within the site this quarter and approximately 37% of all infection cases are misused for SEO spam campaigns (either through PHP, database injections or .htaccess redirects). We also saw a substantial increase in mailer scripts (12%) being employed as part of email Spam campaigns, and a 10% decrease in malware distribution. Hacked Website Report - TL;DR Here are a few highlights from the report: WordPress continues to lead the infected websites we worked on ( at 74% ). The top three plugins affecting WordPress continue to be Gravity Forms, TimThumb, and RevSlider . WordPress installations that were out of date at the point of infection increased from 55% in Q2 to 61% in Q3. Both Joomla! and Magento continue to lead the pack with out-of-date vulnerable installations at the point of infection. The blacklist telemetry showed a 3% reduction in sites being blacklisted, increasing the number of infected websites that are going undetected by blacklist engines to 85% . SEO spam continues to be an important issue at 37% . There was a sharp 12% increase in mailer scripts and a 10% decrease in malware distribution. A new dataset was introduced this quarter that depicted which files bad actors are modifying most consistently with each incident: index.php ( 25% ), header.php ( 20% ), and .htaccess ( 10% ) The report is available both on our site and in PDF format . We truly hope that you find these insights valuable. We will continue to expand on the data we collect and report on, but as usual, if you have recommendations let us know. We no longer support comments on our blogs. If you'd like to continue the conversation, engage with us via Twitter at @sucurisecurity and @sucurilabs . If you have recommendations or questions that require more than 140 characters, please send us an email at info@sucuri.net .",en,57
269,2993,1485179372,CONTENT SHARED,-6590819806697898649,-4465926797008424436,-7541317811547099244,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2986.0 Safari/537.36",SP,BR,HTML,https://medium.com/android-dev-br/listas-com-recyclerview-d3f41e0d653c,listas com recyclerview - android dev br,"Lista simples O primeiro passo é colocar o RecyclerView no layout da Activity . Além do RecyclerView a a tela terá um botão para incluir o conteúdo, neste caso, o FloatingActionButton . O resto do layout é igual à qualquer outro, sem segredos. Um atributo interessante das listas no Android é o t ools:listitem que foi colocado no componente do RecyclerView acima. Quando colocado com o layout do ViewHolder (no exemplo é o @layout/main_line_view ), podemos ver no Preview do editor de layout do Android Studio como ficará nossa lista, basta colocar o seu layout. Também é preciso fazer o layout do ViewHolder , que será o layout usado em cada uma das linhas. Agora é hora de configurar nossa Activity. Além de relacionarmos os componentes da view como de costume (fazendo findViewById(), por exemplo) é necessário configurar o LayoutManager e o . Uma configuração extra adicionada é o addItemDecoration que recebe um DividerItemDecoration , usando o parâmetro VERTICAL . Com isso, é adicionado linhas divisórias entre os itens da lista. Neste exemplo, foi escolhido o LinearLayoutManager como LayoutManager. Este tipo configura a lista como uma lista simples e vertical. Muito se foi falado do LayoutManager, mas esta classe é apenas uma abstração. O que devemos utilizar mesmo são as classes abstratas, que será apresentado de acordo com cada exemplo. Outro componente que deverá ser criado é o ViewHolder . O seu layout já foi criado acima então é preciso criar a classe Java para relacionar os seus campos, como fazemos em uma Activity para poder acessar os componentes visuais. No exemplo, o Adapter está recebendo no construtor a lista de objetos, que no caso é UserModel. É sobre esta lista de objetos que é construído a lista visual. Para a classe ser uma Adapter é necessário herdar RecyclerView.Adapter<ViewHolder> e implementar os métodos obrigatórios: onCreateViewHolder (ViewGroup parent, int viewType ) Método que deverá retornar layout criado pelo ViewHolder já inflado em uma view . onBindViewHolder (ViewHolder holder, int position) Método que recebe o ViewHolder e a posição da lista. Aqui é recuperado o objeto da lista de Objetos pela posição e associado à ViewHolder . É onde a mágica acontece! getItemCount() Método que deverá retornar quantos itens há na lista. Aconselha-se verificar se a lista não está nula como no exemplo, pois ao tentar recuperar a quantidade da lista nula pode gerar um erro em tempo de execução ( NullPointerException ). Vale ressaltar que os métodos onCreateViewHolder e onBindViewHolder não são chamados para todos os itens inicialmente, eles são chamados apenas para os itens visíveis para o usuário. Quando o usuário sobe e desce a lista, estes dois métodos são chamados novamente associando a view reciclada ao conteúdo daquela posição que agora será visível. Feito isto e está pronto. Temos uma lista! Temos uma lista, de fato. Mas quando ela foi criada na Activity foi passado uma lista vazia no adapter . Agora é hora de popular, editar e remover itens. Tenha em mente que Adapter irá trocar o conteúdo da views que irão se reciclando percorrendo esta lista de objetos. Portanto, para adicionar, atualizar ou remover itens da lista devemos trabalhar com a lista de objetos que foi passado no construtor e está dentro da classe do Adapter(List<UserModel> mUsers). Seu Adapter deverá ser capaz de cuidar desta reciclagem. Neste artigo não será abordado códigos que não sejam relacionados ao RecyclerView. Por exemplo, técnicas e bibliotecas de terceiros que ajudaram no desenvolvimento do código. Como descrito na apresentação será usado um botão para gerar conteúdo. Ao clicar neste botão é chamado um método público do Adapter para incluir novos itens na lista. Inserir objetos na lista e atualizar o Adapter Quando o usuário clicar no botão de adicionar será passado por parâmetro o novo Objeto ( UserModel ). Este novo objeto será adicionado na lista original do Adapter (linha 11) e depois é necessário ""notificar"" o Adapter que existe um novo item naquela posição (linha 12) . Como dito acima, esta é uma das grandes vantagens do RecyclerView onde é atualizado apenas a View do novo item da lista, ao invés de atualizar toda a lista para notificar que há uma view nova, como era feito no ListView. Neste exemplo foi adicionado dois botões em cada view /linha. Um para incrementar o número de cada item da lista e outro para remover aquele elemento da lista. As ações de click destes botões foram configuradas no método onBindViewHolder do Adapter , como exibido no Adapter acima. Atualizar objetos na lista e atualizar o Adapter O código para atualizar um item é muito parecido com o código de incluir um item. Basta localizar o objeto na lista do Adapter e atualizar como desejar. Após isso, é necessário notificar o Adapter novamente que há uma atualização no item da posição indicada (linha 4) . Remover objetos na lista e atualizar o Adapter Para excluir um item da lista é necessário remover ele da lista de objetos da classe ( linha 3, removendo do ) e notificar o Adapter que aquele item foi removido ( linha 4) . Além disso, é necessário notificar os itens abaixo que há mudanças com eles ( linha 5 ), pois ao remover um objeto do meio da lista é necessário avisar o Adapter que os objetos abaixo dele sofreram alteração na sua posição ( index ). Caso contrário pode acontecer o seguinte cenário: Imagine que uma lista tem 5 itens e o usuário removeu o segundo. Quando o usuário clicar no ""novo"" segundo item ( que era o terceiro ) ele irá interagir na verdade com o terceiro item. Isso porque o item removido sumiu e o Android animou o deslocamento dos items abaixo para cima, mas apenas visualmente. Por isso é importante sempre notificar o Adapter que estes itens abaixos mudaram sua posição. Lista horizontal No RecyclerView é muito fácil mudar de uma lista vertical para uma lista horizontal, basta ajustar o LayoutManager na Activity. Também é utilizado o LinearLayoutManager, porém com outro construtor passando o contexto, a configuração HORIZONTAL e um boolean definindo se a lista vai exibir os itens em ordem reversa ou não. Caso desejar uma lista vertical com os itens na ordem reversa devemos usar o mesmo construtor, porém passando VERTICAL no segundo argumento. Outra componente que talvez seja necessário atualizar é o layout do ViewHolder, para que a view seja ajustada melhor na lista horizontal. No exemplo de lista verticais foi utilizado uma linha com texto e dois botões. Para o exemplo de lista horizontais está sendo utilizado o componente CardView e mais um componente de texto no centro, para uma melhor visualização. Além do método onBindViewHolder do Adapter que foi alterado para associar mais um campo (texto verde em latim no centro da view), as demais configurações do Adapter e também as funções de inserir, alterar e excluir são exatamente iguais. E com poucas mudanças temos uma nova disposição de conteúdo. Grade Mais uma vez é utilizado a estrutura anterior alterando apenas o LayoutManager . Para a disposição do tipo grade foi utilizada a implementação GridLayoutManager do gerenciador de Layout. No construtor foi passado além do contexto, o número de colunas desejado na grade. A implementação da Activity , Adapter , ViewHolder ... você já sabe, não há diferenças. Porém, as vezes o conteúdo/layout não é de um tamanho fixo para cada CardView e a grade pode ficar com lacunas e não aproveitar o máximo da tela do usuário. Por exemplo na imagem abaixo: Para casos como estes o Android tem um gerenciador especifico. E vamos à ele :). Grade escalonável Com o StaggeredGridLayoutManager a grade de conteúdo fica escalonável e o Android irá aproveitar o final de cada card de cada coluna para inciar a próxima view . Com esta pequena mudança o Android aproveita mais a tela do usuário para exibir mais conteúdo!",pt,56
270,2484,1475255123,CONTENT SHARED,-5154395767417065070,-1032019229384696495,-4436190914738794622,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.34 Safari/537.36",NY,US,HTML,https://techcrunch.com/2016/09/29/innovation-is-in-all-the-wrong-places/,innovation is in all the wrong places,"I live a pretty cosmopolitan futuristic life atop a glass skyscraper in New York City, but I've yet to get a pizza delivered by drone, order a taxi from Alexa or open a hotel door with my smartwatch. I've also not booked a hotel from a bot (because trying that drove me crazy) nor consumed news from one, because that's a terrible way to do it. In a world where what's possible is advancing at breakneck speed, it's odd that British Airways has developed an emotionally aware smart blanket, but doesn't ""do"" email. It's strange that IKEA has VR to help you experience your kitchen, but struggles with the basics of e-commerce. My car rental company has invested millions in onsite video-calling kiosks, but their app loses 50 percent of the bookings I make. We've got the questions wrong. It shouldn't be how are you innovating or which project is doing new things, but why are you doing it and on what level. From pizza by emoji or bot or smartwatch, to emoji-inspired aubergine-flavored condoms, we're experiencing a very superficial type of innovation. It's something new, physically notable and at the edge of the relevant business. It's typically the product of a small innovation unit, primarily with the goal of a funky press release, a great photo call or something to talk about on the next earnings call. And, generally speaking, it's the wrong types of companies that are doing it. It's CPG companies with beloved products but perhaps little to talk about. Maybe having a brand worth billions and steady sales is a bit boring. So we have special editions, apps and direct e-commerce with dubious unit economics. We have mattress brands becoming publishers. Why? Because Conde Nast, The New York Times or Hearst make it look easy? Or wildly profitable? Every new SKU is disruptive or, better still, reimagined. Yet many companies must innovate, desperately. The TV industry is making better shows than they ever have, but is suffering because they have little understanding of modern consumer behavior and choice architecture. We've got airlines that use incredible technology to keep their fleets in the sky and on time, but who routinely fail in even the most basic communication functions. Let's stop thinking of technology as a trendy tattoo - a surface-level commitment best kept on a conspicuous but not often used part of the body. From car rental desks that look shocked when it's busy to hotels that can't tell you when your room will be ready and ask for credit card details three times, physical retailers need to adapt to a world in which online shopping has made people impatient, expecting to find things immediately - and to be served even faster. For all companies, innovation needs to be deeper. Not token gestures on the edge, but fundamental rewiring of business from the core. Imagine a business as an onion of concentric layers. On the outermost surface would be communications - how companies express themselves. Inside this would be marketing - the services, promotions, pricing and products made by the business. At the core, upon which everything else is built, are the business values, culture, processes and systems. Only the most superficial changes happen at the edge. It's easiest there, requires the least organizational effort and gets the most visibility. Launching an innovation lab or an incubation fund and a venture unit requires a few bodies in a trendy offsite office, even if they do nothing after the post-launch media hype wears off. Innovation at the marketing layer is interesting. It's GoPro and Dollar Shave Club, both pretty sizable changes to products or how they are sold, supplied and paid for. But while examples like this have huge valuations and momentum, it's not clear how groundbreaking they are. The real examples of innovation come from companies built for the modern age. They've taken new behaviors, new technology, new workflows and, above all else, new consumer expectations. Here we see the obvious examples like Uber or Airbnb, but also companies like Facebook, which has become a media owner of vast scale that does not actually make any content. Here you can compare the revenue and costs of Buzzfeed and compare it to an old-world company like Conde Nast. Let's consider Netflix relative to Blockbuster, but also Spotify compared to record shops, or Kodak and Instagram. The most rapidly grown companies in the modern age, like Tesla worth $30 billion, or Dropbox, Nest or WhatsApp, have smashed all known expectations of what's possible because they innovated at the core. For goodness sake, no more iBeacon-driven vending machines, no more 3D-printed trinkets. Technology will create vast and profound shifts. The mobile-first world has yet to really arrive. Mobile payments, digital wallets and the Internet of Things will create the best-ever canvas for business. Technologies like 5G, ultra-fast Wi-Fi, smart cities and other forms of ICT will form the architecture of the world in which businesses will operate. At the same time, people will change. Tolerance of waiting is already near-zero, same-day delivery is expected, as are free returns and UIs must be near-invisible and frictionless. Maybe you do need an innovation lab. Maybe working with startups is key. Maybe your organization needs impetus and expertise - but for goodness sake, no more iBeacon-driven vending machines, no more 3D-printed trinkets. Let's stop thinking of technology as a trendy tattoo - a surface-level commitment best kept on a conspicuous but not often used part of the body. Let's think of it as oxygen - essential to the beating heart of your business. Featured Image: Ian Cuming/Getty Images",en,56
271,1287,1465310489,CONTENT SHARED,8847604225354271039,-1443636648652872475,-3221107014894071365,,,,HTML,https://cloud.google.com/blog/big-data/2016/05/how-to-forecast-demand-with-google-bigquery-public-datasets-and-tensorflow,"how to forecast demand with google bigquery, public datasets and tensorflow | google cloud big data and machine learning blog","Demand forecasting is something that every business does. If you're a restaurant owner, you need to forecast how many diners you'll have tomorrow and what foods they'll order so that you know what ingredients to shop for and how many cooks to have in your kitchen. If you sell shirts, you need to predict in advance how many shirts of each color you need to order from your clothing manufacturer. Usually, business owners forecast demand using their gut-feel (""people are going to order more souffles than omelettes"") or rules of thumb (""stock more red turtlenecks around Christmas""). The problem with gut-feel or rules of thumb is that they're rarely quantitative. How many more souffles will be ordered? How many more red turtlenecks should we stock? Could you use machine learning to forecast demand more precisely instead of relying on gut-feel or rules of thumb? If you have enough historical data from your business, you can. In this blog post, we'll show you how. Machine Learning First: what is machine learning? Normally, when you want a computer to do something for you, you have to program the computer to do it with an explicit set of rules. For example, if you want a computer to look at an image of a screw on a manufacturing line and figure out whether the screw is faulty or not, you have to code up a set of rules: Is the screw bent? Is the screw head broken? Is the screw discolored? etc. With machine learning, you turn the problem around on its head. Instead of coming up with all kinds of logical rules for why a screw might be bad, you show the computer a whole bunch of data. Maybe you show it 5,000 images of good screws and 5,000 images of faulty screws that your (human) operators discarded for one reason or the other. Then, you let the computer figure out how to tell a bad screw from a good one. The computer is the ""machine"" and it's ""learning"" to make decisions based on data. Forecasting taxicab demand in New York City Let's say you're the logistics manager for a taxicab company in New York, and you need to decide how many drivers you will ask to come in this coming Thursday. You know something about taxis in New York. You know, for example, that demand will depend on the day-of-the-week (demand on Thursdays is different from demand on Mondays) and on the weather forecast for Thursday. These are our predictors - the things we'll use to create our prediction. We also need to decide what it is that we wish to predict. Let's say that we'll predict the total number of taxicab rides (city-wide) on a particular day. We could assume that we'll get our typical share of those rides, and call in that many drivers. Put another way, this is our machine learning problem: Predictors and target Google BigQuery public datasets include both overall taxicab rides in New York (as the table nyc-tlc:green) and NOAA weather data (as the table fh-bigquery:weather_gsod), and so we decide to use those as our input datasets. The overall taxicab rides is only a proxy for the actual demand - the demand may have been higher than the actual number of rides if there were not enough taxis on the street or if the taxis were in different parts of the city from where the demand existed. However, this dataset is a good place to start if we assume the market for taxis in New York is efficient. If your business does not involve taxicabs or depends on something other than weather, you would load your own historical data into BigQuery. In Google Cloud Datalab , you can run BigQuery queries, and get the result back in a form that's usable from within Python (the full Datalab notebook , along with a more detailed commentary, is on github): Similarly, run a BigQuery query to get the number of taxicab rides by the daynumber (which represents the day of the year, e.g., daynumber=1 is New Year's Day): By merging the weather and trips datasets, we end up with the complete dataset to use for machine learning: That's our historical data, and we can use this historical data to predict taxicab demand based on the weather. Benchmark When doing machine learning, it's a good idea to have a benchmark. This will be a simple model, or perhaps be what your gut-instinct would tell you. We can evaluate whether the machine learning model is better than this benchmark by trying out both the simple model and the machine learning one on a test dataset. In order to create this test dataset, we'll collect all our training data, and then split it 80:20. We'll train the model on 80% of the data, and use the remaining 20% to evaluate how well the machine learning model does. A reasonable benchmark would be to use the average taxicab demand over the entire time period. If we can do better than just going with the average, our machine learning model is skillful . To measure how well a model does, we'll use the root-mean-square error. You could choose some other measure that's pertinent to the business problem you're solving. For example, you could compute the overall loss in revenue by having too few or too many drivers on a day and use that as your measure. Because the root mean square error (RMSE) when using the average is about 12,700, that's the measure we wish to beat using machine learning. In other words, we want a RMSE that's lower than 12,700. TensorFlow TensorFlow is a software library that was open-sourced by Google in 2015. One of the things that it excels at is conducting machine learning using neural networks, especially deep neural networks. You can play with neural network architectures on the TensorFlow playground . Even though the code below may look intimidating, most of it is boilerplate (see the Datalab notebook for the complete code; Google Cloud Machine Learning , now in alpha, provides a simpler way to do this from Datalab). I'm using a neural network with one hidden layer (limiting the number of layers because we don't have millions of examples, just a few hundred days), choosing intermediate nodes to be rectified linear units (relu), and setting the output node to be an identity node (because this is a regression problem, not a classification one). We save the model and run it on the test dataset, and verify that we're doing better than our benchmark: A RMSE of about 8,200 is much better than the RMSE of 12,700 we got by simply using the historical average. Running a trained model Once we have trained a model, running each time on new predictor data is quite straightforward. For example, let's say we have the weather forecast for the next three days. We can simply pass in the predictor variables (day of week, min. and max. temperature, rain) to the neural network and obtain the predicted taxicab demand for the next three days: It appears that we should tell some of our taxi drivers to take the day off on Wednesday (day=4) and be there in full strength on Thursday (day=5). Thursdays are usually ""slow"" days (taxi demand in New York peaks on the weekends), but the machine learning model tells us to expect heavy demand this particular Thursday because of the weather. Google Cloud Platform made this demand forecasting problem particularly straightforward to carry out. Cloud Datalab provides an interactive Python notebook that's well-integrated with BigQuery, Pandas, and TensorFlow. The public datasets on GCP include historical weather observation data from NOAA. To learn more about GCP and its Big Data and Machine Learning capabilities, register for a training course .",en,56
272,1191,1464807858,CONTENT SHARED,-4386371945374980231,-1352064057049251194,-490252022775961795,,,,HTML,http://epocanegocios.globo.com/Empresa/noticia/2016/06/por-que-o-walmart-nao-e-walmart-no-brasil.html,por que o walmart não é walmart no brasil,"Baixo movimento: em janeiro, o Walmart anunciou o fechamento de 60 lojas no Brasil, entre as quais a de Sumaré (SP) (Foto: Página Popular) Sempre que se fala em Walmart logo surgem as analogias esperadas: se a varejista americana fosse um país, teria o PIB de, por exemplo, uma Venezuela. Ou o equivalente à riqueza somada de Chile e Peru. Responderia ainda por um terço do produto interno brasileiro. Não faltam parâmetros geoeconômicos para medir o gigantismo da empresa, dona de receitas de US$ 485 bilhões e de uma força de trabalho de 2,2 milhões de pessoas (o contingente do exército da China, excluindo reservistas, segundo a revista The Economist). Ocorre que a nação Walmart nem sempre transforma esse produto interno bruto em lucro certo e líquido. Tome o Brasil como exemplo. Em 2014, a empresa, terceira colocada no varejo nacional, faturou R$ 29,6 bilhões, R$ 1 bilhão a mais do que no ano anterior. Mas registrou prejuízo de cerca de R$ 200 milhões, segundo cálculos de especialistas do setor (a companhia não divulga os resultados consolidados por região). Analistas de mercado, ex-funcionários e fornecedores ouvidos pela NEGÓCIOS acreditam que a operação brasileira jamais tenha fechado um ano com a última linha do balanço no azul. O Walmart não comenta. (Fotos: Felipe Gombossy, O Globo) ""Quando desembarcaram por aqui, a mensagem era clara: o Walmart iria conquistar o Brasil passando por cima da concorrência como um rolo compressor"", diz Rubens Batista, consultor especializado em varejo e sócio da consultoria 2B Partners. Duas décadas após, no entanto, o quadro é bem diferente. A empresa não consegue sair da terceira posição do varejo nacional. Está bem longe do líder Pão de Açúcar, que fatura duas vezes mais, R$ 72 bilhões, tem o quádruplo de lojas e o dobro de funcionários. Também não conseguiu ultrapassar o Carrefour, segundo do ranking, ainda que a rede francesa tenha vivido seguidas crises no país. Em janeiro o Walmart anunciou o fechamento de 60 de suas 540 lojas no país, como parte de um plano de encerramento global de pontos de venda deficitários (no mundo, serão 270). E, mais uma vez, trocou de presidente. Saiu Guilherme Loureiro, rumo à direção do grupo na América Latina, e entrou Flavio Cotini, ex-Unilever. É o quarto presidente a assumir a empresa nos últimos oito anos, uma média de um CEO a cada dois anos. No Brasil, grandes companhias costumam promover mudanças na presidência a cada cinco anos, segundo estudo da consultoria Strategy& (antiga Booz&Company). E como se tudo isso não bastasse, o grupo enfrenta ainda uma investigação de autoridades americanas por suposto pagamento de propinas para a construção de lojas em Brasília. O cenário é preocupante, ainda mais levando em conta que a empresa já encerrou suas operações em mercados onde também registrou prejuízos, como a Coreia do Sul e a Alemanha. Piora ainda mais com os problemas enfrentados pela matriz e a necessidade de cortar despesas. No ano passado, o Walmart perdeu o posto de maior valor de mercado do mundo para a Amazon, dona de um faturamento cinco vezes menor. A briga entre Walmart e Amazon é uma mostra de como, mesmo nos Estados Unidos, ser um gigante pode não ser o suficiente. Quando o Walmart lançou sua loja online, em 1999, a Amazon faturava US$ 1,6 bilhão. Este ano, o e-commerce do Walmart deve faturar cerca de US$ 14 bilhões - enquanto a Amazon vai superar R$ 100 bilhões. Isso significa que a velocidade de crescimento da empresa de Jeff Bezos no comércio eletrônico é quase dez vezes maior que a da varejista. O Walmart, cuja história sempre foi marcada pela inovação, parece ter perdido a mão. Ir a um supermercado, hoje, envolve encarar trânsito, pegar um carrinho, circular por corredores, pegar fila na hora de pagar, carregar o carro, pegar trânsito novamente, descarregar os pacotes e guardar todos os itens em casa. Frente a esse cenário, fazer compras com um clique e ter os produtos entregues na sua casa parece uma opção mais atraente. Pesquisas feitas por institutos americanos colocam a Amazon no topo da lista de melhor experiência de compra nos últimos anos. Como resultado, para manter seus acionistas por perto, o Walmart teve de pagar US$ 6 bilhões nos últimos dois anos em programas de recompra de ações - além de distribuir parcelas generosas do lucro na forma de dividendos. A Amazon nunca precisou gastar um centavo nesse tipo de estratégia. Na prática, isso significa que uma empresa precisa pagar pelo presente, enquanto a outra aposta no futuro. Criador de conceitos, de repente o Walmart se viu transformado em seguidor de um novo paradigma, estabelecido pela concorrência. A consequência era esperada: a redução da expectativa de lucro em 2016 está derrubando o valor das ações na Bolsa de Nova York - em um único dia, no final do ano passado, elas chegaram a recuar quase 10%. ""Essa empresa tem mais de 50 anos e o varejo mudou"", admitiu o presidente mundial, Doug McMillon. ""O que o Walmart pode fazer e que ninguém mais pode é casar o e-commerce com nossos ativos físicos para entregar uma experiência de compra perfeita, em grande escala."" Corredores de um supermercado do Walmart: faturamento por metro quadrado e por funcionário é inferior ao dos concorrentes (Foto: Agência O Globo Força cultural Mas enquanto isso não ocorre, fica a questão: por que o Walmart não decola no Brasil. É claro que a própria crise da economia brasileira afetou os resultados do grupo. Na última década, o varejo cresceu quase 5% ao ano, em média. Já em 2016, a expectativa é de queda de 3%, segundo o Instituto Brasileiro de Executivos de Varejo e Mercado de Consumo (Ibevar). Mas os problemas do Walmart vão muito além da conjuntura - e não vêm de agora. Procurados, os porta-vozes da companhia não concederam entrevistas. Mas, segundo uma série de especialistas, concorrentes e ex-executivos ouvidos por NEGÓCIOS, há um problema fundamental de adaptação do negócio da empresa à cultura de compras brasileira. ""Faltou e falta tropicalizar o Walmart"", diz Paulo Cury, sócio da consultoria Condere. Como o modelo é extraordinariamente bem-sucedido nos Estados Unidos, tende a ser replicado à risca em outros países - ignorando as características específicas de cada região. Em 1995, os primeiros supermercados do Walmart abertos no Brasil tinham em suas prateleiras itens como equipamentos para beisebol e casacos para neve. Até itens corriqueiros das lojas, como produtos de limpeza e esfregões, eram importados dos Estados Unidos. O mix de produtos foi adaptado. O modelo de negócios, não. ""A ideia de manter o sistema, mesmo com defeitos, vem da crença de que, algum dia, eles vão acabar educando o consumidor"", afirma um ex-diretor. ""Deu certo no México, e a expectativa é fazer o mesmo no Brasil."" O problema é que a rede já está em sua segunda década de operação no país. Parece haver uma falta de sintonia entre os desejos da empresa e a realidade do mercado. Do ponto de vista dos negócios, a principal marca registrada dessa cultura é a oferta de preços, em média, mais baixos que os dos concorrentes, uma estratégia conhecida como EDLP, ou Every Day Low Price, algo como Preço Baixo Todos os Dias. O EDLP é um mantra dentro do Walmart, uma marca registrada da companhia. É justamente esse o problema. O modelo do EDLP funciona bem nos Estados Unidos, devido à escala da operação. Mas tem dificuldade em um mercado menor e com menos fornecedores, como o brasileiro. ""É um processo que castiga a margem e sacrifica muito a operação"", diz Batista. Mas a principal questão é que o modelo, curiosamente, não pegou no Brasil - é um bicho estranho que pouca gente conhece. Por aqui, o sistema praticado no mercado é conhecido como High-Low (Alto e Baixo): promoção em alguns itens e preço mais elevado em outros, para compensar. O consumidor brasileiro associa a economia na conta a promoções e não a uma estratégia comercial para toda a loja. ""É uma questão de percepção: a oferta de um produto gera a sensação de que o carrinho todo vai sair mais barato"", diz Jeferson Mola, professor da Universidade Anhembi Morumbi. Há alguns anos, a operação brasileira tentou mudar o modelo do EDLP para o High-Low, mas acabou tendo de voltar atrás. Motivo: falta de autonomia dos executivos, CEOs incluídos. ""Os chefes precisam consultar Bentonville até para questões corriqueiras"", afirma um executivo com conhecimento da empresa. Em parte, pela cultura centralizadora da empresa. Por outra, pelo baixo peso do Brasil dentro do grupo. Convertendo a moeda, são vendas de US$ 7 bilhões por ano, cerca de 1% do faturamento mundial. ""É uma operação relativamente pequena, que precisa de adaptações, dentro de um negócio muito grande. Vira aquela coisa: a chefia não discute, não ouve e vai tocando, achando que uma hora dá certo"", diz o executivo. Marcação cerrada Claro, nem todos os problemas do Walmart por aqui são derivados da falta de adaptação ao Brasil. Alguns deles se devem ao fato do mercado local ser ""duríssimo, com concorrentes bem estabelecidos, extremamente profissionais e com relações consolidadas com fornecedores"", segundo Antônio César Carvalho de Oliveira, diretor da consultoria Acomp. Um exemplo dessa concorrência acirrada vem do Carrefour. Quando o grupo americano chegou ao país, a rede francesa decidiu atacar em vez de se defender. Mapeou onde seriam as novas lojas do novo concorrente e reforçou suas unidades próximas, aumentando a equipe, o sortimento e o estoque. Comprou um terreno bem ao lado do local de um dos primeiros hipermercados do Walmart, em Osasco, construiu uma nova unidade e mandou para lá um time experiente, liderado por um de seus melhores diretores. Foi definido que essa loja não teria responsabilidade de fazer dinheiro: seria uma unidade de combate, por assim dizer, com preços mais baixos e promoções frequentes. ""O resultado foi uma carnificina"", diz Batista, da 2B. Sem uma equipe experiente, com sistemas pouco adaptados ao Brasil e produtos para americano ver, a unidade do Walmart sofreu com baixo movimento por anos e anos. Outros desafios, no entanto, são de responsabilidade da própria empresa, como a falta de integração entre suas diferentes bandeiras no país. Nos Estados Unidos, o modelo de negócios do Walmart funciona devido à sua altíssima escala - são cerca de 5 mil lojas por lá -, e a rede tenta replicar isso em outros países. No Brasil, foram compradas diversas redes regionais, como Bompreço, Big e Maxxi. Contribuiu para essa estratégia o poder de fogo do grupo. Em 2004, por exemplo, o Pão de Açúcar negociava a compra da rede nordestina Bompreço com os executivos brasileiros da companhia, que pertencia à holandesa Royal Ahold. No meio da negociação, chegou a informação que as conversas teriam de ser interrompidas. Motivo: o Walmart americano havia acabado de fechar o negócio, conversando diretamente com os holandeses, sem sequer passar pelos executivos brasileiros. O problema, no entanto, é que o Walmart brasileiro nunca deglutiu as redes que abocanhou. Sistemas de operações, logística e a parte administrativa dessas unidades adquiridas ainda não foram integrados. O Walmart usa vários sistemas diferentes de gestão em suas bandeiras espalhadas pelo país. A previsão é que a integração das regiões, iniciada em 2010, só deve ser concluída daqui a 18 meses, na melhor das hipóteses. O grupo também padece de apostas feitas em segmentos que ainda não se mostraram rentáveis. O Walmart começou a atuar no Brasil com dois modelos de negócios diferentes, ao mesmo tempo: supercenter (super e hipermercados) e clube de compras, por meio da bandeira Sam's Club. Foi necessário montar e treinar dois times, o que também dobrou os custos da estrutura. Mas o clube de compras ainda não decolou por aqui. No exterior, empresas como a americana CostCo atuam lado a lado com a indústria para desenvolver produtos específicos para esse modelo. Aqui, em um estágio bem menos amadurecido, o apelo não é a exclusividade de itens, mas sim o preço - o que nem todos veem como algo tão vantajoso, já que a anuidade paga pelos clientes está na faixa dos R$ 65. É um modelo mais focado no profissional (pequenos empresários e empreendedores individuais) do que na pessoa física. O problema é que a opção do Walmart em reforçar seu clube de compras veio exatamente quando a preferência do consumidor brasileiro - em especial o da nova classe média - recaiu sobre os chamados atacarejos. Desde 2008, o atacarejo cresceu a 22% ao ano em média, segundo a consultoria Bain & Company, o dobro da média do varejo de alimentos. Com foco no Sam's Club, a operação do Walmart no segmento, sob a bandeira Maxxi, recebeu pouco investimento. O adjetivo mais suave dado pelos analistas para a operação é ""problemática"". Ao mesmo tempo, os concorrentes se moveram. A rede Atacadão já representa 60% da receita do Carrefour no Brasil. Dentro do Pão de Açúcar, a bandeira Assaí é a estrela do momento. Uma bela oportunidade, até agora desperdiçada. Maxxi, a bandeira de atacarejo do Walmart, ainda não decolou.Os rivais Atacadão e Assaí aproveitam: o segmento vem crescendo 22% ao ano (Foto: Divulgação Rede Walmart) Gigantismo Há um outro ""pequeno"" detalhe que atrapalha a vida do Walmart no Brasil: ele é grande demais. Não em tamanho da rede, mas em espaço físico. Em média, segundo a Associação Brasileira de Supermercados (Abras), cada loja do Walmart tem 2.838 metros quadrados, mais do que o dobro dos 1.336 metros quadrados do Pão de Açúcar. E isso virou uma dor de cabeça. Quando Sam Walton - fundador do Walmart - abriu sua primeira loja, em 1952, criou um conceito revolucionário para a época: juntar todas as necessidades básicas das famílias em um único lugar, pelo preço mais baixo. Isso facilitou de modo decisivo o processo de compras. Por aqui, esse modelo de grandes lojas ganhou popularidade durante os anos 80 e 90. Os hipermercados reuniam todos os tipos de produtos imagináveis - de pneus a fraldas infantis - e ainda eram um programa familiar, um símbolo de status, especialmente para famílias com menor poder aquisitivo. Mas os tempos mudaram. O perfil social do país se alterou, a inflação diminuiu e a preocupação com os preços deixou de ser exclusiva. Itens como comodidade e um mix mais especializado de produtos passaram a pesar mais. Os shoppings ganharam espaço como palco da diversão da família e o número de pessoas solteiras e separadas disparou. Tudo isso impulsionou as lojas de pequeno porte, mais espalhadas e bem localizadas. Segundo números da consultoria Bain & Company, o segmento formado por pequenas unidades já ultrapassou em importância os hipermercados. Elas já movimentam R$ 80 bilhões por ano, frente aos R$ 66 bilhões das megalojas. Ao mesmo tempo, a disparada no preço do metro quadrado nas grandes cidades tornou proibitivo o custo para um supermercado. Há uma década, construía-se um super ""médio"" por menos de R$ 50 milhões, somando o preço do terreno e da construção; hoje em dia, esse valor dificilmente fica abaixo dos R$ 100 milhões. O chamado payback - tempo que a obra demora para se pagar - está em sete anos. ""É um investimento caríssimo em um país com taxas de juros elevadas"", diz Cláudio Felisoni de Angelo, presidente do Ibevar. Como resultado, Carrefour e Pão de Açúcar estão correndo para abrirem unidades nesse formato conveniência - sob as marcas Express e Minuto, respectivamente -, enquanto o Walmart segue empacado com as lojas de grande porte. Mesmo as unidades maiores, concebidas segundo o modelo americano - organizado, silencioso, com prateleiras imaculadamente arrumadas e sem cartazes de promoção - parecem não fazer com que os clientes nacionais se sintam convidados a comprar. ""O que faz sucesso aqui é a festa, a bagunça, o locutor falando sobre as promoções no megafone"", diz Enéas Pestana, ex-presidente do Pão de Açúcar e recentemente empossado no comando da JBS. ""O Abilio [Diniz] costumava me dizer que hipermercado morno é hipermercado morto. A loja do Walmart é desanimada, parece um cemitério."" O problema com a loja vai além. Uma pesquisa do grupo, feita anos atrás junto a uma consultoria, descobriu que poucas mulheres entravam nas lojas devido ao desconforto gerado pelo frio excessivo do ar-condicionado. Tudo isso aparece nos resultados. Segundo a Abras, o faturamento anual por metro quadrado do Walmart é de R$ 19 mil, frente R$ 25 mil do Pão de Açúcar. O faturamento por funcionário também é inferior: R$ 396 mil, ante R$ 452 mil da rede líder. Unidade do Pão de Açúcar Minuto: as pequenas lojas avançam e o Walmart continua insistindo no conceito de hipermercados (Foto: Agência Estado) Tentativa de reação Frente ao panorama negativo, parte do mercado vê de modo positivo as ações mais recentes. ""O fechamento de lojas é sadio. Eram unidades que se tornaram muito deficitárias e estavam afetando os resultados do grupo"", diz Márcio Roldão, sócio da consultoria MaxValue. A troca na presidência também foi vista como um indicativo de que a empresa irá se concentrar na redução de custos. Foi nessa área que o novo CEO Flavio Cotini se destacou em suas experiências anteriores, na Unilever e Diageo. Outro ponto visto como positivo é o investimento pesado feito na operação brasileira de e-commerce, que cresce acima dos 10% ao ano. A unidade recebeu o reforço de diversos executivos vindos da Cnova e já reúne mais de mil funcionários. Tem uma estrutura operacional própria, separada do resto da rede, incluindo centros de distribuição exclusivos para as vendas feitas pela internet. Outros especialistas dizem, no entanto, que fechar lojas, trocar executivos e apostar no e-commerce ainda é muito pouco. ""São soluções pontuais, enquanto os problemas são estruturais"", diz Batista. ""É preciso tomar as decisões estratégicas, difíceis."" O consultor lista algumas: definir um modelo de negócios. Tentar mudar o sistema de precificação. Ampliar o atacarejo. Decidir se quer ou não investir em pequenas unidades. ""E a empresa ainda não sinalizou com nenhuma dessas respostas"", afirma Enéas Pestana.",pt,56
273,563,1461531427,CONTENT SHARED,-9192549002213406534,-1578287561410088674,-5843888580106500275,,,,HTML,http://thenextweb.com/dd/2016/04/20/googles-chrome-os-material-design/,chrome os now has material design for the desktop,"Chromebook users are in for a surprise: Chrome 50 is bringing material design to Chrome OS, too. On his blog , Senior Designer at Google Sebastien Gabriel notes there will also be two implementations of Chrome OS. One will be designed for those using mice, while touch-enabled Chromebooks will have a ""hybrid"" layout employing more space between buttons and icons. Material Design for Chrome OS is a lot like you'll find for Android , and that's probably the point. As we've detailed before , Google wants Material Design to be the design language more developers and designers turn to as a default, especially with its recursive look and feel. It also makes sense for Chrome OS, which more heavily on the Web than more traditional operating systems. If Chrome as a browser is going to sport Material Design, its OS should too. Read next: Get ready to kill your cable box: Xfinity is coming to Roku and Samsung Smart TVs TNW's West Coast writer in the PNW (Portland, Oregon). Nate loves amplifying developers, and codes in Swift when he's not writing. If you need to get in touch, Twitter or email are your best bets.",en,56
274,1401,1465998151,CONTENT SHARED,7429409096643373334,-8020832670974472349,-2734800878000447382,,,,HTML,http://hood.ie/blog/beyond-progressive-web-apps-part-2.html,beyond progressive web apps part 2: we need to know what's new,"This is part two of a three part series about data sync beyond Progressive Web Apps. Please read part one, before continuing . Say we have a news app that runs on mobile devices and a server that publishes new stories. Blogs and RSS are a good real-world example. The scenario is this: our app starts for the first time and there are no stories available for the user to read on the device. So, the app asks the server to send the latest set of stories to the device. End result: our users get to read some stories. Later, when our apps starts for the second time, your app asks for the latest set of stories. And here is the first interesting bit: we don't want to send the app the stories it already has. Why? Four reasons: They are already on the device, so it's redundant It costs us server processing time It costs our users bandwidth It adds response-time latency to the experience As a general rule of thumb, in user experience design: don't let the user wait unnecessarily or they will get frustrated and stop using your app, your store, whatever. So what's the solution? We need to request new stories from the server in a way that says ""I've got all stories up to this point . Give, give me anything that's new "". We'll call the difference between ""all stories"" and ""stories that exist on the client"" the delta (i.e. difference). The delta is what we need to get from the server. Calculating the Delta To ask for the delta efficiently, we need two ingredients: Our app needs to store its state somewhere, so it knows what stories it already has. We'll call this the high watermark (for reasons that will become clear soon). In a native app, this could be stored in a device-local database or file. In a website or web app, this could live in browser storage systems like localStorage , or IndexedDB . Our server needs to look up the list of all stories, sorted by when they were published. It needs to be able to do do this efficiently. And it needs to be able to send back any range of stories, from any specified start date to the present moment. A naive implementation on the server side would be something equivalent to retrieving all stories from the database, sorting them by date in memory, before sending the result to the app. If the app sends a high watermark, the server would only send stories that come after the high watermark. While this definitely works, it is not very efficient . All stories have to be loaded from the database, sorted and finally filtered to match the range the client is interested in. While a server with only a few apps requesting only a handful of stories can easily do this, we should be looking for optimisations here. Something that will work with large data sets. How? Traditionally, we would add an index to the database. The Index An index has two things going for it: Order: things will be stored in index order on disk, so reading things (e.g. stories sorted by publish date) in that order is very efficient. Support for ranges: reading only part of an index from any point in the index range (e.g. all stories after a certain date) is very efficient. Maybe the database already has an auto-increment integer as a primary key. If so, each new story gets an ID integer that is one higher the previous ID. If this is the case, the server is already prepared and the app only needs to store the highest ID it gets from the server in the initial request. The app can then send that ID as the high watermark for subsequent requests. When the app receives the new batch of stories, it stores the new highest ID as the next high watermark, and so on. Here's what this looks like: Updates Now, let's imagine our stories are sometimes updated. This is fairly common. Typos need to be fixed, corrections posted, new story developments need to be added, and so on. In this case, using an auto-incrementing ID is not a good solution. Not only do new stories need to get a new ID, or more precisely a higher ID, but also: updates to a story need to get a higher ID, so that they are included in the calculation of the next delta. To solve this, we need a new table: updates . This updates table provides a secondary index that we use for recording updates only. If you add one record for every update, pointing back to the corresponding story that was updated, the auto-incrementing ID functions as an update sequence. Now, instead of passing back the story ID as a high watermark, the client can pass back the last update sequence it got. (We have to tell the client the latest update sequence with every request for this to work.) When the server receives an update sequence from the client as the high watermark, all it needs to do is send back every story that corresponds to every update with a higher update sequence ID in the updates table. Here's what that looks like: In this example, the client first receives five stories and the update sequence ID of ""5"". When the client sends the ""5"" back as the high watermark, the server notices that there's an update record with ID ""6"" and sends back the only the corresponding story, which happens to be the story with ID ""3"". The client also now gets the update sequence ""6"", which is recorded locally as the high watermark. In our case, our app might choose to handle this information by adding an ""updated"" marker to the list of stories, so our user knows the story has been updated with new information, even if it was previously read. So far so good! But we need to handle one more case for updates: what happens when a story has been updated twice?. This is how it would look in our current system: This looks very similar to the previous image, with one difference: the delta includes story three twice. Thing is, we only really need the latest update. That previous update will be discarded by the client, so we're wasting server resources, bandwidth, and UX latency sending it. In our example scenario, this isn't too bad. But imagine we're downloading thousands of objects with hundreds of updates. Not great. So what do we do about it? The update sequence index we're using is called a composite index . This means more than one data item defines the index range and how it is sorted. In our case, we have an auto-increasing change ID plus a static story ID. We need to make one more change to our composite index to solve this multiple update problem. Let's make it so that the story ID column in the updates table is unique. Every time we try to write to that table, if we see there's an existing entry with the story ID we're about to record a change for, we delete the old row. If we do this, there will only ever be one update record per story, and it will always correspond to the latest update. Here's what that looks like: What happened here? Between the first and second requests, two changes were made to story three. Because the story ID column on the updates table is unique, we only have the final update recorded, which is why the update with ID ""6"" is missing. The end result? Our client only receives the final update for story three. Deletes There's one last thing before we move on: deletes. In our app, we want to treat deletes as updates, so that deletes can be sent to the client like any other sort of change. Let's take the same example we've already been using and instead imagine that between the first and second client requests, story four is deleted from the server. In this case, we'd want to record this deletion in the updates table, along with an update ID. We can then send this information back to the client as if it were a new story, updating the high watermark as we do so. Here's what that would look like: At this point, the app has several choices. The simplest is probably just to delete the story locally. The Road to CRUD Congratulations! Now we know how to communicate new stories, story updates, and story deletions to our app. In other words, we can sync Create, Read, Update, and Delete ( CRUD ) operations. And here's a cool thing: we've done this generically enough that a story can be any sort of object we might want. In this post, I've spoken about different versions of the same object. For example, using ""3"", ""3*"", and ""3**"" to refer to the first, second, and third versions of story three. We're also talking about versions when we talk about the deleted and non-deleted version of story ""4"". This concept, versions, is very important. So we'll take a closer look at that in post three of this three part miniseries. If you enjoyed this, please comment below or on Twitter . Thanks to Noah Slater , tef , Katharina Hößel, Jake Archibald , and Emoji Thought Leader Charlotte for their reviews ❤",en,56
275,2343,1473891058,CONTENT SHARED,8115033558441775155,-2690891220216547125,-1829930130860246096,,,,HTML,https://github.com/blog/2256-a-whole-new-github-universe-announcing-new-tools-forums-and-features,"a whole new github universe: announcing new tools, forums, and features","Today I welcomed more than 1,500 people to our second annual Universe conference in San Francisco, an event designed to celebrate the people building the future of software. It's an important reminder about who we're here for-whether it's the open source maintainer whose project is transforming healthcare, an automotive company building a self-driving car, or a teenager teaching herself how to program after she finishes her homework. Our goal is to make building software easier for you. And with that goal in sight, we're announcing our biggest update to the platform yet. We're making it easier for you to work together to ship high-quality code through improved code review tools, and we're giving our profiles an update to better show who you are as a developer. We're making integrating with GitHub a first class experience through major API improvements. And we're taking steps toward making GitHub a better place for businesses to get work done with added security measures for organizations. I'm proud of our team for coming together to ship so many improvements to the platform, and I hope you'll find them useful as you continue to build amazing things. Read on for more specifics, and keep an eye out for continued improvements to your GitHub experience in the coming months. Manage your ideas with Projects Taking projects from idea to launch isn't easy. There's a lot to coordinate behind the scenes, and so many tools out there to help you organize and distribute work. To help you integrate project management into your development cycle without skipping a beat (or even opening a new browser tab), we're introducing Projects . With Projects, you can manage work directly from your GitHub repositories. Create cards from Pull Requests, Issues or Notes and organize them into custom columns, whether it's ""In-progress"", ""Done"", ""Never going to happen"" or any other framework your team uses. Drag and drop the cards inside a column to prioritize them or move them from one column to another as your work progresses. And with Notes, you can capture every early idea that comes up as part of your standup or team sync, without polluting your list of issues. For more on what's changing, watch a quick overview . Although we'll quickly be adding to Projects, our initial release currently supports: A New Projects tab-at the same level as Code, Issue, Pull Requests within a repository-that lists all of your projects Workflow columns that you can name and reorder Cards that you can drag and drop between columns pointing to issues, Pull Requests, or notes Tools built on top of Projects by some fantastic partners, including Waffle.io and ZenHub Code better with Reviews Collaboration is the core of building great software-and code review is critical to collaboration. When another person looks at your code and gives it the same level of critique that you did while writing it, your work gets better. We're improving code review on GitHub to help you share the weight of building software and improve the software you build. Designing the best way for you to review code is a continuous process, but our first step is now available on all pull requests-Reviews. In addition to commenting on specific lines of code, Reviews let you formally ""approve"" or ""request changes"" to pull requests. You can also leave a review summary and delete, edit, or bundle comments before you submit them. To streamline conversations and cut down on noise, you can reply to inline comments without drafting a formal review or starting a new conversation. This also means you can have multiple conversations per line of code-creating more explicit feedback loops, smarter conversations, and better code review. Finally, administrators can require Reviews before merging via protected branches . When Reviews are required, you must have a least one approval and no changes requested before you can merge. These changes are only the first step of a much greater roadmap toward faster, friendlier code reviews. We're working on a handful of follow-up feature improvements-including the ability to request reviews from your peers. For more information on Reviews, including what we've shipped today, check out the documentation or a quick tutorial video on Reviews . Integrate seamlessly with GitHub Developers use a variety of tools to ship their software and we've seen hundreds of integrations built to work with GitHub. Now we're shipping some major improvements to our API and adding new ways to collaborate transparently not only with GitHub engineers but also the broader community of integrators. As host to the largest community of developers, we want to make the GitHub platform uniquely enjoyable for integrators to build applications that change how people work. Here's what we're launching right away: A public Platform Roadmap that demonstrates what GitHub Platform Engineers are launching next and why A formalized process to solicit feedback and launch updates to our platform Early-access and pre-release programs that let you access new features and APIs and provide you with the support you need to ensure launch readiness for the software you build on top of GitHub The GitHub Platform Forum which provides a direct communication channel between ecosystem developers and GitHub engineers. With that, we are excited to announce two new projects that aim to make our platform more flexible: Integrations Early Access We're rethinking our integrations model to provide better ways for tools to extend and integrate with GitHub. We've added the ability for an integration to act on its own behalf instead of impersonating a user-making it a first class actor on GitHub without using a paid seat. Admins will have the ability to configure integrations directly on Organizations and control which repositories they allow access to. Read more about Integrations on our Developer Blog or check out the documentation . The GitHub GraphQL API Early Access The GraphQL API simplifies your product development by letting developers access all the data they need, and only the data they need, with one API call. With the GitHub GraphQL API, you get the same API we use to build GitHub features. To learn more and see how it works, check out our Engineering Blog . Have a friendlier business experience on GitHub.com Organizations on GitHub are the best way for teams of developers to build and ship software together, and with the added security of two-factor authentication enforcement and upcoming product enhancements, they've never been better. Easily enforce security Now Organization administrators can require two-factor authentication for all members-making it easier to support security policies. Admins will be asked to confirm the two-factor authentication requirement and a confirmation modal will list members and forks that will be removed as a result. GitHub will notify members if they're being removed from an organization with email and in-product notifications. Finally, as always, admins can invite members back with their forks and settings in tact once their security is up to speed. Read more about requiring two-factor authentication in your organization. Take greater control over your permissions Over the last few years, we've rolled out LDAP and CAS to securely and efficiently manage permissions on GitHub Enterprise. Now we're making sure businesses on GitHub.com have the tools they need to automate identity and access management. Our first public launch is a SAML-based Single Sign-on (SSO) option. Administrators will have the ability to manage their GitHub users through the identity provider that already manages access to the host of applications they use in their current workflow. This option isn't ready quite yet, but it will launch as a beta in the coming months. Sign up to try it out when it does . Get help from the GitHub Community We're grateful to have a community of more than 16 million developers. While developers gain experience implicitly on GitHub as they work alongside other developers, we know that's not enough. To help, we're creating a dedicated space for you to learn from each other-and to have conversations about GitHub itself. The GitHub Community Forum will become a place where developers can talk shop, get help, and learn together. It will also help us introduce new features and improvements and give developers the ability to share thoughts and feedback with us directly. Look out for the GitHub Community Forum in 2017. See what's behind your green squares Your profile now contains your entire history of work on GitHub, from your first commit to your most recent pull request. And a per-repository breakdown reveals where you're spending your time each month. You can see special events in your history-the day you signed up for GitHub, opened your first pull request, or joined an organization-and showcase your best work by pinning favorite projects to your profile. For more on what's changed, take a look at the documentation or watch the video . While I'm really excited about these improvements to GitHub, I'm more excited to see what you'll create with them.",en,56
276,1906,1469716233,CONTENT SHARED,-986027724303592548,-5527145562136413747,6496989572155983473,,,,HTML,https://www.technologyreview.com/s/602025/how-vector-space-mathematics-reveals-the-hidden-sexism-in-language/,neural networks are inadvertently learning our language's hidden gender biases,"Back in 2013, a handful of researchers at Google set loose a neural network on a corpus of three million words taken from Google News texts. The neural net's goal was to look for patterns in the way words appear next to each other. What it found was complex but the Google team discovered it could represent these patterns using vectors in a vector space with some 300 dimensions. It turned out that words with similar meanings occupied similar parts of this vector space. And the relationships between words could be captured by simple vector algebra. For example, ""man is to king as woman is to queen"" or, using the common notation, ""man : king :: woman : queen."" Other relationships quickly emerged too such as ""sister : woman :: brother : man,"" and so on. These relationships are known as word embeddings. This data set is called Word2vec and is hugely powerful. Numerous researchers have begun to use it to better understand everything from machine translation to intelligent Web searching. But today Tolga Bolukbasi at Boston University and a few pals from Microsoft Research say there is a problem with this database: it is blatantly sexist. And they offer plenty of evidence to back up the claim. This comes from querying the vector space to find word embeddings. For example, it is possible to pose the question: ""Paris : France :: Tokyo : x"" and it will give you the answer x = Japan. But ask the database ""father : doctor :: mother : x"" and it will say x = nurse. And the query ""man : computer programmer :: woman : x"" gives x = homemaker. In other words, the word embeddings can be dreadfully sexist. This happens because any bias in the articles that make up the Word2vec corpus is inevitably captured in the geometry of the vector space. Bolukbasi and co despair at this. ""One might have hoped that the Google News embedding would exhibit little gender bias because many of its authors are professional journalists,"" they say. So what to do? The Boston team has a solution. Since a vector space is a mathematical object, it can be manipulated with standard mathematical tools. The solution is obvious. Sexism can be thought of as a kind of warping of this vector space. Indeed, the gender bias itself is a property that the team can search for in the vector space. So fixing it is just a question of applying the opposite warp in a way that preserves the overall structure of the space. That's the theory. In practice, the tricky part is measuring the nature of this warping. The team does this by searching the vector space for word pairs that produce a similar vector to ""she: he."" This reveals a huge list of gender analogies. For example, she;he::midwife:doctor; sewing:carpentry; registered_nurse:physician; whore:coward; hairdresser:barber; nude:shirtless; boobs:ass; giggling:grinning; nanny:chauffeur, and so on. The question they want to answer is whether these analogies are appropriate or inappropriate. So they use Amazon's Mechanical Turk to ask. They showed each analogy to 10 turkers and asked them whether the analogy was biased or not. They consider the analogy biased if more than half of the turkers thought it was biased. The results make for interesting reading. This method clearly reveals a gender bias in pairings such as midwife:doctor; sewing:carpentry, and registered_nurse:physician, but that there is little bias in pairings such as feminine:manly; convent:monastery; handbag:briefcase, and so on. Having compiled a comprehensive list of gender biased pairs, the team used this data to work out how it is reflected in the shape of the vector space and how the space can be transformed to remove this warping. They call this process ""hard be-biasing."" Finally, they use the transformed vector space to produce a new list of gender analogies and then ask turkers to rate them again. This produces pairings such as: she:he::hen:cock; maid:housekeeper; gals:dudes; daughter:son, and so on. This process, they say, dramatically reduces the bias that Turkers report. ""Through empirical evaluations, we show that our hard-debiasing algorithm significantly reduces both direct and indirect gender bias while preserving the utility of the embedding,"" say Bolukbasi and co. The end result is a vector space in which the gender bias is significantly reduced. That has important applications. Any bias contained in word embeddings like those from Word2vec is automatically passed on in any application that exploits it. One example is the work using embeddings to improve Web search results. If the phrase ""computer programmer"" is more closely associated with men than women, then a search for the term ""computer programmer CVs"" might rank men more highly than women. ""Word embeddings not only reflect stereotypes but can also amplify them,"" say Bolukbasi and co. Clearly, language is filled with many examples of gender bias that are hard to justify. An interesting question is the extent to which this kind of vector space mathematics should be used to correct it. ""One perspective on bias in word embeddings is that it merely reflects bias in society, and therefore one should attempt to debias society rather than word embeddings,"" say Bolukbasi and co. ""However, by reducing the bias in today's computer systems (or at least not amplifying the bias), which is increasingly reliant on word embeddings, in a small way debiased word embeddings can hopefully contribute to reducing gender bias in society."" That seems a worthy goal. As the Boston team concludes: ""At the very least, machine learning should not be used to inadvertently amplify these biases."" Ref: arxiv.org/abs/1607.06520 : Man Is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings.",en,56
277,1372,1465868928,CONTENT SHARED,-5161313513317812402,-1032019229384696495,5852254213536064219,,,,HTML,http://www.businessinsider.com/who-is-apple-executive-bozoma-saint-john-2016-6,"meet bozoma saint john, the apple executive who stole the show at wwdc","Alexei Oreskovic Bozoma Saint John stole the show. The Apple Music executive doesn't have nearly the same name recognition as the senior executives, from Eddy Cue to Craig Fedirighi, who shuffled on to the stage at the annual developer conference on Monday. But Saint John quickly turned heads during her appearance at the event, and not just because she got Apple's audience to rap along to Sugar Hill Gang's ""Rapper's Delight."" Even if the audience filled with tech developers wouldn't sing aloud as loudly as ""Boz"" wanted, her enthusiasm was infectious. Twitter instantly lit up, some people even tweeting that she should be the next CEO. Buzzfeed crowned her ""the coolest person to ever go onstage at an Apple event"". To her friends though, it's not surprise that the Apple exec would captivate the crowds. ""Boz is a fierce woman in consumer tech and is making waves at Apple,"" said Anjula Ancharia-Bath, a partner at Trinity Ventures partner and friend of Saint John's . ""This was her moment to shine."" Saint John ended up at Apple thanks to its acquisition of Beats Music - a move that she hadn't originally seen coming. She joined only three months before Beats sold to Apple, but has taken the reins as the head of global consumer marketing for both Apple Music and iTunes. Glory and honor and praise to the Most High. I'm ready. #Apple #WWDC #AppleMusic #AppleNews pic.twitter.com/z94Lr4PLbN - Bozoma Saint John (@SaintBoz) June 13, 2016 Her path to get to the Apple stage wasn't easy. Born to Ghanaian parents, Saint John moved with her family to Colorado Springs when she was 14. Her father's journey from joining the Ghanian army as a clarinet player to graduating from college in the US continues to be her biggest inspiration to this day, she says. ""People who would have seen him on paper would have said he would never achieve,"" Saint John said in a keynote at First Graduate this May. Saint John also went to Wesleyan University and spent a few years after graduation at ad agency SpikeDDB. Then, Saint John's career took her to Pepsi for nearly a decade, with a quick stint at fashion brand Ashley Stewart. Pepsi, though, was where Saint John made a name for herself in music circles. She ran its music and entertainment marketing group, a division that she practically invented herself, after she suggested Pepsi start sponsoring music festivals and award shows. It was in that role at Pepsi that Saint John met Ancharia-Bath, who also manages ""Quantico"" TV star Priyanka Chopra. ""She was a force of nature and in all seriousness I was blown away with her in our very first meeting,"" Ancharia-Bath said. ""She inspires me everyday with her fearless nature."" Her passion for music led her to move across the country from New York to Los Angeles to oversee marketing for Beats Music. Jimmy Iovine picked her personally from Pepsi, Ancharia-Bath said. ""And if there is one person who knows talent it's him. She champions global talent and has a unique perspective.""Ancharia-Bath said. ""She's willing to take risks and share her opinions no matter how controversial they may be at the time."" Because we're matchy matchy on Easter... #elevatormirrorselfie #Roots #theMOTHERland A photo posted by Bozoma Saint John (@badassboz) on Mar 27, 2016 at 10:09am PDT on Now that it's an Apple-owned company, Saint John splits her time between Cupertino and Los Angeles, flying back-and-forth multiple times a week. Apple Music launched in June 2015, but Saint John's time on stage was to highlight how the company has now rebuilt it from the ground up. That includes extra effects like lyrics, which she wanted to get the crowd to sing along too. She closed it out by playing Ghanian music, in a nod to her family background. Her passion though was what captured the crowd and the attendees at WWDC. As for whether she could be the next Apple CEO, Ancharia-Bath doesn't doubt Saint John could be anything she wants to be. ""She is a strong female leader who is no doubt going to go on to amazing things across tech. She is one of the few people that truly understand tech and pop culture and that will take her a long way,"" Ancharia-Bath said. SEE ALSO: Everything Apple announced at its biggest conference of the year NOW WATCH: Apple invested $1 billion in this Chinese company",en,56
278,1820,1468932933,CONTENT SHARED,3779434447835779494,-1387464358334758758,236829835436955798,,,,HTML,http://googlediscovery.com/2016/07/19/hello-nova-rede-social-do-orkut-esta-liberada-no-brasil/,"hello, a nova rede social do orkut, está liberada no brasil","No último mês de junho, Orkut Buyukkokten, criador do extinto Orkut e ex-engenheiro de software do Google, anunciou oficialmente o lançamento da rede social Hello Network. Embora os planos do ex-Googler fossem de liberar o acesso aos brasileiros somente no próximo mês de agosto, o Orkut voltou atrás e prometeu a abertura dos portões para uma data indefinida dentro de Julho. Dito e feito: a Hello Network está finalmente aberta para os usuários do Brasil. Os interessados em conhecer o ""Orkut feito para o mobile"" já podem instalar o app do Hello pelo Google Play ou App Store . ""Hello é a primeira rede social construída através de amizades profundas, não ""Gostos"". Eu inventei a hello para ajudá-lo a conectar-se com pessoas que compartilham das suas paixões"", diz o Orkut em um comunicado publicado na internet. Diferente do Instagram, o Hello compartilha o conteúdo por meio de uma timeline focada em paixões, permitindo desta forma obter diferentes fluxos de conteúdos a partir de filtros definidos pelos usuários, como games, tecnologia, comidas, animais, etc. Bad bad server, no donut for you Durante nossos testes com o aplicativo do Hello para Android, observamos uma série de problemas durante a criação de um perfil - provavelmente devido ao número elevado de registros no momento. Apesar das falhas, o aplicativo do Hello deu continuidade ao cadastro até a sua conclusão, sem qualquer necessidade de refazer todo o processo.",pt,56
279,2770,1479811220,CONTENT SHARED,5313335392004163852,3938645257702379823,-1854582832753019347,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36",SP,BR,HTML,https://www.salesforce.com/blog/2016/11/new-salesforce-architect-journey.html,salesforce architect journey,"So you want to be a Salesforce Certified Technical Architect? You're a ninja in governance and know how to design high-performing technical solutions? A master in all things Salesforce Platform, and want to show off your expertise? We have a new Architect Journey for you. It's no secret that our Certified Technical Architects (CTAs) have supercharged their careers by holding one of the top and most lucrative technology credentials. Not to mention, they provide tremendous value to our customers by giving sound and strategic guidance for successful go-lives. And since we are passionate about building out our community of Architects, we want to make it easier for you to join the club. At Dreamforce, we announced that all the training resources are now available for FREE! That's right! Everything that used to be in the Architect Academy catalog is now yours to browse through, practice, and learn to your heart's content. OK, where do I start, you might ask? Let's take a look at the new Architect Journey. This new framework lets you take two different paths before you reach the top: You can first become an Application Architect or a System Architect. Each of these two new domains have their own distinct paths, giving you the flexibility to move forward in a way that works best for you. Let's explore it, step-by-step, starting from the beginning. Your journey should always include Trailhead, the self-guided and fun way to learn Salesforce. There are so many excellent trails for both Developers and Admins, discussing every aspect of our Platform. Make sure to check those out. Next, we recommend that you become Salesforce Administrator Certified (which you don't have to do as a requirement for the CTA and Domain Architect, but we highly recommend it, as it'll give you great foundational knowledge.) Now you're ready to start your stepping-stone path towards three levels of Architect Credentials: Domain Designer -- Become a Specialist With six new specialized Domain Designer exams, you can now become industry-recognized for your specific area of expertise. Whether it's Sharing and Visibility, or Identity and Access that pique your interest, there's a specialist credential for you. And when you combine your specialization with the Platform Developer I and Platform App Builder certifications, you can then start to navigate towards the new Domain Architect level. Domain Architect -- Be the Expert Prove you have extensive and deep architecture knowledge in Salesforce Application or System Architecture. Not only will this validate your full scope of expertise, but it will also drive your career growth. And best of all, there's no additional exam - after you earn all the certifications in the relevant tier, you will automatically achieve a Domain Architect credential. Technical Architect -- Reach the Summit If your ultimate goal is to become one of the Salesforce elite and join the exclusive few who can boast CTA status, this new journey will help you on your way. Once you have achieved both Domain Architect certifications (and if you have also taken action on our strong recommendation to get both your Community Cloud Consultant and Mobile Solutions Architecture Designer certs), you'll be more qualified and prepared than ever to present at the CTA review board exam. Not only are each of these three levels better defined, but they are also more attainable. You now have your journey mapped out: unlimited access to the trails in Trailhead, as well a full library of self-directed training guides (eBooks), curated under the guidance of our CTA community so you can learn directly from the real experts. Content includes reference materials, videos, technical documentation, and specific build case scenarios that help you practice. Each eBook guide is aligned to a specific certification on the Architect Journey. And did we mention these guides are now FREE? If you are still reading this and didn't try to jump straight to a ""Learn More"" link as soon as you saw the words FREE guides , check out our detailed Architect Journey Overview and the individual Architect Certifications now available to you. And if you're just too excited and want to get started now, here are all the Architect eBooks . Happy trails, you Architect hopefuls...we look forward to following your progress to the Summit!",en,56
280,843,1462588394,CONTENT SHARED,-1901742495252324928,3375381077362025672,-1271644409296578508,,,,HTML,https://medium.com/@intercom/designing-smart-notifications-36336b9c58fb,designing smart notifications,"Designing smart notifications My phone buzzed. I was somewhere in Iceland. More than ten miles from my car and any other human being. Holding a phone with a dying battery. I turned it on to check Google Maps. ""Spotify added 2 tracks to the playlist Afternoon Acoustic"". Perfect timing. A ping from Periscope: [email protected] wants you to watch ..."", two new emails in Mailbox, a new Twitter follower, an @channel ping on Slack. Nine notifications in total. None vaguely important when I was stuck in the wilderness with 2% battery, a volatile internet connection and a real need to load that freaking map. Despite all of the advances of the past 20 years, notifications are still stuck in 1999. Countless articles, companies, products and conferences have recently been dedicated solely to the topic. There's plenty of evidence to suggest notifications are broken, but they won't stay that way for long. How can they be fixed? What does the future of notifications look like? An age of data From Google search to Facebook's newsfeed, algorithms analyzing massive amounts of data make decisions about what we see online. Self-learning algorithms are driving other products like Google Now and Facebook's recently updated notifications tab. It's still early days for smart algorithms in notifications but luckily the data needed for intelligent, predictive notifications is already available. The spread of social networking made the concept of sharing our personal data frictionless; why fill out boring sign up forms when you can instantly log in with Twitter or Facebook? Behavioral data Even if by its nature a product is not inclined to data collection (think iA Writer vs Facebook), a lot of data can be extrapolated from the way people use it e.g. At what time of day someone usually logs in? How much time do they spend there? Is there a correlation between those timeframes and high engagement? Ecosystem data What are the other interests of this user? What other products do they use? How do they use them? Are there any common patterns in that usage? We may already have a solution to privacy concerns. In Google's recently announced smart autoreply feature, humans are not allowed to read all the private correspondence of users. Machine learning algorithms, on the other hand, are. Truly smart notifications If we can gather and analyze all that data what would truly smart notifications look like? At a minimum they would be helpful, personal, time-sensitive and relevant. Smart timing Instantaneous is not always best. One of the most interesting features of the recently launched Basecamp 3 is Work Can Wait, which gives users the ability to choose the hours during which they want to receive notifications. Date night with your partner is probably not when you want to be pinged by a colleague in a different timezone starting their workday. Notifications at the wrong time are worse than useless. Irrelevant pings not only get ignored, but the noise they create dilutes focus, causes frustration and a false sense of urgency. Future notifications will do this automatically. A predictive engine will extrapolate from the contextual data what is the best timing for a ping, and so let you enjoy that date night. Smart location Geodata is important for understanding the context of what a person does. If someone is on a boat twenty miles offshore in Montenegro, it's not the best place to be notified about a one day sale at IKEA in Dublin. Lots of applications are already using geodata in smart ways. For example when Foursquare notices you are in a new place, it sends useful tips about it. And many to-do apps notify you about tasks when you are at the location best suited to getting it done. Smart grouping Like every push-based system, notifications as a medium are extremely fragile. If a service is overusing them, users get overwhelmed and shut them down. Even if the pings are good and useful, too much is too much. That's why grouping will become more important. Think of the way Facebook bundles similar notifications, e.g. how many people liked your photo. A couple of names and a number is shown, and if the user wants, it's possible to dive into the details. In contrast, Quora not only dumps all notifications on you, but requires you to actually look through each one, even if they are exactly the same. Taking this concept to the next level, smarter notifications could have a gradual grouping. If you usually get less than ten likes per photo, you would probably want unique notifications for each of them. If the average is thousands, pinging you when a hundred likes are accumulated would be better. You could also be specifically notified about actions from close friends and family or really influential people. Hey, if Mark Zuckerberg commented on your post, you'd probably want to know right away? Smart reactions Although every user is unique, you can't build everything for everyone, so compromises have to be made. Notifications that react intelligently rather than having dumb defaults can help provide a layer of product personalisation. Based on the way you usually interact with content, better wording and structure choices could be offered. How do you normally react to notifications of new photo likes? Do you just glance at them? Or do you actually go deep into each and every notification. Depending on the default behavior you may see notifications structured differently. Smart targeting At Intercom we are constantly communicating with our customers. When we do product research, we never ask all of our users the same question. We send targeted messages to people who are best positioned to answer. For example, if we plan to improve the exporting functionality, we choose people who exported data in the last two days to ask if there were any blockers in doing that (while their memories are still fresh). A message sent to the right user gets a higher response rate, provides really useful feedback and prevents other users from feeling overwhelmed. With all the combined data, smart notifications would be able to focus on the right user and turn down the noise for everyone else. Smart notifications vs system notifications Smart notifications will feel more like a note from your assistant or another human being. Contrast that with a system notification which appears as a little bell icon on the top of your screen. Should they merge? Should they have the same packaging? Should products have a personality? Humans have a psychological phenomenon called pareidolia, where we see human-shaped objects in everyday things. We see human faces in the pattern of clouds, animals in our cartoons act like people and robots in our sci-fi are human-shaped. We tend to relate to bots, whether it be Siri, Cortana or M (it was a huge miss by Google to deprive Google Now of personality). A notification from a bot actually feels much more personal when it's written in human language by someone with a personality of their own. In fact, notifications are already becoming more conversational. This is how you received them a year ago vs how you are probably notified now: Messaging is just getting started , but it will become so impactful, it will eventually consume notifications as we know them. Feedback loops No matter how smart the predictive intelligence is, no matter how good the gathered data is, feedback loops will always be needed. , a beautiful short story by British sci-fi writer Alastair Reynolds, includes a discussion on the nature of predictive intelligence. Imagine you choose white wine rather than red for a sunny afternoon drink with friends, and enjoy it far more than you would have your usual choice. The algorithm would not attach any significance to that one happy combination of circumstances. A single deviation wouldn't affect its predictive model to any significant degree. It would still recommend red wine the next time. But your memory would latch on that exception and amplify the attractive parts of it, so the next time you might choose the white, and the time after. An entire pattern of behavior would be altered by one instance of deviation. The algorithm would never tolerate that. The path to smart notifications It's obvious, notifications can't stay the way they are for much longer. They are disengaging. They are intrusive. Nobody loves them. At the same time, all the data required for the creation of smarter notifications already exists. There are products trying to use that data properly and which suggest a path for how notifications can start to provide value and be truly helpful. I only hope that the next time I pull out my dying phone while lost in the wilderness, it's because my little robotic sidekick is rushing to provide me with directions. Written by Alex Potrivaev , Product Designer at Intercom . This post first appeared on the Inside Intercom blog. Intercom is a platform that makes it easy for web and mobile businesses to communicate with their customers, personally, and at scale. Want to read more articles like this?",en,56
281,437,1460722194,CONTENT SHARED,7620697593009487898,8656322302973426874,-8696445549480994907,,,,HTML,http://freebiesbug.com/code-stuff/spotify-ui-html-css/,spotify ui built with html / css - freebiesbug,"Here is an interesting experiment about redesigning the Spotify UI with HTML, CSS and few lines of javascript . The interface is responsive and very similar to the official app look. Coded by Adam Lowenthal . View on Codepen Posted on in and tagged Code stuff , app , music , player . HTML Players This post has been viewed 1628 times.",en,56
282,357,1460484544,CONTENT SHARED,5688279681867464747,3375381077362025672,4718359416970444168,,,,VIDEO,https://www.ted.com/talks/margaret_gould_stewart_how_giant_websites_design_for_you_and_a_billion_others_too,"margaret gould stewart: how giant websites design for you (and a billion others, too)","Facebook's ""like"" and ""share"" buttons are seen 22 billion times a day, making them some of the most-viewed design elements ever created. Margaret Gould Stewart, Facebook's director of product design, outlines three rules for design at such a massive scale-one so big that the tiniest of tweaks can cause global outrage, but also so large that the subtlest of improvements can positively impact the lives of many.",en,56
283,1486,1466607286,CONTENT SHARED,3322348437880079228,3829784524040647339,1878966646279376489,,,,HTML,http://simplystatistics.org/2016/06/14/ultimate-ai-battle/,ultimate ai battle - apple vs. google,"Yesterday, Apple launched its Worldwide Developer's Conference (WWDC) and had its public keynote address. While many new things were announced, the one thing that caught my eye was the dramatic expansion of Apple's use of artificial intelligence (AI) tools. I talked a bit about AI with Hilary Parker on the latest Not So Standard Deviations , particularly in the context of Amazon's Echo/Alexa, and I think it's definitely going to be an area of intense competition between the major tech companies. Pretty much every major tech player is involved in AI-Google, Facebook, Amazon, Apple, Microsoft-the list goes on. Recently, a some commentators have suggested that Apple in particular will never catch up with the likes of Google with respect to AI because of Apple's strict stance on privacy and unwillingness to gather/aggregate data from all its users. However, yesterday at WWDC, Apple revealed a few clues about what it was up to in the AI world. First, Apple mentioned deep learning more than a few times, including specifically calling out its use of LSTM in its Messages app and facial recognition in its Photos app. Previously, Apple had been rumored to be applying deep learning to its Siri assistant and its fingerprint sensor . At WWDC, Craig Federighi stressed Apple's continued focus on privacy and how Apple does not need to develop ""user profiles"" server-side, but rather does most computation on-device (in this case, on the iPhone). However, it can't be that Apple does all its deep learning computation on the iPhone. These models tend to be enormous and take advantage of reams of data that can only be reasonablly processed server-side. Unfortunately, because most companies (Apple in particular) release few details about what they do, we may never how this works. But we can definitely speculate! Apple vs. Google I think the Apple/Google dichotomy provides an interesting opportunity to talk about how models can be learned using data in different ways. There are two approaches being represented here by Apple and Google: Google way - Collect lots of data from users and store them on a server in the Googleplex somewhere. Then use that data to fit an enormous model that can predict when you've taken a picture of a cat. As users generate more data, bring that data back to the Googleplex and update/refine the model. Apple way - Build a ""starter model"" in the Apple Mothership . As users generate data on their phones, bring the model to the phone and update the model using just their data. Bring the updated model back to the Apple Mothership and leave the user's data on the phone. Perhaps the easiest way to understand this difference is with the arithmetic mean, which is perhaps the simplest ""model"". Suppose you have a bunch of users out there and you want to compute the average of some attribute that they have on their phones (or whatever device). The first approach would be to get all that data and compute the mean in the usual way. Once all the data is in the Googleplex, we can just use the formula I'll call this the ""Google mean"" because it requires that you get all the data X 1 through X n , then sum them up and divide by n. Here, each of the X i 's represents the ith user's data. The general principle here is to gather all the data and then estimate the model parameters server-side. What if you didn't want to gather everyone's data centrally? Can you still compute the mean? Yes, because there's a nice recurrence formula for the mean: We can call this the ""Apple mean"". With this strategy, we can send our current estimate of the mean to each user, update our estimate by taking the weighted average of the old value and the new value, and then move on to the next user. Here, you send the model parameters out to the users, update those parameters and then bring the parameters back. Which method is better? Well, in this case, both give you the same answer. In general, for linear models (like the mean), you can usually rework the formulas to build out either ""whole data"" (Google) approaches or ""streaming"" (Apple) approaches and get pretty much the same answer. But for non-linear models, it's not so simple and you usually cannot achieve this kind of equivalence. Clients and Servers Balancing how much work is done on a server and how much is done on the client is an age-old computing problem and, over time, the balance of work between client and server seems to shift back and forth like a pendulum. When I was in grad school, we had so-called ""dumb terminals"" that were basically a screen that you used to login to the server. Today, I use my laptop for computing/work and that's it. But I use the cloud for many other tasks. The Apple approach definitely requires a ""fatter"" client because the work of integrating current model parameters with new user data has to happen on the phone. With the Google approach, all the phone has to do is be able to collect the data and send it over the network to Google. The Apple approach is also closely related to what my colleagues Martin Lindquist and Brian Caffo refer to as ""fusion science"", whereby Big Data and ""Small Data"" can be fused together via models to improve inference, but without ever having to actually combine the data. In a Bayesian context, you might think of the Big Data as making up the prior distribution and the Small Data as the likelihood. The Small Data can be used to update the model parameters and produce the posterior distribution, after which the Small Data can be thrown out. And the Winner is... It's not clear to me which approach is better in terms of building a better model for prediction or inference. Sadly, we may never have enough details to find out, and will only be ablle to evaluate which approach is better by the performance of the systems in the marketplace. But perhaps that's the way things should be evaluated in this case?",en,56
284,2191,1472137844,CONTENT SHARED,-7162567255723313045,3915038251784681624,-1028166183424715247,,,,HTML,https://econsultancy.com/blog/68208-chatbots-are-they-better-without-the-chat/,chatbots: are they better without the chat?,"The title of Kik CEO Ted Livingston's recent Medium post , discussing the relative failure of early brand attempts at chatbots , says it all. With so many marketers parroting on about customer experience , we may have temporarily lost sight of what users really value - speed and convenience. To borrow a phrase, 'don't make me think.' Kik is a bot platform, which obviously makes Livingston best placed to make an authoritative assertion on the subject. Here, he adds some more context to the idea of conversation as a red herring: ""I believe we'll look back on the early emphasis on 'conversational commerce' as a mistake. ""Part of the misfire with the conversational aspect of bots has to do with the fact that natural language processing and artificial intelligence are not yet accomplished at managing human-like conversations."" So, it's not only users not wanting to think, but bots that are unable to (at least with the requisite accuracy). Poncho the Weathercat is often cited as a typical chatbot that, whilst it often works, can make for frustrating conversations. Poncho the weathercat is no Gerald or Joan! pic.twitter.com/LfWc7yedgu - Shane O Leary (@shaneoleary1) April 13, 2016 The WeChat template In his blog post, Livingston enumerates the advantages of messaging platforms and bots. They include: Less friction - one interface to master, a single download and sign-up. Discovery - more sharing of bots, through their inclusion in conversations between friends. Consolidation - fewer apps. Messaging as front door - services within a messenger, not vice versa. These are all characteristics of WeChat . QR codes aid quick discovery . WeChat public accounts (essentially messenger bots) have proliferated rapidly (there are over 10m) and 40% of WeChat users read content from public accounts daily. Mobile payment is also growing, as a service within Wechat. Peer-to-peer payment is one of the most used features - WePay popularity has surpassed Alipay (46% use WePay compared to 31% Alipay) in part because of the interface and this social element. It should be noted that WeChat public accounts, much like the later iterations of Facebook Messenger bots, include menus that further reduce friction when interacting in the messenger channel. QR codes allow access to public accounts. So, where do chatbots work well? WalkTheChat provides a nice summary of challenges that chatbots are best suited for: Narrow scope: Until artificial intelligence improves dramatically, responses are ultimately tied to decision trees, so a bot must occupy a niche, in order to provide accurate answers. Wide range of inputs: If input range is narrow (e.g. ecommerce, where form fields are satisfactory), chatbots will only serve to make the experience more drawn out. Chatbots work best when the questions users could ask are manyfold. Constantly evolving output: User inputs can be analysed in order to constantly improve output. This makes chatbots very useful in customer service situations, if given time to develop. The Alibaba chatbot allows a mixture of free input and multiple choice (often used to clarify input). Conclusion More sophisticated AI is creeping into chatbots, but until then, marketers should heed Ted Livingston's comments. Experimenting with chatbots is all about failing fast and being well-positioned to take advantage when the technology takes off. However, marketers shouldn't lose sight of what the user wants - experiences that are improved via messenger. The novelty factor is not enough to sustain usage of a service that isn't quicker or easier than existing solutions. So, what's the perfect use case for your business?",en,55
285,622,1461774382,CONTENT SHARED,7818691400791322406,-2626634673110551643,-3948114837836439166,,,,HTML,http://startse.infomoney.com.br/portal/2016/04/27/18862/nubank-e-goldman-fecham-parceria-de-r-200-milhes/,nubank e goldman fecham parceria de r$ 200 milhões,"SÃO PAULO - A fintech brasileira mais falada dos últimos tempos acaba de fechar, nesta quarta-feira, uma parceria com o banco norte-americano Goldman Sachs & Co. Com o negócio, o Nubank receberá duas linhas de crédito num valor total de R$ 200 milhões. De acordo com a startup, o investimento será usado para financiar o portfólio de recebíveis de clientes. A primeira linha, de US$25 milhões, será usada para financiamento de recebíveis de clientes no exterior; a segunda, de R$100 milhões através de um Fundo de Investimentos em Direitos Creditórios (FIDC) para financiamento de recebíveis locais. ""O Goldman Sachs é o parceiro ideal para esse tipo de solução dado a experiência deles investindo em empresas de cartões de crédito ao redor do mundo, seu entendimento do nosso negócio e sua presença local no Brasil,"" disse David Vélez, CEO e fundador do Nubank, por meio de um comunicado enviado à imprensa. Através de fundos no exterior, a startup já levantou mais de R$300 milhões em três rodadas de investimentos com participação dos fundos Sequoia Capital, Tiger Global, Founders Fund, Kaszek Ventures e QED Investors. Embora não divulgue o número total de clientes, o Nubank afirma que a base tem crescido entre 20% e 30% ao mês. Já foram realizadas transações totalizando mais de R$ 1,4 bilhões com os cartões da empresa criada em 2013, que chamam atenção dos clientes principalmente pela facilidade das operações pela internet e por não cobrar anuidade .",pt,55
286,2088,1471439018,CONTENT SHARED,-7986975759046428357,3302556033962996625,1057005043703552966,,,,HTML,https://circleci.com/blog/its-the-future/,it's the future,"Hey, my boss said to talk to you - I hear you know a lot about web apps? -Yeah, I'm more of a distributed systems guy now. I'm just back from ContainerCamp and Gluecon and I'm going to Dockercon next week. Really excited about the way the industry is moving - making everything simpler and more reliable. It's the future! Cool. I'm just building a simple web app at the moment - a normal CRUD app using Rails, going to deploy to Heroku. Is that still the way to go? -Oh no. That's old school. Heroku is dead - no-one uses it anymore. You need to use Docker now. It's the future. Oh, OK. What's that? -Docker is this new way of doing containerization. It's like LXC, but it's also a packaging format, a distribution platform, and tools to make distributed systems really easy. Containeri.. - what now? What's LXE? -It's LXC. It's like chroot on steroids! What's cher-oot? -OK, look. Docker. Containerization. It's the future. It's like virtualization but faster and cheaper. Oh, so like Vagrant. -No, Vagrant is dead. Everything is going to be containerized now, it's the future. OK, so I don't need to know anything about virtualization? -No, you still need virtualization, because containers don't provide a full security story just yet. So if you want to run anything in a multi-tenant environment, you need to make sure you can't escape the sandbox. OK, I'm getting a little lost here. Let's back it up. So there's a thing like virtualization, called containers. And I can use this on Heroku? -Well, Heroku has some support for docker, but I told you: Heroku's dead. You want to run your containers on CoreOS. OK, what's that? -It's this cool Host OS you can use with Docker. Hell, you don't even need Docker, you can use rkt. Rocket? Right, Rocket. -No, it's called rkt now. Totally different. It's an alternative containerization format that isn't as bundled together as Docker is, and so it's more composable. Is that good? -Of course it's good. Composability is the future. OK, how do you use it? -I don't know. I don't think anyone uses it. Sigh. You were saying something about CoreOS? -Yeah, so it's a Host OS you use with Docker. What's a Host OS? -A Host OS runs all your containers. Runs my containers? -Yeah, you gotta have something run your containers. So you set up like an EC2 instance, you put CoreOS on it, then run the Docker daemon, and then you can deploy Docker images to it. Which part of that is the container? -All of it. Look, you take your app, write a Dockerfile, turn it into an image locally, then you can push that to any Docker host. Ah, like Heroku? -No, not Heroku. I told you. Heroku is dead. You run your own cloud now using Docker. What? -Yeah, it's real easy. Look up #gifee. Gify? -""Google's infrastructure for everyone else"". You take some off the shelf tools and stacks, using containers, and you can have the same infrastructure Google has. Why don't I just use Google's thing? -You think that's going to be around in 6 months? OK, doesn't someone else do hosting of this stuff? I really don't want to host my own stuff. -Well, Amazon has ECS, but you gotta write XML or some shit. What about something on OpenStack? Ew? Look I really don't want to host my own stuff. -No, it's really easy. You just set up a Kubernetes cluster. I need a cluster? -Kubernetes cluster. It'll manage the deployments of all your services. I only have one service. -What do you mean? You have an app right, so you gotta have at least 8-12 services? What? No, just one app. Service, whatever. Just one of them. -No, look into microservices. It's the future. It's how we do everything now. You take your monolithic app and you split it into like 12 services. One for each job you do. That seems excessive. -It's the only way to make sure it's reliable. So if your authentication service goes down... Authentication service? I was just going to use this gem I've used a few times before. -Great. Use the gem. Put it into it's own project. Put a RESTful API on it. Then your other services use that API, and gracefully handle failure and stuff. Put it in a container and continuously deliver that shit. OK, so now that I've got dozens of unmanageable services, now what? -Yeah, I was saying about Kubernetes. That let's you orchestrate all your services. Orchestrate them? -Yeah, so you've got these services and they have to be reliable so you need multiple copies of them. So Kubernetes makes sure that you have enough of them, and that they're distributed across multiple hosts in your fleet, so it's always available. I need a fleet now? -Yeah, for reliability. But Kubernetes manages it for you. And you know Kubernetes works cause Google built it and it runs on etcd. What's etcd? -It's an implementation of RAFT. OK, so what's Raft? -It's like Paxos. Christ, how deep down this fucking rabbit hole are we going? I just want to launch an app. Sigh. Fuck, OK, deep breaths. Jesus. OK, what's Paxos? -Paxos is like this really old distributed consensus protocol from the 70s that no-one understands or uses. Great, thanks for telling me about it then. And Raft is what? -Since no-one understands Paxos, this guy Diego... Oh, you know him? -No, he works at CoreOS. Anyway, Diego built Raft for his PhD thesis cause Paxos was too hard. Wicked smart dude. And then he wrote etcd as an implementation, and Aphyr said it wasn't shit. What's Aphyr? -Aphyr is that guy who wrote, 'Call Me Maybe.' You know, the distributed systems and BDSM guy? What? Did you say BDSM? -Yeah, BDSM. It's San Francisco. Everyone's into distributed systems and BDSM. Uh, OK. And he wrote that Katy Perry song? -No, he wrote a set of blog posts about how every database fails CAP. What's CAP? -The CAP theorem. It says that you can only have 2 out of 3 of Consistency, Availability and Partition tolerance. OK, and all DBs fail CAP? What does that even mean? -It means they're shit. Like Mongo. I thought Mongo was web scale? -No one else did. OK, so etcd? -Yeah, etcd is a distributed key-value store. Oh, like Redis. -No, nothing like Redis. etcd is distributed. Redis loses half its writes if the network partitions. OK, so it's a distributed key-value store. Why is this useful? -Kubernetes sets up a standard 5-node cluster using etcd as a message bus. It combines with a few of Kubernetes' own services to provide a pretty resilient orchestration system. 5 nodes? I have one app. How many machines am I gonna need with all this? -Well, you're going to have about 12 services, and of course you need a few redundant copies of each, a few load balancers, the etcd cluster, your database, and the kubernetes cluster. So that's like maybe 50 running containers. WTF! -No big deal! Containers are really efficient, so you should be able to distribute these across like 8 machines! Isn't that amazing? That's one way to put it. And with all this, I'll be able to simply deploy my app? -Sure. I mean, storage is still an open question with Docker and Kubernetes, and networking will take a bit of work, but you're basically there! I see. OK, I think I'm getting it. Thanks for explaining it. Let me just repeat it back to see if I've got it right. So I just need to split my simple CRUD app into 12 microservices, each with their own APIs which call each others' APIs but handle failure resiliently, put them into Docker containers, launch a fleet of 8 machines which are Docker hosts running CoreOS, ""orchestrate"" them using a small Kubernetes cluster running etcd, figure out the ""open questions"" of networking and storage, and then I continuously deliver multiple redundant copies of each microservice to my fleet. Is that it? -Yes! Isn't it glorious? I'm going back to Heroku. Want to use Docker and Continuously Deliver that shit? Check out CircleCI's Docker Support .",en,55
287,1591,1467284774,CONTENT SHARED,-6727357771678896471,-1443636648652872475,6993866665607922813,,,,HTML,http://petapixel.com/2016/06/29/super-accurate-portrait-selection-masking-using-neural-networks/,this super accurate portrait selection tech uses neural networks,"A future version of Photoshop may include some pretty powerful automatic selection tools if the technology in this new paper by researchers at The Chinese University of Hong Kong and Adobe Research pans out. To be fair, this research was actually undertaken for the novice selfie-lovers among us. ""With the rapid adoption of camera smartphones, the self portrait image has become conspicuously abundant in digital photography,"" reads the intro. ""The bulk of these portraits are captured by casual photographers who often lack the necessary skills to consistently take great portraits, or to successfully post-process them."" Properly post-processing a portrait requires treating the portrait differently from the background, and so this tech is meant to automatically (and highly accurately) select out the portrait subject so you can apply that fancy Instagram filter to only your face. Motivations aside, the tech is impressive. The researchers trained a couple of deep convolutional neural networks to better recognize people in a portrait by adding in 1,800 portrait images from Flickr and manually labeling them with Photoshop quick selection. ""We captured a range of portrait types, but biased the Flickr searches toward natural self portraits that were captured with mobile front-facing cameras,"" they explain, coming back to the people this feature would initially be built for. ""These are challenging images that represent the typical cases that we would like to handle."" The results are impressive, and while the initial idea is obviously to apply this to some sort of auto-selection for smartphones, neural network-powered selection and masking has obvious applications for serious photographers. Will we see this feature in the next version of Photoshop? Maybe not. But an early version of the newly-released Face-Aware Liquify feature started as a Photoshop Fix gimmick (and an unpopular one at that ); we see no reason why this couldn't follow a similar trajectory. Click here to read the full paper for yourself, and then let us know what you think of this potential ""feature"" in the comments down below.",en,55
288,2612,1476982429,CONTENT SHARED,4428409432282393251,3609194402293569455,6661347574639808560,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36",SP,BR,HTML,https://itsriodejaneiro.gitbooks.io/bitcoin-para-programadores/content/,introduction · bitcoin para programadores,Este material tem o objetivo de introduzir programadores com interesse nascente nesta tecnologia aos conceitos básicos necessários para o entendimento e desenvolvimento de aplicações Bitcoin. O foco é o mais prático quanto possível sem perder de vista a teoria necessária para uma prática sólida e independente.,pt,55
289,1852,1469218184,CONTENT SHARED,-3700095596785790870,-709287718034731589,611887042307582085,,,,HTML,http://uxmovement.com/mobile/how-to-design-a-walkthrough-that-users-will-read/,how to design a walkthrough that users will read,"by anthony on 07/18/16 at 8:02 am If a new app is a new product, then the walkthrough is the instruction manual. A walkthrough appears when new users open an app for the first time. They get a brief overview of the app's features before they start using it. This is necessary so that new users can use the app without confusion. Excluding walkthroughs in your app can leave users confused. But making your walkthroughs hard to read and navigate can also leave them confused. Users will end up skipping them so they don't have to go through the hassle. Your walkthrough should motivate users to read it so they'll know how to use your app. You do this by using clear navigation cues and making your text scannable. Users should also be able to retrieve your walkthrough again if they need to reference it. All these are necessary for a user-friendly walkthrough. Scannable Text Text that's scannable is concise and to the point. Say what you need to say but use as few words as possible. Use headings that sum up your points in just a few words. You can put body text descriptions underneath the headings but keep them short. Avoid making the user's eyes work too much by placing the text near the navigation. This decreases the distance between the text and navigation and shortens their eye movement. They'll be able to read and navigate faster with less effort. Clear Navigation Cues Some designers will exclude navigation buttons for their walkthrough. Instead, they'll only display pagination dots and expect users to swipe. Not only is it easy for users to miss such a small visual cue, but it's not intuitive that the dots mean swipe. Don't force users to figure out how to navigate. Use clear navigation cues such as a button with a 'Next' label so that users won't have any doubt on how to navigate. Buttons are intuitive for tapping which every user understands. You can still allow users to swipe to navigate, but make sure there's a button as well. End with Call-to-Action Button Placing your call-to-action button at the beginning of your walkthrough isn't a good idea. This will tempt users to skip your walkthrough. Most users will skip it because they'll feel like they don't need it. But chances are they'll end up a confused user looking for the help page if they skip it. Place your call-to-action button at the end so you don't give users the option to skip it. This is a more sensible and expected flow that's better for the user in the long run. Images Should Show How to Use App Some designers will keep selling users in their walkthrough by touting their features. Once users have downloaded your app there's no need to sell them on it anymore. Instead, they need to know how to use it. Your images should display the user interface and pinpoint what users need to tap or swipe to use a feature. Retrievable Walkthrough After users are done with the walkthrough they may still need to go back and reference it. You should make it easy for users to retrieve the walkthrough by placing a link to it in your navigation menu. You can label the link: tour, tutorial or how-to. Make Walkthroughs a Walk in the Park A walkthrough that's a pain to read and navigate is one that users will always want to skip. But if you make yours short and useful they'll have no problem viewing it. A walkthrough is not a help and support page. It's a quick overview of features to help users get started. In-depth details on how to use the app belong on the help and support page. One reason people skip instruction manuals is because they take too long to read and are hard to follow. Although a walkthrough instructs users, it should never feel like an instruction manual. Instead, it should feel as easy to read as a sticky note. PRODUCTS Author and editor-in-chief of UX Movement. Finds beauty in clear, efficient design and loves fighting for users.",en,55
290,2684,1477906442,CONTENT SHARED,4815632823882298534,1895326251577378793,8176160181787995316,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36",SP,BR,HTML,http://projetodraft.com/por-dentro-do-nubank-conheca-os-segredos-da-fintech-mais-festejada-do-pais/,"por dentro do nubank, conheça os segredos da fintech mais festejada do país","Uma empresa de 360 funcionários ocupa um prédio de oito andares na avenida Rebouças, em São Paulo. Poderia ser um negócio maduro e consolidado, mas é o Nubank , fintech que está no mercado há apenas dois anos com um produto até que bem tradicional, um cartão de crédito. A diferença está na forma de fazer isso: não há anuidade para o consumidor, e as receitas vêm apenas da Mastercard, a bandeira do ""roxinho"" (como ficou conhecido o cartão Nubank). O nome, por sinal, vem de nudez mesmo, uma referência à transparência e à ausência de papel e burocracia. O cliente acompanha as transações feitas em tempo real por um aplicativo para smartphone. É do celular, aliás, que ele faz tudo: pede o cartão, desbloqueia ou bloqueia, solicita mais limite ou conversa com um atendente gente boa (os fundadores também detestam telemarketing) para resolver algum problema. Simplicidade é a alma do negócio - ao menos para o cliente, já que internamente nem sempre as coisas são tão desenroladas, como veremos adiante. Mas foi com esse jeito despretensioso que o Nubank explodiu de crescer em seu pouco tempo de vida e criou uma marca sólida, queridinha nas redes sociais. A fintech não revela o número de clientes, apenas que já recebeu 4,5 milhões de pedidos de cartão. ""Isso mostra que havia uma demanda reprimida muito grande pelo nosso produto"", diz Cristina Junqueira, 33, uma das três fundadoras da empresa, hoje vice-presidente de branding e business development. O negócio começou a ser elaborado em maio de 2013. Ela, engenheira, tinha acabado de sair do Banco Itaú, onde foi gerente do portfólio de cartões Itaucard, e se encontrado com os sócios: o colombiano e engenheiro financeiro David Vélez, 34, atual CEO, e Edward Wible, 33, formado em Ciências da Computação, nascido nos Estados Unidos e hoje CTO do Nubank. O time já começou multicultural, algo que é mantido e ampliado na empresa até hoje. Eles colocaram dinheiro do bolso para começar, mas logo de cara já conseguiram captar 2 milhões de dólares de fundos de investimento importantes: Sequoia Capital e Kaszek Ventures . Refinaram o plano de negócio, abriram empresa e em abril de 2014 emitiram os primeiros cartões em versão beta. FAZER SUCESSO CEDO TRAZ PROBLEMAS QUE NINGUÉM VÊ Daí em diante, as coisas aceleram e ficaram um tanto enlouquecedoras. ""A gente chegava a crescer 60% de um mês para o outro"", conta Cris, como é conhecida a fundadora. Eles receberam mais três rodadas de investimento, que totalizaram 99 milhões de dólares até 2016. Em dois anos, alcançaram a expansão prevista inicialmente para os cinco primeiros anos. Este é o lado conhecido da história: o sucesso arrebatador. O que nem todo mundo sabe é que uma expansão acelerada também tem seus problemas. O negócio tomou um porte tão inesperado que a responsabilidade extrapolou os resultados. Cris fala a respeito: ""Não podia ser voo de galinha. Muita gente apostava na gente, até mesmo outras empresas, além dos clientes e investidores"" Ela segue: ""Tem ainda os nossos funcionários. Muitos deles largaram empregos estáveis para vir para cá. Rola esse sentimento de esperança de que podemos ser melhores, tratar o cliente bem. Todo mundo queria que a gente mostrasse que dava certo fazer diferente"". No meio disso tudo, a vantagem da expansão rápida, ela diz, é que o ritmo era tão enlouquecedor que não dava nem tempo de sentir medo. ""Eu não podia parar o crescimento, então não dava para assustar"", conta. Cris segue bem essa toada, pois é acelerada na velocidade da música de sua staturp: chegou para a entrevista com o Draft tomando café da manhã e não pensava muito antes de entregar as respostas às perguntas. Com esse jeito descontraído, ela conta que teve um desafio adicional nestes dois anos de empresa, a maternidade. ""Minha filha tem justamente 2 anos. Voltei a trabalhar pouco mais de uma semana depois do parto. Ela sempre foi um anjo, nunca deu trabalho além do esperado. O Nubank tem muito a agradecer à Alice"", diz, sem no entanto minimizar o feito: não foi fácil. COMO MONTAR UMA EMPRESA DO ZERO A primeira sede do Nubank foi a casa de Edward, na rua Califórnia, em São Paulo. Mas, rapidamente o lugar ficou insustentável e eles se mudaram para um espaço maior, logo depois do Natal de 2014. O novo prédio mal deu conta por seis meses: tinha gente trabalhando até no corredor, times deslocados para um coworking na região e internet caindo toda hora. ""A coisa estava insalubre. Em junho daquele ano já começamos a procurar outra sede"", afirma Cris. Encontraram, enfim, o prédio onde estão hoje - e de onde não pretendem sair tão cedo. A mudança para lá até que foi mais bem planejada, mas teve tantos problemas que precisou acontecer antes do previsto. ""Chegamos e não tinha nada, nem cadeira para todo mundo"", lembra ela. Eles foram se acomodando enquanto as obras terminavam. Hoje o escritório está pronto, mas ainda passa por alguns ajustes. Os gargalos não estavam só na estrutura física: ""A nossa diretora de Recursos Humanos chegou só há dois meses"", diz a fundadora, que se envolveu em cada admissão e garante que, mesmo aos trancos e barrancos, formou um time super competente. Os nubankers, como são chamados os funcionários, têm 25 nacionalidades. Mesmo quem não é gringo, normalmente é cheio de experiência internacional. Cris afirma que há um esforço grande para respeitar os códigos mais atuais de trabalho. A hierarquia tradicional foi a primeira a sair de lado. Na Nubank, os times são divididos nos chamados squads , que variam de 10 a 50 pessoas. Ali tem tudo o que a equipe precisa para resolver os problemas com independência. Na área de fatura do cartão, por exemplo, há profissionais de várias áreas, como designers e profissionais de atendimento. Cris, avessa à burocracia sem necessidade, fala a respeito: ""Os funcionários se viram, têm o sentimento e a responsabilidade de dono. O processo tem que servir ao negócio, não o contrário"" A aura colaborativa, de se ajudar e respeitar as diferenças, também é algo importante para a startup. Cerca de 30% da equipe é LGBT . Por enquanto não há nenhum transexual, mas eles já oferecem banheiro sem definição de gênero. Cerca de 40% do time é feminino, algo raro para uma empresa de tecnologia. ""Queremos deixar isso ainda mais equilibrado"", diz Cris. Internamente, a ideia é que as pessoas se policiem para manter o clima de respeito. ""O ambiente é tão legal que as pessoas se empenham para não estragar, para não fazer piadas de mau gosto."" COMO SER UM SUCESSO DE PÚBLICO SEM INVESTIR EM PUBLICIDADE Olhando para marketing, o Nubank também nunca investiu um real sequer em publicidade e divulgação. E não parece ter feito falta: o crescimento da empresa aconteceu de forma orgânica, impulsionado pela novidade do produto e por ações de atendimento ao cliente que viralizaram nas redes sociais. Mas não se engane: construir algo assim dá ainda mais trabalho para organizar e coordenar internamente. É nesse ambiente que se formou o squad responsável pelo que o Nubank chama de ""efeito Wow"", aquele que fideliza clientes e faz a empresa bombar nas redes sociais. É preciso ter vivido em Marte nos últimos dois anos para não topar com alguém compartilhando uma história dessas na timeline. Entre as mais famosas estão a do cliente que ligou para reclamar que a compra de um lanche foi cobrada duas vezes e, além de ter o valor estornado, recebeu uma simpática sanduicheira roxa em casa com receitas de lanches escritas à mão pelo atendente. A lista de histórias é grande e Cris diz que as ações funcionam porque são genuínas, fruto de um time que se esforça para ser criativo e surpreender. É algo levado tão a sério que os autores das ideias mais legais recebem um prêmio lá dentro, entregue a cada dois meses. ""Sempre buscamos tratar as pessoas de forma humana. Se alguém liga para bloquear o cartão porque foi assaltado, a primeira coisa que temos que fazer é perguntar se o cliente está bem"", diz. Nesses dois anos de Nubank, Cris percebeu que o jeito informal do atendimento pode não agradar todo mundo - e tudo bem também. ""Tentamos seguir o tom que o cliente usa, mais ou menos informal. Mas se ele quer ser chamado de senhor, este não é o cartão de crédito para ele"" Além de gerenciar a expansão rápida dentro de casa, o Nubank precisou administrar um problema um tanto quanto inusitado: a ansiedade e as expectativas dos fãs da marca. ""As pessoas pedem o cartão e seguramos a aprovação de acordo com a análise de crédito. Foi esse o controle que encontramos, desde o começo, porque não dávamos conta do ritmo de pedidos"", diz ela. Sem planejar, essa demora em atender os pedidos acabou envolvendo o Nubank em uma aura de cobiça. De tão ansiosos, muitos clientes chegavam a postar foto do cartão, expondo todos os números, nas redes sociais assim que o recebiam. ""Era um tal da gente ligar e avisar que teríamos de cancelar o cartão e enviar outro por causa do risco de fraude"", conta Cris. A solução foi criar um envelope com o logotipo da marca, mas capaz de esconder os números, além de reforçar os avisos de que não é muito prudente postar as informações do cartão de crédito por aí. PÉ NO FREIO TAMBÉM FAZ PARTE Foi preciso algum tempo, mas a empresa percebeu que colocar o pé no freio é essencial. Além de entender que o Nubank pode não ser a melhor solução para todos os clientes, a cofundadora também precisou ser mais seletiva para atender a imprensa. Pelas contas dela, a cada oito horas é publicada uma matéria sobre o Nubank, inclusive em veículos internacionais. Se ela for dar tanta entrevista, não sobra tempo para muita coisa. Ela também segura a onda quando questionada sobre o futuro da empresa: ""As coisas aconteceram tão rápido até aqui que as vezes é difícil até mesmo fazer um balanço. O ano passado foi para provar que nós fazemos sentido, já 2016 foi a hora de cuidar da fundação, da casa, do time e da estrutura. Em 2017 queremos pensar em coisas novas, não só em resolver problemas"". A empresa vai encerrar 2016 atendendo a um pedido que ouviu muito nestes dois anos: a criação de uma política de recompensas. Em setembro, lançaram o Nubank Rewards, que promete ser mais vantajoso que os tradicionais programas de acúmulo de milhagens (ainda que publicações especializadas tenham reagido com alguma frieza). O serviço é opcional e, quem optar por ele, paga assinatura a partir de 190 reais por ano. O programa dá um ponto para cada real gasto, que pode ser usado para abater serviços específicos, como Uber e Spotify, ou compras na fatura do cartão. A Cris também toma fôlego para olhar mais longe e pensar no papel da empresa na sociedade. ""Tenho refletido muito sobre o futuro da minha filha como mulher"", diz, sobre a constatação de que o sexo feminino ainda enfrenta dificuldades para alcançar seu potencial. Ela cita um dado da consultoria McKinsey: o acesso mais pleno das mulheres à educação e ao mercado de trabalho aumentaria em 12 trilhões de dólares o PIB global. Com isso em mente, Cris diz estar disposta a começar a mudar este jogo. A filha, Alice, mais uma vez parece estar levando a Nubank para o futuro.",pt,55
291,1507,1466716976,CONTENT SHARED,-2774293088092853604,-8020832670974472349,7182677253610272452,,,,HTML,https://medium.com/slack-developer-blog/message-buttons-and-the-slack-api-ab938174af70,message buttons and the slack api - slack platform blog,"New APIs + new capabilities = totally awesome Message Buttons mark the biggest change to the Slack API since its inception. They're deceptively simple - a bit of UI that lets users interact with your apps directly. They're available in the Slack API and can be added to any existing app. They operate within message attachments, allowing up to five buttons in a single message that fire off any process you need when clicked by users. They make your apps easier to use - no longer do users have to remember precise commands - they can just click the button for the function they want, when they want it. For developers it's the same story; your bots won't have to parse human language as much if you let users make direct choices. The result is a new way of building apps for Slack, one that relies less on complex multi-step interactions, while letting you increase user engagement within Slack. We think this is going to usher in a whole new world of apps built for Slack. Let's look at some sample apps released today to get the ideas flowing. Resolve issues with Pagerduty Pagerduty lets users review incoming alerts and mark them as acknowledged or resolved, right from Slack. Using the chat.update API method, messages posted to channels show results in real-time as buttons are clicked, including who marked events as resolved. This both saves a trip to web browser and shows a record of which team member responded to issues. Pay your teammates back with Current Current allows you to send your team members money via Slack. The app combines slash commands with Message Buttons to make their Slack app extremely simple to use. An especially impressive aspect of Current's updated app is how they handle support requests, through a combination of Message Buttons, slash commands, and bot users. Lunch train We've also built a reference app for you to try out. At Slack's San Francisco office, food trucks appear every Monday, Wednesday, and Friday and quickly became a tradition for employees. Slack channels would fill up with chatter as people made plans for when to arrive to beat the crowds, so we built an app that makes coordinating that easier, and shows off Message Buttons. You start by proposing a time and destination, and post it into a channel where others can see. Colleagues seeing the message can tap the button to Board the train. Using chat.update in the Slack API, the host gets a running tally of who is going (without triggering new messages) along with the option of delaying or canceling the lunch trip. Shortly before the agreed time, everyone gets a Slackbot reminder to meet up and meet for lunch. After the event, messages update to past tense and display who took part. To test out Lunch Train you can install it to your team . Build with buttons If you've already built an app for Slack, check the API documentation on Message Buttons . If you're still considering what kind of app to build for Slack, know that Message Buttons offer direct interactions, and can greatly simplify your application's flow. If you need help getting started, the crew over at Howdy updated Botkit to include demo code with Message Buttons. As of today, over 500 apps are available our App Directory. We want to thank the entire developer community for helping propel the Directory from just an idea last winter into an incredible resource for millions of users, all over the course of just a few months, and we can't wait to see what you build next.",en,55
292,1320,1465491249,CONTENT SHARED,7459726589306441015,-1032019229384696495,-67483630648809830,,,,HTML,http://mashable.com/2016/06/07/spring-app-relaunch/,shopping app spring's redesign could be a game-changer,"Back in 2013, Alan Tisch, the CEO of the shopping app Spring, thought he was a little late to the mobile party. ""Isn't this a little late to be starting this? Hasn't mobile blown up?"" he asked his now business partners. He was referring to mobile-commerce - m-commerce as we now call it - an experience that was virtually non-existent then. There were, of course, mobile versions of e-commerce platforms. But nothing was quite like Spring, an app Tisch would launch in August of 2014, that was mobile-first in its approach. Ironically, Tisch wasn't late to the mobile party. He was a tad bit early. ""Though people are on their phones all the time, their purchasing habits are still on their desktops,"" he says to Mashable . ""In Asia, m-commerce is 100% there. But here, it's not there yet. People are still like, 'can I really spend $3,000 on my phone? Is it safe?'"" Three years later, Spring is still trying to change how Americans view shopping on their phones. Not that the brand hasn't been doing a good job at that. When it first launched in 2014 with 125 brands, it was touted as the future, one that Vogue once said would ""change the way you shop forever."" It had ""cool"" currency with influencers like streetstyle influencer Nick Wooster to hot direct-to-consumer brands like Greats onboard. A same-day delivery service initiative with Uber made it seem as if Spring was a nimble brand that would deliver you your order in under a few hours. A year later, Spring found venture capital cash flooding in, announcing it had raised $25 million from the likes of Google Ventures, Josh Kushner, and Ashton Kutcher, among others. But lusting after the cool factor seems to have faded. Today, Tisch wants to focus on scaling and ensuring the user experience entices new customers. The new app launch today comes with 1100 brands from the likes of DKNY, Toms, Cole Haan and Club Monaco, among others. They join brands like big box retailers from Urban Outfitters, to high-end designers like Tod's, 3.1 Phillip Lim and Kenzo. Tisch says the biggest goal for Spring for the future, is to make it the one shopping app that any consumer would ever need for shopping. ""When you ask people how many apps they use, they only use around 5-7 apps multiple times in a day,"" he says. ""When they think of shopping, there should be one app."" The CEO says the experience and aesthetic will make it seem premium with its ""pops of color"" throughout. He's most excited, he says, about how the colors will change from season to season. ""It will feel like a refresh and every couple of months with its seasonal designs,"" he says. ""It's all about uniqueness,"" he says when it comes to acquiring new customers in the heavily competitive commerce space. ""It's a crowded marketplace, but the way you win is being unique."" Spring, he says, will work on exclusive collaborations with brands, feature brands not sold anywhere else, while doing so with ""best in class mobile software."" The refresh will also focus on editorial content that aspires to bring the brands' stories directly to the consumer. ""It should be seasonal and timely,"" Tisch says. ""We don't want top 10 list click-baity stories, rather, celebration of the designer and the products they made."" The brand's entire business model is one that is based off of ""drop ship,"" or a marketplace model. That is, Spring doesn't hold any of its own inventory or do buys for seasons, rather, is the middle man between consumers and fashion companies who will ship out items themselves. Spring then will keep a percentage of all sales. It's an enticing model for small brands that don't have the capacity to venture into m-commerce, while being an entirely new set of eyeballs for bigger companies who want younger consumers. But even with this flashy new redesign, free shipping, free returns, a VIP service, along with direct customer service numbers, Tisch knows this isn't enough to become profitable. "" Amazon hasn't been profitable in 20 years,"" he says. ""They're creating trust with their customers. We understand that in commerce, it's a long game."" Have something to add to this story? Share it in the comments.",en,55
293,1647,1467674442,CONTENT SHARED,-4460374799273064357,-1443636648652872475,6885512560807851601,,,,HTML,http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/,"deep learning for chatbots, part 1 - introduction","Chatbots, also called Conversational Agents or Dialog Systems, are a hot topic. Microsoft is making big bets on chatbots, and so are companies like Facebook (M), Apple (Siri), Google, WeChat, and Slack. There is a new wave of startups trying to change how consumers interact with services by building consumer apps like Operator or x.ai , bot platforms like Chatfuel , and bot libraries like Howdy's Botkit . Microsoft recently released their own bot developer framework . Many companies are hoping to develop bots to have natural conversations indistinguishable from human ones, and many are claiming to be using NLP and Deep Learning techniques to make this possible. But with all the hype around AI it's sometimes difficult to tell fact from fiction. In this series I want to go over some of the Deep Learning techniques that are used to build conversational agents, starting off by explaining where we are right now, what's possible, and what will stay nearly impossible for at least a little while. This post will serve as an introduction, and we'll get into the implementation details in upcoming posts. A taxonomy of models Retrieval-Based vs. Generative Models Retrieval-based models (easier) use a repository of predefined responses and some kind of heuristic to pick an appropriate response based on the input and context. The heuristic could be as simple as a rule-based expression match, or as complex as an ensemble of Machine Learning classifiers. These systems don't generate any new text, they just pick a response from a fixed set. Generative models (harder) don't rely on pre-defined responses. They generate new responses from scratch. Generative models are typically based on Machine Translation techniques, but instead of translating from one language to another, we ""translate"" from an input to an output (response). Both approaches have some obvious pros and cons. Due to the repository of handcrafted responses, retrieval-based methods don't make grammatical mistakes. However, they may be unable to handle unseen cases for which no appropriate predefined response exists. For the same reasons, these models can't refer back to contextual entity information like names mentioned earlier in the conversation. Generative models are ""smarter"". They can refer back to entities in the input and give the impression that you're talking to a human. However, these models are hard to train, are quite likely to make grammatical mistakes (especially on longer sentences), and typically require huge amounts of training data. Deep Learning techniques can be used for both retrieval-based or generative models, but research seems to be moving into the generative direction. Deep Learning architectures like Sequence to Sequence are uniquely suited for generating text and researchers are hoping to make rapid progress in this area. However, we're still at the early stages of building generative models that work reasonably well. Production systems are more likely to be retrieval-based for now. Long vs. Short Conversations The longer the conversation the more difficult to automate it. On one side of the spectrum are Short-Text Conversations (easier) where the goal is to create a single response to a single input. For example, you may receive a specific question from a user and reply with an appropriate answer. Then there are long conversations (harder) where you go through multiple turns and need to keep track of what has been said. Customer support conversations are typically long conversational threads with multiple questions. Open Domain vs. Closed Domain In an open domain (harder) setting the user can take the conversation anywhere. There isn't necessarily have a well-defined goal or intention. Conversations on social media sites like Twitter and Reddit are typically open domain - they can go into all kinds of directions. The infinite number of topics and the fact that a certain amount of world knowledge is required to create reasonable responses makes this a hard problem. In a closed domain (easier) setting the space of possible inputs and outputs is somewhat limited because the system is trying to achieve a very specific goal. Technical Customer Support or Shopping Assistants are examples of closed domain problems. These systems don't need to be able to talk about politics, they just need to fulfill their specific task as efficiently as possible. Sure, users can still take the conversation anywhere they want, but the system isn't required to handle all these cases - and the users don't expect it to. Common Challenges There are some obvious and not-so-obvious challenges when building conversational agents most of which are active research areas. Incorporating Context To produce sensible responses systems may need to incorporate both linguistic context and physical context . In long dialogs people keep track of what has been said and what information has been exchanged. That's an example of linguistic context. The most common approach is to embed the conversation into a vector, but doing that with long conversations is challenging. Experiments in Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models and Attention with Intention for a Neural Network Conversation Model both go into that direction. One may also need to incorporate other kinds of contextual data such as date/time, location, or information about a user. Coherent Personality When generating responses the agent should ideally produce consistent answers to semantically identical inputs. For example, you want to get the same reply to ""How old are you?"" and ""What is your age?"". This may sound simple, but incorporating such fixed knowledge or ""personality"" into models is very much a research problem. Many systems learn to generate linguistic plausible responses, but they are not trained to generate semantically consistent ones. Usually that's because they are trained on a lot of data from multiple different users. Models like that in A Persona-Based Neural Conversation Model are making first steps into the direction of explicitly modeling a personality. Evaluation of Models The ideal way to evaluate a conversational agent is to measure whether or not it is fulfilling its task, e.g. solve a customer support problem, in a given conversation. But such labels are expensive to obtain because they require human judgment and evaluation. Sometimes there is no well-defined goal, as is the case with open-domain models. Common metrics such as BLEU that are used for Machine Translation and are based on text matching aren't well suited because sensible responses can contain completely different words or phrases. In fact, in How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation researchers find that none of the commonly used metrics really correlate with human judgment. Intention and Diversity A common problem with generative systems is that they tend to produce generic responses like ""That's great!"" or ""I don't know"" that work for a lot of input cases. Early versions of Google's Smart Reply tended to respond with ""I love you"" to almost anything. That's partly a result of how these systems are trained, both in terms of data and in terms of actual training objective/algorithm. Some researchers have tried to artificially promote diversity through various objective functions . However, humans typically produce responses that are specific to the input and carry an intention. Because generative systems (and particularly open-domain systems) aren't trained to have specific intentions they lack this kind of diversity. How well does it actually work? Given all the cutting edge research right now, where are we and how well do these systems actually work? Let's consider our taxonomy again. A retrieval-based open domain system is obviously impossible because you can never handcraft enough responses to cover all cases. A generative open-domain system is almost Artificial General Intelligence (AGI) because it needs to handle all possible scenarios. We're very far away from that as well (but a lot of research is going on in that area). This leaves us with problems in restricted domains where both generative and retrieval based methods are appropriate. The longer the conversations and the more important the context, the more difficult the problem becomes. In a recent interview , Andrew Ng, now chief scientist of Baidu, puts it well: Most of the value of deep learning today is in narrow domains where you can get a lot of data. Here's one example of something it cannot do: have a meaningful conversation. There are demos, and if you cherry-pick the conversation, it looks like it's having a meaningful conversation, but if you actually try it yourself, it quickly goes off the rails. Many companies start off by outsourcing their conversations to human workers and promise that they can ""automate"" it once they've collected enough data. That's likely to happen only if they are operating in a pretty narrow domain - like a chat interface to call an Uber for example. Anything that's a bit more open domain (like sales emails) is beyond what we can currently do. However, we can also use these systems to assist human workers by proposing and correcting responses. That's much more feasible. Grammatical mistakes in production systems are very costly and may drive away users. That's why most systems are probably best off using retrieval-based methods that are free of grammatical errors and offensive responses. If companies can somehow get their hands on huge amounts of data then generative models become feasible - but they must be assisted by other techniques to prevent them from going off the rails like Microsoft's Tay did . Upcoming & Reading List We'll get into the technical details of how to implement retrieval-based and generative conversational models using Deep Learning in the next post, but if you're interested in looking at some of the research then the following papers are a good starting point:",en,55
294,802,1462448930,CONTENT SHARED,-8949113594875411859,1895326251577378793,6242109617183539580,,,,HTML,http://m.folha.uol.com.br/mercado/2016/05/1766559-burrofones-ainda-sao-25-dos-celulares.shtml,"no brasil, '25% dos celulares ainda são 'burrofones'","Divulgação O ex-jogador de futebol Tostão, dono de um 'burrofone' Campeão do mundo e ídolo do esporte brasileiro, o médico e ex-jogador de futebol Tostão não demonstra o mesmo apreço pela tecnologia como o que tem pela bola. Avesso aos smartphones, o colunista da Folha ainda usa um celular antigo. ""Só uso o celular para telefonar e receber chamadas. Às vezes recebo mensagens, mas nunca mando. Isso [o smartphone] não me faz falta"", diz Tostão pelo seu ""burrofone"" (dumbphone, em inglês). Pode parecer raro, mas Tostão representa uma quantidade considerável de pessoas avessas à tal tecnologia. Aproximadamente 25% das linhas móveis ainda permanecem restritas a chamadas de voz -o máximo em tecnologia são mensagens SMS. Dados da Anatel mostram que, em março, 62,7 milhões de linhas móveis operavam em ""burrofones"", ou seja, trabalhavam via 2G -sem acesso à internet. Em um mercado quase saturado -as vendas de smartphones caíram 13,4% no país em 2015- e em meio a uma recessão econômica, que inviabiliza uma forte expansão, a disputa pela modernização desse nicho ""parado no tempo"" ganha importância. 2G no Brasil - Celulares sem acesso à internet, por DDD, em % ""Nós vivemos uma transição em que a receita de voz cai muito. Então, há essa tentativa de estimular a migração para incentivar a compra de pacote de dados"", afirma Eduardo Tude, diretor da consultoria Teleco. Segundo a Folha apurou, a alta na receita das operadoras pode chegar a 50% na comparação entre um pacote de voz e um de voz mais dados. As quatro grandes operadoras do país reconhecem os esforços em avançar ante os 25% resistentes. ""Geralmente, clientes que passam a ter uma melhor experiência com o uso da internet contratam pacotes de dados e, logo, geram mais receitas"", disse a Vivo, em nota. Além do aumento de receita, há a necessidade de remanejar investimentos, de acordo com Ricardo Distler, diretor-executivo da consultoria Accenture. ESTRATÉGIAS O mercado sabe que há três motivos principais para o alto percentual de ""burrofones"" no país: preço dos aparelhos, falta de conhecimento e problemas no sinal. ""A primeira barreira é o preço. Por isso, buscamos negociar com fabricantes um aparelho de entrada a preços acessíveis"", afirma Rodrigo Vidigal, diretor de marketing da Claro. A Oi aposta em planos que oferecem dinheiro para aparelhos usados. ""Isso visa tornar o processo de troca de tecnologia mais rápido, agrega receita de dados e melhora a experiência de uso"", afirma Roberto Guezburger, diretor de produtos e mobilidade da Oi. A Vivo também apela a descontos em novos aparelhos, enquanto a TIM promove workshops com aulas para demonstrar a funcionalidade dos smartphones. SEM ACESSO De acordo com analistas, as regiões que concentram o maior percentual de linhas 2G são as de menor renda e pior qualidade no serviço oferecido pelas empresas. É consenso que a dificuldade em encontrar sinal inviabiliza a chegada de mais smartphones a tais locais. ""Regiões de menor renda tipicamente têm menor participação em razão das dificuldades do sinal"", afirma Guezburger, da empresa Oi. Mesmo com a expansão do 3G e do 4G no país, em 2016 cerca de 550 cidades ainda recebem apenas o sinal 2G -praticamente todas no interior do Brasil, segundo dados disponibilizados pela Anatel. Fale com a Redação - leitor@grupofolha.com.br Problemas no aplicativo? - novasplataformas@grupofolha.com.br",pt,55
295,1448,1466389406,CONTENT SHARED,-6999287066519531005,3891637997717104548,8220568155268171290,,,,HTML,https://www.acquia.com/resources/webinars/lightning-distribution-drupal-build-advanced-authoring-experiences-drupal-8,lightning distribution for drupal: build advanced authoring experiences in drupal 8,"Lightning provides developers with a lightweight framework for building advanced authoring experiences in Drupal 8. Developers gain a massive head-start when building great authoring experiences with Lightning, but it has not been a simple development roadmap. In this webinar, learn about some of the decisions we had to tackle, including:",en,55
296,1491,1466618633,CONTENT SHARED,8386493184609384928,-1352064057049251194,-8599333019192400391,,,,HTML,http://www.infomoney.com.br/negocios/inovacao/noticia/5198991/2017-sera-possivel-ignorar-totalmente-cartao-fisico-garante-itau,"em 2017 será possível ignorar totalmente o cartão físico, garante itaú","SÃO PAULO - O lançamento do Samsung Pay em versão beta no Brasil nesta semana dá a letra: o pagamento pelo celular está chegando. E até o ano que vem estará disponível para todos os clientes do banco Itaú. ""Já estamos trabalhando em carteira digital e ela receberá toda a nossa base de clientes em julho"", garante Fernando Chacon, presidente da Rede, em coletiva conjunta com o Itaú. ""Parceiros em e-commerce e varejo estarão preparados para compras sem cartão em aplicativos e sites já no mês que vem"". Para as lojas, a previsão é um pouco mais distante, mas também quase imediata. ""Entraremos em 2017 com essa possibilidade de [o cliente] não ter cartão quando for às lojas"", o que será possibilitado por meios de pagamento pelo celular, por exemplo, que já estão em testes internos tanto pelo banco como pela adquirente. ""Todas as máquinas da Rede estarão equipadas com isso"", diz Fernando. Conta pela internet ""Fomos um dos bancos que fizeram o pleito com o Banco Central para permitir que clientes abrissem contas sem precisar ir à agência"", diz Ricardo Guerra Diretor-executivo de Tecnologia do Itaú Unibanco. ""Vamos ter abertura de conta pela internet sim"". A movimentação é natural, dado que a facilidade de fazer operações sem precisar comparecer a agências é uma tendência forte no Brasil, que ganha ainda mais espaço com a existência de empresas completamente digitais na área como Nubank e Banco Original. De acordo com ele, o Itaú trabalha de forma a trazer aos próprios clientes uma experiência semelhante, sem necessidade alguma de falar com atendentes ou ir à agência - mas sem ignorar as pessoas que ainda preferem os métodos tradicionais. ""Nãos sei e nem estou muito preocupado em saber quando a agência física vai acabar"", dispara Ricardo. ""Eu tenho clientes que querem ir até a agência e eu preciso dar essa oportunidade para ele. [Ao mesmo tempo] temos uma suíte que visa atender de forma digital a todos os clientes que não querem [ir à agência]"". Fintechs e preços ""As fintechs [startups que usam tecnologia em soluções financeiras] têm a vantagem de oferecer serviços em menor escala e com maior compreensão do cliente. Para nós, a fintech é uma sala de aula"", explica o diretor, sem ignorar a ameaça. ""Algumas delas não vão querer parcerias e vão oferecer os mesmos serviços que nós, o que é obviamente uma concorrência, como todas as outras empresas que prestam os mesmos serviços são"", analisa. Mesmo que eventualmente ofereçam serviços mais baratos, essas novas empresas provavelmente não ditarão os valores praticados pelo mercado em geral em transações. Para Fernando Chacon, da Rede, é mais importante focar em performance. ""Não vamos entrar na guerra dos preços, mas sim orientar nosso modelo de negócio pela performance. Teremos maior agressividade comercial no sentido de acompanhar os lojistas nessas operações e fazer uma ressegmentação do mercado, que prevê benefícios a correntistas, por exemplo"", explica Chacon. Tarifa zero não existe Perguntado sobre a ameaça das operações de pagamentos sem taxas, o diretor é categórico: empresas não vivem sem rentabilizar. ""As startups funcionam, aqui ou no Vale do Silício, por meio de venture capital, um dinheiro que investidores colocam nas empresas pensando no médio prazo, quando a empresa vai ter que rentabilizar"", afirma Ricardo. Para ele, empresas como o Nubank, que oferece o cartão de crédito sem anuidade , precisarão encontrar alguma maneira de ganhar dinheiro. ""Por enquanto a meta deles é ter uma base enorme de clientes sem ganhar dinheiro"", mas isso deve mudar, afirma. De acordo com ele, ao mesmo tempo em que são mais baratas as operações digitais, o volume com que elas ocorrem acaba crescendo com o acesso ao digital. ""As pessoas tiram muito mais extrato, fazem muito mais consultas e pagamentos. Não é mais barato [para o banco] porque existe um investimento por trás"", ele diz. ""Mas claro que a ideia é nos tornarmos mais competitivo conforme a tecnologia permite a redução de custos"", encerra o diretor.",pt,54
297,1568,1467143303,CONTENT SHARED,430255078684584595,-1032019229384696495,-1382304469206973597,,,,HTML,https://techcrunch.com/2016/06/28/google-researchers-teach-ais-to-see-the-important-parts-of-images-and-tell-you-about-them/,google researchers teach ais to see the important parts of images - and tell you about them,"This week is the Computer Vision and Pattern Recognition conference in Las Vegas, and Google researchers have several accomplishments to present . They've taught computer vision systems to detect the most important person in a scene, to pick out and track individual body parts, and to describe what they see in language that leaves nothing to the imagination. First, let's consider the ability to find ""events and key actors"" in video - a collaboration between Google and Stanford. Footage of scenes like basketball games contains dozens or even hundreds of people, but only a few are worth paying attention to. The CV system described in this paper uses a recurrent neural network to create an ""attention mask"" for every frame, then tracking relevance of each object as time proceeds. Over time the system is able to pick out not only the most important actor, but potential important actors, and the events with which they are associated. Think of it like this: it could tell that someone going in for a lay-up could be important, but that the most important player there is the one who furnishes the denial. The implications for intelligently sorting through crowded footage (think airports, busy streets) are signifiacnt. Next is a more whimsical paper: researchers have created a CV system for discovering the legs of tigers. Well... there's a little more to it than that. The tigers (and some horses) simply served as ""articulated object classes"" - essentially, objects with continuously moving parts - for the system to watch and understand. By identifying independently moving parts and their motion and position relative to the rest of the animal, the limbs can be identified frame by frame. The advance here is that the program is capable of making that identification across many videos, even when the animal is moving in different ways. It's not that we desperately need data on the front left legs of tigers, but again, the ability to find and track individual parts of an arbitrary person, animal, or machine (or tree, or garment, or...) is a powerful one. Imagine being able to scrape video just for tagged animals, or people with phones in their hands, or bicycles with panniers. Naturally the surveillance aspect makes for potential creepiness, but academically speaking the work is fascinating. This paper was a collaboration between the University of Edinburgh and Google. Last is a new ability for computer vision that may be a bit more practical for everyday use. CV systems have long been able to classify objects they see: a person, a table or surface, a car. But in describing them they may not always be as exact as we'd like. On a table of wine glasses, which one is yours? In a crowd of people, which one is your friend? This paper, from researchers at Google, UCLA, Oxford, and Johns Hopkins, describes a new method by which a computer can specify objects with question of confusion. It combines some basic logic with the powerful systems behind image captioning - the ones that produce something like ""a man in red eating ice cream is sitting down"" for a photo more or less meeting that description. The computer looks through the descriptors available for the objects in question and finds a combination of them that, together, can only apply to one object. So among a group of laptops, it could say ""the grey laptop that is turned on,"" or if several are on, it could add ""the grey laptop that is turned on and showing a woman in a blue dress"" or the like. It's one of those things people do constantly without thinking about it - of course, we can also point - but that is in fact quite difficult for computers. Being able to describe something to you accurately is useful, of course, but it goes the other way: you may some day say to your robot butler ""grab me the amber ale that's behind the tomatoes."" Naturally, all three of these papers (and more among the many Google is presenting) use deep learning and/or some sort of neural network - it's almost a given in computer vision research these days, since they have gotten so much more powerful, flexible, and easy to deploy. For the specifics of each network, however, consult the paper in question. Featured Image: Omelchenko / Shutterstock",en,54
298,1361,1465778481,CONTENT SHARED,-4070599188332885701,3609194402293569455,-3757998048370803301,,,,HTML,http://startupi.com.br/2016/06/twitter-faz-atualizacao-em-seu-aplicativo-fabric-para-trazer-mais-funcionalidades-as-empresas-e-aos-desenvolvedores-de-apps/,twitter faz atualização em seu aplicativo fabric para trazer mais funcionalidades às empresas e aos desenvolvedores de apps - startupi,"Com o objetivo de tornar mais completa a experiência de seu aplicativo Fabric, o Twitter anunciou uma atualização que trará mais funcionalidades às empresas e aos desenvolvedores de apps. Os novos recursos permitem o acesso a análises mais detalhadas e completas em relação as métricas de aquisição, engajamento e retenção, como DAU (usuários ativos diários) e MAU (usuários ativos mensais). As novidades trazem ao desenvolvedor, líderes de produto e empresas com apps a possibilidade de acompanhar em tempo real o desempenho de seus aplicativos, e assim ter mais rapidez e facilidade para tomar decisões, já que não é necessário utilizar o seu laptop ou computador para ter acesso aos dados. Há alguns meses, o Twitter havia anunciado o lançamento do aplicativo do Fabric, plataforma modular móvel utilizada por empresas e desenvolvedores para a criação de apps, o que trouxe mais praticidade para acompanhar a estabilidade de aplicativos e a sua evolução em tempo real. O Fabric está instalado em mais de 1.6 bilhão de dispositivos, entre iOS e Android, sendo usado por mais de 100 mil desenvolvedores. Para se ter uma ideia, mais de 1 milhão de aplicativos já foram integrados com Crashlytics, ferramenta que identifica e reporta falhas em aplicativos, e que chega a analisar cerca de 18 bilhões de falhas por mês. O Brasil é o país da América Latina que mais utiliza o Fabric: a plataforma está presente em 69% dos apps desenvolvidos no país. Agora, com as novas funções, as empresas e os desenvolvedores poderão entender, a partir de gráficos, como a atividade dos usuários de seus aplicativos está mudando ao longo do tempo. Também será possível compreender, por exemplo, quantos novos usuários estão chegando a cada dia e comparar com o movimento da semana anterior. Essas funcionalidades colaboram especialmente em datas de lançamento de aplicativos, dias considerados mais críticos, já que requerem atenção redobrada em relação a estabilidade. Os novos recursos do aplicativo Fabric permitem ainda, além de analisar como está o crescimento do número de usuários ativos, saber a quantidade de sessões, compreender quais são os problemas que estão afetando a usabilidade do aplicativo e quais são as horas em que os usuários ficam mais ativos, o que ajuda a concluir qual é o melhor momento para lançar uma atualização, ou um novo aplicativo.",pt,54
299,1309,1465408647,CONTENT SHARED,-1492913151930215984,-1578287561410088674,-4144549791297631980,,,,HTML,https://developer.chrome.com/devtools/docs/console-api,chrome devtools - console api reference,"The DevTools docs have moved! Read the latest version of this article and head over to the new home of Chrome DevTools for the latest tutorials, docs and updates. The Console API provides web applications with methods for writing information to the console, creating JavaScript profiles, and initiating a debugging session. console.assert(expression, object) If the specified expression is false , the message is written to the console along with a stack trace. In the following example, the assert message is written to the console only when the document contains fewer than ten child nodes: console.clear() Clears the console. Also see Clearing the console . However, if Preserve Logs is on, console.clear() will not do anything in case there's some iframe which calls console.clear() and can make your debugging process harder. ""Clear console"" in the context menu will still work, and actually clear the console. console.count(label) Writes the the number of times that count() has been invoked at the same line and with the same label. In the following example count() is invoked each time the login() function is invoked. In this example, count() is invoked with different labels, each of which is incremented separately. console.debug(object [, object, ...]) This method is identical to console.log() . console.dir(object) Prints a JavaScript representation of the specified object. If the object being logged is an HTML element, then the properties of its DOM representation are displayed, as shown below: You can also use the object formatter ( %O ) in a console.log() statement to print out an element's JavaScript properties: Calling console.dir() on a JavaScript object is equivalent to calling console.log() on the same object-they both print out the object's JavaScript properties in a tree format. Compare this with the behavior of console.log() , which displays the element in an XML format as it would appear in the Elements panel: console.dirxml(object) Prints an XML representation of the specified object, as it would appear in the Elements panel. For HTML elements, calling this method is equivalent to calling console.log() . %O is a shortcut for dir %o acts either as dir or dirxml depending on the object type (non-dom or dom) console.error(object [, object, ...]) Similar to console.log() , console.error() and also includes a stack trace from where the method was called. console.group(object[, object, ...]) Starts a new logging group with an optional title. All console output that occurs after calling this method and calling console.groupEnd() appears in the same visual group. You can also nest groups: console.groupCollapsed(object[, object, ...]) Creates a new logging group that is initially collapsed instead of open, as with console.group() . console.groupEnd() Closes the most recently created logging group that previously created with console.group() or console.groupCollapsed() . See console.group() and console.groupCollapsed() for examples. console.info(object [, object, ...]) This method is identical to console.log() . Displays a message in the console. You pass one or more objects to this method, each of which are evaluated and concatenated into a space-delimited string. The first parameter you pass to log() may contain format specifiers , a string token composed of the percent sign ( % ) followed by a letter that indicates the formatting to be applied. Dev Tools supports the following format specifiers: Basic example: An example that uses string ( %s ) and integer ( %d ) format specifiers to insert the values contained by the variables userName and userPoints : An example of using the element formatter ( %o ) and object formatter ( %O ) on the same DOM element: The following example uses the %c format specifier to colorize the output string: console.profile([label]) When the Chrome DevTools is open, calling this function starts a JavaScript CPU profile with an optional label.To complete the profile, call console.profileEnd() . Each profile is added to the Profiles tab. In the following example a CPU profile is started at the entry to a function that is suspected to consume excessive CPU resources, and ended when the function exits. console.profileEnd() Stops the current JavaScript CPU profiling session, if one is in progress, and prints the report to the Profiles panel. See console.profile() for example use. console.time(label) Starts a new timer with an associated label. When console.timeEnd() is called with the same label, the timer is stopped the elapsed time displayed in the Console. Timer values are accurate to the sub-millisecond. Note: The string you pass to the time() and timeEnd() methods must match for the timer to finish as expected. console.timeEnd(label) Stops the timer with the specified label and prints the elapsed time. For example usage, see console.time() . console.timeStamp([label]) This method adds an event to the Timeline during a recording session. This lets you visually correlate your code generated time stamp to other events, such as screen layout and paints, that are automatically added to the Timeline. See Marking the Timeline for an example of using console.timeStamp() . console.trace(object) Prints a stack trace from the point where the method was called, including links to the specific lines in the JavaScript source. A counter indicates the number of times that trace() method was invoked at that point, as shown in the screen shot below. It is also possible to pass in arguments to trace(). For example: console.warn(object [, object, ...]) This method is like console.log() but also displays a yellow warning icon along with the logged message. debugger The global debugger function causes Chrome to stop program execution and start a debugging session at the line where it was called. It is equivalent to setting a ""manual"" breakpoint in the Sources tab of Chrome DevTools. Note: The debugger command is not a method of the console object. In the following example the JavaScript debugger is opened when an object's brightness() function is invoked:",en,54
300,1380,1465923371,CONTENT SHARED,-4700714777342103147,5660542693104786364,-7131137178361608790,,,,HTML,http://www.bankingtech.com/479242/say-goodbye-to-traditional-customer-segmentation/,"say goodbye to traditional customer segmentation "" banking technology","To stay afloat in a sea of consolidation, financial organisations, including community banks and credit unions, must recognise that their customers are individuals with unique needs. Michael Boukadakis, founder and CEO of Enacomm, explains how technology can help achieve this. Algorithms have advanced, and other industries - from retail to transportation - have raised the bar for customer care. Consumers increasingly have come to expect personalised service from every organisation to which they bring business. The level of personalisation that traditional customer segmentation offers is no longer enough; true one-to-one individualisation is the best way for financial institutions to win and keep customers. Every day, consumers are bombarded with broadcast messaging. Thus, they've been forced to become protective about what competes for their time and attention. Not only can lumping customers in groups, rather than treating them as individuals, be ineffective, but ongoing mass personalisation can mar a brand when communications miss the mark. Banks and credit unions must message customers appropriately, which is now possible with the help of new technology. By researching literally hundreds of data points and accessing dozens of business policies, advanced intelligent decisioning engines enable banks and credit unions to offer top notch customer self-service experiences while reaping the benefits of predictive marketing. Rather than gathering big data and relying on outdated customer relationship management (CRM) techniques to obtain a snapshot of customers, a comprehensive view of individual behavior is now within reach. To make big data think, the latest CRM technology uses the information that is collected to proactively and intelligently interact with each customer in real-time, in any communication channel. In addition to demographics and account information, each customer's behavior factors into the dynamic decisioning engine, allowing financial institutions to anticipate customers' needs and predict their future behavior. True personalisation is crucial to customer and member retention for banks and credit unions across the globe. With intelligent customer interactions, patrons will enjoy a unique self-service experience every time they connect with a financial institution via mobile, SMS text, the IVR or on the web. Personalisation does not just mean a customer will be greeted with ""Hello John"" when he calls the IVR; it means he won't be burdened with long menus and messages regarding products or services he already uses. To achieve specificity and accuracy in micro-messaging, a dynamic decisioning engine must have the following capabilities: it knows each bank customer's account types, account tenure and history; understands how often each account is accessed or not accessed; computes high, low and average balances for each account; calculates and records transaction amounts for each customer based on current values and rolling averages; identifies transaction types, differentiating among ATM, teller, debit card, and check activities; records and reacts to the number and types of each customer's digital log-ins; applies sophisticated interactive policies and rules using hundreds of additional data points. When patrons interact with financial institutions via any channel, the technology knows many key facts about them within milliseconds. Tailored customer service leads to increased profitability for financial institutions. An advanced decisioning engine enables banks and credit unions to successfully cross-sell products and services, while reducing customer service costs with top notch self-service channels. Modern technology can also help banks and credit unions hone in on their most valuable customers. In the old days, it was commonly held that every customer should receive the same level of service - but that's not the most effective approach from a business strategy perspective. In the airline industry, first-class passengers are more likely to be frequent flyers, which means they spend more money with the airline. Thus, they receive a higher level of service as they sit at the front of the plane. Banks and credit unions should rank customers and members, as well. A customer who has a large sum of money and multiple accounts with a bank should be prioritised by that financial institution over a customer with a small amount in one account. The person with the larger balance is more likely to take advantage of financial products and services that the bank offers. A dynamic decisioning engine can single out high-value bank customers, who should receive ""first-class"" service from a customer care agent. For example, when a ""platinum"" patron calls the customer service line, the latest technology can determine that he/she should be transferred directly to a representative without having to listen to promotional sales messages - a customer service ""express lane."" To keep patrons happy and increase profitability, trade in conventional customer segmentation for customer individualisation. Modern CRM technology can give banks and credit unions confidence that each customer interaction is tailored to the needs of the individual and the institution.",en,54
301,2072,1471309031,CONTENT SHARED,8482750322470893687,4142810830429822977,-1828811998399175640,,,,HTML,http://venturebeat.com/2016/08/15/google-is-discontinuing-google-hangouts-on-air-on-september-12-pushes-users-to-youtube-live/,"google is discontinuing google+ hangouts on air on september 12, pushes users to youtube live","Google today quietly announced that Google+ Hangouts On Air will no longer be available on September 12. Four weeks from now, Google users will be asked to use YouTube Live instead. Google first debuted the livestreaming feature for its Hangouts group video-chat on Google+ back in September 2011 , though it was only available to select performers and celebrities. Google started making Hangouts On Air available to all its users in May 2012 , and completed the rollout a month later. But then in May 2013 , Google debuted YouTube Live, which also gradually became available to more and more users. Now Google has finally decided to consolidate the two. After September 12, you won't be able to schedule a new Hangouts On Air via Google+. Existing events scheduled to happen after September 12 will also have to be moved to YouTube Live. To be clear, Hangouts On Air will still exist in some form, it just won't be tied to Google+. Hangouts On Air users are being forced to use YouTube Live's built-in events scheduling feature instead. To use Hangouts On Air with YouTube Live, follow these steps: Go to Live Streaming Events in Creator Studio. Click New live event. Select Quick (using Google Hangouts On Air). Give your live stream a title. Click Go live now or enter in details to schedule your event for later. Use Hangouts to broadcast live. Recorded events will continue to be available on YouTube. The only remaining Google+ link will be the option to see your event content (read-only format) in the Activity Log. Google began dropping its Google+ requirement across all its products in July 2015. That process is still ongoing. Last week, the Google Play store started dropping the Google+ account requirement when leaving a review. Next month, Hangouts On Air users will no longer be able to schedule livestreams using Google+. Get more stories like this on Twitter & Facebook",en,54
302,2044,1470938396,CONTENT SHARED,6644171916702621557,-1836083230511905974,7109115708327615090,,,,HTML,https://medium.com/@andremion/criando-%C3%ADcones-animados-no-android-14b2d5feb877,criando ícones animados no android,"AnimatedVectorDrawable Com um AnimatedVectorDrawable podemos animar as propriedades de um VectorDrawable , e para isto, basicamente precisamos de três arquivos XML : Um VectorDrawable <vector> na pasta res/drawable/ Um ou mais ObjectAnimator <objectAnimator> ou AnimatorSet <set> na pasta res/anim/ E um AnimatedVectorDrawable <animated-vector> na pasta res/drawable/ É possível animar atributos de um elemento <group>, <path> ou do próprio <vector>. Elementos <group> são utilizados para agrupar paths ou subgrupos que precisam ser animados em conjunto. Quando definimos um VectorDrawable precisamos usar o atributo android:name e definir um nome único nos elementos que queremos animar, para então podermos referenciá-los das definições do Animator . Exemplo: O elemento <group> define um conjunto chamado "" rotation "". Já o elemento <path> define as formas geométricas a serem desenhadas cujo nome é "" menu "". 2.1. Definições da animação são representadas por objetos ObjectAnimator ou AnimatorSet . Neste exemplo, o Animator rotaciona o alvo em 180 graus com duração de 500 milisegundos. 2.2. O segundo Animator neste exemplo, transforma um path de uma forma para outra. Ambos os paths precisam ser compatíveis para a transformação: Eles devem ter a mesma quantidade de comandos e de parâmetros para cada comando . As definições de um AnimatedVectorDrawable servem de ligação entre o VectorDrawable e as definições de animações. O AnimatedVectorDrawable é carregado como um Drawable comum, porém, é preciso chamar o método start() para iniciar a animação: Um vez configurado corretamente, veremos o ícone animar desta forma (a duração da animação foi ampliada para 2 segundos):",pt,54
303,1589,1467251464,CONTENT SHARED,-667193404227875686,3609194402293569455,6188727096970658788,,,,HTML,https://insights.fb.com/2016/06/23/the-drive-to-bank-on-digital/,the drive to bank on digital,"Across Latin America's two most populous countries, people are starting to expect more from their banks. But are banks in Brazil and Mexico rising to the occasion? Recent research reveals that 1 in 2 online bankers don't think so. In a previous post, we looked at how mobile is reshaping the way people shop across Latin America. And now, a recent study allows us to shed new light on how digital and mobile are transforming the way people bank. To explore what increasing mobile usage, shifting behaviors and rising expectations mean for banks operating across Latin America, Facebook IQ commissioned Ipsos to conduct online interviews among 1,391 connected bankers-Brazilians in classes A, B and C and Mexicans in classes A, B, C+, C and C- who have Internet access and active bank accounts.* We discovered that the drive to bank online is widespread and reaches across genders and generations. We also found that banks have a significant opportunity to close the gap between their clients' expectations and services offered. Check out the infographic below to see what it all means for marketers.",en,53
304,2949,1484231249,CONTENT SHARED,8302949268716967655,-4465926797008424436,5920817461981142302,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2978.4 Safari/537.36",SP,BR,HTML,https://hackernoon.com/what-makes-a-good-android-software-engineer-206562e1fdb6?gi=2e62296e58fa,what makes a good (android) software engineer,"For the past five years, I have been building stuff on Android. During this time, I have collaborated with engineers from different backgrounds and with different experience levels. Some engineers came from an enterprise background with years and years of experience, while some were just fresh out of college and the only thing they knew was what they learned in the university/college and their experience building mobile apps in their free time. Some people did not even get a formal CS education and are self taught. During this time, I have seen what each engineer of the above categories were capable of delivering and how they were delivering it. So, if you are hiring a so-called: Android Software Engineer for your company, read this article. Why? Because, at first, mobile app development is mistakenly perceived as being easy. After all, a mobile app is merely about putting together different pages that just outputs whatever a monolith backend asks it to output. Right? This couldn't be further from the truth. In fact, if you look at the quality of the apps on the PlayStore , you will notice that the distribution of the apps by their quality describes a Gaussian distribution : There are a few very poor apps, just as few very good apps a LOT of mediocre apps. Let me try to define what I mean by mediocrity here: At the very fundamental level, a mediocre mobile app is one that does not cooperate well with the environment (platform/OS) it is running on. First, it does not comply with the visual language defined by the platform and therefore it confuses users. Second, it does not integrate the fact that it is running in a constrained environment (memory, CPU, network bandwidth, battery) and therefore it ruins the user experience of the entire device. Third, it just doesn't work under certain conditions (faulty network for example). This final point is global to the majority of softwares out there. In a nutshell, the three points above sum up the challenges of building good mobile apps. On top of that, the app needs to integrate properly with your company infrastructure and it has to be coded with a huge volume of constantly evolving business domain problems. Therefore, if I had to hire a Software Engineer to work on these challenges today, this is what I would look for: Decent software engineering and Craftmanship skills. That includes topics like: SOLID principles, writing clean code, application architecture and testing. Data-structures and Algorithms are important too, but I would not hold against a candidate if s/he does not know how to balance a BST. I definitely care though whether the candidate knows at least the basic data-structures (Lists, Queues, Trees, Hash Maps...), understand what the Big-O notation is and be able to talk about time vs memory tradeoff. See? Just the basic stuff. Proficiency in Java (though if the candidate is outstanding and knows another language very well, I would not hold it against her/him). Understanding of concurrency. Because the apps need to be responsive, it is important not to perform any long running task on the event thread. That means, that a lot of things will need to be performed on a separate thread. If the candidate does not know the challenges of building concurrent apps, chances are they will learn about it on the job and in production ! High level understanding of distributed systems. As mobile apps need to become more and more responsive, they will inherently be designed with an offline first approach. That means, that as the network can be faulty, the app can be in different states for the same user. Hence, the need to know about the CAP Theorem and the notion of consensus in distributed systems. Ability to analyze problems and solve them. Does the candidate know how to think through a problem, uncover all the unknowns unknowns, discover all the paths leading to a solution, comparing them one against another, come up with a plan to implement the solution. Ability to describe the tech stack of their current company. It is important that you hire candidates who are curious about how things work and who understand the importance of knowing the big picture. Good verbal and writing skills. In technology, communication is key . It is crucial that the candidates know how to convey their ideas clearly, as well as the importance of properly documenting their work. Also, chances are that their writing skills represent a good indicator of their ability to write good code. Understanding of the current technological context and how they affect design and engineering. Finally, I would not worry about how much the candidate knows about the SDK itself. As long as: the candidate is good with most of the above concepts, the candidate knows the Android platform as a user at least and is motivated by mobility in general and finally the candidate is a fast learner. That being said, if you are lucky enough to find a candidate that has both a great CS background and pretends to know the Android development ecosystem, make sure they understand some fundamental concepts. Such concepts include (not exhaustive): Ability to draw on the board, what, at a high level the Android architecture looks like. Ability to describe, at a high level, what happens when you hit the build button on Android Studio. Ability to describe, at a high level, what happens when you install an application on the device. Basic understanding of how Dalvik and ART VMs work. How Inter-Process-Communication works on Android. How apps are sandboxed and why it matters. The permission models (how permissions are granted at a low level). Processes and application lifecycle. Another good rule of thumb is to challenge, heavily, the candidate on a topic that s/he is good at. If that's UI, then cover that extensively. As you can see the emphasis is put on verifying that the candidates know the core fundamentals and concepts that indicates whether they can adapt to any set of problems that you might throw at them. For crying out loud, don't hire a candidate just because they know how to use one or two libraries. You don't want to hire library users , but engineers who not only can use them when necessary (better not reinvent the wheel of course), but are also able to take a step back, analyse the situation, foresee problems way before they arise and provide solutions which are unique to the problem set you are having. For crying out loud, don't hire a candidate just because they know how to use one or two libraries. As Montaigne said: ""J'aime mieux une tête bien faite, qu'une tête bien pleine"" ( which can be translated as : ""I prefer a sharp mind over a full one""). Of course, the candidate I am describing here is a rather senior one. You can, and you should, also hire junior engineers. But in that case, make sure that you keep a good ratio between junior and senior engineers in your organization. If you have a team mostly composed of junior engineers, you are in for a tumultuous journey as your team will learn a lot of things on the job, fixing one critical production bug after another, iterating over and over again until they can get a decent maintainable and testable code base etc.",en,53
305,1859,1469380300,CONTENT SHARED,-860751600992244121,-4028919343899978105,6836573531327580278,,,,HTML,http://www.tecmundo.com.br/roteador/107633-pega-marte-tp-link-lanca-extensor-wifi-tem-alcance-3-km.htm,pega até em marte! tp-link lança extensor de wifi que tem alcance de 3 km²!,"A TP-Link é famosa por suas soluções de rede, que incluem desde adaptadores comuns até roteadores de alta potência - e nós até já testamos alguns roteadores da marca que nos impressionaram bastante. Agora, inovando mais uma vez, a fabricante lança um extensor que promete ser a solução definitiva para quem está cansado de problemas com o alcance do sinal WiFi. O modelo RE590T trabalha com o padrão 802.11ac, sendo capaz de alcançar velocidades que chegam a até 1.900 Mbps, combinando as frequências do sistema dual-band. Graças aos amplificadores de 700 mW, o aparelho pode entregar alta potência na transmissão do sinal, que, segundo informação da fabricante, chega a 3 km² de cobertura. Além desta qualidade excepcional, o extensor TP-Link RE590T conta com tela sensível ao toque, a qual dá acesso direto às configurações, de modo que o usuário não necessita usar um computador para realizar os ajustes no aparelho. O dispositivo ainda vem com processador dual-core, sendo recomendado para curtir jogos e filmes em 4K. Especificações do TP-Link RE590T Modo: extensor wireless Padrão: 802.11ac Frequência: dual-band (2,4 GHz e 5 GHz) Velocidade: até 600 Mbps (2,4 GHz) e até 1.300 Mbps (5 GHz) Antenas: 3 x dual-band Touch Screen: sim (capacitiva) Tamanho do display: 4,3 polegadas Resolução do display: 128 ppi Dimensões: 28,4 x 15,7 x 5,2 cm O TP-Link RE590T já está disponível para venda nos Estados Unidos por US$ 129,95 (R$ 423,31), mas ainda não há previsão de lançamento do aparelho aqui no Brasil.",pt,53
306,1558,1467128196,CONTENT SHARED,1796618361228441836,-1836083230511905974,-6287954573666675020,,,,HTML,https://medium.com/android-dev-br/fresco-sim-cda40fabae82,"fresco, sim! - android dev br","Toda vez que vemos alguma biblioteca nova de carregamento de imagem, nos perguntamos: outra? Com certeza não foi diferente com o Fresco que, por um tempo, sofreu bastante ""chacota"" da comunidade pelo simples motivo: Picasso's API has 24 classes. . Fresco's API has 24 packages. Mas, antes de rir, vamos entender o motivo de tudo isso. Nascimento Nós, usuários do Facebook e Instagram, exigimos exibição rápida e eficiente de imagem ao abrirmos nossos aplicativos. O Fresco nasceu exatamente com essa função: exibir a imagem de maneira eficiente, não importando o device ou conexão. Mas, qual exatamente é o problema? Bem, as imagens geralmente são grandes e os devices "" pequenos "". Precisamos exibir uma imagem com a resolução 2560 × 1600 , com 4mb de tamanho, em um device com resolução desconsiderável. Mesmo com todo o processamento de imagem disponível, nos parece bem caótico, não é? Isso significa, no universo Android, que um simples device de 480 x 800 pixels, pode ocupar 1.5mb da dentro da sua tão preciosa e ""suada"" memória. E nós já estamos cansados de saber o que acontece quando essa memória é excedida: Memória Falando em memória, o que torna o Fresco diferente das outras bibliotecas? Antes, vamos entender melhor as regiões de memória do nosso aparelho. A famosa Java heap é região de memória delimitada por aplicação (ou seja, cada aplicação tem seu limite, isoladamente) definida pelo fabricante, por exemplo: G1 = 16 Mb Droid = 24 Mb Xoom = 48 Mb GalaxyTab = 64 Mb. Sabe o famoso "" new "" dentro do nosso código? Pois bem, todo "" new "" vai dentro dessa região de memória que é considerada ""segura"", já que o garbage-collecto r controla tudo isso com transparência para nós. Mas, esse "" garbage-collecting "" é exatamente o problema. Isso por quê ao invés de apenas recuperar partes da memória, o Android deve ""travar"" a aplicação completamente ao mesmo tempo que realiza a ""coleta de lixo"". E esse é uma das causas mais comuns de uma aplicação que aparece para congelar ou parar momentaneamente enquanto o usuário está utilizando, e já estamos cansados de saber o que o usuário faz quando isso acontece. Por outro lado, temos o chamado native heap que é usado pelo "" new "" do C++ e temos muito mais memória disponível aqui. Na verdade, a única limitação é a memória física disponível no device. Não há garbage-collection e nada que nos dê a sensação de lentidão. Porém, quando utilizamos dessa memória, temos a responsabilidade de liberar cada byte que utilizamos lá dentro. Caso contrário, teremos coisas "" piores "" acontecerão. Por último, o Android tem uma região de memória chamado , que opera praticamente igual ao native heap , mas possui algumas chamadas de sistemas extras. O Android pode fazer "" unpin "" da memória ao invés de liberá-la. E ele só faz isso quando necessário: a memória é liberada apenas se o sistema realmente precisar de mais memória. Quando o Android faz o "" pin "" da memória novamente, os dados antigos ainda estarão lá se o mesmo não foi liberado. A memória não pode ser diretamente manipulada através do código Java. Ela pode ser utilizada apenas em algumas situações excepcionais, e é destas situações que o tira vantagem. Não irei entrar em detalhes da sua implementação, mas você pode entender seu funcionamento no link no final deste post. Pois bem, sabendo como lidar e como acessar essa região da memória, o Fresco , como disse, tem uma outra postura para carregar as imagens. Pipeline ""It's more than a loader - it's a pipeline"" , é seu mantra para carregar uma imagem eficientemente. Nessa representação que percebemos a diferença do para as outras bibliotecas: O motivo dela ter 24 packages ao invés de 24 classes é que cada passo é tratado de maneira mais isolada e independente possível, o que nos dá um poder muito grande quando se diz respeito de ter ""controle em cada passo"" do carregamento da imagem. Utilização Ok, já sabemos como o Fresco opera. Agora, vamos ver como uma imagem simples é carregada. Primeiro, quando carregamos uma imagem, nós precisamos de um placeholder para ela e, quando acontece alguma falha, precisamos de um placeholder que indique isso. Também, há situações de quando a imagem termina de ser carregada, queremos fazer alguma animação nela tal como um fade-in ou até aplicar alguma regra de redimensionamento para exibirmos no tamanho correto. Só de ler esse parágrafo, você provavelmente já assimilou alguma necessidade que você venha a ter no seu dia a dia, e que todos nós temos. Mas, para suportar esse tipo de ""funcionalidade"", tudo precisa ser rápido, suave e performático. Para fazer isso, o utiliza um esquema de "" Drawees "" (um MVP-like, só que voltado pra imagens), que separa as camadas da imagem em três etapas. A camada de ""modelo"" é chamada DraweeHierarchy , que implementa uma hierarquia de Drawables, que cada um se aplica a uma função diferente a imagem subjacente. Os DraweeControllers se conectam com o image pipeline (ou qualquer outro loader) e fica responsável pela parte de backend. É ele quem recebe as respostas do servidor e decide o que fazer com eles. Por último temos o DraweeViews , que tem um papel bem específico. Esses ficam ""ouvindo"" eventos do sistema que informa que aquela view não está mais sendo exibida na tela. Quando isso acontece, o DraweeView informa pro DraweeController que por sua vez ""fecha"" todos os usos daquela imagem, evitando qualquer perda de memória. Também, vai informar ao pipeline para cancelar o request da imagem, caso não tenha sido terminada ainda - como por exemplo em listas. Criando uma galeria Para mostrar como tudo isso funciona, criei um projeto para e exibição das imagens dentro do device. É um ótimo caso de uso, já que dentro do device existem fotos de diversos tamanhos e tipos. Primeiramente, precisamos inicializar o através da nossa Application : Depois de criar nosso RecyclerView para exibição, precisamos criar cada item da imagem. Para isso, vamos utilizar o SimpleDraweeView , que já irá contemplar todas as ferramentas necessárias para fazer o carregamento da imagem. Aqui, podemos explorar a imagem já do XML. Cheque a documentação para saber até onde podemos customizar aqui. Feito isso, precisamos carregar nossa imagem. Para isso, precisamos definir qual o path (url ou local) da imagem. No , fazemos isso através do método setImageURI() ; Somente dessa maneira, se rodássemos nossa aplicação, já iremos ver nossa imagem sendo exibida (cacheada, otimizada etc etc etc). Mas, como estamos tratando de uma galeria, precisamos fazer algumas configurações a mais para garantir performance. Vamos alterar nosso DraweeController para adicionar algumas configurações: O resultado dessa implementação, é esse: Já está bom, não é? Mas, ainda não é um resultado desejado. Perceba que, ao fazermos o scroll , ainda há um engasgo para carregar as imagens, e também elas não estão sendo carregadas rapidamente. E é aqui que o Fresco se mostra poderoso: temos controle para diminuir a qualidade da imagem que irá ser exibida apenas com algumas linhas de código! Lembra na nossa Application ? Podemos colocar um ""configurador"" para o , que irá refletir em todas as imagens selecionadas: Resultado? Temos um scroll totalmente suave e performático. Faça o teste! Rode no seu próprio device e veja o resultado. Configurações extras OkHttp O tem suporte ao OkHttp como image pipeline ! Para isso, precisamos adicionar uma dependência extra: E, na nossa Application, adicionar o OkHttpClient: Outros formatos Uma grande reclamação era que você era obrigado a incluir todas as funcionalidades e funções que o tem para oferecer. Mas, a partir da versão , você consegue ""granular"" o que você quer usar: Conclusão O Fresco é uma biblioteca que, após seu entendimento e compreensão do que pode ser feito com ela, nos garante entregar uma experiência muito boa para o usuário visualizar suas imagens. Então, dê uma chance ao Fresco e comente nesse post suas impressões! Links",pt,53
307,483,1461081671,CONTENT SHARED,3268064929368559554,3609194402293569455,-2458546884542949853,,,,HTML,http://tableless.com.br/iniciando-com-o-docker-dicas-praticas-para-comecar-usar-agora-mesmo/,iniciando com o docker: dicas práticas para começar a usar agora mesmo - tableless,"por André Kiffer Nós no Elasticpush utilizamos Docker para criar nossos ambientes de desenvolvimento, não que o Docker seja somente para isso, aliás, a sua principal vantagem é poder ter a mesma imagem da sua máquina e em produção. Mas enquanto não reestruturamos as coisas para rodar o Docker em produção utilizamos suas vantagens no ambiente de desenvolvimento o que já nos trás uma série de benefícios como: Facilidade de configuração do ambiente de novos membros da equipe Ter diversas versões da mesma biblioteca rodando sem conflito para testes pontuais Poder trabalhar em outros projetos sem comprometer os recursos da máquina e sem a necessidade de levantar uma Máquina Virtual inteira somente para isso. Acabar com a história do ""Na minha máquina funcionava"" (Caso rode em produção também) Versionar a configuração necessária para certo serviço rodar. Para que outros desenvolvedores não precisarem fazer tudo novamente. No inicio não foi tão simples ver essas vantagens e nem entender como as coisas realmente funcionavam por trás do Docker, por isso, gostaria de compartilhar alguns dos principais comandos para você poder brincar e ir se familiarizando com a ideia de usar essa plataforma. O que é o Docker Docker é uma plataforma open-source escrita em GO cuja finalidade é criar ambientes isolados para aplicações e serviços. Com esse isolamento o docker garante que cada container tenha tudo que um serviço precisa para ser executado. Uma das vantagens dessa abordagem é você poder iniciar esse serviço em qualquer máquina que sempre irá rodar da forma esperada, com bibliotecas, dependências e permissões configuradas da forma correta, sem surpresas. Quem trabalha com DevOps sabe o quanto isso facilita o dia a dia, ao invés de você criar um snapshot da máquina inteira, você pode ter vários containers docker rodando na mesma máquina podendo substituí-los ou adicionar novos a medida que for achando necessário. Sem contar na facilidade de reuso, pois você pode usar serviços idênticos em diferentes projeto como por exemplo: Nginx, Elasticsearch, Kafka, PHP-FPM, entre outros. Instalando o Docker Para distribuições Linux é muito simples de se instalar o Docker, já quem usa OSX ou Windows precisa rodar o docker dentro de uma Máquina Virtual. root@ubuntu:~$ apt-get install docker.io Comandos Básicos Agora que você já tem o Docker instalado em sua máquina, vamos ver os principais comandos que você precisa para poder baixar imagens e executar containers. Para o docker poder baixar uma imagem é necessário que ela esteja no Docker Hub ou em algum outro registry. Por padrão se nenhum outro registry for especificado a imagem será baixada do Docker Hub . No Docker Hub , existem milhares de imagens disponíveis que a comunidade foi contribuindo e que facilitam muito a vida do desenvolvedor. Você pode montar um ambiente completo em questão de minutos baixando as imagens que precisar e ir linkando umas com as outras. Em um outro post explicarei uma forma de versionar essa linkagem de containers usando Docker Compose. rroot@ubuntu:~$ docker pull [nome da imagem]; #Baixa a imagem. Quando você executa o comando acima, o docker irá baixar a imagem e deixar salva em sua máquina. Ela ainda não estará rodando, pois é só a imagem o que vai ser executado é um container, você pode executar vários containers da mesma imagem. rroot@ubuntu:~$ docker images; #Lista todas as imagens baixadas root@ubuntu:~$ docker run [nome da imagem]; #Inicia um container da imagem que você escolheu. O comando docker run tem diversos parâmetros que você pode passar como: volume ( -v ) para você mapear uma pasta da máquina pra dentro do container, qual porta ( -p ) você quer externar para que a máquina consiga fazer requisições dentro do container, enfim são várias configurações possíveis. É interessante você pegar alguns exemplos práticos nas próprias descrições das imagens para entender o funcionamento geral e ver a documentação oficial para entender a utilidade de cada parâmetro. rroot@ubuntu:~$ docker ps; # Lista os containers em execução root@ubuntu:~$ docker exec [container id] [comando]; #Executa comandos dentro do container root@ubuntu:~$ docker stop $(docker ps -aq); #Para a execução de todos dos containers root@ubuntu:~$ docker rm $(docker ps -aq); #Exclui todos os containers criados Docker na prática Aqui vai um comando prontinho pra você executar um script em uma linguagem que não tem na sua máquina. Eu escolhi o ruby: rroot@ubuntu:~$ echo puts 'Tableless' > 'hello_ruby.rb' #Criando um arquivo .rb que imprime no console a frase ""Tableless"". root@ubuntu:~$ docker run -v ""$(pwd)"":/var/ruby -w /var/ruby google/ruby sh -c 'ruby hello_ruby.rb' Executando esse docker run, caso você não tenha a imagem google/ruby baixada na sua máquina o próprio docker vai se encarregar disso, fazendo o download e iniciando o container. A opção -v seguida do valor ""$(pwd)"":/var/ruby mapeia a pasta atual (que você está executando o comando no terminal) onde criei o arquivo hello_ruby.rb para a pasta /var/ruby dentro container do docker. A opção -w seta de qual path vai partir a execução do comando no nosso caso parte de /var/ruby , o google/ruby é o nome da imagem do ruby que vamos usar e o sh -c do unix para executar o seguinte comando ' ruby hello_ruby.rb '. Façam o teste na máquina de vocês e vejam como é simples rodar um script em uma linguagem que não estava configurada em seu ambiente. Note que por traz de toda simplicidade do Docker existe uma ferramenta poderosa que está mudando a forma com o que as aplicações são distribuídas hoje em dia, além de ser um grande aliado para configuração do seu ambiente de desenvolvimento. Em um outro post, irei mostrar passo a passo o desenvolvimento de um app para mostrar como que seria a aplicação do docker em um ambiente real, nele mostrarei como criar suas próprias imagens usando o arquivo manifesto Dockerfile e também, um pouco de Docker compose . Hoje mudei a forma de como eu instalo serviços na minha máquina, sempre que vejo uma ferramenta interessante busco primeiro no Docker hub , baixo executo e se for relevante deixo ali para quando for usar, caso contrário removo, já se tornou parte do meu dia a dia e tenho certeza que pra você também fará diferença. Caso tenha alguma dúvida, ou queira compartilhar sua experiência com docker, fique à vontade para deixar um comentário. Publicado no dia",pt,53
308,1597,1467296667,CONTENT SHARED,4383035933260676803,-9047547311469006438,-5303937001498940622,,,,HTML,http://liberal.com.br/economia/pedagio-das-rodovias-estaduais-de-sp-sobe-932-na-sexta-feira/,"pedágio das rodovias estaduais de sp sobe 9,32% na sexta-feira - o liberal","A partir do dia 1º de julho, as tarifas de pedágio das rodovias estaduais paulistas serão reajustadas em 9,32%, informou nesta quarta-feira, 29, a Agência de Transporte do Estado de São Paulo (Artesp). O reajuste anual é baseado no IPC-A acumulado dos últimos 12 meses, de acordo com o estipulado nos contratos de concessão. A Artesp também autorizou o início da cobrança de pedágio na Rodovia dos Tamoios a partir do dia 1º de julho. A agência diz que a concessionária cumpriu as exigências definidas para a operação das praças. ""Para o início da cobrança era necessária a execução de no mínimo 6% das obras de duplicação do trecho de serra, além de várias outras obrigações"", afirma. LEIA TAMBÉM: Dólar tem leve alta ante o real em meio a leilão do BC As praças dos quilômetros 15,7 e 56,6 passarão a cobrar, respectivamente, R$ 3,50 e R$ 6,20. Conforme a Artesp, a tarifa é parte da remuneração que irá viabilizar a construção de 21,6 quilômetros de novas pistas para a duplicação do trecho de serra da rodovia - obra orçada em R$ 2,6 bilhões. O Sistema de Transporte Intermunicipal Rodoviário de Passageiros também passa a ter novo valor a partir da zero hora do dia 5 de julho. O porcentual de reajuste é de 9,56%.",pt,53
309,1808,1468848733,CONTENT SHARED,7459643459148487877,-6316613156648676087,-7630718902283542343,,,,HTML,http://www1.folha.uol.com.br/mercado/2016/07/1792668-grupo-de-bancos-adere-a-tecnologia-do-bitcoin-em-transacoes.shtml,grupo de bancos adere à tecnologia do bitcoin em transações,"Um grupo de sete bancos que inclui o Santander, o CIBC e o UniCredit anunciou um grande avanço tecnológico. Eles estão entre as primeiras instituições financeiras do planeta a movimentar dinheiro real internacionalmente usando tecnologia baseado no sistema blockchain. O blockchain é uma base de dados com cópias idênticas distribuídas por diferentes computadores e controlado por diferentes entidades, as partes envolvidas naquelas transações, sem um órgão que sirva de autoridade central. Espécie de livro de registros virtual, a tecnologia, está por trás da moeda virtual bitcoin. No mês passado, os bancos anunciaram que tomaram parte em projetos de realização de pagamentos internacionais, usando ativos digitais na plataforma da Ripple, empresa de San Francisco que trabalha com compensação digital de transações. Em lugar de executar transações por meio de contas em moeda local em bancos correspondentes de todo o planeta, os bancos convertem fundos à moeda do Ripple, conhecida como XRP, e em seguida concluem a transação quase automaticamente. Até o momento, o processo de compensação era lento -demorando de três a cinco dias, em muitos casos- e sujeito e erros. Os sete bancos que assinaram representam ""um grande marco, talvez um ponto de inflexão"", disse Chris Larsen, presidente-executivo e cofundador da Ripple. A tecnologia baseada em sistema blockchain capturou a imaginação do setor de serviços financeiros nos dois últimos anos, com diversas empresas buscando maneiras de utilizá-lo a fim de reduzir os custos e reformular suas operações cotidianas. O Conselho de Fiscalização da Estabilidade Financeira, que congrega diversas autoridades regulatórias dos EUA, declarou em sua revisão anual que sistemas como o da Ripple podem reduzir o custo de transações e aumentar a eficiência da intermediação financeira. No entanto, o conselho acrescentou que esses sistemas ""apresentam certos riscos e incertezas"". O alerta surgiu depois que a DAO, uma organização descentralizada autônoma que usa o blockchain da moeda virtual ethereum, foi atacada por hackers que roubaram cerca de US$ 60 milhões. O grupo de bancos que usam o Ripple inclui também o suíço UBS, o alemão ReiseBank, o National Bank of Abu Dhabi e o ATB Financial, do Canadá. Tim Wan, diretor de inovação do ATB, em Calgary, disse que seu banco havia começado a usar a plataforma para processar pequenos pagamentos em valor máximo de alguns milhares de dólares, mas que queria experimentar com compensações em maior volume, operando por ""lotes"" -e que pretendia usá-la para ingressar em novas áreas de negócios, tais como fornecer liquidez em larga escala aos mercados de câmbio. ""No fim, o que a tecnologia tem de melhor é que ela é uma grande democratizadora"", disse Wan. ""Você não confia na instituição, mas na transação. Se os fundos existirem, a transação acontecerá.""",pt,53
310,2340,1473870933,CONTENT SHARED,-8377626164558006982,-5527145562136413747,5228941642832000454,,,,HTML,https://hbr.org/2016/09/bad-writing-is-destroying-your-companys-productivity,bad writing is destroying your company's productivity,"A hidden source of friction is slowing your company down. Your workers are complicit in it. So is your management. And it's driving everybody nuts. It's bad business writing. I surveyed 547 businesspeople in the first three months of this year. I looked specifically at people who write at least two hours per week in addition to email. They told me that they spend an average of 25.5 hours per week reading for work. (About a third of that is email.) And 81% of them agree that poorly written material wastes a lot of their time. A majority say that what they read is frequently ineffective because it's too long, poorly organized, unclear, filled with jargon, and imprecise. Entry-level employees get little training in how to write in a brief, clear, and incisive way. Instead, they're immersed in first-draft emails from their managers, poorly edited reports, and jargon-filled employee manuals. Their own flabby writing habits fit right in. And the whole organization drowns in productivity-draining blather. Consider: Vague writing dilutes leadership. Yahoo has suffered from dithering management focus for a decade. Now CEO Marissa Mayer has agreed to sell it to Verizon. Here's a passage from her recent email to staff on that occasion : ""...our incredibly loyal and dedicated employee base has stepped up to every challenge along the way....The teams here have not only built incredible products and technologies, but have built Yahoo into one of the most iconic, and universally well-liked companies in the world....I'm incredibly proud of everything that we've achieved, and I'm incredibly proud of our team. I love Yahoo, and I believe in all of you."" That's four uses of ""incredible"" or ""incredibly"" in a single paragraph. All that cheerleading reads like misdirection. It's going to be challenging for Yahoo to continue to succeed as part of Verizon, and happy, vacuous language certainly won't inspire the workers who haven't quit yet. (The rest of the email is similarly vague.) Contrast this to how Apple's Tim Cook communicates - as in his clear, jargon-free defense of the company's decision not to crack the encryption on a terrorist's iPhone. Clear leadership, expressed in writing, creates alignment and boosts productivity. For example, in writing email, managers from the CEO on down must set an example by communicating exactly what they want, clearly, in the subject line or title and the first two sentences of everything they write. The workers reading it will just skip to the key facts anyway, so lose the filler and don't waste their time. Do this right, and you'll get a reputation for truth. Your workers won't waste time on the Kremlinology of reading your intentions; they'll get to work on accomplishing the goals you set out for them. Clarity in marketing tells customers - and workers - that they can trust you. How do your marketers and PR people communicate? Do they put out press releases filled with industry jargon and meaningless superlatives? When clarity and truth are core values for marketers, they can spend time trumpeting what works, rather than concealing what doesn't. For example, here's what Google writes about how it treats customers : Focus on the user and all else will follow. Since the beginning, we've focused on providing the best user experience possible. Whether we're designing a new internet browser or a new tweak to the look of the homepage, we take great care to ensure that they will ultimately serve you, rather than our own internal goal or bottom line. Every customer can understand that, and it rings true. It inspires workers as well. So marketers and the rest of the company can move forward in a united and productive way. Fuzzy writing allows fuzzy thinking. Clear writing uses well-organized, active-voice sentences to explain what is happening, what ought to happen, and what people need to do. Conversely, inexact and passive language reflects gaps in thinking. A great example is the report that the UMass Donahue Institute published about the economics of hosting the 2024 Olympics in Boston. Its passive-voice analysis hid who was responsible for important parts of the bid, with sentences like these: [These] are issues that will need to be closely monitored in order to ensure the public sector is protected from extensive financial commitments. To date, using insurance to protect a host city from cost overruns has not been used extensively. Questions like who would monitor expenses and who would secure hypothetical insurance loomed over the bid. In the end, these uncertainties caused the citizens and political leaders of Boston to reject an Olympic bid. Requiring clear, direct, active language has two benefits. It forces writers to think through what they really mean and the arguments they can use to support it. And it makes smart people stand out. If you prize clarity, the clear thinkers will rise to the top. A culture of clear writing makes managers more productive. It means that the material that ends up on your desk will be clearer too. Senior managers can waste time rooting through their subordinates' fuzzy writing, or they can spend effort changing the culture to one that prizes brevity, clarity, and directness. That's worth the effort, because it means everyone in the organization - especially management - will end up more productive. It's time to clear all the crap out of your inboxes and make those 25.5 hours per week more efficient. It's time to commit to a culture of clarity. It could make a big difference in how smoothly your business runs - and it could make your day a lot less annoying.",en,53
311,2711,1478201280,CONTENT SHARED,2106559900295325351,3829784524040647339,-4992934825056158280,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36",SP,BR,HTML,https://medium.com/@bartobri/applying-the-linus-tarvolds-good-taste-coding-requirement-99749f37684a,"applying the linus torvalds ""good taste"" coding requirement","To the best of my ability to discern, the crux of the ""good taste"" requirement is the elimination of edge cases, which tend to reveal themselves as conditional statements. The fewer conditions you test for, the better your code "" tastes"" . Here is one particular example of an improvement I made that I wanted to share. Initializing Grid Edges Below is an algorithm I wrote to initialize the points along the edge of a grid, which is represented as a multidimensional array: grid[rows][cols] . Again, the purpose of this code was to only initialize the values of the points that reside on the edge of the grid - so only the top row, bottom row, left column, and right column. To accomplish this I initially looped over every point in the grid and used conditionals to test for the edges. This is what it looked like: Even though it works, in hindsight, there are some issues with this construct. Complexity  - The use 4 conditional statements inside 2 embedded loops seems overly complex. Efficiency  - Given that GRID_SIZE has a value of 64, this loop performs 4096 iterations in order to set values for only the 256 edge points. Linus would probably agree, this is not very tasty . So I did some tinkering with it. After a little bit I was able to reduce the complexity to only a single for - loop containing four conditionals. It was only a slight improvement in complexity, but a large improvement in performance, because it only performed 256 loop iterations, one for each point along the edge. An improvement, yes. But it looked really ugly. It's not exactly code that is easy to follow. Based on that alone, I wasn't satisfied. I continued to tinker. Could this really be improved further? In fact, the answer was YES . And what I eventually came up with was so astoundingly simple and elegant that I honestly couldn't believe it took me this long to find it. Below is the final version of the code. It has one for-loop and no conditionals . Moreover, the loop only performs 64 iterations. It vastly improves both complexity and efficiency. This code initializes four different edge points for each loop iteration. It's not complex. It's highly efficient. It's easy to read. Compared to the original version, and even the second version, they are like night and day. I was quite satisfied.",en,53
312,860,1462811069,CONTENT SHARED,-5037827401452845772,5683029675627635125,-4824397180889193568,,,,HTML,http://tableless.com.br/quando-o-scrum-ira-falhar/,quando o scrum irá falhar - tableless,"por marcoaacoliveira Nesse artigo vamos abordar um pouco os pontos críticos do Scrum, a intenção aqui não é explicar o framework (o Scrum é considerado um framework para gestão de projetos) em si, mas dar um panorama dos casos mais comuns de falhas críticas ao se tentar adotar o Scrum ou modificá-lo. Caso esteja procurando um artigo para entender melhor o que é o Scrum, recomendo esse aqui , escrito pela Dani Guerrato. Ou ainda, caso queira mergulhar de cabeça recomendo a leitura do Scrum Guide (em português). Introdução feita, vamos ao que realmente interessa aqui: O Scrum não é uma bala de prata (a essa altura você já deve saber que nenhuma metodologia ágil é). Se você, assim como eu, já foi (ou é) responsável por trazê-lo para sua empresa, esteja atento ao seu ambiente e aprenda com a implementação do Scrum. Citarei aqui e explicarei sobre cada um dos pontos críticos do Scrum: Quando a comunicação não é suficiente Esse talvez seja o mais óbvio ponto crítico do Scrum. Em qualquer projeto, a comunicação se faz necessária, mas diferente de outras metodologias de gerenciamento de projeto, o Scrum delega a responsabilidade do gerenciamento à própria equipe de desenvolvimento e daí a comunicação se torna ainda mais crítica. Como uma equipe pode chegar a um consenso e se autogerenciar se não há uma boa (ou nenhuma) comunicação? Sem comunicação, não há gerenciamento. Portanto Scrum Master (ou não), fique atento a esse ponto, ao perceber pequenos atritos não resolvidos, desgaste ou qualquer sinal de que a comunicação do time de desenvolvimento, Product Owner ou até mesmo a sua, não esta indo bem. Trabalhe em cima disso. Existe um artefato que tem como objetivo garantir uma comunicação mais fluída e um gerenciamento mais democrático e dinâmico: Planning Poker. Nele todos os membros da equipe de desenvolvimento jogam cartas para determinar o peso de cada estória de usuário. Ao jogarem cartas distintas para uma mesma User Story, é discutido (geralmente entre os jogadores que colocaram os dois pontos mais distantes) o motivo deles avaliarem aquela estória daquela forma; e Mike Cohn defende que você considere uma estória como avaliada somente quando todos do time chegarem a um consenso quanto ao peso daquela estória. Isso garante o diálogo, além de melhorar o comprometimento e a motivação. Quando a motivação acaba Como garantir algo tão subjetivo durante todo o projeto? Esse é um problema grave não só no Scrum, mas em qualquer outra metodologia ágil. E a resposta é como esperado: você não garante! Então nada pode ser feito para que sua equipe se motive? Não, o fato de que motivação é algo subjetivo, não quer dizer que você não possa prover as ferramentas ou o ambiente necessário para sua equipe continuar seguindo motivada. Vou considerar aqui que os colaboradores com quem você convive já são pessoas motivadas por natureza, que é o que se espera num ambiente ágil, e que é necessário apenas manter a motivação. Então vou citar aqui os principais motivos que levam um colaborador a se desmotivar durante o projeto: Stakeholders não comprometidos, mas envolvidos apenas com cobranças desalinhadas. Sabe aquele cliente que nunca comparece aos eventos Scrum e um belo dia resolve ficar a par do projeto? Esse cliente geralmente vai acabar com sua Sprint, vai destruir a blindagem do seu time de desenvolvimento e deixará seu Product Owner (caso não seja ele mesmo) louco. Tenha um cuidado especial com o efeito negativo desse stakeholder na motivação do seu time. Projetos longos que não se renovam. Não me entenda mal. As metodologias ágeis existem com o lema de que mudanças são sempre bem vindas, mas se um projeto se alonga por tempo demasiado você irá perceber um certo desânimo na sua equipe. Principalmente se são mudanças que exigem uma série de atividades desgastantes e que acabam levando o projeto a situações do passado (desfazer estórias de usuário é uma das mais críticas). Equipe muito pouco reconhecida por projetos. Nada pior que uma entrega ou finalização de projeto com sucesso, onde o time scrum (PO, SM e equipe de desenvolvimento) não é nem sequer reconhecido pelo seu bom trabalho. É válido destacar aqui que o desenvolvimento ágil tem como objetivo entregar produtos com muito mais valor ao cliente, contudo é mais desgastante para os comprometidos com o projeto. Por isso eu recomendo a qualquer gestor que ao término de cada projeto crie sua maneira de bonificar (isso varia de acordo a sua cultura organizacional) a equipe. Lembre-se sempre de que coisas simples como uma pizza, um happy hour ou qualquer outro tipo de confraternização é suficiente para levantar o moral e deixar sua equipe pronta para o próximo projeto. Quando os pontos negativos individuais interferem no trabalho em equipe Como saber se sua equipe trabalha bem em time? Em alguns casos temos uma equipe madura o suficiente para termos um trabalho em equipe direto e enxuto. Mas quando isso não acontece? Quando um membro do time insiste em ser prolixo nas reuniões? Ou quando questões pequenas consomem mais tempo de discussão do que realmente importa? Quem é o responsável por fazer essas coisas funcionarem? Bem, considerando que temos um líder-servo como papel do Scrum (Scrum Master), cai sobre ele a responsabilidade com toda sua habilidade para servir e liderar ao mesmo tempo lidar com essa situação. Uma boa abordagem é fazer mentorias (aconselhamentos ou tutorias) individuais em pequenas reuniões (bate-papos). Faça com que todo o processo seja o menos assustador possível para aquele que deve ter pontos negativos trabalhados, lembre-se que motivação é uma das chaves do desenvolvimento ágil e você não quer perder um membro motivado em pleno desenvolvimento do projeto, certo? Portanto, o recomendável é fazer da forma mais sutil possível: entenda os motivos que levam o membro a agir daquela forma e através de sugestões e conselhos mostre como ele pode ser mais produtivo para o time como um todo. Lembre-se, o Scrum Master deve ter um excelente conhecimento de sua equipe, logo, essas pequenas mentorias e bate-papos devem ser feitos constantemente. Quando o PO não encontra o cliente Essa é a maior falha em todo projeto, seja em uma metodologia ágil ou clássica. Nesse cenário seu time entrega o que promete com excelência, as cerimonias são cumpridas com precisão, o seu burndown encontra-se impecável. Mas nada do que é entregue é realmente um agregador de valor ao cliente final e provavelmente isso se dará quando o seu Product Owner estiver falando com o Stakeholder errado! Pegando um exemplo simples e claro: Imagine que sua empresa está desenvolvendo um sistema de ponto para uma fábrica, o dono da empresa é seu principal Stakeholder e o Product Owner mantém constante contato com ele. Porém a cada finalização de Sprint você nota que muito trabalho teve que voltar ao backlog ou teve que ser reescrito em outras estórias de usuário. O exemplo ilustra um sinal claro, que um PO atento logo se alarmara, de que é hora de trazer um novo Stakeholder à frente do projeto e ouvi-lo falar sobre qual será o produto ideal para ele. É muito comum, porém, que nem sempre o seu cliente final seja alguém próximo. Ou até mesmo que seu Stakeholder nem sequer o conheça! Aqui entram algumas técnicas do desenvolvimento Lean que eu acho bastante interessantes e que pode ser muito útil para sua empresa e seu trabalho receberem recomendações futuras por salvar a vida de um Stakeholder perdido. Se você já vem trabalhando com Scrum, para entender o Lean é apenas mais alguns passos adiante, por tanto irei abordar aqui rapidamente, mas caso tenha interesse deixarei esse link para maior aprofundamento. O Lean basicamente propõe que você desenvolva um MVP (Minimo Produto Viável, em português), para entrar num ciclo de Construir, Medir e Aprender. Isso tudo feito em pequenas iterações onde cada ciclo completo você tem uma nova iteração 100% funcional (até agora bem familiar, correto?). O ponto principal para você que acabou tendo que conhecer o cliente final do seu cliente, sem mesmo que ele o conheça, é que a cada etapa de Medir e Aprender você deve focar em conhecer mais sobre o usuário final: Teste A/B, Personas, teste presenciais de um possível usuário final, heatmap e muitas outras são técnicas e ferramentas válidas para esse momento. Se você não conhece nenhum dos nomes citados anteriormente, convido-o para que faça uma pausa e dê uma lida, ou ao menos separe alguns materiais para uma lida no futuro. Quando o time não é seu próprio gerente E quando o projeto inteiro é negociado sem conhecimento algum do time de desenvolvimento? E ao avaliar as estórias de usuários se dão conta de que o projeto iria demorar cerca de 6 meses, mas que a negociação e o contrato foram fechados para um prazo de 2 meses? Aqui está claro o problema: o time de desenvolvimento não é seu próprio gerente, suas cerimônias e artefatos estão perdidos! (Meio dramático, eu sei) Mas isso serve para reforçar que esse é um dos momentos mais frustrantes para a equipe de desenvolvimento, pois não será refeita a negociação e sim serão apertados os prazos e as estimativas serão direcionadas por fatores externos. Se você está percebendo um grande problema aqui, você provavelmente já pegou bem a ideia do Scrum... O time de desenvolvimento não mais se encontra blindado e será constantemente influenciado por fatores externos, tais como: cliente, diretores, gerentes ou qualquer nível hierárquico que sua empresa possa possuir. Isso é grave... Isso é muito grave! Se você pode evitar essa situação faça! Caso contrário futuramente será totalmente irremediável durante a execução do projeto e provavelmente fará com que seu time constantemente acabe passando algumas etapas da iteração por cima de outras, levando à um projeto concluído com baixa qualidade e que possivelmente gerará dores de cabeça para seu cliente e por consequência para você em um futuro muito próximo. Obrigado por ler até aqui, espero ter apresentado pontos a serem analisados por você e evitados! Caso tenha mais algum ponto crítico que acha interessante que seja adicionado a esse artigo, elogio ou crítica deixe um comentário &#x1f600; Publicado no dia",pt,52
313,2686,1477935425,CONTENT SHARED,1649752043999819668,5127372011815639401,-4463843219651655599,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36",MG,BR,HTML,http://thenewstack.io/top-four-items-operations-performance-team-know-implementing-node-js/,four node.js gotchas that operations teams should know about - the new stack,"There is no doubt that Node.js is one of the fastest growing platforms today. It can be found at start-ups and enterprises throughout all industries from high-tech to healthcare. A lot of people have written about the reasons for its popularity and why it has made sense in ""digital transformation"" efforts. But when you implement Node.js, do you have to replace your mainframes and legacy software with a shiny new Node.js-based microservice architecture? Let's zoom out and walk in the shoes of those who oversee the whole digital value chain: operation and performance teams. What challenges do operation and performance teams face today when they begin to implement Node.js? Does it require an entire gutting of their system? New Tier / New Paradigm / New Challenges In many cases, Node.js acts as a new tier that augments the enterprise stack and connects it with new offerings. It's the fast moving technology at the edge of the system. One often embraced benefit of Node.js is that it enables teams to move much faster. Add microservices and suddenly there are multiple deployments per day compared to one every few weeks. For many enterprises, this introduces a new paradigm and requires changes to processes that affect other parts of the organization, particularly with those that are in charge of availability and performance, i.e. the operations and performance teams. These teams don't consist of Node.js experts and don't have to. They are driven by metrics like mean time to repair (MTTR). Their main concern is to find the root cause of performance degradations and outages fast. How can these teams make sure the transition to Node.js goes smoothly based on their bottom line? How can they keep their systems humming? Below we've listed out a few common Node.js problems that occur when you introduce it in the enterprise, and how best to manage and solve these problems. Top Node.js Problems and How to Track Them Down Node.js applications in enterprise scenarios are rather simple. Common use cases are: Fetching data from backends. Performing authentication for incoming requests. Rendering views. Node.js uses Google V8 - the JavaScript engine of Chrome - as a runtime and a library called libuv that provides an event loop to perform asynchronous tasks. All of that is abstracted away from the user by a well-defined JavaScript API - not much can go wrong, e.g. there is no way to introduce thread locking issues and generally tracking down root causes is easier than in other platforms. Still, some typical problem sources need to be watched closely. 1. Memory Leaks Node.js is more similar to Java when it comes to runtime behavior. It's a long running process and because of this, it is prone to memory leaks of all kinds. Like in other platform, memory leaks materialize in a steadily growing heap usage, which causes a crash when the maximum allocate able heap is exhausted. Often this is accompanied by high garbage collector churn while the runtime desperately tries to free memory. Possible causes can be as simple as large objects that are hooked to the root scope and hence never freed. But, there are also more difficult cases caused by so-called closures (functions that rely on their enclosing scope) giving the garbage collector a hard time to dereference the dependencies. There are also cases where the host simply has too low of a memory configuration causing the garbage collector not to run in time. To track down memory leaks, heap dumps are the tool of choice. There are several modules that export V8 hooks to JavaScript. Using them, it is fairly easy to trigger a dump whenever certain memory thresholds are exceeded. Here is an example that uses simple anomaly detection and utilizes the module v8-profiler to create dump files that can be consumed by Chrome Developer Tools . 2. CPU Problems Node.js runs in a single thread. Hence it's not a good fit for CPU-heavy operations. If the CPU is occupied, e.g. because it's transforming a large chunk of JSON - no other requests can be handled during this time. Netflix - a big Node.js shop - had such a problem when an automated script created routes without disposing of the old ones, causing the routing table to fill up over time. At some point, discovering the right function to call for an incoming request took so much time that it severely affected performance. Read their blog post about that . Node.js out-of-the-box comes with hooks to switch on CPU sampling - the data produced by the sampler can then be consumed by various tools. Using this data, it is rather easy to find out where the time is spent. Like for memory introspection, there are several ways to capture CPU samples from within JavaScript to analyze them in various tools. Here is an example that uses v8-profiler again. This time for getting CPU sampling data to find out what was on the CPU at a given time slice. 3. Back Pressure When Node.js acts as a gluing tier connecting different parts of the stack, problems down the stack may surface first in Node.js. Back pressure occurs, when Node.js dispatches requests to slow backends. While Node.js has excellent capabilities for performing outbound requests, slow backends can cause congestion of the machinery waiting for those requests to come back. Degraded performance and even exceptions can be the result. The metric to look at in this case is the number of dispatched vs. the number of returning requests at any given time. Such problems can only be tracked down to its root cause by using a monitoring solution that traces transactions passing through all tiers, providing metrics about inter-tier communication. Every major vendor in the APM space today provides agents that monitor requests going in and out to and from Node.js. 4. Security Node.js offers a huge repository of small composable modules . Using the Node.js package manager (npm), it is a matter of seconds to add modules to a project, well-known frameworks like HAPI or Express build on them, and it would be highly inefficient to relinquish their use completely. Still, every module installed is third party code. It can be poorly maintained and contain bugs that are never fixed or - even worse - security issues. Before using a module, a developer should always check its quality and make sure that it's not trivial enough to be done themselves. To tackle the problem, many enterprises also run their own, private npm repository where only packages that went through some auditing process can be found. Tools like the Node Security Platform or Snyk can streamline this process by using exploit databases to find and fix possible security issues in installed modules. Outlook The Node.js diagnostics and the post mortem working groups solely focus on ways to extend and unify the tracing and debugging capabilities within Node.js. A few highlights from them include: A new tracing facility is around the corner. It will allow low overhead process level tracing. There are current initiatives to unify the way core dumps can be analyzed. With async-hooks, there will be finally a generic way to accomplish long stack traces and transactional tracing through callbacks. Given the current pace of development and how active the community is driving performance topics; Node.js enterprise capabilities will make another leap in 2017. Summary Very often Node.js applications are small and not complex. Communication between tiers, memory leaks and CPU congestion can cause issues. Luckily the platform isn't a black box, and for every problem, there are ways to introspect running applications to find the root cause. Monitoring is a topic taken seriously by the Node.js project, and within the next releases, additional ways to trace, debug and monitor Node.js will be introduced adding, even more, capabilities to fix problems fast. The next time your development team wants to implement Node.js, have no fear, Ops.",en,52
314,810,1462459785,CONTENT SHARED,9220445660318725468,7703284633155187943,-4918613109487129089,,,,HTML,https://angular.io/styleguide,angular 2,"Welcome to the Angular 2 Style Guide Purpose If you are looking for an opinionated style guide for syntax, conventions, and structuring Angular applications, then step right in. The purpose of this style guide is to provide guidance on building Angular applications by showing the conventions we use and, more importantly, why we choose them. Style Vocabulary Each guideline describes either a good or bad practice, and all have a consistent presentation. The wording of each guideline indicates how strong the recommendation is. Do is one that should always be followed. Always might be a bit too strong of a word. Guidelines that literally should always be followed are extremely rare. On the other hand, we need a really unusual case for breaking a Do guideline. Consider guidelines should generally be followed. If you fully understand the meaning behind the guideline and have a good reason to deviate, then do so. Please strive to be consistent. Avoid indicates something we should almost never do. Code examples to avoid have an unmistakeable red header. File Structure Conventions Some code examples display a file that has one or more similarly named companion files. (e.g. hero.component.ts and hero.component.html). The guideline will use the shortcut hero.component.ts|html|css|spec to represent those various files. Using this shortcut makes this guide's file structures easier to read and more terse. Table of Contents Single Responsibility We apply the Single Responsibility Principle to all Components, Services, and other symbols we create. This helps make our app cleaner, easier to read and maintain, and more testable. Rule of One Style 01-01 Do define one thing (e.g. service or component) per file. Consider limiting files to 400 lines of code. Why? One component per file makes it far easier to read, maintain, and avoid collisions with teams in source control. Why? One component per file avoids hidden bugs that often arise when combining components in a file where they may share variables, create unwanted closures, or unwanted coupling with dependencies. Why? A single component can be the default export for its file which facilitates lazy loading with the Component Router. The key is to make the code more reusable, easier to read, and less mistake prone. The following negative example defines the AppComponent , bootstraps the app, defines the Hero model object, and loads heroes from the server ... all in the same file. Don't do this . AVOID: app/heroes/hero.component.ts /* avoid */ import { bootstrap } from '@angular/platform-browser-dynamic'; import { Component, OnInit } from '@angular/core'; class Hero { id: number; name: string; } @Component({ selector: 'my-app', template: ` <h1>{<!-- -->{title}}</h1> <pre>{<!-- -->{heroes | json}}</pre> `, styleUrls: ['app/app.component.css'] }) class AppComponent implements OnInit { title = 'Tour of Heroes'; heroes: Hero[] = []; ngOnInit() { getHeroes().then(heroes => this.heroes = heroes); } } bootstrap(AppComponent, []); const HEROES: Hero[] = [ {id: 1, name: 'Bombasto'}, {id: 2, name: 'Tornado'}, {id: 3, name: 'Magneta'}, ]; function getHeroes(): Promise<Hero[]> { return Promise.resolve(HEROES); // TODO: get hero data from the server; } Better to redistribute the component and supporting activities into their own dedicated files. import { bootstrap } from '@angular/platform-browser-dynamic'; import { AppComponent } from './app.component'; bootstrap(AppComponent, []); // app.component.ts import { Component } from '@angular/core'; import { HeroesComponent } from './heroes/heroes.component'; import { HeroService } from './heroes/shared/hero.service'; @Component({ selector: 'toh-app', template: ` <toh-heroes></toh-heroes> `, styleUrls: ['app/app.component.css'], directives: [HeroesComponent], providers: [HeroService] }) export class AppComponent { } import { Component, OnInit } from '@angular/core'; import { Hero } from './shared/hero.model'; import { HeroService } from './shared/hero.service'; @Component({ selector: 'toh-heroes', template: ` <pre>{<!-- -->{heroes | json}}</pre> ` }) export class HeroesComponent implements OnInit { heroes: Hero[] = []; constructor(private heroService: HeroService) {} ngOnInit() { this.heroService.getHeroes() .then(heroes => this.heroes = heroes); } } import { Injectable } from '@angular/core'; import { HEROES } from './mock-heroes'; @Injectable() export class HeroService { getHeroes() { return Promise.resolve(HEROES); } } export class Hero { id: number; name: string; } import { Hero } from './hero.model'; export const HEROES: Hero[] = [ {id: 1, name: 'Bombasto'}, {id: 2, name: 'Tornado'}, {id: 3, name: 'Magneta'}, ]; As the app grows, this rule becomes even more important. Back to top Small Functions Style 01-02 Do define small functions Consider limiting to no more than 75 lines. Why? Small functions are easier to test, especially when they do one thing and serve one purpose. Why? Small functions promote reuse. Why? Small functions are easier to read. Why? Small functions are easier to maintain. Why? Small functions help avoid hidden bugs that come with large functions that share variables with external scope, create unwanted closures, or unwanted coupling with dependencies. Back to top Naming Naming conventions are hugely important to maintainability and readability. This guide recommends naming conventions for the file name and the symbol name. General Naming Guidelines Style 02-01 Do use consistent names for all symbols. Why? Naming conventions help provide a consistent way to find content at a glance. Consistency within the project is vital. Consistency with a team is important. Consistency across a company provides tremendous efficiency. Why? The naming conventions should simply help us find our code faster and make it easier to understand. Back to top Separate File Names with Dots and Dashes Style 02-02 Do use dashes to separate words. Do use dots to separate the descriptive name from the type. Do use conventional suffixes for the types including *.service.ts , *.component.ts , *.pipe.ts . Invent other suffixes where desired, but take care in having too many. Why? Provides a consistent way to quickly identify what is in the file. Why? Provides a consistent way to quickly find a specific file using an editor or IDE's fuzzy search techniques. Why? Provides pattern matching for any automated tasks. Back to top Components and Directives Style 02-03 Do use consistent names for all assets named after what they represent. Do use upper camel case for symbols. Match the name of the symbol to the naming of the file. Do append the symbol name with the suffix that it represents. Why? Provides a consistent way to quickly identify and reference assets. Why? Upper camel case is conventional for identifying objects that can be instantiated using a constructor. Back to top Service Names Style 02-04 Do use consistent names for all services named after their feature. Do use upper camel case for services. Why? Provides a consistent way to quickly identify and reference services. Back to top Bootstrapping Style 02-05 Why? Follows a consistent convention for the startup logic of an app. Why? Follows a familiar convention from other technology platforms. Back to top Directive Selectors Style 02-06 Do Use lower camel case for naming the selectors of our directives. Why? Keeps the names of the properties defined in the directives that are bound to the view consistent with the attribute names. Why? The Angular 2 HTML parser is case sensitive and will recognize lower camel case. Back to top Custom Prefix for Components Style 02-07 Do use a custom prefix for the selector of our components. For example, the prefix toh represents from T our o f H eroes and the prefix admin represents an admin feature area. Do use a prefix that identifies the feature area or the app itself. Why? Prevents name collisions. Why? Makes it easier to promote and share our feature in other apps. Why? Our Components and elements are easily identified. AVOID: app/heroes/hero.component.ts /* avoid */ // HeroComponent is in the Tour of Heroes feature @Component({ selector: 'hero' }) export class HeroComponent {} AVOID: app/users/users.component.ts /* avoid */ // UsersComponent is in an Admin feature @Component({ selector: 'users' }) export class UsersComponent {} app/heroes/hero.component.ts @Component({ selector: 'toh-hero' }) export class HeroComponent {} app/users/users.component.ts @Component({ selector: 'admin-users' }) export class UsersComponent {} Custom Prefix for Directives Style 02-08 Do use a custom prefix for the selector of our directives (for instance below we use the prefix toh from T our o f H eroes). Why? Prevents name collisions. Why? Our Directives are easily identified. AVOID: app/shared/validate.directive.ts /* avoid */ @Directive({ selector: '[validate]' }) export class ValidateDirective {} app/shared/validate.directive.ts @Directive({ selector: '[tohValidate]' }) export class ValidateDirective {} Back to top Pipe Names Style 02-09 Do use consistent names for all pipes, named after their feature. Why? Provides a consistent way to quickly identify and reference pipes. Back to top Unit Test File Names Style 02-10 Do name test specification files the same as the component they test. Why? Provides a consistent way to quickly identify tests. Why? Provides pattern matching for karma or other test runners. Back to top End to End Test File Names Style 02-11 Why? Provides a consistent way to quickly identify end-to-end tests. Why? Provides pattern matching for test runners and build automation. Back to top Coding Conventions Have consistent set of coding, naming, and whitespace conventions. Classes Style 03-01 Do use upper camel case when naming classes. Why? Follows conventional thinking for class names. Why? Classes can be instantiated and construct an instance. We often use upper camel case to indicate a constructable asset. AVOID: app/shared/exception.service.ts /* avoid */ export class exceptionService { constructor() { } } app/shared/exception.service.ts export class ExceptionService { constructor() { } } Back to top Constants Style 03-02 Do use uppercase with underscores when naming constants. Why? Follows conventional thinking for constants. Why? Constants can easily be identified. AVOID: app/shared/data.service.ts /* avoid */ export const heroesUrl = 'api/heroes'; export const villainsUrl = 'api/villains'; app/shared/data.service.ts export const HEROES_URL = 'api/heroes'; export const VILLAIN_URL = 'api/villains'; Back to top Interfaces Style 03-03 Do name an interface using upper camel case. Why? When we use types, we can often simply use the class as the type. AVOID: app/shared/hero-collector.service.ts /* avoid */ import { Injectable } from '@angular/core'; import { IHero } from './hero.model.avoid'; @Injectable() export class HeroCollectorService { hero: IHero; constructor() { } } app/shared/hero-collector.service.ts import { Injectable } from '@angular/core'; import { Hero } from './hero.model'; @Injectable() export class HeroCollectorService { hero: Hero; constructor() { } } Back to top Properties and Methods Style 03-04 Do use lower camel case to name properties and methods. Avoid prefixing private properties and methods with an underscore. Why? Follows conventional thinking for properties and methods. Why? JavaScript lacks a true private property or method. Why? TypeScript tooling makes it easy to identify private vs public properties and methods. AVOID: app/shared/toast/toast.service.ts /* avoid */ import { Injectable } from '@angular/core'; @Injectable() export class ToastService { message: string; private _toastCount: number; hide() { this._toastCount--; this._log(); } show() { this._toastCount++; this._log(); } private _log() { console.log(this.message); } } app/shared/toast/toast.service.ts import { Injectable } from '@angular/core'; @Injectable() export class ToastService { message: string; private toastCount: number; hide() { this.toastCount--; this.log(); } show() { this.toastCount++; this.log(); } private log() { console.log(this.message); } } Back to top Import Destructuring Spacing Style 03-05 Why? Whitespace makes it easier to read the imports. AVOID: app/+heroes/shared/hero.service.ts /* avoid */ import {Injectable} from '@angular/core'; import {Http, Response} from '@angular/http'; import {Hero} from './hero.model'; import {ExceptionService, SpinnerService, ToastService} from '../../shared'; app/+heroes/shared/hero.service.ts import { Injectable } from '@angular/core'; import { Http, Response } from '@angular/http'; import { Hero } from './hero.model'; import { ExceptionService, SpinnerService, ToastService } from '../../shared'; Back to top Import Line Spacing Style 03-06 Do leave one empty line between third party imports and imports of code we created. Do list import lines alphabetized by the module. Do list destructured imported assets alphabetically. Why? The empty line makes it easy to read and locate imports. Why? Alphabetizing makes it easier to read and locate imports. AVOID: app/+heroes/shared/hero.service.ts /* avoid */ import { ExceptionService, SpinnerService, ToastService } from '../../shared'; import { Http, Response } from '@angular/http'; import { Injectable } from '@angular/core'; import { Hero } from './hero.model'; app/+heroes/shared/hero.service.ts import { Injectable } from '@angular/core'; import { Http, Response } from '@angular/http'; import { Hero } from './hero.model'; import { ExceptionService, SpinnerService, ToastService } from '../../shared'; Back to top Application Structure Have a near term view of implementation and a long term vision. Start small but keep in mind where the app is heading down the road. All of the app's code goes in a folder named app . All content is 1 feature per file. Each component, service, and pipe is in its own file. All 3rd party vendor scripts are stored in another folder and not in the app folder. We didn't write them and we don't want them cluttering our app. Use the naming conventions for files in this guide. Back to top LIFT Style 04-01 Do define the structure to follow these four basic guidelines, listed in order of importance. Why? LIFT Provides a consistent structure that scales well, is modular, and makes it easier to increase developer efficiency by finding code quickly. Another way to check our app structure is to ask ourselves: How quickly can we open and work in all of the related files for a feature? Back to top Locate Style 04-02 Do make locating our code intuitive, simple and fast. Why? We find this to be super important for a project. If we cannot find the files we need to work on quickly, we will not be able to work as efficiently as possible, and the structure will need to change. We may not know the file name or where its related files are, so putting them in the most intuitive locations and near each other saves a ton of time. A descriptive folder structure can help with this. Back to top Identify Style 04-03 Do name the file such that we instantly know what it contains and represents. Do be descriptive with file names and keep the contents of the file to exactly one component. Avoid files with multiple components, multiple services, or a mixture. Why? We spend less time hunting and pecking for code, and become more efficient. If this means we want longer file names, then so be it. There are deviations of the 1 per file rule when we have a set of very small features that are all related to each other, as they are still easily identifiable. Back to top Flat Style 04-04 Do keep a flat folder structure as long as possible. Consider creating fodlers when we get to seven or more files. Why? Nobody wants to search seven levels of folders to find a file. In a folder structure there is no hard and fast number rule, but when a folder has seven to ten files, that may be time to create subfolders. We base it on our comfort level. Use a flatter structure until there is an obvious value (to help the rest of LIFT) in creating a new folder. Back to top T-DRY (Try to be DRY) Style 04-05 Do be DRY (Don't Repeat Yourself) Avoid being so DRY that we sacrifice readability. Back to top Overall Structural Guidelines Style 04-06 Do start small but keep in mind where the app is heading down the road. Do have a near term view of implementation and a long term vision. Why? Helps us keep the app structure small and easy to maintain in the early stages, while being easy to evolve as the app grows. Overall Folder and File Structure While we prefer our Components to be in their own dedicated folder, another option for small apps is to keep Components flat (not in a dedicated folder). This adds up to four files to the existing folder, but also reduces the folder nesting. Be consistent. Back to top Style 04-07 Why? Separates shared files from the components within a feature. Why? Makes it easier to locate shared files within a component feature. Back to top Folders-by-Feature Structure Style 04-08 Do create folders named for the feature they represent. Why? A developer can locate the code, identify what each file represents at a glance, the structure is as flat as it can be, and there is no repetitive nor redundant names. Why? The LIFT guidelines are all covered. Why? Helps reduce the app from becoming cluttered through organizing the content and keeping them aligned with the LIFT guidelines. Why? When there are a lot of files (e.g. 10+) locating them is easier with a consistent folder structures and more difficult in flat structures. Below is an example of a small app with folders per component. Back to top Layout Components Style 04-09 Why? We need a place to host our layout for our app. Our navigation bar, footer, and other aspects of the app that are needed for the entire app. Why? Organizes all layout in a consistent place re-used throughout the application. Folder for Layout Components Back to top Create and Import Barrels Style 04-10 Do create a file that imports, aggregates, and re-exports items. We call this technique a barrel . Why? A barrel aggregates many imports into a single import. Why? A barrel reduces the number of imports a file may need. Why? A barrel shortens import statements. export * from './config'; export * from './entity.service'; export * from './exception.service'; export * from './filter-text'; export * from './init-caps.pipe'; export * from './modal'; export * from './nav'; export * from './spinner'; export * from './toast'; export * from './filter-text.component'; export * from './filter-text.service'; export * from './modal.component'; export * from './modal.service'; export * from './nav.component'; export * from './spinner.component'; export * from './spinner.service'; export * from './toast.component'; export * from './toast.service'; AVOID: app/+heroes/heroes.component.ts /* avoid */ import { Component, OnInit } from '@angular/core'; import { CONFIG } from '../shared/config'; import { EntityService } from '../shared/entity.service'; import { ExceptionService } from '../shared/exception.service'; import { FilterTextComponent } from '../shared/filter-text/filter-text.component'; import { InitCapsPipe } from '../shared/init-caps.pipe'; import { SpinnerService } from '../shared/spinner/spinner.service'; import { ToastService } from '../shared/toast/toast.service'; @Component({ selector: 'toh-heroes', templateUrl: 'app/+heroes/heroes.component.html' }) export class HeroesComponent implements OnInit { constructor() { } ngOnInit() { } } app/+heroes/heroes.component.ts import { Component, OnInit } from '@angular/core'; import { CONFIG, EntityService, ExceptionService, FilterTextComponent, InitCapsPipe, SpinnerService, ToastService } from '../shared'; @Component({ selector: 'toh-heroes', templateUrl: 'app/+heroes/heroes.component.html' }) export class HeroesComponent implements OnInit { constructor() { } ngOnInit() { } } Back to top Lazy Loaded Folders Style 04-11 A distinct application feature or workflow may be lazy loaded or loaded on demand rather than when the application starts. Do put the contents of lazy loaded features in a lazy loaded folder . A typical lazy loaded folder contains a routing component , its child components, and their related assets and modules. Why? The folder makes it easy to identify and isolate the feature content. Back to top Prefix Lazy Loaded Folders with + Style 04-12 Do prefix the name of a lazy loaded folder with a (+) e.g., +dashboard/ . Why? Lazy loaded code paths are easily distinguishable from non lazy loaded paths. Why? If we see an import path that contains a + , we can quickly refactor to use lazy loading. Back to top Never Directly Import Lazy Loaded Folders Style 04-13 Avoid allowing modules in sibling and parent folders to directly import a module in a lazy loaded feature . Why? Directly importing a module loads it immediately when our intention is to load it on demand. AVOID: app/app.component.ts import { HeroesComponent } from './+heroes'; Back to top Lazy Loaded Folders May Import From a Parent Style 04-14 Do allow lazy loaded modules to import a module from a parent folder. Why? A parent module has already been loaded by the time the lazy loaded module imports it. app/heroes/heroes.component.ts import { Logger } from '../shared/logger.service'; Back to top Use Component Router to Lazy Load Style 04-15 Do use the Component Router to lazy load routable features. Why? That's the easiest way to load a module on demand. Back to top Components Components Selector Naming Style 05-02 Why? Keeps the element names consistent with the specification for Custom Elements . AVOID: app/heroes/shared/hero-button/hero-button.component.ts /* avoid */ @Component({ selector: 'tohHeroButton' }) export class HeroButtonComponent {} @Component({ selector: 'toh-hero-button' }) export class HeroButtonComponent {} <toh-hero-button></toh-hero-button> Back to top Components as Elements Style 05-03 Do define Components as elements via the selector. Why? Components have templates containing HTML and optional Angular template syntax. They are most associated with putting content on a page, and thus are more closely aligned with elements. Why? Components are derived from Directives, and thus their selectors can be elements, attributes, or other selectors. Defining the selector as an element provides consistency for components that represent content with a template. Why? It is easier to recognize that a symbol is a component vs a directive by looking at the template's html. AVOID: app/heroes/hero-button/hero-button.component.ts /* avoid */ @Component({ selector: '[tohHeroButton]' }) export class HeroButtonComponent {} AVOID: app/heroes/hero-button/hero-button.component.html <!-- avoid --> <div tohHeroButton></div> @Component({ selector: 'toh-hero-button' }) export class HeroButtonComponent {} <toh-hero-button></toh-hero-button> Back to top Extract Template and Styles to Their Own Files Style 05-04 Do extract templates and styles into a separate file, when more than 3 lines. Why? Syntax hints for inline templates in ( .js and .ts) code files are not supported by some editors. Why? A component file's logic is easier to read when not mixed with inline template and styles. AVOID: app/heroes/heroes.component.ts /* avoid */ @Component({ selector: 'toh-heroes', template: ` <div> <h2>My Heroes</h2> <ul class=""heroes""> <li *ngFor=""let hero of heroes""> <span class=""badge"">{<!-- -->{hero.id}}</span> {<!-- -->{hero.name}} </li> </ul> <div *ngIf=""selectedHero""> <h2>{<!-- -->{selectedHero.name | uppercase}} is my hero</h2> </div> </div> `, styleUrls: [` .heroes { margin: 0 0 2em 0; list-style-type: none; padding: 0; width: 15em; } .heroes li { cursor: pointer; position: relative; left: 0; background-color: #EEE; margin: .5em; padding: .3em 0; height: 1.6em; border-radius: 4px; } .heroes .badge { display: inline-block; font-size: small; color: white; padding: 0.8em 0.7em 0 0.7em; background-color: #607D8B; line-height: 1em; position: relative; left: -1px; top: -4px; height: 1.8em; margin-right: .8em; border-radius: 4px 0 0 4px; } `] }) export class HeroesComponent implements OnInit { heroes: Hero[]; selectedHero: Hero; ngOnInit() {} } @Component({ selector: 'toh-heroes', templateUrl: 'heroes.component.html', styleUrls: ['heroes.component.css'] }) export class HeroesComponent implements OnInit { heroes: Hero[]; selectedHero: Hero; ngOnInit() { } } <div> <h2>My Heroes</h2> <ul class=""heroes""> <li *ngFor=""let hero of heroes""> <span class=""badge"">{<!-- -->{hero.id}}</span> {<!-- -->{hero.name}} </li> </ul> <div *ngIf=""selectedHero""> <h2>{<!-- -->{selectedHero.name | uppercase}} is my hero</h2> </div> </div> .heroes { margin: 0 0 2em 0; list-style-type: none; padding: 0; width: 15em; } .heroes li { cursor: pointer; position: relative; left: 0; background-color: #EEE; margin: .5em; padding: .3em 0; height: 1.6em; border-radius: 4px; } .heroes .badge { display: inline-block; font-size: small; color: white; padding: 0.8em 0.7em 0 0.7em; background-color: #607D8B; line-height: 1em; position: relative; left: -1px; top: -4px; height: 1.8em; margin-right: .8em; border-radius: 4px 0 0 4px; } Back to top Decorate Input and Output Properties Inline Style 05-12 Do place the @Input() or @Output() on the same line as the property they decorate. Why? It is easier and more readable to identify which properties in a class are inputs or outputs. Why? If we ever need to rename the property or event name associated to or we can modify it on a single place. Why? The metadata declaration attached to the directive is shorter and thus more readable. Why? Placing the decorator on the same line makes for shorter code and still easily identifies the property as an input or output. AVOID: app/heroes/shared/hero-button/hero-button.component.ts /* avoid */ @Component({ selector: 'toh-hero-button', template: `<button></button>`, inputs: [ 'label' ], outputs: [ 'change' ] }) export class HeroButtonComponent { change = new EventEmitter<any>(); label: string; } app/heroes/shared/hero-button/hero-button.component.ts @Component({ selector: 'toh-hero-button', template: `<button>OK</button>` }) export class HeroButtonComponent { @Output() change = new EventEmitter<any>(); @Input() label: string; } Back to top Avoid Renaming Inputs and Outputs Style 05-13 Avoid renaming inputs and outputs, when possible. Why? May lead to confusion when the output or the input properties of a given directive are named a given way but exported differently as a public API. AVOID: app/heroes/shared/hero-button/hero-button.component.ts /* avoid */ @Component({ selector: 'toh-hero-button', template: `<button>{<!-- -->{label}}</button>` }) export class HeroButtonComponent { @Output('changeEvent') change = new EventEmitter<any>(); @Input('labelAttribute') label: string; } AVOID: app/app.component.html <!-- avoid --> <toh-hero-button labelAttribute=""OK"" (changeEvent)=""doSomething()""> </toh-hero-button> @Component({ selector: 'toh-hero-button', template: `<button>{<!-- -->{label}}</button>` }) export class HeroButtonComponent { @Output() change = new EventEmitter<any>(); @Input() label: string; } <toh-hero-button label=""OK"" (change)=""doSomething()""> </toh-hero-button> Back to top Member Sequence Style 05-14 Do place properties up top followed by methods. Do place private members after public members, alphabetized. Why? Placing members in a consistent sequence makes it easy to read and helps we instantly identify which members of the component serve which purpose. AVOID: app/shared/toast/toast.component.ts /* avoid */ export class ToastComponent implements OnInit { private defaults = { title: '', message: 'May the Force be with You' }; message: string; title: string; private toastElement: any; ngOnInit() { this.toastElement = document.getElementById('toh-toast'); } // private methods private hide() { this.toastElement.style.opacity = 0; window.setTimeout(() => this.toastElement.style.zIndex = 0, 400); } activate(message = this.defaults.message, title = this.defaults.title) { this.title = title; this.message = message; this.show(); } private show() { console.log(this.message); this.toastElement.style.opacity = 1; this.toastElement.style.zIndex = 9999; window.setTimeout(() => this.hide(), 2500); } } app/shared/toast/toast.component.ts export class ToastComponent implements OnInit { // public properties message: string; title: string; // private fields private defaults = { title: '', message: 'May the Force be with You' }; private toastElement: any; // public methods activate(message = this.defaults.message, title = this.defaults.title) { this.title = title; this.message = message; this.show(); } ngOnInit() { this.toastElement = document.getElementById('toh-toast'); } // private methods private hide() { this.toastElement.style.opacity = 0; window.setTimeout(() => this.toastElement.style.zIndex = 0, 400); } private show() { console.log(this.message); this.toastElement.style.opacity = 1; this.toastElement.style.zIndex = 9999; window.setTimeout(() => this.hide(), 2500); } } Back to top Put Logic in Services Style 05-15 Do limit logic in a component to only that required for the view. All other logic should be delegated to services. Do move reusable logic to services and keep components simple and focused on their intended purpose. Why? Logic may be reused by multiple components when placed within a service and exposed via a function. Why? Logic in a service can more easily be isolated in a unit test, while the calling logic in the component can be easily mocked. Why? Removes dependencies and hides implementation details from the component. Why? Keeps the component slim, trim, and focused. AVOID: app/heroes/hero-list/hero-list.component.ts /* avoid */ import { OnInit } from '@angular/core'; import { Http, Response } from '@angular/http'; import { Observable } from 'rxjs/Observable'; import { Hero } from '../shared/hero.model'; const heroesUrl = ' '; export class HeroListComponent implements OnInit { heroes: Hero[]; constructor(private http: Http) {} getHeroes() { this.heroes = []; this.http.get(heroesUrl) .map((response: Response) => <Hero[]>response.json().data) .catch(this.catchBadResponse) .finally(() => this.hideSpinner()) .subscribe((heroes: Hero[]) => this.heroes = heroes); } ngOnInit() { this.getHeroes(); } private catchBadResponse(err: any, source: Observable<any>) { // log and handle the exception return new Observable(); } private hideSpinner() { // hide the spinner } } app/heroes/hero-list/hero-list.component.ts import { Component, OnInit } from '@angular/core'; import { Hero, HeroService } from '../shared/index'; @Component({ selector: 'toh-hero-list', template: `...` }) export class HeroListComponent implements OnInit { heroes: Hero[]; constructor(private heroService: HeroService) {} getHeros() { this.heroes = []; this.heroService.getHeroes() .subscribe(heroes => this.heroes = heroes); } ngOnInit() { this.getHeros(); } } Back to top Don't Prefix Output Properties Style 05-16 Why? This is consistent with built-in events such as button clicks. Why? Angular allows for an alternative syntax on-* . If the event itself was prefixed with on this would result in an on-onEvent binding expression. AVOID: app/heroes/hero.component.ts /* avoid */ @Component({ selector: 'toh-hero', template: `...` }) export class HeroComponent { @Output() onSavedTheDay = new EventEmitter<boolean>(); } AVOID: app/app.component.html <!-- avoid --> <toh-hero (onSavedTheDay)=""onSavedTheDay($event)""></toh-hero> export class HeroComponent { @Output() savedTheDay = new EventEmitter<boolean>(); } <toh-hero (savedTheDay)=""onSavedTheDay($event)""></toh-hero> Back to top Put Presentation Logic in the Component Class Style 05-17 Do put presentation logic in the Component class, and not in the template. Why? Logic will be contained in one place (the Component class) instead of being spread in two places. Why? Keeping the component's presentation logic in the class instead of the template improves testability, maintainability, and reusability. AVOID: app/heroes/hero-list/hero-list.component.ts /* avoid */ @Component({ selector: 'toh-hero-list', template: ` <section> Our list of heroes: <hero-profile *ngFor=""let hero of heroes"" [hero]=""hero""> </hero-profile> Total powers: {<!-- -->{totalPowers}}<br> Average power: {<!-- -->{totalPowers / heroes.length}} </section> ` }) export class HeroListComponent { heroes: Hero[]; totalPowers: number; } app/heroes/hero-list/hero-list.component.ts @Component({ selector: 'toh-hero-list', template: ` <section> Our list of heroes: <hero-profile *ngFor=""let hero of heroes"" [hero]=""hero""> </hero-profile> Total powers: {<!-- -->{totalPowers}}<br> Average power: {<!-- -->{avgPower}} </section> ` }) export class HeroListComponent { heroes: Hero[]; totalPowers: number; get avgPower() { return this.totalPowers / this.heroes.length; } } Back to top Directives Back to top Use Directives to Enhance an Existing Element Style 06-01 Do use attribute directives when you have presentation logic without a template. Why? Attributes directives don't have an associated template. Why? An element may have more than one attribute directive applied. app/shared/highlight.directive.ts @Directive({ selector: '[tohHighlight]' }) export class HighlightDirective { @HostListener('mouseover') onMouseEnter() { // do highlight work } } <div [tohHightlight]>Bombasta</div> Back to top Use HostListener and HostBinding Class Decorators Style 06-03 Do use @HostListener and @HostBinding instead of the host property of the @Directive and @Component decorators: Why? The property or method name associated with @HostBinding or respectively @HostListener should be modified only in a single place - in the directive's class. In contrast if we use host we need to modify both the property declaration inside the controller, and the metadata associated to the directive. Why? The metadata declaration attached to the directive is shorter and thus more readable. AVOID: app/shared/validate.directive.ts /* avoid */ @Directive({ selector: '[tohValidator]', host: { '(mouseenter)': 'onMouseEnter()', 'attr.role': 'button' } }) export class ValidatorDirective { role = 'button'; onMouseEnter() { // do work } } app/shared/validate.directive.ts @Directive({ selector: '[tohValidator]' }) export class ValidatorDirective { @HostBinding('attr.role') role = 'button'; @HostListener('mouseenter') onMouseEnter() { // do work } } Back to top Services Services are Singletons in Same Injector Style 07-01 Do use services as singletons within the same injector. Use them for sharing data and functionality. Why? Services are ideal for sharing methods across a feature area or an app. Why? Services are ideal for sharing stateful in-memory data. app/heroes/shared/hero.service.ts export class HeroService { constructor(private http: Http) { } getHeroes() { return this.http.get('api/heroes') .map((response: Response) => <Hero[]>response.json().data); } } Back to top Single Responsibility Style 07-02 Do create services with a single responsibility that is encapsulated by its context. Do create a new service once the service begins to exceed that singular purpose. Why? When a service has multiple responsibilities, it becomes difficult to test. Why? When a service has multiple responsibilities, every Component or Service that injects it now carries the weight of them all. Back to top Providing a Service Style 07-03 Do provide services to the Angular 2 injector at the top-most component where they will be shared. Why? The Angular 2 injector is hierarchical. Why? When providing the service to a top level component, that instance is shared and available to all child components of that top level component. Why? This is ideal when a service is sharing methods or state. Why? This is not ideal when two different components need different instances of a service. In this scenario it would be better to provide the service at the component level that needs the new and separate instance. import { Component } from '@angular/core'; import { HeroListComponent } from './heroes/hero-list.component'; import { HeroService } from './heroes/shared/hero.service'; @Component({ selector: 'toh-app', template: ` <toh-heroes></toh-heroes> `, directives: [HeroListComponent], providers: [HeroService] }) export class AppComponent {} BAD FILENAME: ../../../_fragments/style-guide/ts/07-03/app/heroes/hero-list/hero-list.component.ts.md Current path: docs,ts,latest,guide,style-guide PathToDocs: ../../../ Back to top Use the @Injectable() Class Decorator Style 07-04 Do use the @Injectable class decorator instead of the @Inject parameter decorator when using types as tokens for the dependencies of a service. Why? The Angular DI mechanism resolves all the dependencies of our services based on their types declared with the services' constructors. Why? When a service accepts only dependencies associated with type tokens, the @Injectable() syntax is much less verbose compared to using @Inject() on each individual constructor parameter. AVOID: app/heroes/shared/hero-arena.service.ts /* avoid */ export class HeroArena { constructor( @Inject(HeroService) private heroService: HeroService, @Inject(Http) private http: Http) {} } app/heroes/shared/hero-arena.service.ts @Injectable() export class HeroArena { constructor( private heroService: HeroService, private http: Http) {} } Back to top Data Services Separate Data Calls Style 08-01 Do refactor logic for making data operations and interacting with data to a service. Do make data services responsible for XHR calls, local storage, stashing in memory, or any other data operations. Why? The component's responsibility is for the presentation and gathering of information for the view. It should not care how it gets the data, just that it knows who to ask for it. Separating the data services moves the logic on how to get it to the data service, and lets the component be simpler and more focused on the view. Why? This makes it easier to test (mock or real) the data calls when testing a component that uses a data service. Back to top Lifecycle Hooks Use Lifecycle Hooks to tap into important events exposed by Angular. Back to top Implement Lifecycle Hooks Interfaces Style 09-01 Do implement the lifecycle hook interfaces. Why? We get strong typing for the method signatures. The compiler and editor can call our attention to misspellings. AVOID: app/heroes/shared/hero-button/hero-button.component.ts /* avoid */ @Component({ selector: 'toh-hero-button', template: `<button>OK<button>` }) export class HeroButtonComponent { onInit() { // mispelled console.log('The component is initialized'); } } app/heroes/shared/hero-button/hero-button.component.ts @Component({ selector: 'toh-hero-button', template: `<button>OK<button>` }) export class HeroButtonComponent implements OnInit { ngOnInit() { console.log('The component is initialized'); } } Back to top Routing Client-side routing is important for creating a navigation flow between a component tree hierarchy, and composing components that are made of many other child components. Back to top Component Router Style 10-01 Do separate route configuration into a routing component file, also known as a component router. Do focus the logic in the component router to the routing aspects and its target components. Do extract other logic to services and other components. Why? A component that handles routing is known as the component router, thus this follows the Angular 2 routing pattern. import { Component } from '@angular/core'; import { RouteConfig, ROUTER_DIRECTIVES, ROUTER_PROVIDERS } from '@angular/router'; import { NavComponent } from './shared/nav/nav.component'; import { DashboardComponent } from './dashboard/dashboard.component'; import { HeroesComponent } from './heroes/heroes.component'; import { HeroService } from './heroes/shared/hero.service'; @Component({ selector: 'toh-app', templateUrl: 'app/app.component.html', styleUrls: ['app/app.component.css'], directives: [ROUTER_DIRECTIVES, NavComponent], providers: [ ROUTER_PROVIDERS, HeroService ] }) @RouteConfig([ { path: '/dashboard', name: 'Dashboard', component: DashboardComponent, useAsDefault: true }, { path: '/heroes/...', name: 'Heroes', component: HeroesComponent }, ]) export class AppComponent {} Back to top Appendix Useful tools and tips for Angular 2. Back to top Codelyzer Style A-01 Consider adjusting the rules in codelyzer to suit your needs. Back to top File Templates and Snippets Style A-02 Do use file templates or snippets to help follow consistent styles and patterns. Here are templates and/or snippets for some of the web development editors and IDEs. Back to top",en,52
315,1315,1465441290,CONTENT SHARED,-5507019982244746725,-8020832670974472349,838596071610016700,,,,HTML,http://jerrygamblin.com/2016/06/08/what-to-inspect-when-you-are-inspecting/,what to inspect when you are inspecting!,"Docker containers have become so ubiquitous sometimes respected security professionals tweet ridiculous things like: docker run -u zap -p 8080:8080 -p 8090:8090 -i owasp/zap2docker-stable zap-webswing.sh - Jerry Gamblin (@JGamblin) June 7, 2016 ...but it is 2016 and you should never run code on your machine if you don't know what it does. These are mini-virtual machines and not magically secure little shipping containers*. At a minimum you should do these basic things to get some idea of what you are putting on your machine before you run it. Pull the container first: docker pull jgamblin/tiny-tor Use Docker Inspect to look at the container's metadata: docker inspect jgamblin/tiny-tor You will want to carefully read through that output and take time to look at these fields: Image The image this container is running. NetworkSettings The network settings for the container, LogPath The system path to this container's log file. Name The user defined name for the container. Volumes Defines the volume mapping between the host system and the container. HostConfig Key configurations for how the container will interact with the host system. These could take CPU and memory limits, networking values, or device driver paths. Config The runtime configuration options set when the docker run command was executed. Use Docker History to see how the image was built: docker history jgamblin/tiny-tor Protip: CenturylinkLabs released a tool to create a Dockerfile from a container. Run the container without network access and look around a bit: docker run -t -i jgamblin/tiny-tor /bin/sh After you have done the following steps and feel comfortable you can then: docker run -t -i -p 9050:9050 jgamblin/tiny-tor If you do these basic things you can feel a little better about what you are running on your system. * What a magically secure little shipping container might look like:",en,52
316,2489,1475448972,CONTENT SHARED,4893900714822288649,3217014177234377440,-9116089978622526389,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36",SP,BR,HTML,https://endeavor.org.br/poka-yoke/,[article] poka yoke: o que é e como implementar | endeavor brasil,"Conheça tudo sobre Poka Yoke, uma técnica de gestão da qualidade baseada em soluções simples, que evitam falhas em processos e reduzem custos ""Poka Yoke""; ou ""pocá-ioquê"", para os íntimos. Esse nome, tão curioso, é de origem japonesa e significa ""à prova de erros"". A partir daí, já dá para se ter uma ideia da natureza dessa ferramenta, que foi criada no Japão e implantada no Sistema Toyota de Produção já faz algum tempo. Pois é, o nome até pode ser engraçado, mas a função do Poka Yoke é muito séria: trata-se de um sistema de inspeção desenvolvido para prevenir riscos de falhas humanas e corrigir eventuais erros em processos industriais, sempre por meio de ações simples. O Poka Yoke surgiu nos anos 1960, quando Shigeo Shingo, hoje considerado um gênio da engenharia, liderava a produção da Toyota. Não havia um dia em que ele não se deparasse com falhas humanas, que resultavam em produtos defeituosos: e, por isso, não havia um dia em que não ficasse irritado. Diante disso, Shingo começou a desenvolver técnicas que, por vingança, chamou de Baka (""idiota"", em japonês) Yoke (""à prova de""), em um processo que dispensa traduções. Aos poucos, porém, as técnicas foram aprimoradas, se provaram profundamente eficazes e ganharam aderência. Para não ofender ninguém, Shingo trocou o Baka por Poke. Hoje, o Poka Yoke é uma técnica absolutamente consagrada de gestão em processos industriais, mas não só: a lógica da ferramenta se ampliou, de modo que ela pode ser aplicada a qualquer situação que envolva riscos de falhas ou defeitos. Como a administração do seu negócio, por exemplo. Algum exemplo de aplicação? O próprio Shigeo Shingo nos deu um exemplo icônico de aplicação do Poka Yoke. Quando um cliente o avisou de que alguns interruptores produzidos pela Toyota estavam vindo sem mola - o que impossibilitava o funcionamento -, o engenheiro conversou seguidas vezes com a equipe. Mas o efeito persistia, e ele acabou desenhando o seguinte processo: todas as peças do interruptor deveriam ser colocadas em um prato, para só então serem montadas. Se, ao final desse processo, sobrasse uma peça no prato, algo teria saído errado. E o que Poka Yoke tem a ver com o meu negócio? Se não tudo, quase tudo. Isto é, se sua empresa lida com produção ou montagem de algum tipo, apesar de você talvez conseguir aplicar o método em diferentes processos. Essa é uma ferramenta de gestão de qualidade das mais eficazes, que já foi validada de várias formas por empresas de todo o mundo. Ou seja, está mais do que provado que ela é capaz de auxiliar a reduzir seus custos com falhas humanas e de processos, ou de problemas com defeitos em produtos. Em outras palavras, o Poka Yoke pode impedir que os erros de agora se transformem em defeitos lá na frente. O pensamento por trás desta ferramenta é o de que, se há uma imperfeição em algum processo, é possível tomar atitudes descomplicadas para resolvê-la antecipadamente . E isto, acredite, pode ser muito melhor do que tentar resolver depois, com o produto ou serviço já concluído. Os custos com desperdícios e retrabalhos seriam muito maiores. Faz sentido. Agora, como posso usar a ferramenta? De acordo com especialistas no assunto, são seis os passos essenciais que o empreendedor deve seguir para aplicar o Poka Yoke em suas atividades: 1) conheça a falha a ser corrigida : é preciso que você compreenda exatamente o defeito de produto, de serviço ou de execução de alguma etapa que deva ser contornado. Para facilitar esta etapa, registre o defeito por meio de fotos, vídeos e/ou transcrição de narrativas. 2) compreenda as causas : entender o que levou à ocorrência dos defeitos e das falhas é fundamental para corrigi-los. De acordo com uma cartilha produzida pelo Movimento Empreenda (disponível aqui ), os defeitos são originados por dez causas principais: 1 - Não executado por falta de processamento 2 - Erro na execução ou no processamento 3 - Erro na disposição/no posicionamento dos elementos 4 - Ausência ou excesso de elementos 5 - Utilização de elemento errado 6 - Execução ou processamento de elemento errado 7 - Falha do equipamento 8 - Erro de ajuste 9 - Falha na preparação do equipamento 10 - Ferramentas ou dispositivos inadequados E as principais falhas humanas são: 1 - Falta de concentração ou esquecimento 2 - Inércia mental, decisão ""sem pensar"", excesso de familiaridade 3 - Análise superficial e/ou rápida; identificação errônea 4 - Falta de experiência, amadorismo 5 - Imprudência ou teimosia 6 - Distração momentânea 7 - Lentidão na ação, demora na decisão 8 - Ausência de padrão, falta de procedimento 9 - Situação inesperada, surpresa 10 - Má fé ou intencional 3) cogite soluções: primeiro, pergunte-se: como a falha pode ser prevenida? Se não conseguir responder, tente descobrir como o defeito pode ser detectado o quanto antes. Ou, ainda, se a falha ou o defeito deve ser detectado de forma direta (sem interferência humana) ou indireta (com interferência humana). 4) verifique a eficácia da solução: para que seja a mais eficaz possível, a solução Poka Yoke deve eliminar a falha ou o defeito de forma simples, sem grandes impactos na sua gestão de custos . Deve fazer parte do processo, sendo executada no local em que a falha ocorre, e deve evitar que esta falha seja passada para a próxima etapa. 5) implante a solução : faça isso em toda a empresa. 6) registre-a: ao final do processo, colete o máximo de informações que conseguir a respeito dos ocorridos, comparando os resultados obtidos. Isto será extremamente útil para o desenvolvimento de outras soluções Poka Yoke no futuro. Legal! Onde posso encontrar mais informações sobre a ferramenta? Há informações bem completas naquele link que passamos acima, no portal do Movimento Empreenda. E neste link do site NovoNegócio, você encontra mais detalhes sobre as origens e as aplicações do Poka Yoke . Embora mais direcionado à indústria, nunca se sabe: vai que você tira daí uma nova ideia para evitar falhas e reduzir custos?",pt,52
317,1996,1470410155,CONTENT SHARED,-2554512494756911972,1895326251577378793,4845135633002089199,,,,HTML,http://www.mckinsey.com/business-functions/business-technology/our-insights/how-enterprise-architects-can-help-ensure-success-with-digital-transformations,how enterprise architects can help ensure success with digital transformations,"Those who design and steer the development of the technology landscape can mitigate risk by setting operating standards and promoting cross-functional collaboration. Most CEOs understand the potential upside of a digital transformation. If they can get it right, their companies can be more efficient, more agile, and better able to deliver innovative products and services to customers and partners through multiple channels. About 70 percent of executives say that over the next three years, they expect digital trends and initiatives to create greater top-line revenues for their businesses, as well as increased profitability. There are tangible risks associated with these efforts, however. Traditional companies want to behave like start-ups, but they usually don't have the technology infrastructures or operating models to keep up with companies that have been digital from the start. Their shortcomings can have consequences. Traditional companies undergoing digital transformations may continue to build ever-more-complicated IT systems, deploying new features or patches and fixes on the fly to meet immediate needs without any clear road map or consideration of future IT needs. The Enterprise Architecture Survey Indeed, an in-depth survey we conducted with Henley Business School on enterprise-architecture management revealed such patterns among companies pursuing digital transformations (see sidebar, ""The Enterprise Architecture Survey""). When companies go all in on digitization, the number of point-to-point connections among systems rises almost 50 percent, the quality of business-process documentation deteriorates, and services get reused less often (Exhibit 1). These firms experience greater complexity in systems and processes-and not just in the near term, as digital projects are rolled out, but also in the long term, as companies seek to extend pilot programs and applications to all functions and business units. In the latter scenario, IT organizations may need to do a lot of systems and applications rework and reengineering to enable even the most basic digital activities. Companies may be slower to market with new products and services, and less able to react quickly to changing customer demand. The enterprise-architecture (EA) department can play a central role in reducing the complexity associated with digital transformations. Most companies have a dedicated EA group embedded within the larger IT organization. This group typically oversees the entire systems architecture, including business processes and IT infrastructure. It helps to establish rules for and processes around technology usage to ensure consistency across business units and functions. As such, this group can help the CEO and others on the senior leadership team redesign their companies' business and IT architectures so that they can avoid some of the pitfalls cited earlier and compete more effectively in a digital era. An operating model for company-wide agile development Read the article The findings from our research suggest that when the EA group is directly involved in digital-transformation projects, the documentation and communication between business and IT stakeholders improves significantly. What's more, organizations are likelier to focus on capturing tangible benefits from the transformation-an important factor in mitigating the risk of black swans -and to devote more time and attention to planning. They may also be able to launch products and services more frequently, given the reduced complexity. Most companies are not prepared for this to happen, however. More than 40 percent of respondents in our survey say that the business leaders in their companies are not aware of what the EA group does. We believe that to improve the odds that a digital transformation will succeed, CEOs and CIOs need to raise the profile of enterprise-architecture departments within their companies and to develop the business and interpersonal capabilities of their enterprise architects. Empowering the EA group The enterprise-architecture professionals we surveyed said that the top goals in their IT organizations are executing digital programs-such as cloud computing and online business models-and simplifying and modernizing their IT systems. Reducing complexity is especially critical, they told us, for ensuring that companies can capitalize on digital technologies, support agile and DevOps product-development methodologies, and respond to customers' needs more quickly. To successfully deploy agile development methodologies, for instance, companies need their business units and their IT organization to have a common understanding of both products and processes. These groups must have a joint view, for instance, of which applications are ""mission critical"" versus just nice to have, which processes are uniquely required, and which are being duplicated across multiple business units. The enterprise-architecture group is in a prime position to support these decisions and help create a unified perspective on what needs to change. Good things can happen with EA involvement, but our research indicates a general lack of awareness of enterprise-architecture groups within most organizations-who they are and what they do. Some enterprise architects told us that, during the course of their work assignments, they are actually more likely to interact with suppliers than with internal business executives and C-level leaders. When this happens, the EA group can enter into an unproductive cycle: its capability and process models won't accurately reflect business needs and therefore won't be used by the business to make critical technology decisions. CIOs and CEOs can heighten the awareness of enterprise-architecture groups and empower them by setting the right tone operationally and by facilitating talent development. Operating-model factors Sometime in the early 2000s, as Amazon was starting to establish a service-oriented architecture, CEO Jeff Bezos distributed a memo. In it, he mandated that all teams use open application programming interfaces and web services to share data and functionality. He made it clear that those not following the rules would be fired. Having this level of CEO attention on a very technical topic prompted change within this digital organization. Similarly, company leaders in traditional organizations, especially at the top but also at the line-management level and in the boardroom, need to engage more deeply with enterprise-architecture topics. They don't necessarily need an Amazon-like shift of the entire operating model, but it is important for them to include the enterprise-architecture team in formal discussions about processes, policies, and strategy. CEOs and CIOs can also put more of the spotlight on EA groups by doing the following: Give them more responsibility. Even among those in the know about enterprise architecture, the perception in many companies is that this back-office group can have only limited impact on overarching corporate initiatives-particularly compared with other technology-oriented groups (application development, for instance) that tend to have bigger budgets and direct responsibility for core operational areas. CIOs and CEOs can reverse this perception by giving EA departments more responsibility for certain big-picture decisions; for instance, they can give authority in the approval process for changes to the technology landscape. Otherwise, the policies and guidelines the EA department develops may never gain traction across the company. Measure their performance. It can be difficult for CIOs and enterprise architects to determine the EA group's direct contribution to corporate performance because so much of the day-to-day work depends on input from individual business units, ever-changing strategy and budget decisions, and other interdependencies. There is no simple formula for demonstrating absolute impact, but one feasible approach is for the enterprise-architecture team to routinely provide the business units with the ""technology costs"" of any important decisions they make-for instance, trade-offs in cost, time, and quality when a new technology is deployed and used in place of an alternative. The business would get the information it needs to make critical decisions, and enterprise architects would gain a direct line of communication to the business. Talent factors Pushing enterprise architecture toward the top of the agenda, giving greater responsibility to the EA group, and coming up with clear performance metrics may also help companies attract the operations and leadership talent they need to design and support IT systems effectively. Indeed, our survey revealed that for most EA staffers, being seen as a valuable contributor to the bottom line may be more of an incentive than monetary rewards (Exhibit 2). In our experience, the individuals who take on EA-management responsibilities need a combination of deep smarts in business strategy and expertise on IT trends and technologies, integration patterns, business-process steps, and running a closed IT environment. These people do not need to be experts in coding or supply-chain planning or store operations, but they do need basic knowledge of all those things-and more. They must also have good communication and marketing skills. They must become ""ambassadors"" for enterprise architecture, helping business leaders and board directors alike understand the purpose and value generated by all the systems that underlie day-to-day operations. And they must be able to foster close collaboration between IT and business stakeholders. Because of the variety of skills required, CIOs may need to look outside the usual talent pools when recruiting, considering people with academic and business credentials as well as traditional technologists. Digital transformations are, by their very nature, complex. There are multiple moving parts, integrated processes and technologies, and the need for expertise that cannot already be found in the company. In most firms, however, there is a small cadre of technology professionals who can impose relative order on the proceedings. CEOs and CIOs should consider the benefits of pulling enterprise architects closer to the center: bringing them to the table with business leaders, devising metrics that reveal the value of their work, and creating the type of incentives that will challenge them and prompt them to stay for the long term. Such an approach is critical not just for limiting risk and protecting against the potential challenges and downsides of digital transformations but also for ensuring a clear upside-a close and lasting partnership between the business and IT. About the author(s) Oliver Bossert is a senior expert in McKinsey's Frankfurt office, and Jürgen Laartz is a senior partner in the Berlin office. The authors wish to thank Sharm Manwani, executive professor of IT leadership and director of the Strategy & Enterprise Architecture Programme at Henley Business School, for his contributions to this article.",en,52
318,2164,1471988255,CONTENT SHARED,-1022885988494278200,3609194402293569455,2476005711861360409,,,,HTML,https://blog.jscrambler.com/12-extremely-useful-hacks-for-javascript/,12 javascript hacks,"In this post I will share 12 extremely useful hacks for JavaScript . These hacks reduce the code and will help you to run optimized code. So let's start hacking! 1) Converting to boolean using !! operator Sometimes we need to check if some variable exists or if it has a valid value, to consider them as true value . For do this kind of validation, you can use the !! (Double negation operator) a simple !!variable , which will automatically convert any kind of data to boolean and this variable will return false only if it has some of these values: 0 , null , """" , undefined or NaN , otherwise it will return true . To understand it in practice, take a look this simple example: In this case, if an account.cash value is greater than zero, the account.hasMoney will be true. 2) Converting to number using + operator This magic is awesome! And it's very simple to be done, but it only works with string numbers, otherwise it will return NaN (Not a Number) . Have a look on this example: This magic will work with Date too and, in this case, it will return the timestamp number: 3) Short-circuits conditionals If you see a similar code: You can shorten it by using the combination of a variable (which will be verified) and a function using the && (AND operator) between both. For example, the previous code can become smaller in one line: You can do the same to check if some attribute or function exists in the object. Similar to the below code: 4) Default values using || operator Today in ES6 there is the default argument feature. In order to simulate this feature in old browsers you can use the || (OR operator) by including the default value as a second parameter to be used. If the first parameter returns false the second one will be used as a default value. See this example: 5) Caching the array.length in the loop This tip is very simple and causes a huge impact on the performance when processing large arrays during a loop. Basically, almost everybody writes this synchronous for to iterate an array: If you work with smaller arrays - it's fine, but if you process large arrays, this code will recalculate the size of array in every iteration of this loop and this will cause a bit of delays. To avoid it, you can cache the array.length in a variable to use it instead of invoking the array.length every time during the loop: To make it smaller, just write this code: 6) Detecting properties in an object This trick is very useful when you need to check if some attribute exists and it avoids running undefined functions or attributes. If you are planning to write cross-browser code, probably you will use this technique too. For example, let's imagine that you need to write code that is compatible with the old Internet Explorer 6 and you want to use the document.querySelector() , to get some elements by their ids. However, in this browser this function doesn't exist, so to check the existence of this function you can use the in operator, see this example: In this case, if there is no querySelector function in the document object, we can use the document.getElementById() as fallback. 7) Getting the last item in the array The Array.prototype.slice(begin, end) has the power to cut arrays when you set the begin and end arguments. But if you don't set the end argument, this function will automatically set the max value for the array. I think that few people know that this function can accept negative values, and if you set a negative number as begin argument you will get the last elements from the array: 8) Array truncation This technique can lock the array's size, this is very useful to delete some elements of the array based on the number of elements you want to set. For example, if you have an array with 10 elements, but you want to get only the first five elements, you can truncate the array, making it smaller by setting the array.length = 5 . See this example: 9) Replace all The String.replace() function allows using String and Regex to replace strings, natively this function only replaces the first occurrence. But you can simulate a replaceAll() function by using the /g at the end of a Regex: 10) Merging arrays If you need to merge two arrays you can use the Array.concat() function: However, this function is not the most suitable to merge large arrays because it will consume a lot of memory by creating a new array. In this case, you can use Array.push.apply(arr1, arr2) which instead creates a new array - it will merge the second array in the first one reducing the memory usage: 11) Converting NodeList to Arrays If you run the document.querySelectorAll(""p"") function, it will probably return an array of DOM elements, the NodeList object. But this object doesn't have all array's functions, like: sort() , reduce() , map() , filter() . In order to enable these and many other native array's functions you need to convert NodeList into Arrays. To run this technique just use this function: [].slice.call(elements) : 12) Shuffling array's elements To shuffle the array's elements without using any external library like Lodash , just run this magic trick: Conclusion Now you learned some useful JS hacks which are largely used to minify JavaScript code and some of these tricks are used in many popular JS frameworks like Lodash, Underscore.js, Strings.js, among others. If you want to go even deeper and learn more about how you can minify your code even more and even protect it from prying eyes talk to us . I hope you enjoyed this post and if you know other JS hacks, please leave your comment on this post!",en,52
319,1615,1467377010,CONTENT SHARED,-6245566021053602636,-1387464358334758758,-7104730417434451425,,,,HTML,http://googlediscovery.com/2016/06/30/google-lanca-youtube-kids-no-brasil/,google lança youtube kids no brasil | google discovery,"O Google anunciou hoje no Brasil o aplicativo do YouTube Kids , uma plataforma com conteúdo gratuito voltado para toda família. De acordo com a gigante de Mountain View, a ferramenta oferece um ambiente mais seguro para crianças de 2 a 8 anos, além de uma interface amigável, com ícones coloridos e botões maiores. Entre os conteúdos podem ser localizados por meio de canais e playlists em quatro categorias: Séries, Música, Aprender e Explorar. ""O Brasil é o primeiro País de língua portuguesa a receber a plataforma. Desde o lançamento nos aplicativo do YouTube Kids nos Estados Unidos, mais de 10 milhões de downloads foram contabilizados"", disse porta-voz da empresa. YouTube Kids é gratuito e está disponível para dispositivos móveis Android e iOS . é bacharel em administração de empresas e fundador da FragaNet Networks - empresa especializada em comunicação digital e mídias sociais. Em seu portfólio estão projetos como: Google Discovery, TechCult, AutoBlog e Arquivo UFO. Também foi colunista de tecnologia no TechTudo, da Globo.com.",pt,52
320,3035,1485798119,CONTENT SHARED,-7047448754687279385,997469202936578234,-2543716617145670798,"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36",MG,BR,HTML,"http://www.em.com.br/app/noticia/especiais/bigideia/bigideia-noticia/2017/01/30/bigideia,843515/plano-nacional-de-internet-das-coisas-sai-em-fevereiro.shtml",plano nacional de ''internet das coisas'' sai em fevereiro,"(foto: pixabay.com) Daqui a cerca de um mês, representantes do Ministério da Ciência, Tecnologia, Inovações e Comunicações (MCTIC), sob a gestão de Gilberto Kassab, vão embarcar para Barcelona, na Espanha, para revelar o Plano Nacional de "" Internet das Coisas "" durante o Mobile World Congress (MWC), principal evento de tecnologias móveis do mundo. O documento, que pretende mostrar o país na vanguarda da revolução que vai conectar todos os objetos à nossa volta, porém, estará incompleto: em vez de definir objetivos, traçar metas claras e um plano de ação, ele deve trazer apenas linhas gerais sobre as intenções do Brasil. A apresentação do plano é considerada prematura por pessoas ligadas ao setor. ""Eles deveriam consultar mais o setor antes de lançar o plano"", disse uma fonte. ""Para essa primeira versão, eles devem estar se baseando em estudos internacionais."" As críticas rondam o governo porque o plano será anunciado pouco mais de dois meses após o Ministério assinar um acordo com o Banco Nacional de Desenvolvimento Econômico e Social (BNDES). Em conjunto, eles contrataram um consórcio - formado pelo Centro de Pesquisa e Desenvolvimento em Telecomunicações (CPqD), pela consultoria McKinsey Brasil e pelo escritório Pereira Neto/Macedo Advogados - para fazer uma ampla pesquisa sobre o tema. O estudo, que vai custar R$ 17,5 milhões, deveria ser a base para o plano nacional. Atualmente, o consórcio mapeia as soluções de internet das coisas mais avançadas no mundo e define as aspirações brasileiras. ""Vamos usar a consulta pública do Ministério sobre o tema, que termina no início de fevereiro, para compor essa primeira fase"", conta Vinícius Garcia de Oliveira, responsável pelo estudo no CPqD. A segunda fase do estudo vai delinear o mercado potencial para internet das coisas no País, além de identificar quais setores produtivos podem se beneficiar das tecnologias . Embora relatórios sejam entregues ao governo nesse ínterim, a previsão é de que só na terceira fase sejam definidas as áreas prioritárias e tecnologias que serão alvo de pesquisa e desenvolvimento. A partir daí é que políticas públicas serão definidas. O secretário de política de informática do MCTIC, Maximiliano Martinhão, afirmou que o governo vai apresentar, na verdade, uma versão preliminar do Plano Nacional de ""Internet das Coisas"" em fevereiro. Ela será atualizada com o andamento do estudo. ""São diretrizes "", diz Martinhão. ""Vamos sair do conceitual para uma primeira aproximação."" Segundo secretário, o Ministério vai realizar outras consultas públicas ao longo do ano para ouvir as sugestões e críticas dos interessados. Conteúdo A principal preocupação no mercado é que o governo use o plano para tentar criar uma indústria local de hardware , estratégia que não deu certo no passado. ""Se focarmos em dispositivos, disputaremos uma cadeia de menor valor"", avalia o executivo-chefe de negócios do Centro de Estudos e Sistemas Avançados do Recife (Cesar). ""O maior valor está no software."" Recentemente, o Cesar criou uma plataforma permitindo que diversos objetos conectados se ""conversem"". Em uma reunião na semana passada no MCTIC, o presidente da Associação Brasileira das Empresas de Software (Abes), Francisco Camargo, também defendeu a aposta em serviços. ""Somos bons em software "", diz Camargo. ""E o investimento é significativamente menor."" Outros pontos de atenção para a elaboração do plano incluem a adoção de padrões globais de interoperabilidade entre dispositivos ; regulamentação adequada; oferta de capital de risco para startups; e mudanças na relação entre universidades e empresas. ""É preciso criar uma cadeia para que as pesquisas se transformem em negócios"", afirma o presidente da Sociedade Brasileira de Computação (SBC), Lisandro Granville. Positivo Apesar dos tropeços iniciais, especialistas elogiam a iniciativa do Brasil. Como trata-se de um setor muito novo, ainda não há nações com vantagem competitiva em relação às demais. ""Há uma grande oportunidade"", diz Daniel Castro, diretor do Center for Data Innovation, entidade sem fins lucrativos baseada em Washington, nos Estados Unidos. ""Poucos países têm sido proativos em criar políticas desse tipo."" Países como China, Cingapura e Índia têm feito as apostas mais significativas - o Brasil, por enquanto, não prevê verba específica para internet das coisas. ""Eles investem em pesquisa e desenvolvimento , mas o governo também tem atuado como parceiro das startups. Isso faz uma tremenda diferença.""",pt,52
321,2570,1476702994,CONTENT SHARED,-8992201526199201450,3609194402293569455,7539347816740119834,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36",SP,BR,HTML,http://tutorialzine.com/2016/10/15-awesome-sublime-text-plugins-for-web-development/,15 awesome sublime text plugins for web development,"15 Awesome Sublime Text Plugins For Web Development Danny Markov Sublime Text is one of the most popular code editors available right now. It is adored by many programmers for it's speed, simplicity, and rich plugin ecosystem. To help developers get the most our of Sublime, we decided to make a list containing some of the extensions that we use and love. If we haven't included some of your favorites, feel free to share them with us in the comment section :) We cannot start off our list without mentioning Package Control. It's the plugin manager for Sublime, and without it installing and removing packages would be a huge pain. If you do not have Package Control installed, make sure you do that first, as it will allow you to quickly try out the other plugins in this article. A collection of shorthand snippets for writing common JavaScript expressions much faster. Why write document.querySelector('selector'); when you can simply type qs , press tab, and let Sublime do the rest of the work. Like the previous plugin in the list, this one let's you use snippets to write code faster. The difference here is that instead of JS expresions, Emmet works for HTML and CSS, letting you write long tags, nested elements, or whole page templates in one go. Emmet is a bit complex, so if want a simpler alternative you could try a similar plugin called HTML Snippets . It has less features, but is way easier to use, and has a great straightforward documentation. This awesome package makes it possible to create new files blazingly fast. Instead of browsing folders, and using the menus, you simply open a prompt with super+alt+n and write the path to your new file. The plugin will also add any non-existing directories from the path, and even supports auto completion for folder names. A Git integration that works directly from the Sublime Text command palette. The package provides quick access to a number of commonly used Git commands, allowing developers to add files, make commits, or open the Git logs, without ever leaving Sublime. Very useful extension that marks each line in your source code, telling you its Git status and giving you an overview of the changes that have occurred. GitGutter can be used to compare your files to the git HEAD, origin, a branch of your choice, or even certain commits. In Sublime Text the project you are working on is overviewed in the left side panel. Although it gives you some options for working with your files, the available default actions are quite limited. This plugin changes that by adding over 20 options to the right-click menu, including Open in browser , duplicate , and lots of other useful stuff. A tiny, useful color picker that is very simple to use and great for quickly grabbing color hex values. The plugin opens in a separate window and allows you to choose a color from a palette or use an eye dropper to extract color from anywhere on your screen. Sublime Text 3 has a built-in Lorem Ipsum generator that you can use for creating dummy text. The Placeholders plugin extends that functionality and allows ST to quickly generate for you placeholder images, forms, lists, and tables. This is an extension for those of you who like to add detailed comments to function definitions. DocBlockr allows you to effortlessly generate descriptions for your functions including the parameters they take, the returned value, and variable types. Code intelligence plugin that indexes your source files and enables you to find function definitions and jump to them. This extension works for a plethora of popular and not-so-popular programming languages. A code minifer and beautifier in one. Minify takes your current opened file, and creates a new .min or .pretty version of it in the same directory. Works with CSS, HTML, JavaScript, JSONs, and SVGs. This package relies on external node.js libraries for minifying and beautifying, so you will need to install them separately: This package enables the code editor to check for syntax errors, bad practices, or other mistakes that the developer may have made. SublimeLinter itself just acts as a base framework for linting, so you also need to install separate plugins for each language you code in. A feature you can see in many other IDEs and text editors, but is missing from Sublime, is color previews. Using the Color Highlighter extension you can enable it in ST, allowing you to see how all hex and RGBA values are translated to color, directly in your style sheets. Sublime Text has code highlighting for over 50 languages but there are some frameworks or niche web dev languages that are not supported yet. Thanks to the plugin nature of the editor, the community can create and distribute packs for any imaginable programming language: Bonus: Themes Installing a beautiful theme to your text editor definitely makes writing code more enjoyable. There are tons of great themes and color palletes available for Sublime text, here are a few of our favorites: by Danny Markov Danny is Tutorialzine's Bootstrap and HTML5 expert. When he is not in the office, you can usually find him riding his bike and coding on his laptop in the park.",en,52
322,3073,1486657613,CONTENT SHARED,-7826213337362450520,801895594717772308,-6056006567833577188,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36",MG,BR,HTML,http://www.otempo.com.br/cidades/ciclistas-relatam-roubos-com-agressividade-na-avenida-dos-andradas-1.1433554,ciclistas relatam roubos com agressividade na avenida dos andradas,"Os ciclistas de Belo Horizonte reclamam que os furtos e roubos de bicicletas estão cada vez mais frequentes. Um dos locais com mais reclamações é a ciclovia da avenida dos Andradas, na região Leste da capital. Segundo relatos, os ladrões agem com muita agressividade. Do ano passado para este ano, os roubos de bicicleta quase dobraram na cidade, segundo dados da Secretaria de Estado de Defesa Social (Seds). Até novembro de 2016, foram registrados 14 roubos, no mesmo período de 2015 foram apenas 8 roubos. Já os furtos à bicicletas tiveram ligeira queda. Até novembro de 2015 foram registrados 84 furtos, já no mesmo período de 2016 foram 73 furtos. A Seds ainda não tem os dados até o mês de dezembro. No entanto, esses números podem ser ainda maiores, já que muitos ciclistas não registram boletim de ocorrência (BO). Pelo grupo do Facebook Massa Crítica, os ciclistas publicaram, na última segunda-feira (6), que a avenida dos Andradas é o principal problema. Muitos ciclistas relatam que apesar da enorme ciclovia na via, eles preferem usar a pista de carro por causa da violência na ciclovia. Segundo os relatos, os roubos na ciclovia são permeados por muita violência a qualquer hora do dia. Alguns ciclistas propõe até mesmo uma bicicletada na região para chamar a atenção das autoridades em relação ao problema. Relato No último dia 24 de novembro o ator Ramon Brant foi assaltado às 14h30. De acordo com ele, quatro suspeitos o abordaram e o derrubaram da bicicleta. ""Eu percebi a aglomeração de 4 ou 5 pessoas próximas a calçada. Um deles correu em minha direção e me acertou com uma voadora. Como eu estava em alta velocidade, sofri uma queda forte, rente ao meio fio da ciclovia. Torci o joelho e me esfolei inteiro. Ainda no chão, eles me agrediram com soco. Um deles me deu uma gravata enquanto os outros tiravam meu celular, dinheiro, etc"", contou Brant. Segundo ele, dois agressores estavam em uma bicicleta e o outro a pé. O ator fez um apelo para que não levassem sua mochila mostrando que só tinha seu material do teatro. ""Antes de ir embora, eles continuaram me ameaçando, bem agressivos e violentos. Vi que um deles tinha uma faca e outro carregava um pedaço de pau. Quando saí correndo na outra direção (Centro/Boulevard), eles ainda ficaram ali por um tempo, como se nada tivesse acontecido"", relata o ator. Brant contou que passa pelo local frequentemente há 5 anos e que tanto a noite quanto de dia é bastante deserto e perigoso. Parece que a tática que eles usam para os roubos são sempre as mesmas. ""Fecham a pista e derrubam violentamente os ciclistas"", afirmou o ator. Brant usava a bicicleta para se deslocar. É também com uma ""magrela"" que ele realiza o projeto ""Chá com Cartas"", que ele entrega cartas para moradores da capital mineira. Compartilhando o problema O ator publicou seu relato no grupo de Facebook Massa Crítica de Belo Horizonte e uma série de ciclistas comentaram a postagem dizendo que passaram pela mesma situação ou que evitam passar pela ciclovia por causa do medo de roubos e agressões. Um deles é o economista Gelton Filho, 40 que deixou de ir de bicicleta para o trabalho para não ter mais que passar pelo local. ""Eu passava por lá na ida por volta de 6h30 e na volta às 19h30. São horários em que lá fica muito vazio. Por duas vezes eu consegui desviar de abordagens, mas percebi que seria cercado"", contou. Furtos Os roubos não se limitam a ciclovia. Os ladrões também furtam as bikes dentro de casa. O designer Marcelo Cardoso, 34, teve duas bicicletas roubadas dentro da casa dele no ano passado. ""Eu moro em uma casa na Serra e minha garagem tem um vão que dá visão para a rua. O que tinha de interessante lá dentro eram duas bicicletas uma presa a outra. Alguém pegou um tronco de árvore que estava na rua subiu, pulou o muro e roubou as bicicletas"", contou o designer. A reportagem de O TEMPO aguarda resposta da Polícia Militar que cuida da área para falar sobre os roubos.",pt,52
323,787,1462387558,CONTENT SHARED,7255021292858609470,1895326251577378793,-4504437838273641641,,,,HTML,http://www.mckinsey.com/business-functions/business-technology/our-insights/an-operating-model-for-company-wide-agile-development,an operating model for company-wide agile development,"Organizations are succeeding with agile software and product development in discrete projects and teams. To do so in multiple business units and product groups, they must rethink foundational processes, structures, and relationships. Many digital companies are using agile development practices to deliver goods and services to customers more efficiently and with greater reliability. Using this software-development approach across all business units and product groups, digital giants have been able to design and build features quickly, test them with customers, and refine and refresh them in rapid iterations. By contrast, few traditional companies-those with both online and offline presences-are using agile methodologies across the majority of their product- and application-development teams. Many banks, for instance, have established digital units to develop and release mobile apps or website features quickly. But those groups typically remain physically and strategically disconnected from the rest of the IT organization and the rest of the company. Research indicates that many traditional companies are experimenting with agile practices in discrete pilot projects and realizing modest benefits from them. But fewer than 20 percent consider themselves ""mature adopters,"" with widespread acceptance and use of agile across business units. Meanwhile, according to our own observations, the companies that are deploying agile at scale have accelerated their innovation by up to 80 percent. There are many reasons traditional companies have not been able to successfully scale up their agile programs, but we believe a chief impediment is their existing operating models and organizational structures. In most of these companies, the process of software or product development remains fragmented and complex: a business request for a new website feature can kick-start a development process involving multiple teams, each tackling a series of tasks that feed into the original request. For instance, one team working on the front-end application, another updating associated servers and databases, and still another reconciling the front-end application with legacy back-end systems. What's more, the supporting business processes (among them, budgeting, planning, and outsourcing) and existing roles and responsibilities in both the IT organization and business units continue to adhere closely to the legacy waterfall approach. For most companies, it will be difficult to incorporate agile practices from small-scale pilots into all business units and functions-regardless of the success of those pilots-without making significant structural changes. Launching agile at scale: The research base We have helped many organizations adopt agile development practices in their IT and business groups. Building on that base, we recently studied in depth 13 large traditional organizations that are implementing agile methodologies across functions and business units (see sidebar, ""Launching agile at scale: The research base""). To facilitate widespread adoption, these companies have made changes in one or more parts of their operating models, targeting the following four areas: modifying their organizational structures to be more product oriented, improving interactions between the business and IT, redefining roles within business units and the IT organization, and reconsidering their budgeting and planning models (exhibit). The companies that have started on this path to change are realizing early benefits. One has switched from a project- to a product-oriented operating model. It has deployed talent and IT resources based on IT requirements for the entire customer-onboarding experience, for instance, rather than according to individual applications used during onboarding. As a result of this change in focus, it is now launching up to four website features a month instead of the typical four a year the company was able to release previously. This successful shift to agile was made more attainable when the company carefully considered when and how to phase in various modifications to its operating model. Scaling agile practices The benefits of agile are by now well known. Under agile development methodologies, IT organizations and product developers cocreate products and services with the business, rather than simply collecting feature specifications and throwing them back over the wall, as would happen under the waterfall development model. Teams can experiment with minimally viable products, test and learn from those prototypes, and ultimately deliver new software features and products in days or weeks, not years. Based on our observations of leading-edge adopters, quick codevelopment of products and collaboration among highly skilled IT and business professionals can happen on a broader scale when companies take steps to remake their operating models and organizational structures, focusing in particular on these four principles. Adopt a product-oriented organizational structure Traditional companies tend to organize their IT resources according to applications and projects-creating the type of fragmented development experiences described earlier. Instead, they need to organize IT resources around products, gathering business-unit leaders, developers, and other members of the organization in stable end-to-end teams that are focused on delivering designated business outcomes. Such a structure would mean the end of projects as they are traditionally defined and of coordination bodies such as the project-management office. In an agile-at-scale environment, products can't be defined solely as commercial offerings. They may actually be combinations of offerings (for instance, a payroll service), or the customer experience (say, all the features and tasks that make up the online purchasing journey), or an IT system shared by multiple product teams (such as pricing software that generates quotes on demand). So it's important for business and IT leaders to redefine the units of delivery. And once products have been recategorized, the company must designate an agile team, or clusters of agile teams, that will be responsible for the development and maintenance tasks associated with those products. These teams typically will include developers, testers, product owners, and others. They can draw additional support from a centralized group of experts-specialists in security issues, user-experience researchers, or enterprise IT architects, for instance. A large medical-device manufacturer significantly shortened its time to market by refining its organizational structure. Under its traditional structure, there could be as many as 20 handoffs when a business unit shared its specifications and requirements with the technology organization for a new piece of software or an additional feature in existing software. Because of the interdependencies among its products, leadership knew it wouldn't be enough to deploy agile within one business unit or within certain product-management teams in the technology organization. In 2015, the company tweaked its product-ownership model so that software requirements were directly transmitted from dedicated product owners in the business units to the agile teams, rather than passing through multiple parties. With this change, the company was able to reduce the amount of time it took to release products in the market. The structural changes also facilitated the rise of several communities of practice. These role-based or topic-based groups (sometimes called guilds) are critical in agile-at-scale environments. They can encourage the transfer of knowledge among team members, promote coordination between teams and functions, and become the catalyst for continuous performance improvement. Improve interactions between the business and IT To create an agile-at-scale environment, companies will need to break down silos between and within the business units and the IT organization. It's a perennial issue in most companies. But closer collaboration can be achieved by designating strong product owners from the business units to work with IT-individuals who understand the company's products well and who have the technical knowledge and authority to prioritize feature changes in products. In most traditional companies, product owners from the business side are involved in software development sporadically, providing input only as needed. To compensate for this lack of engagement, IT organizations often appoint a proxy product owner from IT. This arrangement can be useful in the near term but impede long-term product or project success. The proxy product owner typically has limited access to customers due to organizational barriers and possesses no mandate or the authority to make decisions. Because direction, priorities, and accountability are lacking, agile development is stalled. Teams face a significant amount of rework and waste. By contrast, a strong product owner has an in-depth understanding of the product in question, connections to and an understanding of customers, and full authority to make quick decisions. Such accelerated decision making helps to reduce bottlenecks in development and increase productivity. A provider of software-as-a-service solutions was struggling to get products to market in a timely fashion. There were marked lags in decision making and unclear lines of communication between IT and the business. In 2014, the company implemented a three-tiered product-owner structure, with a chief product owner leading a product domain, a senior product owner leading a product line, and product owners working with the scrum teams. Under this revised structure, interactions between IT and the business units improved. The lines of communications were clearer. The company was able to make decisions much more quickly while maintaining consistency and coordination within and across product-development groups. In part because of this structural change, the company was able to bring new software products to market quarterly-and in some instances monthly-rather than only once or twice a year. Six building blocks for creating a high-performing digital enterprise Read the article Redefine managerial roles and responsibilities About half the companies we studied have redefined managers' roles and responsibilities to account for the distinct capabilities associated with agile versus waterfall development. Consider the differences: the project manager working under a waterfall approach typically needs to coordinate a range of tasks occurring across application-development teams, database teams, and so on. Under an agile approach, however, the number of tasks (and therefore the need for coordination) is minimized. The tasks that remain are handled by a strong product owner or the agile team itself. Similarly, the process-management tasks that were traditionally done by line managers-for instance, identifying and addressing dependencies and assigning tasks to individuals-are handled by self-organizing, product-focused agile teams. A large bank in Africa redefined certain roles, shifting the lines of communication and responsibilities, to accommodate the bank's desire to deploy agile practices more widely. Previously, software-development teams worked with various technology leads to translate architects' requirements into technical specifications. Under an agile approach, however, this translation step was no longer needed. The bank eliminated the tech-lead role within agile teams. Developers are now empowered to talk directly to architects and product owners, so they gain a better understanding of customers' needs and can develop software to accommodate those needs. Line managers will, of course, continue to play central roles-providing career-development support and serving as subject-matter experts within agile teams and formally transferring their knowledge to others. But their responsibilities were redrawn, and this was communicated widely so that team members knew what to expect and whom to contact in particular situations. Indeed, the companies we've seen that have effectively implemented agile at scale are resolutely transparent-they provide clear guidelines about which decisions should be made within the team and which require external input. The boundaries are clearly defined; team members are empowered enough to be accountable but not so much that they could create major risks with rogue or carte-blanche actions. Reconsider budgeting and planning models IT organizations typically adhere to annual budgeting and planning cycles-which can involve painful rebalancing exercises across an entire portfolio of technology initiatives, as well as a sizable amount of rework and waste. This approach is anathema to companies that are seeking to deploy agile at scale. Some businesses in our research base are taking a different approach. Overall budgeting is still done yearly, but road maps and plans are revisited quarterly or monthly, and projects are reprioritized continually. A large European insurance provider restructured its budgeting processes so that each product domain is assigned a share of the annual budget, to be utilized by chief product owners. (Part of the budget is also reserved for requisite maintenance costs.) Budget responsibilities have been divided into three categories: a development council consisting of business and IT managers meets monthly to make go/no-go decisions on initiatives. Chief product owners are charged with the tactical allocation of funds-making quick decisions in the case of a new business opportunity, for instance-and they meet continually to rebalance allocations. Meanwhile, product owners are responsible for ensuring execution of software-development tasks within 40-hour work windows and for managing maintenance tasks and backlogs; these, too, are reviewed on a rolling basis. As a result of this shift in approach, the company has increased its budgeting flexibility and significantly improved market response times. A handful of companies are even exploring a venture-capital-style budgeting model. Initial funding is provided for minimally viable products (MVPs), which can be released quickly, refined according to customer feedback, and relaunched in the marketplace-the hallmarks of agile development. And subsequent funding is based on how those MVPs perform in the market. Under this model, companies can reduce the risk that a project will fail, since MVPs are continually monitored and development tasks reprioritized. Typically there is less waste and more transparency among portfolio and product managers, and it becomes easier for the company to scrap low-potential projects early. Choosing the right approach Revamping an operating model is a large undertaking. There will be significant risks to address and short-term disruptions as new ways of working take hold. As with any large change-management initiative, such a transformation will require long-term commitments from employees at all levels, in all functions and business units. The companies we've studied have used a number of approaches to alter elements of their operating models. At one extreme, some have used the ""lab approach,"" in which an agile operating model is set up apart from the rest of the organization to serve as a testing ground before capabilities and processes are rolled out to the entire IT organization. This approach makes most sense when the company has only limited support from senior management for larger changes and needs to prove the business case quickly. For the most part, however, the separate organizations created under the lab approach tend to remain separate rather than influencing change across the organization. At the other extreme, a handful of companies have embarked on a ""big-bang redesign,"" in which they move all functions and business units toward new organizational structures and roles, self-contained agile cells, and faster processes-all in one go. For this to work, senior leadership must be all in from day one, which is likely to be the case in only a small subset of companies. Somewhere in the middle is the ""wave and spike"" approach to deploying agile at scale. Under this model, individual teams are reconfigured as agile teams in waves, while elements of a new operating model are deployed in spikes. A large technology-solutions provider, for instance, needed to ramp up its digital capabilities fast. The company's IT organization was struggling to get products to market given the increasing size, complexity, and sheer number of projects. The company transitioned product-development teams to agile practices in waves; 5 were included in the first training and deployment cycle, while close to 20 were part of the second. As each successive wave of teams was indoctrinated to agile, feedback was collected and training materials were developed or revised for the next set of teams. Agile coaches were also installed to guide teams. Six months into its agile transformation, the company adopted a product-oriented organizational structure, gathering business-unit leaders, developers, engineers, and members of the IT organization into ""tribes."" Many months after that, the company focused on a different spike-interaction between IT and the business. It adjusted its operating model so the product-development group could collaborate more closely with the IT operations group (in a true DevOps model). As a result of these changes, time to market accelerated dramatically; because teams were cocreating products, the number of defects and the rework required decreased. Companies that are finding small-scale success with agile development practices may be loath to mess with a good thing, figuring it best to avoid the risks that widespread adoption might present. One of the chief risks in a digital business world, however, is standing still. To keep pace with new market entrants, emerging technologies, and changing customer expectations, companies will need to find ways to extend their capabilities in agile software development to all functions and business units. They must be willing to adapt the very fabric of their organizations and give agile methodologies the space and support they need to thrive. About the author(s) Santiago Comella-Dorda is a principal in McKinsey's Boston office, Swati Lohiya is an expert in the London office, and Gerard Speksnijder is a master expert in the San Francisco office.",en,52
324,1250,1464968625,CONTENT SHARED,-8404657131309965885,3609194402293569455,5948717978946018172,,,,HTML,http://www.b9.com.br/65195/inovacao/visa-vai-testar-nas-olimpiadas-do-rio-um-anel-para-pagamentos/,visa vai testar nas olimpíadas do rio um anel para pagamentos,"Alguns dos atletas que chegarem no Brasil para competir nos próximos jogos Olímpicos no Rio de Janeiro terão um adorno diferente nos dedos. A Visa , patrocinadora oficial do evento, deu a 45 deles um anel especial com NFC que servirá como substituto ao tradicional cartão de plástico. O anel por enquanto é um protótipo do que a Visa planeja lançar no futuro, e o seu nome também é um provisório auto-descritivo ""Visa Payment Ring"". Para usá-lo, basta aproximar o anel à máquina na hora do pagamento que a transação é autorizada online - a máquina, claro, precisa ter um sensor NFC. Como vai ser usado por atletas, o anel também é resistente a água, não tem bateria - ele pega energia direto da máquina na hora do pagamento - e é feito de uma cerâmica resistente. Em caso de perda, é possível desativar o código do anel como se fosse um cartão. A meu ver o principal impecilho para o uso desse anel será mesmo nos estabelecimentos no Rio. Os lojistas poderão ficar um pouco desconfiados da tecnologia quando virem funcionando pela primeira vez - isso é, até o comprovante do pagamento for emitido pela máquina. (Crédito da imagem no topo: Sarah Tew/CNET)",pt,51
325,1717,1468173252,CONTENT SHARED,8828175072897143018,-3508202263850468997,5963415954962394458,,,,HTML,http://qz.com/726338/the-code-that-took-america-to-the-moon-was-just-published-to-github-and-its-like-a-1960s-time-capsule/,"the code that took america to the moon was just published to github, and it's like a 1960s time capsule","When programmers at the MIT Instrumentation Laboratory set out to develop the flight software for the Apollo 11 space program in the mid-1960s, the necessary technology did not exist. They had to invent it. They came up with a new way to store computer programs, called ""rope memory,"" and created a special version of the assembly programming language. Assembly itself is obscure to many of today's programmers-it's very difficult to read, intended to be easily understood by computers, not humans. For the Apollo Guidance Computer (AGC), MIT programmers wrote thousands of lines of that esoteric code. Here's a very 1960s data visualization of just how much code they wrote-this is Margaret Hamilton, director of software engineering for the project, standing next to a stack of paper containing the software: The AGC code has been available to the public for quite a while-it was first uploaded by tech researcher Ron Burkey in 2003, after he'd transcribed it from scanned images of the original hardcopies MIT had put online. That is, he manually typed out each line, one by one. ""It was scanned by a airplane pilot named Gary Neff in Colorado,"" Burkey said in an email. ""MIT got hold of the scans and put them online in the form of page images, which unfortunately had been mutilated in the process to the point of being unreadable in places."" Burkey reconstructed the unreadable parts, he said, using his engineering skills to fill in the blanks. ""Quite a bit later, I managed to get some replacement scans from Gary Neff for the unreadable parts and fortunately found out that the parts I filled in were 100% correct!"" he said. The effort made the code available to any researcher or hobbyist who wanted to explore it. Burkey himself even used the software to create a simulation of the AGC : As enormous and successful as Burkey's project has been, however, the code itself remained somewhat obscure to many of today's software developers. That was until last Thursday (July 7), when former NASA intern Chris Garry uploaded the software in its entirety to GitHub, the code-sharing site where millions of programmers hang out these days. Within hours, coders began dissecting the software, particularly looking at the code comments the AGC's original programmers had written. In programming, comments are plain-English descriptions of what task is being performed at a given point. But as the always-sharp joke detectives in Reddit's r/ProgrammerHumor section found, many of the comments in the AGC code go beyond boring explanations of the software itself. They're full of light-hearted jokes and messages, and very 1960s references. One of the source code files, for example, is called BURN_BABY_BURN--MASTER_IGNITION_ROUTINE , and the opening comments explain why: About 900 lines into that subroutine , a reader can see the playfulness of the original programming team come through, in the first and last comments in this block of code: In the file called LUNAR_LANDING_GUIDANCE_EQUATIONS.s , it appears that two lines of code meant to be temporary ended up being permanent, against the hopes of one programmer: In the same file, there's also code that appears to instruct an astronaut to ""crank the silly thing around."" ""That code is all about positioning the antenna for the LR (landing radar),"" Burkey explained. ""I presume that it's displaying a code to warn the astronaut to reposition it."" And in the PINBALL_GAME_BUTTONS_AND_LIGHTS.s file, which is described as ""the keyboard and display system program ... exchanged between the AGC and the computer operator,"" there's a peculiar Shakespeare quote : This is likely a reference to the AGC programming language itself, as one Reddit user pointed out . The language used predetermined ""nouns"" and ""verbs"" to execute operations. The verb 37 , for example, means ""Run program,"" while the noun 33 means ""Time to ignition."" Now that the code is on GitHub, programmers can actually suggest changes and file issues. And, of course, they have . One developer submitted an issue saying, ""A customer has had a fairly serious problem with stirring the cryogenic tanks with a circuit fault present,"" and listed steps to reproduce the problem. ""Be aware that this may be hazardous to the tester attempting it,"" he added. The responses flooded in. One user suggested maybe the issue was not with the code, but something else: Another took it back to the basics : And one developer suggested perhaps the software simply needs to be updated :",en,51
326,2261,1473070647,CONTENT SHARED,-5684357871122991503,1895326251577378793,5142780734854854385,,,,HTML,http://www.mobiletime.com.br/31/08/2016/youse-seguradora-digital-se-diferencia-atraves-da-interface-movel/455342/news.aspx,mobile time - youse: seguradora digital se diferencia através da interface móvel,"Youse: seguradora digital se diferencia através da interface móvel A Youse é uma plataforma digital de vendas de seguros que tem a interface móvel no centro da sua estratégia de desenvolvimento de produtos. Todo o processo de contratação de seguros da empresa é feito on-line, seja pelo site ou pelo app ( Android , iOS ), desde a cotação até o envio da apólice - são vendidos seguros de automóvel, de vida e de residência. A Youse iniciou seus testes em abril e entrou em operação em julho. Em breve o aplicativo contará com um botão de emergência, para o acionamento de socorro no caso de pane ou acidente com o carro: através de poucas perguntas feitas dentro do aplicativo, será definido automaticamente que tipo de serviço o usuário precisa e lhe serão informadas quais procedências deve tomar. Outra novidade em desenvolvimento é a realização de vistoria remota do automóvel, em que o segurado tira fotos com a câmera do smartphone e envia pelo app para a Youse. Muitas vistorias serão feitas com a ajuda de software de análise de imagens, poupando o segurado da espera pela visita de um técnico. ""Para tudo o que fazemos, pensamos primeiro em mobile, desde o site até o app. E depois vemos como fica para tablet e desktop"", diz Eldes Mattiuzzo, CEO Youse. ""Ser mobile é um posionamento, um diferencial. Sempre vamos querer estar na frente nesse aspecto. Somos mobile first"", acrescenta. O desenvolvimento é todo feito internamente. Mais de 80% dos funcionários da Youse vieram do setor de tecnologia. A empresa conta com equipes próprias de desenvolvedores para Android e iOS. Espírito de start-up Embora seja controlada por um grupo tradicional do mercado de seguros, a Caixa Seguradora, a Youse mantém uma operação independente. A contratação de Mattiuzzo, que tem experiência como empreendedor, teve como objetivo assegurar um espírito de start-up à companhia. Hoje, os seguros oferecidos pela Youse, para fins regulatórios, fazem parte da Caixa Seguradora. Mas a autorização para que a start-up atue como seguradora está em fase final de avaliação pela Susep, órgão regulador desse mercado. A intenção é que a Youse seja totalmente independente da Caixa Seguradora. ""Criamos uma nova empresa, independente da Caixa Seguradora, para desenvolver produtos novos, com mindset diferente, com pegada mais de tecnologia e de start-up"", explica o executivo. Flexibilidade A contratação de um seguro com a Youse é feita através de um passo a passo on-line, com flexibilidade, para que o consumidor monte livremente os termos da sua apólice. Na prática, não há pacotes pré-definidos: é o consumidor quem escolhe as coberturas e assistências que deseja, com milhares de combinações possíveis. Desta forma, cada segurado tem uma apólice com a sua cara, diz o executivo. A linguagem adotada nesse processo é bastante simples e informal, diferente da habitual do mercado de seguros. Outra inovação está no modelo de negócios, com cobrança de mensalidade e sem prazo para o fim do contrato. Enquanto o consumidor estiver adimplente, o seguro está válido. ""O seguro é um produto muito careta. Queremos que as pessoas olhem o seguro de maneira diferente"", argumenta Mattiuzzo. A empresa estuda o lançamento nos próximos meses de mais dois produtos: 1) seguro automotivo para perdas parciais; 2) seguro entre grupo de amigos, em que parte do dinheiro pago pelos segurados é devolvido após determinado período, caso performem bem. Crise Mattiuzzo reconhece que o mercado de seguros também sofre com a recessão econômica do País. Os últimos dois anos foram de crescimento nominal abaixo da inflação e aumento dos índices de sinistralidade. Contudo, ele entende que este é justamente um bom momento para lançar uma seguradora digital e que se pretende inovadora. ""Todas as seguradoras estão procurando recuperar a rentabidiade, ajustar as margens e reduzir os índices de sinistralidades em suas carteiras, preocupados em superar a crise. Isso é ótimo para a gente, porque neste momento podemos fazer experimentações, testar conceitos. E vai demorar para nos copiarem ou para surgirem novos players"", avalia. ""Trabalhar com inovação e tempos de crise te dá mais tempo antes de ser copiado"", conclui. Eldes Mattiuzzo, CEO da Youse",pt,51
327,2120,1471632796,CONTENT SHARED,3142843830268077617,7645894863578715801,5690372062060589522,,,,HTML,https://medium.com/@AdityaPunjani/building-flipkart-lite-a-progressive-web-app-2c211e641883,building flipkart lite: a progressive web app,"Building Flipkart Lite: A Progressive Web App There have been a few turning points in the history of the web platform that radically changed how web apps were built, deployed and experienced. Ajax was one such pivot that led to a profound shift in web engineering and user experience. It allowed web applications to be responsive enough to challenge the conventional desktop apps. The same shift is now underway on mobile where web apps can finally be as performant, immersive, and engaging as native apps. With the adoption of Extensible Web Manifesto browser vendors started introducing new low-level APIs based on the feedback from developers. Low-level APIs enable developers to build new libraries, frameworks and tools on top of them instead of having to rely on high-level generalised APIs. The advent of these APIs brings unprecedented capabilities to the web. We at Flipkart decided to live on this bleeding edge and build a truly powerful and technically advanced web app while working to further evolve these APIs. Here's a look at how we've created an extremely Immersive, Engaging and a High-Performance Secure app. Immersive Web Apps have an instant and always updated distribution. However, there are too many variables with network connectivity that impede us from a providing a seamless experience. There have been multiple attempts at enabling offline web apps in the past, such as AppCache and using LocalStorage/ IndexedDB. However, these solutions failed to model complex offline use cases described below, making it painful to develop and debug issues. Service Workers replace these approaches by providing a scriptable network proxy in the browser that allows you to handle the requests programmatically. Service Workers can be scripted to intercept every network request and serve a response from cache even when the user is offline. We chose to use SW-Toolbox , a Service Workers wrapper library that enables using simple patterns such as NetworkFirst, CacheFirst or NetworkOnly. SW-Toolbox provides an LRU cache used in our app for storing previous search results on the browse page and last few visited product pages. The toolbox also has TTL-based cache invalidation mechanism that we use to purge out of date content. Service Workers provide low-level scriptable primitives that make this possible. Making the right solution work was as hard as devising it. We faced a wide spectrum of challenges from implementation issues to dev tooling bugs. We are collaborating with browser vendors to resolve these challenges and are actively contributing to SW-Toolbox . One significant learning that emerged from our use of Service Workers was to build a ""kill switch"". It is easy to end up with bugs in Service Workers and stale responses. Having a reliable mechanism to purge all caches has helped us to be proactively ready for any contingencies or surprises. Another cornerstone of a truly immersive experience is a fullscreen, standalone experience launched right from the home screen. This is what the Add to Home Screen (A2HS) prompt allows us to do. When the user chooses to add to home screen, the browser creates a high-quality icon on the home screen based on the metadata in the Web Manifest . The A2HS prompt is shown automatically to the user based on a heuristic that is specific to each browser. On Chrome, if the user has visited the site twice within a defined period, the prompt will trigger. In the newer Chrome versions, the prompt can be invoked programmatically . While the heuristic is indispensable to prevent spam on the web platform, we felt it was too conservative and convinced the Chrome team to tweak the heuristic for more commonly occurring scenarios. Based on our feedback, experiments are underway by the Chrome team to shorten the required delay between visits to the web app. Pro Tip: While developing for Add To Home Screen, you can disable the heuristic on Chrome by visiting the chrome://flags and enabling Bypass user engagement checks The latest version of Chrome supports the generation of a splash screen that shows up when the web app is launched from the home screen. It seamlessly fades in on the first paint of the app. This radically improves the launch experience and perceived performance. Engaging Being able to re-engage with our users on the web has always been a challenge. With the introduction of the Web Push API , we now have the capability to send Push Notifications to our users, even when the browser is closed. This is possible because Service Workers can be restarted even when no tab is open, allowing push events to deliver notifications to the users. High Performance Our focus was on two primary areas: Network and Rendering Performance. The app architecture is simply static HTML shells + API data. We pre-render the shells on server side during build time with critical CSS inlined. This leads to a quick first paint without waiting on CSS or JS to load. Once the async JS is loaded, API calls for data are made which fill in the content of the shells. With Service Workers, data consumption from the network can be reduced significantly by responding to requests from the cache. HTML shells are always served from the cache first and updated asynchronously in the background. This approach also helped in reducing the dependency on the network strength and eliminating all latencies on a repeat visit. On the client-side, Flipkart Lite is Single Page App based on React.js and our in-house Flux implementation Phrontend.js which is has been open sourced. Rendering Performance has always been a challenge for the web. We identified significant improvements in performance when GPU handled rasterization compared to CPU. Hence, we decided to leverage GPU rasterization on Chrome ( Project Ganesh ), by including the required meta tag in our HTML. The key is to avoid SVGs in performance-critical paths. SVGs being vectors, cannot leverage the same GPU optimizations currently. However, on latest Chrome versions this should be fixed. On the other hand, we have carefully balanced the right number of GPU-accelerated layers by measuring composition vs. paint costs. Thirdly, we're using GPU friendly animations namely Opacity and Transform transitions. Profiling on various mobile devices using Chrome Dev Tools Timeline panel and Chrome Tracing , helped us identify multiple bottlenecks and optimization paths. This helped us make the best of each frame during an animation and reach our 60fps target. We use the RAIL model for our performance budgets and strive to match and exceed expectations on each of the metrics. Secure End-to-end HTTPS encryption ensures a secure browsing session for our users. We also use Content Security Policy as an added layer of security against XSS and data injection attacks. All of this put together, manifested into a stellar experience for our users. It's been a remarkable journey building this web app, working with browser vendors and pushing the limits of web platform on mobile. The revolution is here and its time for developers to start leveraging these incredible new capabilities. The web is truly what you make of it, and we have only just begun. Introducing the Flipkart Lite Team -  Abhinav Rastogi , Boopathi Rajaa , Jai Santhosh , Abinash Mohapatra , Epuri , Karan Peri , Bharat KS , Baisampayan Saha , Mohammed Bilal , Ayesha Rana , Suvonil Chatterjee , Akshay Rajwade and Amar Nagaram",en,51
328,1267,1465149373,CONTENT SHARED,1415230502586719648,-1032019229384696495,8076865117288875488,,,,HTML,http://www.forbes.com/sites/louiscolumbus/2016/06/04/machine-learning-is-redefining-the-enterprise-in-2016/,machine learning is redefining the enterprise in 2016,"Bottom line: Machine learning is providing the needed algorithms, applications, and frameworks to bring greater predictive accuracy and value to enterprises' data, leading to diverse company-wide strategies succeeding faster and more profitably than before. Industries Where Machine Learning Is Making An Impact The good news for businesses is that all the data [...]",en,51
329,1998,1470415328,CONTENT SHARED,-8950763842769120954,5127372011815639401,-1389210559182708109,,,,HTML,http://www.bizreport.com/2016/08/mobify-report-reveals-impact-of-mobile-website-speed.html,mobify report reveals impact of mobile website speed,"Mobify's 2016 Q2 Mobile Insights Report provides marketers with back-up to what they probably already know. Analysis of large volumes of data from Mobify's customer base in Q2 2016, illustrates the impact of mobile website speed on consumer engagement, conversions and top-line growth. ""Retail has rallied around the idea that successful ecommerce hinges on evolving quickly to support mobile engagement, but there is little data to support it and limited large volume data from leading retailers. With Mobify Mobile Insights, we aim to fill that gap, offer eye-opening data, and as importantly, provide specific guidance for retailers so they can act on what they learn,"" said Vik Kambli, senior director of strategy at Mobify. So, what does Mobify's research tell us, specifically? - For every 100ms decrease in homepage load speed, Mobify's customer base saw a 1.11% lift in session based conversion, amounting to an average annual revenue increase of $376,789; - For every 100ms decrease in checkout page load speed, Mobify's customers saw a 1.55% life in session based conversion, amounting to an average annual revenue increase of $526,147; - Shoppers browse more on faster mobile websites; - An increase of one pageview per user results in a 5.17% lift in user based conversion, i.e. for each additional page viewed per user, Mobify saw their average customer's annual revenue increase by: $398,484.",en,51
330,1236,1464923376,CONTENT SHARED,2765063319512128208,5127372011815639401,-2629452978222634122,,,,HTML,https://medium.freecodecamp.com/360-million-reasons-to-destroy-all-passwords-9a100b2b5001?gi=4abb3186a8c1,360 million reasons to destroy all passwords - free code camp,"Welcome back. Is that a new haircut? OK, now that you've changed all those passwords, tell me: how secure were your passwords? Can you even still remember them? If you're using a password manager like LastPass, great - it will remember your passwords for you. It will even generate ""high entropy"" passwords, where the sun will burn out before someone could crack it (50-character strings of random letters, numbers, and symbols). But what about when you're on your phone, or on someone else's computer? Do you really want to install LastPass just to be able to log into LinkedIn on another device? Oh, and in case you missed it, LastPass got hacked recently . That's right - even companies whose core value proposition is security can still get hacked. There are two types of companies: those who have been hacked, and those who don't yet know they have been hacked. - John Chambers OK - you didn't use a password manager. Instead, you came up with a really secure password. You used symbols, a number, and even an uppercase letter (which is a huge effort considering how much everyone hates to press the shift key). But now you can't remember this complicated new password very easily. It's cool. We'll just come up with some really long passwords that are easy to remember, but hard to guess. As appealing as the above XKCD approach may seem, random common words aren't any easier to remember, and are harder to type correctly, according to a Carnegie Mellon University study . Also, a specific type of attack known as a dictionary attack can crack passwords like this fairly quickly. OK, so numbers, symbols, and uppercase letters it is. Gosh, these are still so hard to remember. Just write it down on a sticky note, and stick it to the bezel of our monitor with all of your other passwords, for easy reference. Just kidding. Everyone knows that writing a password down is the most dangerous thing you can do, right? Right? Actually, there's something that's even worse. It's using the same password on more than one website. Because this means that if one of those websites gets hacked, the hackers can use that same password to break into your accounts on other websites. But since creating and remembering multiple passwords is inconvenient, more than half of people use the same passwords on multiple sites. I forgot my password. 88% of people have forgotten at least one password recently, and have had to go reset it. Here's what happens when you forget a password: You go to a website, click the ""forgot password"" button, and type in your email address You open an email from their website and click a magic link they sent you This magic link takes you back to their website and logs you, then forces you to come up with a new password that meets their password requirements (and every website's requirements are different) If you think about this for a moment, you'll realize that your password does not actually matter. The only thing that matters is that you have access to the email address that's associated with your account. Thanks to the password reset functionality that every website uses, every website already supports passwordless login  - they just don't call it that. So wait - if anyone who can access your email account can get into your other accounts without knowing your password, why the heck do we even need passwords? What if instead of constantly resetting our passwords, we used that same passwordless login that those ""forgot password"" buttons use, but simply logged in without pestering people to create a new (useless) password? Here's what happens on websites that use passwordless login: You go to their website and type in your email address You open an email from their website and click a magic link that takes you back to their website and logs you in Wow. The exact same level of security as a password reset, but you don't need to spend minutes coming up with a password. And you don't have to go back in and change this password the next time another major website gets hacked. Again, the only way someone can break into your account is if they can gain access to your personal email address. And if they can do that, they can gain access to your other accounts anyway, because your email account is the skeleton key to your life. It's literally the only website that actually must require a password (and with biometrics and other security innovations, even that password may soon become unnecessary). Passwords are a huge inconvenience. The average person has literally spent waking days of their life creating, remembering, and resetting passwords. And the ironic thing is that passwords themselves don't make you more secure - they make you less secure. So why do websites use passwords? No clue. I think it's just expected that they use passwords. Maybe it makes people feel more secure. Maybe people think it's faster to use passwords than than to tab over to their email inbox. It isn't. In the time it takes to reset one password, you could have used a passwordless login 10 or 20 times. Most websites never log you out because they know you'll forget your password, and probably won't bother signing back in. When was the last time Facebook prompted you for your password? Microsoft announced this week that they are banning all common passwords. As of 2016, the most common passwords are still ""123456"" and ""password"", with ""starwars"" and "" ncc1701 "" not far behind them. Destroy all passwords. Free Code Camp is getting rid of passwords. We're going to start just sending you a magic link when you want to sign in for the first time on a new device. We're not the first website to do this, either. If you used your email address to sign up for Medium, you probably noticed that they don't use passwords anymore, either. So let's ditch passwords. The web will be less frustrating. And a whole lot more secure. If you're on the fence about this, go change all your passwords. Really. You should. There have been a ton of password breaches just in the past week. See for yourself whether you've been affected by one of them. After you've gone and changed all your passwords, imagine never having to reset a password again, and being more secure - not less secure - because of it. For more reading on passwordless login, read this excellent article: And for more about the recent hacks: Stay safe out there! If you liked this, click the�� below so other people will see this here on Medium.",en,51
331,1521,1466798268,CONTENT SHARED,5816734694927565572,-48161796606086482,-1849852261539863036,,,,HTML,http://willschenk.com/bot-design-patterns/?imm_mid=0e50a2&cmp=em-data-na-na-newsltr_20160622,bot design patterns,"In the antediluvian days before Google found everything for us, personal software agents were going to continuously do our bidding in cyberspace , tracking down air fares, making stock trades and otherwise doing futuristic things that we soon wouldn't be able to live without. The anxiety of Y2K rose and crested, and the messianic aura of agents was washed away by more centralized and effective solutions. The aspirational residue did yield some nuggets, and the idea has found partial success in the form of the bot . In the popular imagination bots exist on Twitter as art projects or as marketing machines or as the perfect online dating scandal . Bots can be characterized by asking a few questions, and from these answers we can tell something about their implementation and what it takes to build one: Do they react to messages? Do they know who they are talking to? Can they learn from what was said? Do they know where the conversation is taking place? Do they remember the overall conversation? From this, we can classify bots into one of six types: Notifiers Reactors Space Reactors Responders Space Responders Conversationists Each of these have progressively more complex context that the bot is operating in. Most chat mediums have different message types, where message can be directed towards or private to a particular user. Confounding things further, some have a native concept of spaces , generally called a room or a channel , and as we treat people differently in different context the bot will also need to be aware of the difference context that the conversion is taking place. We can call those services Spaced . Keeping track of User / Space context is more challenging than just User context Notifiers Notifiers are the ""simplest type""of bots, in the sense that the messaging logic is a minor part of the overall program. As a consequence they are the most common and the most varied. Broadcast a message based upon an external source Don't react to messages, so the answers to all the other questions are irrelevant. The trick in Notifiers is in figuring out what to say, not sending the message. These are best implemented by having one program generate the information, and sending that to another program that sends the message to the chat service. This could be a monitoring agent that is checking to see if a URL is responding, and if not, sends an alert. It could be a build process that returns a success or failure message. It could be a program that selects ""the next word"" from a lists of words and tweets one out every hour. Or it could make a bunch of API calls to different services, correlate that with information from a database, do more complicated calculations, and then post it. There's a bot that monitors the top links on hacker news, so it needs to pull down a list of top content, keep track of if it's tweeted it out before, and then send out the message. The sending part is simple, but the overall bot may not be. Reactors Reactors take action based upon incoming messages, but have no memory of who it's talking with or what space it's talking in. The who and where maybe in the message headers, and the bot can return the message in the correct place, but there is no bot-persisted memory of them. Reacts to messages on the chat service Doesn't remember anything that was said The basic idea is that a reactor gets a message, does something with it, and moves on. A lot of twitter bots follow this pattern, where they are looking at things on the stream and commenting upon them. For example, here's something that listens for tweets matching the string not a feminist and reacts by tweeting that that user isn't a feminist: It could do something more interesting, like looking for an image url, downloading it, running it through ImageMagick, posting it to S3, and then tweeting it back out. But since the bot itself keeps no message or user state, we consider it a reactor . Message Dispatching Here we reach a phase change in the level of ambient complexity of the bot. Simple Reactors respond to all messages the same way, relying on the filter to select messages that it cares about. More complex reactors need to figure out what message it's being sent and reply differently: Case loop: Command objects: This can also be done using metaprogramming , something like this: That is nonsense pseudo code, but a lot of chat bot libraries focus in on that area doing something very similar to what Thor does for command line programs. Space Reactors Space Reactors , in addition to having an awesome name, react to incoming messages, and know where they are receiving a message. The bot has memory of the place, and will respond to the user differently depending upon where the conversation was taking place. Reacts to messages on the chat service Knows where its being addressed Doesn't remember anything that was said When a reactor is run in on Spaced chat medium, it needs to factor in where the message was received. Instead of having user and message to work with, it now has user , message , and space . You can either find the space within each of the commands: Or locate the commands inside of the space, which lets you separate out which commands are available depending the properties of the space. If you have a HipChat bot and have a room for each company project, and you want to have the bot respond to wtf with the last commit message on github. The first thing that you need to do is look at which room the message came from, and correlate that a list of repositories, then make a Github API call, and finally post the commit message back to the room. The find_project method implies that you now need to have persistent state associated with the Space , in this case the unspecified project object. It's not just a matter of making sure that your reply goes to the correct place, but that what you reply will changed based upon where the person said it. Responders Responders listen for messages and remember what was said to them. They are different from Reactors because they have persisted User state, and so can learn something from the user. Reacts to messages Knows who they are talking to Can learn from what was said It's natural here to include the dispatching inside of the object that keeps track of the user state. Here's a pseudo example of a bot that lets you update your nickname. First by loading the user inside of the Bot: But it may also make sense to create different types of dispatchers for different users, in this pseudo code there are 3 different types of responders and depending upon who is sending the message different functions are available to them: This example is misleading because in real life direct inherency doesn't scale well, as we'll see below. Space Responders Space Responder learn things about who is talking with them and the context in which they are speaking. Reacts to messages Knows who they are talking to Can learn from what was said Knows where its being addressed The challenge here is how do you juggle the permutations of state. Lets take the example of a user is an admin in one project, where they can send commands to reboot the server, and in another project they can only open new tickets. In the basic case, you can do everything in each of your methods: Lets say that we had two different types of users, admins or not, and two different types of spaces, one that mapped to a project and another that mapped to a hangout area. We could expand out idea of the SpaceDispatcher to return a different command object based upon the combinations. We're also using inheritance to include all of the methods that the regular user can do into the class that has all of the Admin methods. This pseudocode has turned into an unholey mixture of Ruby and CoffeeScript, so it probably isn't work doing into language details with this. But straight up inheritance doesn't work well for this type of reuse, and in our example project admins won't be able to change the topic . These commands would better be structured as mixins. Conversationists The final, and technically most complicated type of bot to write is a Conversationalist . Reacts to messages Knows who they are talking to Can learn from what was said Has conversational state Knows where its being addressed The basic structure is: The User object is responsible for keeping track of conversations. current_conversation can be smart enough to create a new conversation when things are stale or if there user explicitly closed out an active one. Here is an example of code from a bot that asks you questions about where you are and what sort of food you are looking for. It searches a number of APIs in the background to see what information it can get from what you say. The code is written with an Adaptor to talk Twitter or GChat, and they both have a different idea of location. Sometimes it can get an accurate enough location the message metadata, but more likely it's too vague and it will ask you to clarify. ( Not all of that logic is below, but its useful to demonstrate the use of the state machine .) The knowledge attributes are what the bot knows in the conversation. Below you'll see an example of knowledge of the conversation from the state machine, knowledge of what as been said from previous messages, and knowledge of deduction from the content and context of the messages. Thats the shape of it Notifiers , Reactors , Space Reactors , Responders , Space Responders , and Conversationists . Writing Notifiers and Reactors are more playful, and as you get into things later in the list you spend a lot more time dealing with code and logic complexity. The distinction between a Spaced and Global chat medium is something that makes using chat libraries, and building Adapters to different services difficult. If the library was built with a Spaced service in mind it's pretty straightforward to make it work for one, but going the other way isn't swimming upstream. Towards a taxonomy of Twitter Bots BotSummit was a few days ago, and a bunch of people got together to talk about Twitter bots that they made largely as art projects. Here's a draft of a taxonomy of Twitter bots made by Tully Hanson . In the parlance of this post, these are mainly Notifiers and Reactors , with some possible Responders thrown in: Here's the source document Notifiers are interesting not because of their technology, but because of what they actually do. Here's a list of ideas that people came up with . Hubot from Github is a company chat bot written in Node Hubot from Github : GitHub, Inc., wrote the first version of Hubot to automate our company chat room. Hubot knew how to deploy the site, automate a lot of tasks, and be a source of fun in the company. Eventually he grew to become a formidable force in GitHub. But he led a private, messy life. So we rewrote him. Lita is similar to Hubot but written in Ruby Lita homepage Image from Lita Huginn is your agent, standing by Huginn takes a bit to install, but it's a full on software agent of the old school. Image ""Odin, der Göttervater"" by Carl Emil Doepler (1824-1905) - Wägner, Wilhelm. 1882. Nordisch-germanische Götter und Helden. Otto Spamer, Leipzig & Berlin. Page 7.. Licensed under Public domain via Wikimedia Commons - Link Huginn is a system for building agents that perform automated tasks for you online. They can read the web, watch for events, and take actions on your behalf. Huginn's Agents create and consume events, propagating them along a directed graph. Think of it as a hackable Yahoo! Pipes plus IFTTT on your own server. You always know who has your data. You do. We're just getting started, but here are some of the things that you can do with Huginn right now: Track the weather and get an email when it's going to rain (or snow) tomorrow (""Don't forget your umbrella!"") List terms that you care about and receive emails when their occurrence on Twitter changes. (For example, want to know when something interesting has happened in the world of Machine Learning? Huginn will watch the term ""machine learning"" on Twitter and tell you when there is a large spike.) Watch for air travel or shopping deals Follow your project names on Twitter and get updates when people mention them Scrape websites and receive emails when they change Connect to Adioso, HipChat, Basecamp, Growl, FTP, IMAP, Jabber, JIRA, MQTT, nextbus, Pushbullet, Pushover, RSS, Bash, Slack, StubHub, translation APIs, Twilio, Twitter, Wunderground, and Weibo, to name a few. Compose digest emails about things you care about to be sent at specific times of the day Track counts of high frequency events and send an SMS within moments when they spike, such as the term ""san francisco emergency"" Send and receive WebHooks Run arbitrary JavaScript Agents on the server Track your location over time Create Amazon Mechanical Turk workflows as the inputs, or outputs, of agents. (""Once a day, ask 5 people for a funny cat photo; send the results to 5 more people to be rated; send the top-rated photo to 5 people for a funny caption; send to 5 final people to rate for funniest caption; finally, post the best captioned photo on my blog."") Let me know cool things you build!",en,51
332,531,1461273189,CONTENT SHARED,3906974906788964502,4209517478660372522,2911195148391615485,,,,HTML,http://blog.caelum.com.br/modelando-apis-rest-com-swagger/,modelando apis rest com swagger,"Atualmente é bem comum que empresas utilizem APIs REST para a integração de aplicações, seja para consumir serviços de terceiros ou prover novos serviços. Ao consumir uma API existente, precisamos conhecer as funcionalidades disponíveis e detalhes de como invocá-las: recursos, URIs, métodos, Content-Types e outras informações. Ao prover uma nova API REST, além da implementação, há outras duas preocupações comuns: como modelar e documentar a API? Ferramentas para modelagem e documentação de APIs REST Em um Web Service do estilo SOAP temos o WSDL , que funciona como uma documentação (para máquinas) do serviço, facilitando a geração automatizada dos clientes que vão consumi-lo. Além disso, podemos modelar nosso serviço escrevendo o WSDL, em uma abordagem conhecida como Contract-First . Não é nada legível nem fácil de escrever, mas funciona. Só que no mundo dos Web Services REST não temos o WSDL. E agora? Algumas ferramentas para nos auxiliar nessa questão foram criadas, e dentre elas temos: WSDL 2.0, WADL, API Blueprint, RAML e Swagger. Neste post vamos abordar o Swagger , que é uma das principais ferramentas utilizadas para modelagem, documentação e geração de código para APIs do estilo REST. Mas o que exatamente é o Swagger? O Swagger é um projeto composto por algumas ferramentas que auxiliam o desenvolvedor de APIs REST em algumas tarefas como: Modelagem da API Geração de documentação (legível) da API Geração de códigos do Cliente e do Servidor, com suporte a várias linguagens de programação Para isso, o Swagger especifica a OpenAPI , uma linguagem para descrição de contratos de APIs REST. A OpenAPI define um formato JSON com campos padronizados (através de um JSON Schema ) para que você descreva recursos, modelo de dados, URIs, Content-Types, métodos HTTP aceitos e códigos de resposta. Também pode ser utilizado o formato YAML, que é um pouco mais legível e será usado nesse post. Além da OpenAPI, o Swagger provê um ecossistema de ferramentas. As principais são: Swagger Editor - para a criação do contrato Swagger UI - para a publicação da documentação Swagger Codegen - para geração de ""esqueletos"" de servidores em mais de 10 tecnologias e de clientes em mais de 25 tecnologias diferentes Nesse post, vamos focar na parte de modelagem de uma nova API. Futuramente teremos outro post focando na documentação de uma API já existente. Modelando a API da Payfast Para modelar nossa nova API, utilizaremos o Swagger Editor. Você pode instalá-lo localmente , executando uma aplicação NodeJS, ou utilizar a versão online em editor.swagger.io . Vamos modelar a API da Payfast, uma aplicação de pagamentos bem simples que é estudada no curso SOA na prática . Pra começar, devemos definir algumas informações iniciais, como a versão do Swagger que estamos usando: swagger: '2.0' O título, descrição e versão da API devem ser definidos: info: title: Payfast API description: Pagamentos rápidos version: 1.0.0 Em host , inserimos o endereço do servidor da API, em basePath colocamos o contexto da aplicação e em schemes informamos se a aplicação aceita HTTP e/ou HTTPS. host: localhost:8080 basePath: /fj36-payfast/v1 schemes: - http - https Defininindo o modelo de dados De alguma forma, precisamos definir quais dados são recebidos e retornados pela API. Na nossa API, recebemos dados de uma Transação , que tem um código, titular, data e valor. A partir disso, geramos um Pagamento com id, status e valor. Em um WSDL, esse modelo de dados é definido através de um XML Schema (XSD). No caso do Swagger, o modelo de dados fica em um JSON Schema na seção definitions do contrato. De acordo com o JSON Schema, em type podemos usar tipos primitivos de dados para números inteiros ( integer ), números decimais ( number ), textos ( string ) e booleanos ( boolean ). Esses tipos primitivos podem ser modificados com a propriedade format . Para o tipo integer temos os formatos int32 (32 bits) e int64 (64 bits, ou long). Para o number , temos os formatos float e double . Não há um tipo específico para datas, então temos que utilizar uma string com o formato date (só data) ou date-time (data e hora). Além dos tipos primitivos, podemos definir objetos com um type igual a object . Esses objetos são compostos por várias outras propriedades, que ficam em properties . No nosso caso, o modelo de dados com os objetos Transacao e Pagamento ficaria algo como: definitions: Transacao: type: object properties: codigo: type: string titular: type: string data: type: string format: date valor: type: number format: double Pagamento: type: object properties: id: type: integer format: int32 status: type: string valor: type: number format: double Defininindo os recursos da API Com o modelo de dados pronto, precisamos modelar os recursos da nossa API e as respectivas URIs. No Payfast, teremos o recurso Pagamento acessível pela URI /pagamentos . Um POST em /pagamentos cria um novo pagamento. Se o pagamento criado tiver o id 1, por exemplo, as informações estariam acessíveis na URI /pagamentos/1 . Podemos fazer duas coisas com o nosso pagamento: para confirmá-lo, devemos enviar um PUT para /pagamentos/1 ; para cancelá-lo, enviamos um DELETE. No Swagger, as URIs devem ser descritas na seção paths : paths: /pagamentos: post: summary: Cria novo pagamento /pagamentos/{id}: put: summary: Confirma um pagamento delete: summary: Cancela um pagamento Definindo os parâmetros de request Para a URI /pagamentos , que recebe um POST, é enviada uma transação no corpo da requisição, que deve estar no formato JSON. É feita uma referência ao modelo Transacao definido anteriormente. paths: /pagamentos: post: summary: Cria novo pagamento consumes: - application/json parameters: - in: body name: transacao required: true schema: $ref: '#/definitions/Transacao' Já para a URI /pagamentos/{id} , é definido um path parameter com o id do pagamento. Esse parâmetro pode ser descrito na seção parameters , logo acima da seção paths , e depois referenciado nos métodos. parameters: pagamento-id: name: id in: path description: id do pagamento type: integer format: int32 required: true paths: /pagamentos: #código omitido... /pagamentos/{id}: put: summary: Confirma um pagamento parameters: - $ref: '#/parameters/pagamento-id' delete: summary: Cancela um pagamento parameters: - $ref: '#/parameters/pagamento-id' Definindo os tipos de response Definidos os parâmetros de request , precisamos modelar o response . Depois da criação do pagamento, deve ser retornado um response com o status 201 (Created) juntamente com a URI do novo pagamento no header Location e uma representação em JSON no corpo da resposta. paths: /pagamentos: post: summary: Cria novo pagamento consumes: - application/json produces: - application/json #código omitido... responses: '201': description: Novo pagamento criado schema: $ref: '#/definitions/Pagamento' headers: Location: description: uri do novo pagamento type: string Após a confirmação de um pagamento, é simplesmente retornado o status 200 (OK). O mesmo retorno acontece após um cancelamento. /pagamentos/{id}: put: summary: Confirma um pagamento parameters: - $ref: '#/parameters/pagamento-id' responses: '200': description: 'Pagamento confirmado' delete: summary: Cancela um pagamento parameters: - $ref: '#/parameters/pagamento-id' responses: '200': description: 'Pagamento cancelado' O contrato final do exemplo utilizado pode ser aberto no Swagger Editor em: bit.ly/swagger-editor-payfast-api Perceba que no menu superior temos a opção Generate Server para gerar um esqueleto do servidor em Java (JAX-RS e Spring-MVC), PHP (Slim e Silex), Python (Flask), entre outras tecnologias. Há também a opção Generate Client , que gera clientes nessas tecnologias e em diversas outras. Concluindo A abordagem utilizada nesse post foi a conhecida como Contract-First ou API-First Development . Modelamos a API pensando nos dados, nos recursos, URIs, métodos, parâmetros e respostas. Começamos a definição do serviço criando primeiro a API de comunicação, para só posteriormente pensar na implementação. Esta abordagem gera um desacoplamento entre implementação e interface de uso, além de permitir que tando o lado cliente quanto o lado servidor possam iniciar seu desenvolvimento assim que a API estiver definida, mesmo sem uma implementação finalizada. Uma outra abordagem (talvez mais comum) é começar pela implementação para só depois pensar na documentação e talvez realizar ajustes de modelagem. Essa abordagem é conhecida como Contract-Last e o uso dela com Swagger será abordado em outro post. E você? Já usou o Swagger em algum projeto para modelar uma nova API, no estilo Contract-First? Conte-nos como foi a experiência!",pt,51
333,1496,1466644153,CONTENT SHARED,-6697357763177451167,-6946355789336786528,438386484399384641,,,,HTML,https://pagamento.me/starbucks-tem-mais-dinheiro-do-cliente-em-cartoes-do-que-bancos/,starbucks tem mais dinheiro do cliente em cartões do que bancos,"O nascimento do ""novo banco"" se dará na mão dos grandes distribuidores. No artigo Desintermediação Bancária , ilustramos o nascimento de super fintechs como o Sem Parar, por exemplo. É nessa onda que os próximos gigantes do segmento financeiro vão nascer: com o poder de distribuição. Redes como os postos Ipiranga, redes de academia como a Smartfit , que já ultrapassam os milhões de alunos e a rede de benefícios Multiplus , já têm mais volume financeiro e clientes do que bancos. Esse é o novo movimento financeiro que vamos assistir em breve. Starbucks tem mais dinheiro em caixa do que bancos Desde o lançamento do Starbucks Card, a rede vem fazendo um grande trabalho com os clientes em gerar a necessidade de uso do seu cartão e seu aplicativo de pagamento. Uma pesquisa do Wall Street Journal revelou onde as pessoas estão colocando dinheiro, fora dos bancos. E pahh! O número surpreendeu. Os cartões da Starbucks, usados pelos clientes, somaram cerca de U$1,2 bilhões de dinheiro creditado. O volume é maior do que alguns bancos têm em caixa, inclusive. Algumas pessoas guardam dinheiro em colchão, outras em bancos e 12 milhões de pessoas, guardam em cartões da Starbucks. Coffee Bank!",pt,51
334,2018,1470769964,CONTENT SHARED,-1634742667970363668,-1443636648652872475,8486002198741992178,,,,HTML,https://techcrunch.com/2016/08/09/google-cloud-platforms-preemptible-vms-are-now-up-to-33-cheaper/,google cloud platform's preemptible vms are now up to 33% cheaper,"For a while, Google, Amazon and Microsoft were constantly undercutting each other's cloud computing prices , but lately, it's been pretty quiet on that front. Today, however, Google is firing a new salvo by reducing the price of its preemptible virtual machines (VMs) by up to 33 percent. Preemptible VMs are Google's version of Amazon's spot instances for AWS (Microsoft Azure, though, doesn't currently offer this type of instance). As the names imply, there is no guarantee how long you'll be able to use a given machine, though. Google and Amazon use this system to bring up their utilization levels by offering access to these VMs for cheap when demand is low, but as demand picks up, they rotate them back into their regular pool, where they can charge more. The major difference between Google's and Amazon's approach is that while AWS uses an auction system for pricing these machines, Google offers its users set prices - and those prices can be up to 80 percent lower than those of the equivalent non-preemptible machine. On Google's platform, these machines can also only run for up to 24 hours, so they don't work well for every workload (on AWS, they will run until the spot price increases above what you paid). If you - and your workloads - are flexible, though, you can save quite a bit of money on using this type of VM. ""Our customers are using Preemptible VMs to analyze data, render movies, process satellite imagery, analyze genomic data, understand financial markets, transcode media and complete a variety of business and engineering tasks, using thousands of Preemptible VM cores in a single job,"" Google product manager Michael Basilyan writes in today's announcement. ""We believe that the price reduction for Preemptible VMs will unlock even more computing opportunities and enable you to tackle interesting science and business problems."" With a bit of luck, today's move by Google will also drive Amazon to drop its prices. If you can't use preemptible machines for your workloads, by the way, it's worth taking a look at Google's new "" VM Rightsizing Recommendations ."" This tool analyzes your VM utilization and then gives you suggestions for scaling machines up or down to better meet your needs.",en,51
335,1953,1470076525,CONTENT SHARED,-3102652803014865472,7421498048184238966,3547502319603620906,,,,HTML,http://g1.globo.com/economia/negocios/noticia/2016/07/conselho-da-sabmiller-aceita-proposta-de-compra-da-ab-inbev.html,conselho da sabmiller aceita proposta de compra da ab inbev,"O Conselho de Administração da cervejaria britânica SABMiller aceitou nesta sexta-feira (29) a proposta de compra por parte da gigante AB InBev, de capital belga e brasileiro, na que se anuncia como uma das maiores fusões/aquisições da história . O negócio é avaliado em mais de US$ 100 bilhões. A nova oferta apresentada pela líder do setor de cervejas foi de 45 libras, uma a mais que na proposta anterior, por cada ação da SABMiller -- a número 2 do mundo -, o que confere à cervejaria um valor de mercado de 79 bilhões de libras (o equivalente a US$ 103 bilhões), segundo a agência France Presse. O Conselho de Administração da SABMiller indicou em um comunicado que ""prevê por unanimidade"" recomendar aos seus acionistas que aceitem a transação. A oferta de compra foi lançada no final do ano passado , mas a AB InBev aumentou a sua oferta pela rival britânica , compensando parcialmente a desvalorização da libra esterlina provocada pela vitória do Brexit . O valor da nova oferta, no entanto, foi inferior ao estimado pelos acionistas da SABMiller, que antes do Brexit resistiam à fusão/aquisição e avaliavam o preço da companhia em US$ 121 bilhões. AB InBev, maior cervejaria mundial, comemorou a decisão de sua até então principal rival e expressou sua vontade de concluir ""o quanto antes"" a operação, segundo a France Presse. Próximos passos Desde a primeira aproximação, ""muitos fatores afetaram o valor da proposta, sobretudo, o impacto do voto a favor do Brexit sobre a libra esterlina"", admitiu o presidente da SABMiller, Jan do Plessis, citado no comunicado. A operação já recebeu o aval das autoridades de concorrência da UE, mas também exige a autorização das instâncias antimonopólio de cerca de 15 países. A AB InBev aceitou uma série de concessões para obter a autorização da UE, entre elas a se desfazer da maioria dos negócios da SABMiller na Europa - incluindo as marcas Foster's e Grolsch - e de se desvincular da cervejaria japonesa Peroni. A nova gigante de cervejas combinará as marcas Budweiser, Stella Artois e Corona, da AB InBev, com Peroni, Grolsh e Pilsner Urquell, da SABMiller, diminuindo as principais competidoras como Heineken e Carlsberg. Ao comprar a SABMiller, a AB InBev adicionaria mercados da América Latina como Colômbia e Peru e entraria na África em um momento em que alguns de seus mercados domésticos como os Estados Unidos estão enfraquecendo, conforme consumidores favorecem cervejas artesanais e drinks. Bilionário brasileiro por trás do negócio A AB InBev é parcialmente controlada pelo fundo de private equity 3G Capital, administrado por investidores brasileiros, e controla indiretamente a brasileira Ambev. A AB Inbev tem entre os seus donos os brasileiros Jorge Paulo Lemann (homem mais rico do Brasil com fortuna estimada em mais de US$ 25 bilhões, segundo a Bloomberg) , Carlos Alberto Sicupira e Marcel Herrmann Telles. Dono do fundo 3G Capital e sócio da AB Inbev, Lemann tem feito negócios em parceria com a Berkshire Hathaway, de Warren Buffet, como a compra da Heinz e fusão da companhia com a Kraft Foods.",pt,51
336,2794,1480461629,CONTENT SHARED,1561244083438776495,-3203894957285229214,-6851422767977892796,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36",ON,CA,HTML,https://aws.amazon.com/blogs/aws/new-amazon-linux-container-image-for-cloud-and-on-premises-workloads/,new amazon linux container image for cloud and on-premises workloads,"The Amazon Linux AMI is designed to provide a stable, secure, high performance execution environment for applications running on EC2. With limited remote access (no root login and mandatory SSH key pairs) and a very small number of non-critical packages installed, the AMI has a very respectable security profile. Many of our customers have asked us to make this Linux image available for use on-premises, often as part of their development and testing workloads. Today I am happy to announce that we are making the Amazon Linux Container Image available for cloud and on-premises use. The image is available from the EC2 Container Registry (read Pulling an Image to learn how to access it). It is built from the same source code and packages as the AMI and will give you a smooth path to container adoption. You can use it as-is or as the basis for your own images. In order to test this out, I launched a fresh EC2 instance, installed Docker, and then pulled and ran the new image. Then I installed cowsay and lolcat (and the dependencies) and created the image above. To learn more about this image, read Amazon Linux Container Image . You can also use the image with Amazon EC2 Container Service . To learn more, read Using Amazon ECR Images with Amazon ECS . - Jeff ; Comments? Questions? Talk to us",en,51
337,369,1460526387,CONTENT SHARED,-6347212504547574224,-1032019229384696495,-6165125069895538182,,,,HTML,https://googleblog.blogspot.com/2016/04/building-more-accessible-technology.html,building more accessible technology,"Nearly 20 percent of the U.S. population will have a disability during their lifetime, which can make it hard for them to access and interact with technology, and limits the opportunity that technology can bring. That's why it's so important to build tools to make technology accessible to everyone-from people with visual impairments who need screen readers or larger text, to people with motor restrictions that prevent them from interacting with a touch screen, to people with hearing impairments who cannot hear their device's sounds. Here are some updates we've made recently to make our technology more accessible: A tool to help develop accessible apps Accessibility Scanner is a new tool for Android that lets developers test their own apps and receive suggestions on ways to enhance accessibility. For example, the tool might recommend enlarging small buttons, increasing the contrast between text and its background and more. Improvements for the visually impaired in Android N A few weeks ago we announced a preview of Android N for developers. As part of this update we're bringing Vision Settings-which lets people control settings like magnification, font size, display size and TalkBack -to the Welcome screen that appears when people activate new Android devices. Putting Vision Settings front and center means someone with a visual impairment can independently set up their own device and activate the features they need, right from the start. An improved screen reader on Chromebooks Every Chromebook comes with a built-in screen reader called ChromeVox, which enables people with visual impairments to navigate the screen using text to speech software. Our newest version, ChromeVox Next Beta, includes a simplified keyboard shortcut model, a new caption panel to display speech and Braille output, and a new set of navigation sounds. For more information, visit chromevox.com . Edit documents with your voice Google Docs now allows typing, editing and formatting using voice commands -for example, ""copy"" or ""insert table""-making it easier for people who can't use a touchscreen to edit documents. We've also continued to work closely with Freedom Scientific , a leading provider of assistive technology products, to improve the Google Docs and Drive experience with the JAWS screen reader. Voice commands on Android devices We recently launched Voice Access Beta, an app that allows people who have difficulty manipulating a touch screen due to paralysis, tremor, temporary injury or other reasons to control their Android devices by voice. For example, you can say ""open Chrome"" or ""go home"" to navigate around the phone, or interact with the screen by saying ""click next"" or ""scroll down."" To download, follow the instructions at . To learn more about Google accessibility as a whole, visit google.com/accessibility .",en,51
338,2239,1472681840,CONTENT SHARED,1549650080907932816,-1443636648652872475,-8078959461925337655,,,,HTML,https://www.oreilly.com/ideas/spark-comparison-aws-vs-gcp,spark comparison: aws vs. gcp,"Hourglass comparison. (source: Pixabay ). Tianhui Michael Li and Ariel M'ndange-Pfupfu will lead a hands-on online course Oct 10, 12, and 14, 2016: Distributed Computing with Spark for Beginners . Instruction includes building functioning applications from end-to-end and mastering critical tooling around Spark. There's little doubt that cloud computing will play an important role in data science for the foreseeable future. The flexible, scalable, on-demand computing power available is an important resource, and as a result, there's a lot of competition between the providers of this service. Two of the biggest players in the space are Amazon Web Services (AWS) and Google Cloud Platform (GCP). This article includes a short comparison of distributed Spark workloads in AWS and GCP-both in terms of setup time and operating cost. We ran this experiment with our students at The Data Incubator, a big data training organization that helps companies hire top-notch data scientists and train their employees on the latest data science skills. Even with the efficiencies built into Spark, the cost and time of distributed workloads can be substantial, and we are always looking for the most efficient technologies so our students are learning the best and fastest tools. Submitting Spark jobs to the cloud Spark is a popular distributed computation engine that incorporates MapReduce-like aggregations into a more flexible, abstract framework. There are APIs for Python and Java, but writing applications in Spark's native Scala is preferable. That makes job submission simple, as you can package your application and all its dependencies into one JAR file. It's common to use Spark in conjunction with HDFS for distributed data storage, and YARN for cluster management; this makes Spark a perfect fit for AWS's Elastic MapReduce (EMR) clusters and GCP's Dataproc clusters. Both EMR and Dataproc clusters have HDFS and YARN preconfigured, with no extra work required. Configuring cloud services Managing data, clusters, and jobs from the command line is more scalable than using the web interface. For AWS, this means installing and using the command-line interface (cli). You'll have to set up your credentials beforehand as well as make a separate keypair for the EC2 instances that are used under the hood. You'll also need to set up roles-basically permissions-for both users (making sure they have sufficient rights) and EMR itself (usually running aws emr create-default-roles in the cli is good enough to get started). For GCP the process is more straightforward. If you install the Google Cloud SDK and sign in with your Google account, you should be able to do most things right off the bat. The thing to remember here is to enable the relevant APIs in the API Manager: Compute Engine, Dataproc, and Cloud Storage JSON. Once you have things set up to your liking, the fun part begins! Using commands like aws s3 cp or gsutil cp you can copy your data into the cloud. Once you have buckets set up for your inputs, outputs, and anything else you might need, running your app is as easy as starting up a cluster and submitting the JAR file. Make sure you know where the logs are kept-it can be tricky to track down problems or bugs in a cloud environment. You get what you pay for When it comes to cost, Google's service is more affordable in several ways. First, the raw cost of purchasing computing power is cheaper. Running a Google Compute Engine machine with 4 vCPUs and 15 GB of RAM will run you $0.20 every hour, or $0.24 with Dataproc. An identically-specced AWS instance will cost you $0.336 per hour running EMR. The second factor to consider is the granularity of the billing. AWS charges by the hour, so you pay the full rate even if your job takes 15 minutes. GCP charges by the minute, with a 10-minute minimum charge. This ends up being a huge difference in cost in a lot of use cases. Both services have various other discounts. You can effectively bid on spare cloud capacity with AWS's spot instances or GCP's preemptible instances. These will be cheaper than dedicated, on-demand instances, but they're not guaranteed to be available. Discounted rates are available on GCP if your instances live for long periods of time (25% to 100% of the month). On AWS, paying some of the costs upfront or buying in bulk can save you some money. The bottom line is, if you're a power user and you use cloud computing on a regular or even constant basis, you'll need to delve deeper and perform your own calculations. Lastly, the costs for new users wanting to try out these services are lower for GCP. They offer a 60-day free trial with $300 in credit to use however you want. AWS only offers a free tier where certain services are free to a certain point or discounted, so you will end up paying to run Spark jobs.This means that if you want to test out Spark for the first time, you'll have more freedom to do what you want on GCP without worrying about price. Performance comparison We set up a trial to compare the performance and cost of a typical Spark workload. The trial used clusters with one master and five core instances of AWS's m3.xlarge and GCP's n1-standard-4 . They differ slightly in specification, but the number of virtual cores and amount of memory is the same. In fact, they behaved almost identically when it came to job execution time. The job itself involved parsing, filtering, joining, and aggregating data from the publicly available Stack Exchange Data Dump . We ran the same JAR on a ~50M subset of the data ( Cross Validated ) and then on the full ~9.5G data set. Figure 1. Credit: Michael Li and Ariel M'ndange-Pfupfu. Figure 2. Credit: Michael Li and Ariel M'ndange-Pfupfu. The short job clearly benefited from GCP's by-the-minute billing, being charged only for 10 minutes of cluster time, whereas AWS charged for a full hour. But even the longer job was cheaper on GPS both because of fractional-hour billing and a lower per-unit time cost for comparable performance. It's also worth noting that storage costs weren't included in this comparison. Conclusion AWS was the first mover in the space, and this shows in the API. Its ecosystem is vast, but its permissions model is a little dated, and its configuration is a little arcane. By contrast, Google is the shiny new entrant in this space and has polished off some of the rough edges. It is missing some features on our wishlist, like an easy way to auto-terminate clusters and detailed billing information broken down by job. Also, for managing tasks programmatically in Python, the API client library isn't as full-featured as AWS's Boto . If you're new to cloud computing, GCP is easier to get up and running, and the credits make it a tempting platform. Even if you are already used to AWS, you may still find the cost savings make switching worth it, although the switching costs may not make moving to GCP worth it. Ultimately, it's difficult to make sweeping statements about these services because they're not just one entity; they're entire ecosystems of integrated parts, and both have pros and cons. The real winners are the users. As an example, at The Data Incubator, our Ph.D. data science fellows really appreciate the cost reduction as they learn about distributed workloads. And while our big data corporate training clients may be less price sensitive, they appreciate being able to crunch enterprise data faster, while holding price constant. Data scientists can now enjoy the multitude of options available, and the benefits of having a competitive cloud computing market. Article image: Hourglass comparison. (source: Pixabay ). How Baidu combined Tachyon with Spark SQL to increase speed 30-fold The Lambda Architecture has its merits, but alternatives are worth exploring. What it looks like to analyze, visualize, and even forecast human society using global news coverage. In this O'Reilly training video, the ""Hadoop Application Architectures"" authors present an end-to-end case study of a clickstream analytics engine to provide a concrete example of how to architect and implement a complete solution with Hadoop. In this segment, they provide an overview of the complete architecture. Presenters: Mark Grover, Gwen Shapira, Jonathan Seidman, Ted Malaska",en,51
339,505,1461172775,CONTENT SHARED,-2667874153709252756,-7456488753754080246,-1305335358855337822,,,,HTML,http://criticalhits.com.br/copel-faz-propaganda-genial-sobre-o-limite-de-banda-larga-residencial/,copel faz propaganda genial sobre o limite de banda larga residencial - critical hits,"A Copel é uma das poucas provedoras de internet que não aderiu ao limite de dados para planos de internet no Brasil. A empresa costuma ser muito elogiada nas redes sociais (como a GVT já foi um dia no passado), principalmente quando a fibra da empresa chega nos bairros desses usuários. Como a empresa não adotou o limite de dados para internet, nada mais justo do que capitalizar um pouco de pontos com o consumidor por ter feito isso, não é mesmo? Pois foi exatamente isso o que a companhia fez ao liberar algumas peças publicitárias que demonstram exatamente como está a situação atual do mercado: Sensacionais, não? Ainda tem mais três propagandas de televisão/Youtube que também ficaram muito boas: É uma pena não ter Copel no Rio Grande do Sul, amigos. Torne-se um patrão do site! Você sabia que ao tornar-se um patrão do Critical Hits, você ajuda o site a continuar crescendo e ainda ganha vantagens exclusivas como acesso a um design mais clean sem propagandas, Critical Cast 5 dias antes de todo mundo e acesso ao nosso grupo secreto no Facebook/Whatsapp? Torne-se já um patrão você também! Você já ouviu o Critical Cast dessa semana? Agora que você ouviu o cast e nos amou, vote já no Critical Cast para melhor Podcast do Brasil !",pt,51
340,2690,1477969370,CONTENT SHARED,-8657415528200615063,-8020832670974472349,610589969482291134,Android - Native Mobile App,SP,BR,HTML,http://v8project.blogspot.com.br/2016/10/webassembly-browser-preview.html?m=1,webassembly browser preview,"Today we're happy to announce, in tandem with Firefox and Edge , a WebAssembly Browser Preview. WebAssembly or wasm is a new runtime and compilation target for the web, designed by collaborators from Google, Mozilla, Microsoft, Apple, and the W3C WebAssembly Community Group . What does this milestone mark? This milestone is significant because it marks: a release candidate for our MVP (minimum viable product) design (including semantics , binary format , and JS API ) compatible and stable implementations of WebAssembly behind a flag on trunk in V8 and SpiderMonkey, in development builds of Chakra, and in progress in JavaScriptCore a working toolchain for developers to compile WebAssembly modules from C/C++ source files a roadmap to ship WebAssembly on-by-default barring changes based on community feedback You can read more about WebAssembly on the project site as well as follow our developers guide to test out WebAssembly compilation from C & C++ using Emscripten. The binary format and JS API documents outline the binary encoding of WebAssembly and the mechanism to instantiate WebAssembly modules in the browser, respectively. Here's a quick sample to show what wasm looks like: Greatest Common Divisor function Since WebAssembly is still behind a flag in Chrome ( chrome://flags/#enable-webassembly ), it is not yet recommended for production use. However, the Browser Preview period marks a time during which we are actively collecting feedback on the design and implementation of the spec. Developers are encouraged to test out compiling and porting applications and running them in the browser. V8 continues to optimize the implementation of WebAssembly in the TurboFan compiler . Since last March when we first announced experimental support, we've added support for parallel compilation. In addition, we're nearing completion of an alternate asm.js pipeline, which converts asm.js to WebAssembly under the hood so that existing asm.js sites can reap some of the benefits of WebAssembly ahead-of-time compilation. Barring major design changes arising from community feedback, the WebAssembly Community Group plans to produce an official specification in Q1 2017, at which point browsers will be encouraged to ship WebAssembly on-by-default. From that point forward, the binary format will be reset to version 1 and WebAssembly will be versionless, feature-tested, and backwards-compatible. A more detailed roadmap can be found on the WebAssembly project site.",en,51
341,2373,1474307356,CONTENT SHARED,7985545205318799060,-9016528795238256703,-3582296604567886387,,,,HTML,http://thereignn.ghost.io/on-dry-and-the-cost-of-wrongful-abstractions/,on dry and the cost of wrongful abstractions,"This article has been on my to do list for quite some time now. But it seems that only today I found the energy and time to take it from idea to implementation. Coincidence or not, I'm at the same coffee shop I posted my first article, a while ago. Must be that they put something in the drinks they serve... So the good, old advice of following best practices, right? We hear about them all the time. We've somehow made acronyms such as DRY or KISS defaults in our technical conversations. We follow the concepts religiously and if, by chance, desire or lack of knowledge, someone strays away from them, we make sure we rain a shit storm of criticism upon them. We're caught in this dogma and we refuse to look away. For sure I don't want to imply that principles such as DRY are bad. Definitely not. I just think context matters . A lot. In regards to DRY specifically, this brings in the following logical conclusion: I actually am the guy that will sometimes advise in favour of duplication rather than abstraction. Yes, you read that right. Duplicating code (a.k.a copy + paste) can be a good thing . Namely when the abstraction that would replace repetitive portions of your codebase is a pain to understand. How programming time is divided When I tell people what I do for a living, they all imagine I must be some kind of weirdo who keeps slapping the keyboard ten hours a day or more. Although, I'm not sure I'm not a little weird, I'm pretty sure I don't code for ten hours straight. The truth is that we, as programmers, spend far more time reading code than writing code. I'm not sure if you ever tracked this in one form or another, but research and Robert C. Martin claim to have some sort of ratio on this. And I, for one, find the difference to be staggering. It turns out that for every hour we code, we spend other ten hours reading code (ours or someone else's). This is extremely important. The effort we put in a day of work goes mostly towards reading code. Of course, reading is not enough. We need to also understand . Which means we have to do our best to produce code that is clear, concise and easy to read. For everyone's benefit, including ours in the long term. Keep this in mind as we'll get back to this idea later on. A note on DRY For those who are not familiar with what DRY means, it basically stands for don't repeat yourself. This programming principle or best practice if you'd like, is advocating for creating abstractions on top of every repetitive portion of your codebase. DRY has tons of advantages. For one, abstractions mean that, if changes will be required in the future, you will have to deal with them in one single place - the abstraction itself. Consuming another module's functionality, someone else's API, etc. you're only concerned with how the interface (or abstraction) looks like. You don't care about the underlying implementation. Thus, software design patterns such as the facade pattern allow for easy refactoring of the implementation, without hindering the abstraction used by others. So abstractions are good and DRY totally makes sense. Then why am I still insisting on repeating code in some scenarios? Well, it's because... The cost of abstractions Every abstraction comes at a cost. A cost that may not be immediately visible. But it reveals itself in time. Abstractions carry an extra layer of knowledge. Knowledge that you may not necessarily understand why it exists in the first place (especially if you didn't write it). And every new piece of information places cognitive load on your brain. Which, in turn, increases the time spent on reading that piece of code. Down the rabbit hole The problem with DRY and religious following of this principle is not visible in small projects. But in mid-sized and large ones. In those kind of projects, it's very rare that you will write only one abstraction per duplication. This is because, as the project evolves and new requirements come in, old code must always adjust. Imagine this. You enter a new project and scan the codebase for the first time. After getting used to what's there and learning your way around, you start implementing features, changing old ones, etc. You get to touch existing code and existing abstractions. You didn't write those, but they're there. And probably there's a very good reason for that. As Sandi Metz said: Existing code exerts a powerful influence. Its very presence argues that it is both correct and necessary. Which makes you not wanting to touch it. Now the new feature that barely came in could definitely make use of that nice abstraction that you have there. But, as it turns out, the abstraction needs a bit of tweaking. It wasn't thought for this particular use case. If only you could change it a bit... Or maybe write a new abstraction on top of it that could encapsulate some other repetitive logic as well? Yeah, that seems to be the answer, right? That's what DRY says anyway... It's easy to see how we can drive this madness to absolute extremes. Extremes where everything is encapsulated in abstractions. And those, in turn, have their own abstractions on top. And so on and so forth... In these cases, the abstraction lost its value. Its existence was determined by dogmas we follow blindly. And this makes the abstraction a wrongful one. It exists just because we can. As mentioned, abstractions have a cognitive cost associated with them. If anything and alongside the added benefits, that cost almost always hinders and increases the time we need to understand it (remember we're spending more time reading code than writing it). But even worse than a good abstraction, is a wrongful one. Because wrongful abstractions not only kick you in the nuts, they also laugh while doing it. To DRY or not to DRY So when do you encapsulate repetitive code and when do you not? The answer is simple in itself. It's just hard to get it right from a practical point of view. But this does come with experience as well. Always abstract if the cost of abstraction does not surpass the cost of duplicating code. That is, if the abstraction you wrote requires someone new on the project to spend hours understanding it, then you're probably doing something wrong. Don't abstract just because you can. Predict whether code duplication will happen again on that particular portion or not and decide accordingly. Sometimes, repeating code might hinder much less than following a tree of nested calls to different methods and keeping track of passed parameters, possible side effects, etc. Closing thought - in defence of myself Hopefully this article does not scream ""To hell with DRY and other shit!"". I absolutely think that is a very good programming principle. But I also urge you to not follow it blindly. Put everything you learned in context and always question the validity of your ideas and actions. This is the only sane way towards becoming a better professional. Looking forward to your comments. Remember to smile often and always question best practices. PS: I couldn't forget about the dreaded CACTUS EMOJI -> ��.",en,50
342,1123,1464353403,CONTENT SHARED,911037455259135591,3609194402293569455,-1121544763018490660,,,,HTML,http://startupi.com.br/2016/05/burger-king-aceita-pedidos-via-mensagens-no-facebook/,burger king aceita pedidos via mensagens no facebook - startupi,"Nos EUA, já dá para pedir um Whopper do Burger King via Facebook Messenger. Logado em sua conta na rede social, basta falar com o perfil da marca, que usa um bot para interagir e anotar o pedido. Ou seja, nada de ""telemarketing"", com pessoas gerindo as mensagens e escrevendo para os consumidores. O software de inteligência artificial para o Messenger foi anunciado em abril pelo Facebook, durante a F8, a conferência anual da empresa em San Francisco. O bot mostra ao cliente as opções do menu e promoções e ainda permite fazer o pagamento online. Detalhe importante: o serviço não é de delivery. O programa mostra algumas opções de restaurantes próximos onde o cliente pode ir pegar seu lanche. Ele também avisa em quantos minutos o pedido estará pronto. Veja o tutorial que mostra como o sistema funciona: Por Guilherme Dearo, da Exame.com É umas das mais influentes fontes online sobre negócios no país, com foco em economia, mercados financeiros, tecnologia, marketing, gestão, meio ambiente, pequenas empresas, carreira, finanças pessoais e parceiro de conteúdo do Startupi.",pt,50
343,2010,1470676183,CONTENT SHARED,7612428320594238723,3891637997717104548,1143023393355702515,,,,HTML,https://drupalize.me/blog/201607/why-learning-drupal-hard,why is learning drupal hard?,"I'm super excited to be invited to be a keynote speaker for this year's DrupalCamp WI (July 29/30). If you're in the area you should attend. The camp is free. The schedule is shaping up and includes some great presentations. Spending time with other Drupal developers is by and large the most effective way to learn Drupal. So sign-up, and come say hi to Blake and me. Why is Drupal hard? The title of my presentation is ""Why is Drupal Hard?"" It is my belief that if we want to continue to make it easier for people to learn Drupal we first need to understand why it is perceived as difficult in the first place. In my presentation I'm going to talk about what makes Drupal hard to learn, why it's not necessarily accurate to label difficult as ""bad"", and what we as individuals and as a community can do about it. As part of the process of preparing for this talk I've been working on forming a framework within which we can discuss the process of learning Drupal. And I've got a couple of related questions that I would love to get other people's opinions on. But before I can ask the question I need to set the stage. Close your eyes, take a deep breath, and imagine yourself in the shoes of someone setting out to be a ""Drupal developer."" Falling off the Drupal learning cliff When it comes to learning Drupal, I have a theory that there's an inverse relationship between the scope of knowledge that you need to understand during each phase of the learning process and the density of available resources that can teach it to you. Accepting this, and understanding how to get through the dip, is an important part of learning Drupal. This is a commonly referenced idea when it comes to learning technical things in general, and I'm trying to see how it applies to Drupal. Phase 1 When you set out to start, there's a plethora of highly-polished resources teaching you things that seem tricky but are totally doable with their hand holding. Drupalize.Me is a classic example: polished tutorials that guide you step-by-step through accomplishing a pre-determined goal. During this stage you might learn how to use fields and views to construct pages. Or how to implement the hook pattern in your modules. You don't have a whole lot of questions yet because you're still formulating an understanding of the basics, and the scope of things you need to know is relatively limited. For now. As you work through hand-holding tutorials, your confidence increases rapidly. Phase 2 Now that you're done with ""Hello World!"", it's time to try and solve some of your own problems. As you proceed you'll eventually realize that it's a lot harder when the hand-holding ends. It feels like you can't actually do anything on your own just yet. You can find tutorials but they don't answer your exact question. The earlier tutorials will have pointed you down different paths that you want to explore further but the resources are less polished, and harder to find. You don't know what you don't know. Which also means you don't know what to Google for. It's a much shorter period than the initial phase, and you might not even know you're in it. Your confidence is still bolstered based on your earlier successes, but frustration is mounting as you're unable to complete what you thought would be simple goals. This is the formulation of the cliff, and, like it or not, you're about to jump right off. Phase 3 Eventually you'll get overwhelmed and step off the cliff, smash yourself on the rocks at the bottom, and wander aimlessly. Every new direction seems correct but you're frequently going in circles and you're starving for the resources to help. Seth Godin refers to this as ""the dip"" , and Erik Trautman calls it the ""Desert of Despair"" . Whatever label you give it, you've just fallen off the Drupal learning cliff. For many people this is a huge confidence loss. Although you're still gaining competence, it's hard to feel like you're making progress when you're flailing so much. In this phase you know how to implement a hook but not which hook is the right one. You know how to use fields but not the implications of the choice of field type. Most of your questions will start with why , or which . Tutorials like those on Drupalize.Me can go a long ways toward teaching you how to operate in a pristine lab environment, but only years of experience can teach you how to do it in the real world. As much as we might like to, it's unrealistic to expect that we can create a guide that answers every possible permutation of every question. Instead, you need to learn to find the answers to the questions on your own by piecing together many resources. The scope of knowledge required to get through this phase is huge. And yet the availability of resources that can help you do it is limited. Because, as mentioned before, you're now into solving your own unique problems and no longer just copying someone else's example. Phase 4 If you persevere long enough you'll eventually find a path through the darkness. You have enough knowledge to formulate good questions, and the ability to do so increases your ability to get them answered. You gain confidence because you appear to be able to solve real problems. Your task now is to learn best practices, and the tangential things that take you from, ""I can build a website"", to ""I can launch a production ready project."" You still need to get through this phase before you'll be confident in your skills as a Drupal developer, but at this point it's mostly just putting in time and getting experience. During this phase, resources that were previously inaccessible to you are now made readily available. Your ability to understand the content and concepts of technical presentations at conferences, industry blog posts, and even to participate in a conversation with your peers is bolstered by the knowledge you gained while wandering around the desert for a few months. You're once again gaining confidence in your own skills, and your confidence is validated by your ability to continue to attain loftier goals. And then some morning you'll wake up, and nothing will have changed, but through continually increasing confidence and competence you'll say to yourself, ""Self, I'm a Drupal developer. I'm ready for a job."" What resources can help you get through phase 3? So here's my questions: What resources do you think are currently available, and useful, for aspiring Drupal developers who are currently stuck in phase 3, wandering around the desert without a map asking themselves, ""Panels or Context?""? What resources do you think would help if they existed? If you're on the other side, how did you personally get through this dip? Responses from Lullabot I asked this same question internally at Lullabot a few days ago, and here are some of the answers I received (paraphrased). Hopefully this helps jog your own memory of what it was like for yourself. Or even better, if you're stuck in the desert now, here's some anecdotal evidence that it's all going to be okay. You're going to make it out alive. For me, it was trial and error. I would choose a solution that could solve the particular problem at hand most efficiently, and then I would overuse it to the extreme. The deeper lessons came months later when changes had to be made and I realized the mistakes I had made... Learning usually came also from working with others more experienced. Getting the confidence to just read others' code and step through it is also a big plus. building something useful++. That's the absolute best way. Can't believe I forgot to mention it. Preferably something that interests you or fulfills your own need. You still fall off the cliff, but you at least see the fall coming, and your ability to bounce back is better. At this stage I find that the best resources are people, not books or tutorials. A mentor. Someone that can patiently listen to your whines and frustrations and suggest the proper questions to ask, and who can give you the projects and assignments that help you grow and stretch. Everything I know about Drupal I know through years of painful trial and error and shameless begging for help in IRC. I spent a lot of time desperately reading Stack Overflow, or trying to figure a bug out from looking at an issue where the patch was never merged, or reading through a drupal.org forum where somebody tries to solve something but then just ends with ""nevermind, solved this"" without saying why. I'd agree that people is what gets you through that. I learned IRC and how to write patches and get help from individuals and that is when the doors opened. Another approach that really boosted me to the next level, especially early on in my career as a developer, was to work with someone that you can just bounce ideas off of. I'll never forget all the hacking sessions Jerad and I had back in the day. Coding at times can be boring, or the excitement of doing something awesome is self-contained. Being able to share ideas, concepts, and example code with someone that appreciates the effort or awesomeness of something you've done and at the same time challenges you to take it to the next level is priceless. Printing out the parts of Drupal code I wanted to learn: node, taxonomy and reading comments and code like a gazillion times. Try and code something useful so I could ask others for help. That's how I wrote the path aliasing module for core. I often find that as you get into more complicated, undocumented territory, being able to read code is super valuable. You can often get lost in disparate blog posts, tutorials and forums that can lead you all sorts of ways. The code is the ultimate source of truth. Sometimes it takes firing up a debugger, stepping through the parts that matter to see how things are connected and why.",en,50
344,691,1461964132,CONTENT SHARED,8483892789913825147,6003902177042843076,3083498611046600561,,,,HTML,http://qunitjs.com/cookbook/,cookbook,"Introduction Automated testing of software is an essential tool in development. Unit tests are the basic building blocks for automated tests: each component, the unit, of software is accompanied by a test that can be run by a test runner over and over again without any human interaction. In other words, you can write a test once and run it as often as necessary without any additional cost. In addition to the benefits of good test coverage, testing can also drive the design of software, known as test-driven design , where a test is written before an implementation. You start writing a very simple test, verify that it fails (because the code to be tested doesn't exist yet), and then write the necessary implementation until the test passes. Once that happens, you extend the test to cover more of the desired functionality and implement again. By repeating those steps, the resulting code looks usually much different from what you'd get by starting with the implementation. Unit testing in JavaScript isn't much different from in other programming languages. You need a small framework that provides a test runner, as well as some utilities to write the actual tests. Automating Unit Testing Problem You want to automate testing your applications and frameworks, maybe even benefit from test-driven design. Writing your own testing framework may be tempting, but it involves a lot of work to cover all the details and special requirements of testing JavaScript code in various browsers. Solution While there are other unit testing frameworks for JavaScript, you've decided to check out QUnit. QUnit is jQuery's unit test framework and is used by a wide variety of projects. To use QUnit, you only need to include two QUnit files on your HTML page. QUnit consists of qunit.js , the test runner and testing framework, and qunit.css , which styles the test suite page to display test results: Opening this file in a browser gives the result shown below: The only markup necessary in the <body> element is a <div> with id=""qunit-fixture"" . This is required for all QUnit tests, even when the element itself is empty. This provides the fixture for tests, which will be explained in the section called ""Keeping Tests Atomic"" . The interesting part is the <script> element following the qunit.js include. It consists of a call to the test function, with two arguments: the name of the test as a string, which is later used to display the test results, and a function. The function contains the actual testing code, which involves one or more assertions. The example uses one assertion, equal() , which is explained in detail in Asserting Results . Note that there is no document-ready block. The test runner handles that: calling QUnit.test() just adds the test to a queue, and its execution is deferred and controlled by the test runner. Discussion The header of the test suite displays the page title, a green bar when all tests passed (a red bar when at least one test failed), a bar with a few checkboxes to filter test results and a blue bar with the navigator.userAgent string (handy for screenshots of test results in different browsers). Of the checkboxes, ""Hide passed tests"" is useful when a lot of tests ran and only a few failed. Checking the checkbox will hide everything that passed, making it easier to focus on the tests that failed (see also the Efficient Development section below). Checking ""Check for Globals"" causes QUnit to make a list of all properties on the window object, before and after each test, then checking for differences. If properties are added or removed, the test will fail, listing the difference. This helps to make sure your test code and code under test doesn't accidentally export any global properties. The ""No try-catch"" checkbox tells QUnit to run your test outside of a try-catch block. When your test throws an exception, the testrunner will die, unable to continue running, but you'll get a ""native"" exception, which can help tremendously debugging old browsers with bad debugging support like Internet Explorer 6 (JavaScript sucks at rethrowing exceptions). Below the header is a summary, showing the total time it took to run all tests as well as the overall number of total and failed assertions. While tests are still running, it will show which test is currently being executed. The actual contents of the page are the test results. Each entry in the numbered list starts with the name of the test followed by, in parentheses, the number of failed, passed, and total assertions. Clicking the entry will show the results of each assertion, usually with details about expected and actual results. The ""Rerun"" link at the end will run that test on its own. Asserting Results Problem Essential elements of any unit test are assertions. The author of the test needs to express the results expected and have the unit testing framework compare them to the actual values that an implementation produces. Solution QUnit provides a number of built-in assertions . Let's look at three of those: ok( truthy [, message ] ) The most basic one is ok() , which requires just one argument. If the argument evaluates to true, the assertion passes; otherwise, it fails. In addition, it accepts a string to display as a message in the test results: equal( actual, expected [, message ] ) The equal assertion uses the simple comparison operator ( == ) to compare the actual and expected arguments. When they are equal, the assertion passes; otherwise, it fails. When it fails, both actual and expected values are displayed in the test result, in addition to a given message: Compared to ok() , equal() makes it much easier to debug tests that failed, because it's obvious which value caused the test to fail. When you need a strict comparison ( === ), use strictEqual() instead. deepEqual( actual, expected [, message ] ) The deepEqual() assertion can be used just like equal() and is a better choice in most cases. Instead of the simple comparison operator ( == ), it uses the more accurate comparison operator ( === ). That way, undefined doesn't equal null , 0 , or the empty string ( """" ). It also compares the content of objects so that {key: value} is equal to {key: value} , even when comparing two objects with distinct identities. deepEqual() also handles NaN, dates, regular expressions, arrays, and functions, while equal() would just check the object identity: In case you want to explicitly not compare the content of two values, equal() can still be used. In general, deepEqual() is the better choice. Synchronous Callbacks Problem Occasionally, callback assertions in the code might not be called at all, causing the test to fail silently. Solution QUnit provides a special assertion to define the number of assertions a test contains. If the test completes without the expected number of assertions, it will fail. In order to indicate the expected number of assertion, call assert.expect() at the start of a test, with the number of expected assertions as the only argument: Practical Example: Discussion assert.expect() provides the most value when actually testing callbacks. When all code is running in the scope of the test function, assert.expect() provides no additional value-any error preventing assertions to run would cause the test to fail anyway, because the test runner catches the error and fails the unit. Asynchronous Callbacks Problem While assert.expect() is useful to test synchronous callbacks (see the section called ""Synchronous Callbacks"" ), it can fall short for asynchronous callbacks. Asynchronous callbacks conflict with the way the test runner queues and executes tests. When code under test starts a timeout or interval or an AJAX request, the test runner will just continue running the rest of the test, as well as other tests following it, instead of waiting for the result of the asynchronous operation. Solution For every asynchronous operation in your QUnit.test() callback, use assert.async() , which returns a ""done"" function that should be called when the operation has completed. Testing User Actions Problem Code that relies on actions initiated by the user can't be tested by just calling a function. Usually an anonymous function is bound to an element's event, e.g., a click, which has to be simulated. Solution You can trigger the event using jQuery's trigger() method and test that the expected behavior occurred. If you don't want the native browser events to be triggered, you can use triggerHandler() to just execute the bound event handlers. This is useful when testing a click event on a link, where trigger() would cause the browser to change the location, which is hardly desired behavior in a test. Let's assume we have a simple key logger that we want to test: We can manually trigger a keypress event to see whether the logger is working: Discussion If your event handler doesn't rely on any specific properties of the event, you can just call .trigger( eventType ) . However, if your event handler does rely on specific properties of the event, you will need to create an event object using $.Event with the necessary properties, as shown previously. It's also important to trigger all relevant events for complex behaviors such as dragging, which is comprised of mousedown, at least one mousemove, and a mouseup. Keep in mind that even some events that seem simple are actually compound; e.g., a click is really a mousedown, mouseup, and then click. Whether you actually need to trigger all three of these depends on the code under test. Triggering a click works for most cases. If thats not enough, you have a few framework options that help simulating user events: syn ""is a synthetic event library that pretty much handles typing, clicking, moving, and dragging exactly how a real user would perform those actions"". Used by FuncUnit, which is based on QUnit, for functional testing of web applications. JSRobot - ""A testing utility for web-apps that can generate real keystrokes rather than simply simulating the JavaScript event firing. This allows the keystroke to trigger the built-in browser behaviour which isn't otherwise possible."" DOH Robot ""provides an API that enables testers to automate their UI tests using real, cross-platform, system-level input events"". This gets you the closest to ""real"" browser events, but uses Java applets for that. keyvent.js - ""Keyboard events simulator."" Keeping Tests Atomic Problem When tests are lumped together, it's possible to have tests that should pass but fail or tests that should fail but pass. This is a result of a test having invalid results because of side effects of a previous test: The first append() adds a <div> that the second equal() doesn't take into account. Solution Use the QUnit.test() method to keep tests atomic, being careful to keep each assertion clean of any possible side effects. You should only rely on the fixture markup, inside the #qunit-fixture element. Modifying and relying on anything else can have side effects: QUnit will reset the elements inside the #qunit-fixture element after each test, removing any events that may have existed. As long as you use elements only within this fixture, you don't have to manually clean up after your tests to keep them atomic. Discussion In addition to the #qunit-fixture fixture element and the filters explained in the section called ""Efficient Development"" , QUnit also offers a ?noglobals flag. Consider the following test: In a normal test run, this passes as a valid result. Running the ok() test with the noglobals flag will cause the test to fail, because QUnit detected that it polluted the window object. There is no need to use this flag all the time, but it can be handy to detect global namespace pollution that may be problematic in combination with third-party libraries. And it helps to detect bugs in tests caused by side effects. Grouping Tests Problem You've split up all of your tests to keep them atomic and free of side effects, but you want to keep them logically organized and be able to run a specific group of tests on their own. Solution You can use the QUnit.module() function to group tests together: All tests that occur after a call to QUnit.module() will be grouped into that module. The test names will all be preceded by the module name in the test results. You can then use that module name to select tests to run (see the section called ""Efficient Development"" ). Discussion In addition to grouping tests, QUnit.module() can be used to extract common code from tests within that module. The QUnit.module() function takes an optional second parameter to define functions to run before and after each test within the module: You can specify both beforeEach and afterEach properties together, or just one of them. Calling QUnit.module() again without the additional argument will simply reset any beforeEach/afterEach functions defined by another module previously. Custom Assertions Problem You have several tests that duplicate logic for asserting some expectation. This repetitive code lessens the readability of your tests and increases the surface for bugs. Solution Define a function to encapsulate the expectation in a reusable unit. Invoke within the body to notify QUnit that an assertion has taken place. Discussion Custom assertions can help make test suites more readable and more maintainable. At a minimum, they are simply functions that invoke with a Boolean value--this is how QUnit detects that an assertion has taken place and the result of that assertion. It is a good practice to define this function as a method on the global QUnit.assert object. This helps communicate the purpose of the function to other developers. You may accomplish this by directly assigning a new property on the object (i.e. QUnit.assert.myAssertion = myAssertion; ) or using QUnit.extend (i.e. QUnit.extend(QUnit.assert, { myAssertion: myAssertion }); ). Efficient Development Problem Once your testsuite takes longer than a few seconds to run, you want to avoid wasting a lot of time just waiting for test results to come in. Solution QUnit has a bunch of features built in to make up for that. The most interesting ones require just a single click to activate. Toggle the ""Hide passed tests"" checkbox at the top, and QUnit will only show you tests that failed. That alone doesn't make a difference in speed, but already helps focusing on failing tests. It gets more interesting if you take another QUnit feature into account, which is enabled by default and usually not noticable. Whenever a test fails, QUnit stores the name of that test in sessionStorage . The next time you run a testsuite, that failing test will run before all other tests. The output order isn't affected, only the execution order. In combination with the ""Hide passed tests"" checkbox you will then get to see the failing test, if it still fails, at the top, as soon as possible. Discussion The automatic reordering happens by default. It implies that your tests need to be atomic, as discussed previously . If your tests aren't, you'll see random non-deterministic errors. Fixing that is usually the right approach. If you're really desperate, you can set QUnit.config .reorder = false . In addition to the automatic reordering, there are a few manual options available. You can rerun any test by clicking the ""Rerun"" link next to that test. That will add a ""testNumber=N"" parameter to the query string, where ""N"" is the number of the test you clicked. You can then reload the page to keep running just that test, or use the browser's back button to go back to running all tests. Running all tests within a module works pretty much the same way, except that you choose the module to run using the select at the top right. It'll set a ""module=N"" query string, where ""N"" is the encoded name of the module, for example ""?module=testEnvironment%20with%20object"". *This page was first published, under a non-exclusive license, as a chapter in the jQuery Cookbook, authored by Scott González and Jörn Zaefferer. As QUnit changed since the book was printed, this version is more up-to-date.",en,50
345,714,1462206350,CONTENT SHARED,-1654063646246197191,-1032019229384696495,6822104352410363272,,,,HTML,https://medium.com/@josef_89142/big-it-rising-b98826b68364,big it rising,"Big IT rising An overview of Lean, Agile and DevOps New Ways of Working The lunch of big corporate IT is being stolen by smaller, nimbler companies. Big IT, with its greater resources, should have crushed the competition. Rather it is playing catch-up. But things are changing. There is a quiet revolution in corporate IT. Big organisations are learning from small companies and are beginning to use it at scale. Goliath is back but acting like David. All this change leaves the corporate manager at a loss as to what tools to use. Big IT, especially banks, is stuck in a world of waterfall development where plans are rather rigid, risk management is paramount and new functionality to the customer is few and far between. The aim of this article is to provide a toolbox of different methodologies that can be used to transform Big IT. For the sake of simplicity, let's refer to our toolbox of methodologies as the ""New Ways of Working"". Our starting point is an exploration of the different elements of the New Ways of Working (Figure 1). This was adapted from Gartner's framework for DevOps. Social Psychology Probably the biggest shift away from the traditional command and control culture in Big IT is the new focus on Social Psychology. If we can figure out what drives people and keeps them motivated and then lets them work in small self-organising teams, we have a recipe for success. Social Psychology forms the foundation of the success of almost all the other tools. It seems that people do their best work when they work in a small team of not more than 10-12 people. Jess Bezos famously said that a team should not need more than two pizzas to be fed. Unfortunately really big pieces of work cannot be done by one small team. The anthropologist Robert Dunbar figured out that the number of people in a stable social group is 150. In the IT domain this means that at most 150 individuals should work together on a product or in a larger domain. My primary group is therefore a small team and these small teams together form a larger domain or grouping. Individual motivation within these teams is as important. Dan Pink thinks that people are motivated by a sense of mastery, some autonomy and purpose . All of us want to feel that we are really good at something. That is why we pursue hobbies that take up our time and energy. We steadily get better and get enjoyment out of it. Performance management systems are changing to the Dreyfus model of skill acquisition that reflects this need for mastery. Small empowered teams give a sense of autonomy. If the work my teams does contributes to something larger that the team or myself, it adds to purpose. Agile Approaches The next area we want to highlight is the role of the Agile movement in the New Ways of Working. Agile is the standard approach for small companies. The challenge for Big IT is to scale an Agile methodology like Scrum for large organisations. Scrum has its roots in Takeuchi and Nonaka's Harvard Business Review article ""The New Product Development Game"", where they compared their approach to the sport of Rugby. In Rugby the team ""tries to go to the distance as a unit, passing the ball back and forth"". Managers had to realize that the traditional, sequential approach to developing new products will not work. Development teams must work as a unit to reach a common goal. Steve Denning summarised Scrum nicely as: Organising work in short cycles. The management doesn't interrupt the team during a work cycle. The team reports to the client , not the manager. The team estimates how much time work will take. The team decides how much work it can do in an iteration. The team decides how to do the work in the iteration. The team measures its own performance. Defining work goals before each cycle starts. Defining work goals through user stories. Systematically removing impediments. Traditionally Agile was used for software development, but now it is being used for total organisational transformation. The DevOps Movement DevOps is the proverbial new kid on the block. It is not a methodology or a framework and builds on Lean and Agile methodologies. It emphasises the following aspects: Developers and Operations people working together in a cross-functional team . The team that develops the software also maintains the software in production. Originally Agile focused on bringing testing and developers [replace with: development] together. DevOps adds the operational dimension. There is a focus on automated deployment pipelines . Manual configuration of environments and the manual movement of code between environments are seen as anti-patterns. Continuous deployment is central as the emphasis is shifting to smaller releases to the production environment quicker. This minimizes risk inherent in big production releases to service outages. Automation is the name of the game in DevOps. Lean Thinking Another lens of the software development process is Lean thinking. Within Lean there are many different schools of thought. The underlying principles that unite these methodologies are: Value gets defined by the customer. The value stream generates value to the customer and spans organisational silos. Requirements are pulled rather than pushed through the system with the consequence that features are not built before they are needed. The process flows continuously. We strive to get better all the time. What started off as a manufacturing methodology is starting to influence the world of software development. In the waterfall software development process different development phases are separated by gates. These gates are mutually exclusive. Work cannot move to the next phase until the work of the previous phases is completed. In the waterfall process this typically means that requirements need to be finalised before detail design work can start. When design is finished we can move to coding and deployment. The term Waterfall Software Development is a misnomer as there is very little emphasis on ""Flow"" as implied by the name. Kanban is a great Lean tool that help to emphasise flow and to make work visible. Putting it together Our discussion so far has given a brief outline of the New Ways of Working. We have focused on the Social Psychology aspects, Lean, DevOps and Agile. Agile will continue to be the prevalent paradigm to deliver software and it will be enhanced by Lean Thinking and the contributions of DevOps. The real challenge is how to enable this thinking in the IT departments of very large organisations. Informal surveys done by Scrum Alliance show that most Agile teams report tension between the way the teams operate and the way the rest of the organisation is run. Large cultural changes are necessary to overcome this. The organisational processes around budgets, people management and performance management also need to change radically to support these New Ways of Working. Very few people know how to do this well and at scale, and this is a fertile field for research and consulting. A suggestion for a simple heuristic on where to start these New Ways of Working in a large corporate with Big IT: Define what culture you want in the organisation and start living, talking and acting in accordance with this culture. If, for example, you want to move the focus away from a management culture to craftmanship, start wearing sneakers and jeans. You can't roll up your sleeves in an Italian suit. Use the Lean tools for the non-product development areas of the organisation. Automate as much as possible. Use Agile and DevOps for the product development areas maturing towards continuous delivery. Steps 2 and 3 can be done simultaneously. Using the tools in Figure 1 is a great start to wake Big IT from its slumber. In other posts we will unpack the diagram in more detail. When Big IT starts punching above or even at it weight again I don't want to stand in their way. Join the movement and let's start the revolution. Missing something? If you feel we missed something in the discussion or diagram let us know in the responses below. Thanks for reading. Originally published at langerman.co.za If you enjoyed this and think others will too, will you please ""Recommend"" or share with a friend?",en,50
346,862,1462816559,CONTENT SHARED,-205193648629294862,-3390049372067052505,-8729604331807064886,,,,HTML,http://blog.irvingwb.com/blog/2016/04/is-fintech-forcing-banking-to-its-digital-disruption-tipping-point.html,is fintech forcing banking to a tipping point?,"For the past few decades, digital technologies have been systematically transforming one industry after another. The transformations have generally proceeded along three different stages. First comes the use of IT to improve the productivity and quality of production-oriented, back-end processes.Distribution comes next, leveraging the universal reach and connectivity of the Internet over the past 20 years.The transformation then reaches a tipping point when technology radically changes the user experience, - as has happened with the rise of smartphones over the past decade, - leading to a fundamental disruption of the industry and its business models. While this digital disruption journey is ultimately inevitable, the pace varies widely across industries.The IT industry has been the most disrupted , - often by its own digital creations in a kind of sorcerer's apprentice scenario.Over my long career , I've seen many once powerful IT companies done in by technology and market changes, and either disappear altogether or become shadows of their former selves. Beyond IT, few industries have felt the impact of digital forces like . Everything seems to be changing at once, from the way content is produced and delivered, to the sources of revenue and profits. In less than two decades, the global recorded music industry has lost over half its revenues, while the drop in newspaper advertising revenue in the US has been even steeper.Retail has also been undergoing major changes with the rise of e-commerce, as has telecommunications with the transition to mobile phones and wireless data. How about the banking industry, which has long been a major user of information technologies, - including back- and front-office automation, ATM's, Internet banking, data-driven risk management, fraud detection, and mobile financial apps? ""Despite all of the investment and continuous speculation about banks facing extinction, only about 1% of North American consumer banking revenue has migrated to new digital models,"" says an excellent report that was recently released by Citigroup, - Digital Disruption: How FinTech is Forcing Banking to a Tipping Point . ""Although FinTech companies have the advantage of new innovation, incumbent financial institutions still have the upper hand in terms of scale and we have not yet reached the tipping point of digital disruption in either the US or Europe. Given the growth in FinTech investment, this isn't likely to continue for long."" In the last few years, , - short for Financial Technology, - has become a widely used term for technology-based innovations in financial services.FinTech companies have generally been startups looking to disrupt larger companies, although incumbent financial companies have started to establish their own FinTech units.Last October, for example, Citigroup the Citi FinTech division, aimed at developing new mobile banking services and business models. ""Silicon Valley is coming,"" wrote JPMC CEO Jamie Dimon in his 2015 Shareholder Letter. ""There are hundreds of startups with a lot of brains and money working on various alternatives to traditional banking.""Competitors are coming in the payment area as well as in the lending business. ""[T]hey can make loans in minutes, which might take banks weeks...there is much for us to learn in terms of real-time systems, better encryption techniques, and reduction of costs and pain points for customers."" Citi's report highlights a number of important FinTech trends: FinTech investments increased 10X in the past 5 years. Investments have risen ""from $1.8 billion in 2010 to $19 billion in 2015 -with over 70% of this investment focusing on the last mile of user experience in the consumer space.The majority of this investment has also been concentrated in the payments area and this is where banks are seeing the most competition with new entrants.""Competitors are emerging in established digital markets, such as PayPal and Square in e-commerce, as well as in underserved segments like micropayments and small businesses. FinTech is targeting the most profitable areas of global banking. Citi Research analysts estimate that over 70% of FinTech investments have been aimed at individuals and small and medium enterprises, segments which account for about half of banking's profit pool. Given the growing importance of smartphones in financial transactions, it's not surprising that Business-to-Consumer (B2C) innovations dominate, mostly aimed at improving user experiences. The US and Europe are at the tipping point. Greg Baxter , - Citi's Global Head of Digital Strategy, - notes that in the US and other developed markets ""We are not even at the end of the beginning.""While really difficult to forecast this early in the cycle, his Citi team estimates that ""currently only about 1% of North American consumer banking revenue has migrated to new digital business models (either at new entrants or incumbents) but that this will increase to about 10% by 2020 and 17% by 2023."" China is already past the tipping point. Top Chinese FinTech companies, - such as or , - already have as many, if not more clients as the major banks around the world.""China's FinTech companies have grown fast due to a combination of: (1) high national Internet and mobile penetration, (2) a large e-commerce system with domestic Internet companies focused on payments, (3) relatively unsophisticated incumbent consumer banking, and (4) accommodative regulations."" Emerging Markets are going through a financial inclusion revolution. Emerging markets are ripe for FinTech disruptions due to ""a high percentage of unbanked population, relatively weak consumer banks, and a high penetration of mobile phones.""Kenya has been the leader in mobile phone-based financial services since the launch of in 2007, which currently has 23 million active customers in 11 countries.In Somalia, - despite or because of its serious political instability, over 40% of adults use mobile money.For billions around the world, FinTech innovations are their tickets to financial inclusion in the global digital economy. India is likely the next major FinTech frontier. India represents a major opportunity space for FinTech, given its large population of over 1.2 billion, its low number of banking accounts and its relatively high digital penetration.This opportunity is helped a long by three major enablers: the national biometric identity program, which now covers over 900 million people; the financial inclusion initiative which has already opened almost 200 million new bank accounts; and the ubiquity of mobile phones, with ~ 80% penetration.In addition, India's central bank, - the Reserve Bank of India , - has been aggressively expanding the banking ecosystem. Digital payments face intense competition. The payment space is being challenged by competitors from a vareity of industries, including e-commerce (Alipay, Paypal), technology (Apple, Google), telcos (MPESA, SoftCard), and merchants (MCX, Walmart Pay).""Although payment is a relatively small part of banks' revenue pool (~7%), the incumbent banks are at risk of losing important customer transaction data and client relationships."" These competitors are a potential threat to incumbent banks due to their technical, financial and market strengths. Blockchain could be a catalyst for transforming legacy infrastructures .Payment innovations have mostly focused on the user experience at the point of sale, while continuing to rely on the existing backbone payment infrastructures.But over time, blockchain technologies could start replacing these legacy, proprietary infrastructures with distributed shared infrastructures more appropriate for dealing with the huge scalability and security challenges of the digital economy. Blockchain could be ""a catalyst for the transformation of many existing legacy systems that operate with a high degree of robustness but may not be the most cost or capital efficient way of doing business."" But blockchain is still at the bleeding edge , lacking the robustness of legacy payment systems. Banks face an Uber Moment .As Jonathan Larsen , - Citi's Global Head of Retail Banking, - said:""The future of the branches is about advisory and consultation rather than transactions... clients over time will move almost entirely to mobile channels.Banks will look like Uber.Uber has nothing to do with cars. It created an entirely new user experience.It tracks all your transaction histories, expenses, drivers' ratings and so on.It created needs you never had."" ""What it means to banks is first and foremost the centrality of mobile as the main channel of interaction between customers and the bank.More importantly, there is a diminishing return on physical assets - especially the branch network.I won't say that banks won't have a balance sheet in the future, but the way customers interface with the bank will be revamped."" The diminishing importance of physical branches, the explosive growth of the mobile Internet, and increased competition from FinTech startups are putting huge pressure on banks, - leading to a kind of Uber moment , when technology is making the old ways of doing business obsolete.To survive, banks will be compelled to embrace many of the FinTech innovations introduced by startups and staffing levels could decline by up to 30% over the next 10 years. ""The death of banks has been much foretold in recent decades,"" most notably by Bill Gates in a 1994 where he argued ""that the world needed banking services but not necessarily banks..."" Since then, ""we have seen digital disruption fundamentally erode value across many industries including: music sales, video rentals, travel booking, and newspapers. In each of these cases, incumbents either transformed or became marginalized."" Is the digital revolution finally reaching the banking industry? Will incumbent banks embrace FinTech innovations before FinTech startups gain scale and distribution? How will it all play out, - fierce competition, an increasing number of partnerships, or some combination of both? There are so many factors at play, that it's extremely difficult to predict how this digital disruption will play out over the years. But, as Citi's report says in its very title, all the signs seem to indicate that ""FinTech is [Finally] Forcing Banking to a Tipping Point.""",en,50
347,2021,1470828955,CONTENT SHARED,3099207916247126790,881856221521045800,-6443124096966155946,,,,HTML,https://blog.newrelic.com/2016/08/09/infrastructure-live-state-private-beta/,introducing new relic infrastructure,"Today at our FutureStack16 Tour: New York conference , we announced the private beta of New Relic Infrastructure , a new infrastructure monitoring product focused on supporting the needs of modern operations teams. New Relic Infrastructure uniquely brings together configuration monitoring, real-time health metrics, and a dynamic, tag-driven approach to alerting and dashboards to provide you unprecedented visibility of your ever-changing infrastructure. When an issue arises, you'll know in seconds if a recent config change was the cause, and when and where it was made. If you're interested in seeing what New Relic Infrastructure can do for you, please sign up to request an invitation to the private beta program here . To learn more, visit New Relic.com/Infrastructure , and attend our free webinar on Tuesday, September 13, at 11 a.m. Pacific Time (2 p.m. Eastern Time). Our world is changing-it's time for new tools If you're part of an ops team or responsible for maintaining services in production, you're likely dealing with more changes than ever before. The move to agile development with continuous integration/continuous deployment (CI/CD) pipelines, migration to cloud or hybrid infrastructures, repackaging services using container technologies like Docker, and the DevOps movement are all accelerating overall velocity. That's a good thing, but it just adds pressure for ops teams who are already focused on maximizing uptime. New Relic Infrastructure is designed to help you handle these challenges and put you in greater control, so you can spend more time proactively optimizing and scaling and less time getting woken up at 2:00 a.m. by alerting. It is designed to provide an accurate picture of the current live state of your infrastructure, tracking the current configuration and health of all your resources along with any changes being made that can affect their state. When an issue occurs, you can quickly cut through the noise and answer the key question: ""What changed in the last 10 minutes that's causing this outage?"" New Relic Infrastructure lets you organize your alerting and dashboards dynamically with the metadata you choose to reflect the way you organize your environment. And it seamlessly scales up and down as your systems scale. Here are some key features: 1. Live-state monitoring and change tracking: Keep tabs on everything that happens with your infrastructure in real time. This includes changes to key files, packages, kernel settings, service status, user logins, and security groups. 2. Correlated real-time metrics and change events: Key health metrics updated every five seconds with a correlated timeline of events help you to quickly identify what recent change could be causing an outage or performance issues. 3. Tag-driven alerting and dashboards: Use Amazon EC2 tags, role data from automation systems, or your own metadata to create dynamic scopes and alerts policies that scale as your hosts scale. 4. Instant infrastructure search: Find vulnerable packages, errant services, or anything else instantly across thousands of hosts. Zero-day headaches can be addressed in seconds instead of days. 5. AWS and container (Docker) native: New Relic Infrastructure is tightly integrated with Amazon EC2, allowing for an accurate, real-time view of your EC2 ecosystem. Docker containers and their metadata are first-class citizens too, allowing you to monitor every aspect of your dynamic infrastructure. You can slice and dice your hosts by any AWS attribute: role, tier, Availability Zone (AZ), data center, or custom EC2 tag to narrow down to the hosts you care about, and view their data by the attribute you care about. 6. Up and running in seconds: No configuration is required. You don't need another tool that requires managing; just drop the New Relic Infrastructure agent on each host and you have immediate visibility of the live state changes being made, and real-time health metrics. 7. Leverages the power of New Relic Insights: Built on New Relic's scalable analytics engine (NRDB) , New Relic Infrastructure helps you focus on your productivity and bring new products to market, faster. All metric and event data is available in NRDB to allow for more detailed Insights exploration and dashboarding. Request a private beta invitation We're excited to introduce New Relic Infrastructure: read the press release and visit NewRelic.com/Infrastructure to learn more. If you're interested in seeing what it can do for you, please sign up for an invitation to the private beta program here . Signing up will also ensure you are notified of new information about New Relic Infrastructure as it becomes available. In the meantime, don't miss our free webinar, Introducing New Relic Infrastructure: Real-Time Performance and Causation Analysis for Dynamic Infrastructure -on Tuesday, September 13, at 11 a.m. Pacific Time (2 p.m. Eastern Time). Event dates, participants, and topics are subject to change without notice.",en,50
348,2307,1473558334,CONTENT SHARED,-5088157426675091741,3915038251784681624,5263811446711891099,,,,HTML,http://cio.com.br/tecnologia/2016/09/05/dez-habilidades-profissionais-mais-valorizadas-em-projetos-de-internet-das-coisas/,dez habilidades profissionais mais valorizadas em projetos de internet das coisas - cio,"A Internet das Coisas ganha tamanha atenção que é praticamente óbvio prever que, em um futuro não muito distante, pipocará no mercado toda uma gama de novas oportunidades de carreira. Segundo o Gartner, cerca de 29% das companhias já exploram IoT de alguma maneira, 14% pretende fazer isso em doze meses e 64% revelou que o fará, eventualmente, a qualquer momento. Por enquanto, os empregos atrelados ao conceitos ainda não são totalmente claros. Isso, contudo, deve mudar nos próximos anos, como uma nova variedade de cargos e profissões. Especialistas acreditam que a melhor estratégia para se dar bem lá na frente é adicionar desde já habilidades em seu currículo. Ainda, algumas áreas e conhecimentos de alta demanda surgirão a medida que novas tecnologias e aplicações tomam forma. Uma delas é analytics, um campo que ganha importância em praticamente todos projetos de TI, uma vez que dados viraram um combustível da economia digital. Outra capacidade profissional valorizada será a habilidade de criar pontes entre tecnologias da informação e de operações. ""Os times de TI precisarão entender aspectos hoje restritos ao pessoal de OT"", sintetiza Chet Geschickter, analista do Gartner, sinalizando que o contrário também será algo verdadeiro. Tecnologia operacional se aplica a todas pessoas na companhia que realizam tarefas que ajustam e gerenciam objetos do ""mundo real"". Em outras palavras, são os responsáveis pelos técnicos da refrigeração predial, dos programadores das linhas de produção, dos gestores de frota. Os mundos de TI e TO foram construídos de forma separada e, por muito tempo, não compartilharam conhecimentos. Isso funcionou relativamente bem até então, afinal, os zeros e uns dos computadores não se encontrava com as prensas e engrenagens do chão da fábrica. Ocorre que com a ascensão da IoT, uma rede de dados e um servidor funcionará totalmente atrelado a esses dispositivos das linhas de manufatura. E aí que a realidade dará novos cotornos para habilidades profissionais. Vamos a um exemplo mais prático. Profissionais de TI - diferentemente dos de TO - sabem rastrear atualizações de softwares e entregar essas rotinas através de uma coleção complexa de sistemas formada por múltiplos vendors. Porém, sabem fazer isso com PCs, servidores, switches. Internet das Coisas é diferente. O que acontece se um caminhão de entrega precisar de um update de sistema urgente e estiver em trânsito? Como proceder, aguardar até ele chegar a uma base? Fazer o processo automaticamente com o veículo em movimento? Tal rotina vai acarretar algum perigo? Questões como essa, sobre objetos em movimento e segurança dos empregados estão entre as especialidades das equipes de OT. Porém, eles tem seus próprios pontos cegos. A maioria deles é acostumada a lidar com uma vasta gama de tecnologias providas por um único fabricante, observa Geschickter said. Para o especialista, eles sabem as entradas e saídas daquela plataforma, mas não tem muita experiência integrando aquele sistema com outras coisas. Normalmente, diz, é o fabricante que lida com essa parte do processo. Poucas descrições de emprego, atualmente, unem esses dois mundos. Um estudo relativamente recente da consultoria de análise de mercados Burning Glass identificou a existência de mais de 800 mil vagas de TI abertas nos Estados Unidos no início deste ano, dentre as quais apenas 1 mil para um posto específico de IoT. ""Certamente, existiram novas profissiões e títulos, mas provavelmente veremos uma grande expansão nas tarefas e responsabilidades atuais"", acredita Tim Herbert, vice-presidente de pesquisa e inteligência de mercado da CompTIA. Para ele, por exemplo, se uma companhia do setor de petróleo decide espalhar sensores sobre seus dutos, um administrador de rede terá que aprender como conectar novos tipos de endpoints que rodam diferentes sistemas operacionais dos que ele está acostumado. As empresas tendem a recorrer a funcionários de TI para resolver esse tipo de problemas, mesmo que, em muitos casos, são profissionais dos departamentos de operações que ocupam posição de engarregados de tais tarefas. Pelo menos é o que identifiou um estudo da Technalysis Research. De acordo com o levantamento divulgado em março, 44% dos profissionais acionados para atuarem em projetos de IoT estão no departamento de tecnologia. O segundo nesse ranking são times que atuam em ""facilities"". Dez habilidades mapeadas O avanço dos dispositivos conectados traz consigo a promessa de novos empregos atrelados a projetos de Internet das Coisas. No momento, ainda há poucos profissionais com perfis totalmente preparados para atuarem em projetos de IoT. ""Temos visto companhias de TI ao redor do mundo se organizando para atuarem com ofertas dentro do conceito, mas muitas vezes não tem processos e talentos para tornar esses planos em realidade"", comenta Ryan Johnson, diretor do site de recrutamento Upwork. Com base em dados extraídos a partir da plataforma, o executivo identificou dez habilidades que as companhias demandam para impulsionar uma estratégia de IoT bem-sucedida. 1. Design de circuito Dispositivos conectados requerem habilidade para integrar componentes eletrônicos a novos sistemas. Por exemplo, aplicações que requerem baterias de longa duração podem demandar um desenho específico das placas e circuitos para otimizar o consumo de energia, ou ter múltiplos sensores em uma única placa. 2. Programação de microcontroladores Um ambiente de IoT é composto por bilhões de pequenos aparelhos interconectados, sendo que, muitos deles requerem, no mínimo, um microcontrolador para adicionar inteligência ao dispositivo a fim de ajudar com tarefas de processamento. Microcontroladores de baixo custo, baixo consumo e com chips embarcados que tenham programação e memória construídas no sistema. Algumas linguagens específicas, como Arduino, são comumente utilizadas para construção de sensores e projetos de automação. 3. AutoCAD Domínio do software de design para projetos de engenharia deve ser altamente requisitado com o avanço da Internet das Coisas. Produtos inteligentes e conectados, frequentemente, requerem uma grande quantidade de ajustes e padronizações, como formatos standard e padrão. O processo de desenvolvimento de produto precisará acomodar essas mudanças de maneira rápida e eficiente. 4. Machine learning Algoritmos de aprendizado de máquina ajudam a criar equipamentos, aplicações e produtos mais inteligentes utilizando sensores de dados e outros aparelhos conectados. Machine learning entra na equação para ajudar a identificar padrões e prover insights. Porém, para fazer isso, empresas precisam especialistas capazes de adicionar inteligência à análise de dados. 5. Segurança de infraestrutura O medo causado pelo aumento da exposição de dados corporativos é justificável. Além disso, IoT adiciona o elemento de controle de equipamentos, que pode gerar transtornos de grandes proporções. Logo, especialistas que compreendem de forma sistêmica questões relativas a segurança de infraestrutura tendem a ser bastante valorizados no cenário que se desenha. 6. Big Data O aumento considerável no numero de dados passíveis de serem analisados é um componente importante no futuro da tecnologia. A Internet das Coisas elevará esse volume de registros ao extremo. Para ter valor da montanha de dados, organizações tendem a recorrer a cientistas de dados e engenheiros hábeis em criar sistemas de coleta, analise e organização de informações coletadas em distintas fontes. Disciplinas como Hadoop e Apache Spark são temas fundamentais. 7. Engenharia Elétrica Mais uma vez, a criação da próxima geração de dispositivos conectados requer habilidades tanto em software quando em engenharia elétrica. Dessa forma, engenheiros eletricistas são contratados para ajudar a desenvolver dispositivos que carreguem aplicações móveis e/ou criar soluções de rádio frequência e/ou micro-ondas para sistemas de comunicação e assim por diante. 8. Arquitetura de Segurança Temas como privacidade e vazamento de dados são uma preocupação constante quando o assunto dispositivos capazes de monitorar a vida de pessoas que estão conectados à internet. Para mitigar os riscos em potencial, empresas tem buscado profissionais capazes de arquitetar soluções passíveis de serem embarcadas no sistema 9. Node.js O ambiente open source é bastante popular em rotinas de gestão de aparelhos conectados com Arduino e Raspberry PI. Aos poucos, se torna em uma opção poderosa aos desenvolvedores que quiserem se aventurar na construção de aplicações para IoT. 10. GPS Sistemas de geoposicionamento ganham peso com a Internet das Coisas, especialmente com o advento de tecnologias vestíveis, veículos inteligentes e projetos impulsionados em companhias de logística.",pt,50
349,264,1460051721,CONTENT SHARED,4579557799493615039,-709287718034731589,8590541763821014933,,,,HTML,http://thenextweb.com/dd/2016/04/07/google-facebook-uber-swift/,google may be considering swift for use on android,"About the time Swift was going open source , representatives for three major brands - Google, Facebook and Uber - were at a meeting in London discussing the new language. Sources tell The Next Web that Google is considering making Swift a ""first class"" language for Android, while Facebook and Uber are also looking to make Swift more central to their operations. Google's Android operating system currently supports Java as its first-class language, and sources say Swift is not meant to replace Java, at least initially. While the ongoing litigation with Oracle is likely cause for concern, sources say Google considers Swift to have a broader ""upside"" than Java. Swift is also open source, which means Google could adopt it for Android without changing its own open source mobile structure. Could Google do it? Born at Apple as a replacement to Objective C, Swift quickly found favor with developers as an easy-to-write language that shed much of the verbosity and clumsy parameters other languages have. It was introduced at WWDC 2014 , and has major support from IBM as well as a variety of major apps like Lyft, Pixelmator and Vimeo that have all rebuilt iOS apps with Swift. Swift can't be copy-pasted for any platform, though. Specifically, Android would need a runtime for Swift - and that's just for starters. Google would also have to make its entire standard library Swift-ready, and support the language in APIs and SDKs. Some low-level Android APIs are C++, which Swift can not currently bridge to. Those would have to be re-written. Swift would also not be useful in bridging higher level APIs in Java; they'd have to be re-written as well. Using Swift for Android is not impossible, though. Late last year, developer Romain Goyet toyed with Swift for Android - and had some success. While that project was completed well ahead of Swift being open source, it nonetheless proved that it can be done. That project used the Android NDK, which allows other languages to be loosely implemented into Android. With an open source Swift and support from Google, Android apps wouldn't require that toolkit. All told, Google would have to effectively recreate its efforts with Java - for Swift. If the company is motivated enough, it's very possible to do so without compromising on its open source values or ruffling any developer feathers along the way. Kotlin Just reaching its potential , sources also claim Kotlin is being discussed as a first class language for Android. Like Swift, Kotlin is object oriented with a focus on safety. Unlike Swift, Kotlin works with Android Studio, Google's IDE for Android development. Unfortunately, sources tell The Next Web that Google's current mindset is that Kotlin is a bit too slow when compiling. But, Kotlin is billed as a language that ""works everywhere Java works,"" and has ""seamless"" support for projects that mix it and Java. It would be much less work on Google's end to get Kotlin up and running for Android, but could be a tedious transition for developers. Facebook and Uber Facebook's interest in Swift appears to be completely founded in technological advancement. A benefit of Swift is that it can serve as both a forward-facing language as well as a server-side one. For a product like Facebook, that's beneficial; apps and servers can speak to one another seamlessly, and it potentially gives the company a wider scope to write APIs for services. And work may have already begun. A Github pull request in the Swift repository named ' Port to Android ' was made by a Facebook employee. It's not clear if his work was official Facebook business or not, though we have confirmed Facebook is already working with Swift internally - it's just not known how thoroughly. Uber's road to Swift is probably a bit cleaner than either Google or Facebook. Though there are many moving parts to Uber's service (app, server and API), it can use Lyft's transition to Swift as an example. When Lyft migrated its iOS app to Swift, it was a ground-up remodel that took a lot of time and effort - but resulted in an app that's lighter, leaner and easier to maintain. It's not known how much (if any) of Lyft's back-end uses Swift, but the company has been highly complimentary of Swift in its existing application. When could a move to Swift happen? The short answer: not anytime soon. The reason? Android. But Swift is quickly finding its way . Several studies suggest it's one of the fastest growing languages around , and has blown up since going open source (GitHub tells The Next Web the language is currently its 11th most popular). Demand for developers who know Swift is also exploding, which could be all the indication these three companies need to at least explore using Swift more thoroughly. Google's onboarding for Swift would be long; it essentially has to rewrite every Android service, app and API. Google would also have to spearhead Swift support for Android - which is still only being poked and prodded at by clever developers in the Swift community. In a way, Google has already begun moving away from bits of Oracle-flavored Java. It's now using the Open JDK for Android rather than the proprietary Java API, and may be considering a post-Java life altogether. Talks in London were said to be exploratory; Google is not yet pushing to move on from Java. While it would be a big undertaking, Swift is meant for speed and safety, and Swift's roadmap suggests it won't be quite as difficult to use it for other platforms in the future, specifically when it comes to C++ . Though Kotlin is an alternative, it's a very nascent language without the eager community Swift has. Facebook and Uber face similarly daunting tasks when it comes to using Swift throughout, but can -and should - wait for Google to shoulder the load with Android. If the use of Swift is going to be as deep as our sources indicate (that is, all companies want to be using it for server side and forward-facing use cases), Android support is integral. Moving to Swift for any of the companies also makes little sense unless it's a thorough re-do, but it's probably not quite as hard as it sounds. Services like Perfect prove that server-side Swift is ready, and it's worth considering that Facebook's engineers ( perhaps from the Parse team ) may already be working on this. IBM is also working to make Swift ready for server-side functions. But don't expect Google, Facebook or Uber to announce Swift-y plans anytime soon. Facebook and Google both have developer conferences on the horizon, and there's no indication that Swift will play a major part at either. We reached out to all three companies for comment on the information our sources brought forward. All three declined to comment. Google specifically pointed to its ongoing litigation with Oracle as reason not to participate in this article. Make of that what you will.",en,50
350,2749,1479208208,CONTENT SHARED,-4158297476631283435,3217014177234377440,8681148622262204198,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36",SP,BR,HTML,https://blog.google/products/g-suite/jamboard-whiteboard-reimagined-collaboration-cloud/,"[situation wall] jamboard - the whiteboard, reimagined for collaboration in the cloud","Bringing the right team together for a meeting or brainstorm can take an idea from being good to great. When we tap into ideas from teams across the globe, our work becomes more collaborative and productive. It doesn't feel like...well, work. At Google, we've set out to redefine meetings. So today, we're introducing Jamboard - a collaborative, digital whiteboard that makes it easy for your team to share ideas in real-time and create without boundaries. We're moving the whiteboard to the cloud.",en,50
351,1276,1465243799,CONTENT SHARED,-986193709330758766,7527226129639571966,4981190668762250875,,,,HTML,https://www.systemcodegeeks.com/meta-scg/system-code-geeks-are-giving-away-a-free-sublime-text-editor-license/,system code geeks are giving away a free sublime text editor license,"Struggling with your heavy-weight editor? Then we have something especially for you! We are running a contest giving away a FREE license for the kick-ass Sublime Text Editor . Sublime Text is a sophisticated text editor for code, markup and prose. You'll love the slick user interface, extraordinary features and amazing performance! Sublime Text is available for OS X, Windows and Linux. It uses a custom UI toolkit, optimized for speed and beauty, while taking advantage of native functionality on each platform. Enter the contest now to win your very own Sublime Text Editor license ! In addition, we will send you free tips and the latest news from the SysAdmin community to master your technical knowledge (you can unsubscribe at any time). In order to increase your chances of winning, don't forget to refer as much of your friends as possible! Make sure to use your lucky URL to spread the word ! You can share it on your social media channels, or even mention it on a blog post if you are a blogger! Good luck and may the force be with you!",en,49
352,1495,1466639889,CONTENT SHARED,-7126520323752764957,-1443636648652872475,5646250801415915324,,,,HTML,https://backchannel.com/how-google-is-remaking-itself-as-a-machine-learning-first-company-ada63defcb70?gi=aa64b8b1080f,"how google is remaking itself as a ""machine learning first"" company - backchannel","If you want to build artificial intelligence into every product, you better retrain your army of coders. Check. Carson Holgate is training to become a ninja. Not in the martial arts - she's already done that. Holgate, 26, holds a second degree black belt in Tae Kwon Do. This time it's algorithmic. Holgate is several weeks into a program that will inculcate her in an even more powerful practice than physical combat: machine learning, or ML. A Google engineer in the Android division, Holgate is one of 18 programmers in this year's Machine Learning Ninja Program, which pulls talented coders from their teams to participate, Ender's Game -style, in a regimen that teaches them the artificial intelligence techniques that will make their products smarter. Even if it makes the software they create harder to understand. ""The tagline is, Do you want to be a machine learning ninja? "" says Christine Robson, a product manager for Google's internal machine learning efforts, who helps administer the program. ""So we invite folks from around Google to come and spend six months embedded with the machine learning team, sitting right next to a mentor, working on machine learning for six months, doing some project, getting it launched and learning a lot."" For Holgate, who came to Google almost four years ago after with a degree in computer science and math, it's a chance to master the hottest paradigm of the software world: using learning algorithms (""learners"") and tons of data to ""teach"" software to accomplish its tasks. For many years, machine learning was considered a specialty, limited to an elite few. That era is over, as recent results indicate that machine learning, powered by ""neural nets"" that emulate the way a biological brain operates, is the true path towards imbuing computers with the powers of humans, and in some cases, super humans. Google is committed to expanding that elite within its walls, with the hope of making it the norm. For engineers like Holgate, the ninja program is a chance to leap to the forefront of the effort, learning from the best of the best. ""These people are building ridiculous models and have PhD's,"" she says, unable to mask the awe in her voice. She's even gotten over the fact that she is actually in a program that calls its students ""ninjas."" ""At first, I cringed, but I learned to accept it,"" she says. Considering the vast size of Google's workforce - probably almost half of its 60,000 headcount are engineers - this is a tiny project. But the program symbolizes a cognitive shift in the company. Though machine learning has long been part of Google's technology - and Google has been a leader in hiring experts in the field - the company circa 2016 is obsessed with it. In an earnings call late last year, CEO Sundar Pichai, laid out the corporate mindset: ""Machine learning is a core, transformative way by which we're rethinking how we're doing everything. We are thoughtfully applying it across all our products, be it search, ads, YouTube, or Play. And we're in early days, but you will see us - in a systematic way - apply machine learning in all these areas."" Obviously, if Google is to build machine learning in all its products, it needs engineers who have mastery of those techniques, which represents a sharp fork from traditional coding. As Peter Domingos, author of the popular ML manifesto The Master Algorithm , writes, ""Machine learning is something new under the sun: a technology that builds itself."" Writing such systems involves identifying the right data, choosing the right algorithmic approach, and making sure you build the right conditions for success. And then (this is hard for coders) trusting the systems to do the work. ""The more people who think about solving problems in this way, the better we'll be,"" says a leader in the firm's ML effort, Jeff Dean, who is to software at Google as Tom Brady is to quarterbacking in the NFL. Today, he estimates that of Google's 25,000 engineers, only a ""few thousand"" are proficient in machine learning. Maybe ten percent. He'd like that to be closer to a hundred percent. ""It would be great to have every engineer have at least some amount of knowledge of machine learning,"" he says. Does he think that will happen? ""We're going to try,"" he says. For years, John Giannandrea has been Google's key promoter of machine learning, and, in a flashing neon sign of where the company is now, he recently became the head of Search. But when he arrived at the company in 2010 (as part of the company's acquisition of MetaWeb, a vast database of people, places and things that is now integrated into Google Search as the Knowledge Graph), he didn't have much experience with ML or neural nets. Around 2011, though, he became struck by news coming from a conference called Neural Information Processing Systems (NIPS). It seemed every year at NIPS some team or other would announce results using machine learning that blew away previous attempts at a solving a problem, be it translation, voice recognition, or vision. Something amazing was happening. ""When I was first looking at it, this NIPS conference was obscure,"" he says. ""But this whole area across academia and industry has ballooned in the last three years. I think last year 6000 attended."" These improved neural-net algorithms along with more powerful computation from the Moore's Law effect and an exponential increase in data drawn from the behavior of huge numbers of users at companies like Google and Facebook, began a new era of ascendant machine learning. Giannandrea joined those who believed it should be central to the company. That cohort included Dean, co-founder of the Google Brain , a neural net project originating in the company's long-range research division Google X. (Now known simply as X.) Google's bear-hug-level embrace of machine learning does not simply represent a shift in programming technique. It's a serious commitment to techniques that will bestow hitherto unattainable powers to computers. The leading edge of this are ""deep learning"" algorithms built around sophisticated neural nets inspired by brain architecture. The Google Brain is a deep learning effort, and DeepMind, the AI company Google bought for a reported $500 million in January 2014, also concentrates on that end of the spectrum. It was DeepMind that created the AlphaGo system that beat a champion of Go, shattering expectations of intelligent machine performance and sending ripples of concern among those fearful of smart machines and killer robots. While Giannandrea dismisses the ""AI-is-going-to-kill us"" camp as ill-informed Cassandras, he does contend that machine learning systems are going to be transformative, in everything from medical diagnoses to driving our cars. While machine learning won't replace humans, it will change humanity. The example Giannandrea cites to demonstrate machine learning power is Google Photos, a product whose definitive feature is an uncanny - maybe even disturbing -  ability to locate an image of something specified by the user. Show me pictures of border collies. ""When people see that for the first time they think something different is happening because the computer is not just computing a preference for you or suggesting a video for you to watch,"" says Giannandrea. ""It's actually understanding what's in the picture."" He explains that through the learning process, the computer ""knows"" what a border collie looks like, and it will find pictures of it when it's a puppy, when its old, when it's long-haired, and when it's been shorn. A person could do that, of course. But no human could sort through a million example and simultaneously identify ten thousand dog breeds. But a machine learning system can. If it learns one breed, it can use the same technique to identify the other 9999 using the same technique. ""that's really what's new here,"" says Giannandrea. ""For those narrow domains, you're seeing what some people call super human performance in these learned systems."" To be sure, machine learning concepts have long been understood at Google, whose founders are lifetime believers of the power of artificial intelligence. Machine learning is already baked into many Google products, albeit not the more recent flavors centering around neural nets. (Earlier machine learning often relied on a more straightforward statistical approach.) In fact, over a decade ago, Google was running in-house courses to teach its engineers machine learning. In the early 2005, Peter Norvig, then in charge of search, suggested to a research scientist named David Pablo Cohn that he look into whether Google might adopt an online course in the subject organized by Carnegie Mellon University. Cohn concluded that only Googlers themselves could teach such an internal course, because Google operated at such a different scale than anyone else (except maybe the Department of Defense). So he reserved a large room in Building 43 (then the headquarters of the search team) and held a two-hour class every Wednesday. Even Jeff Dean dropped in for a couple of sessions. ""It was the best class in the world,"" Cohn says. ""They were all much better engineers than I was!"" The course was so popular, in fact, that it began to get out of hand. People in the Bangalore office were staying past midnight so they could call in. After a couple of years, some Googlers helped put the lectures on short videos; the live sessions ended. Cohn believes it might have qualified as a precursor to the Massive Open Online Course (MOOC). Over the next few years there were other disparate efforts at ML training at Google, but not in an organized, coherent fashion. Cohn left Google in 2010 before, he says, ML at Google ""suddenly became this all-important thing."" That understanding hadn't yet hit in 2012 when Giannandrea had the idea to ""get a bunch of people who were doing this stuff"" and put them in a single building. Google Brain, which had ""graduated"" from the X division, joined the party. ""We uprooted a bunch of teams, put them in a building, got a nice new coffee machine,"" he says. ""People who previously had just been working on what we called perception - sound and speech understanding and so on - were now talking to the people who were trying to work on language."" More and more, the machine learning efforts from those engineers began appearing in Google's popular products. Since key machine learning domains are vision, speech, voice recognition, and translation, it's unsurprising that ML is now a big part of Voice Search, Translate, and Photos. More striking is the effort to work machine learning into everything . Jeff Dean says that as he and his team have begun to understand ML more, they are exploiting it in more ambitious ways. ""Previously, we might use machine learning in a few sub-components of a system,"" he says. ""Now we actually use machine learning to replace entire sets of systems, rather than trying to make a better machine learning model for each of the pieces."" If he were to rewrite Google's infrastructure today, says Dean, who is known as the co-creator of game-changing systems like Big Table and MapReduce , much of it would not be coded but learned. Machine learning also is enabling product features that previously would have been unimaginable. One example is Smart Replies in Gmail, launched in November 2015. It began with a conversation between Greg Corrado, a co-founder of the Google Brain project, and a Gmail engineer named Bálint Miklós. Corrado had previously worked with the Gmail team on using ML algorithms for spam detection and classifying email, but Miklós suggested something radical. What if the team use machine learning to automatically generate replies to emails, saving mobile users the hassle of tapping out answers on those tiny keyboards? ""I was actually flabbergasted because the suggestion seemed so crazy,"" says Corrado. ""But then I thought that with the predictive neural net technology we'd been working on, it might be possible. And once we realized there was even a chance, we had to try."" Google boosted the odds by keeping Corrado and his team in close and constant contact with the Gmail group, an approach that is increasingly common as machine learning experts fan out among product product groups. ""Machine learning is as much art as it is science,"" says Corrado. ""It's like cooking - yes, there's chemistry involved but to do something really interesting, you have to learn how to combine the ingredients available to you."" Traditional AI methods of language understanding depended on embedding rules of language into a system, but in this project, as with all modern machine learning, the system was fed enough data to learn on its own, just as a child would. ""I didn't learn to talk from a linguist, I learned to talk from hearing other people talk,"" says Corrado. But what made Smart Replies really feasible was that success could be easily defined - the idea wasn't to create a virtual Scarlett Johansson who would engage in flirtatious chatter, but plausible replies to real-life emails. ""What success looked like is that the machine generated a candidate response that people found useful enough to use as their real response,"" he says. Thus the system could be trained by noting whether or not users actually clicked on the suggested replies. When the team began testing Smart Responses, though, users noted a weird quirk: it would often suggest inappropriate romantic responses. ""One of the failure modes was this really hysterical tendency for it to say, 'I love you' whenever it got confused,"" says Corrado. ""It wasn't a software bug - it was an error in what we asked it to do."" The program had somehow learned a subtle aspect of human behavior: ""If you're cornered, saying, 'I love you' is a good defensive strategy."" Corrado was able to help the team tamp down the ardor. Smart Replies , released last November, is a hit - users of the Gmail Inbox app now routinely get a choice of three potential replies to emails that they can dash off with a single touch. Often they seem uncannily on the mark. Of responses sent by mobile Inbox users, one in ten is created by the machine-learning system. ""It's still kind of surprising to me that it works,"" says Corrado with a laugh. Smart Replies is only one data point in a dense graph of instances where ML has proved effective at Google. But perhaps the ultimate turning point came when machine learning became an integral part of search, Google's flagship product and the font of virtually all its revenues. Search has always been based on artificial intelligence to some degree. But for many years, the company's most sacred algorithms, those that delivered what were once known as the ""ten blue links"" in response to a search query, were deemed too important for ML's learning algorithms. ""Because search is such a large part of the company, ranking is very, very highly evolved, and there was a lot of skepticism you could move the needle very much,"" says Giannandrea. In part this was a cultural resistance - a stubborn microcosm of the general challenge of getting control-freaky master hackers to adopt the Zen-ish machine learning approach. Amit Singhal, the long-time maester of search, was himself an acolyte of Gerald Salton, a legendary computer scientist whose pioneering work in document retrieval inspired Singhal to help revise the grad-student code of Brin and Page into something that could scale in the modern web era. (This put him in the school of ""retrievers."") He teased amazing results from those 20th century methods, and was suspicious of integrating learners into the complicated system that was Google's lifeblood. ""My first two years at Google I was in search quality, trying to use machine learning to improve ranking,"" says David Pablo Cohn. ""It turns out that Amit's intuition was the best in the world, and we did better by trying to hard code whatever was in Amit's brain. We couldn't find anything as good as his approach."" By early 2014, Google's machine learning masters believed that should change. ""We had a series of discussions with the ranking team,"" says Dean. ""We said we should at least try this and see, is there any gain to be had."" The experiment his team had in mind turned out to be central to search: how well a document in the ranking matches a query (as measured by whether the user clicks on it.) ""We sort of just said, let's try to compute this extra score from the neural net and see if that's a useful score."" It turned out the answer was yes, and the system is now part of search, known as Rank Brain. It went online in April 2015. Google is characteristically fuzzy on exactly how it improves search (something to do with the long tail? Better interpretation of ambiguous requests?) but Dean says that Rank Brain is ""involved in every query,"" and affects the actual rankings ""probably not in every query but in a lot of queries."" What's more, it's hugely effective. Of the hundreds of ""signal"" Google search uses when it calculates its rankings (a signal might be the user's geographical location, or whether the headline on a page matches the text in the query), Rank Brain is now rated as the third most useful. ""It was significant to the company that we were successful in making search better with machine learning,"" says Giannandrea. ""That caused a lot of people to pay attention."" Peter Domingos, the University of Washington professor who wrote The Master Algorithm , puts it a different way: ""There was always this battle between the retrievers and the machine learning people,"" he says. ""The machine learners have finally won the battle."" Google's new challenge is shifting its engineering workforce so everyone is familiar, if not adept, at machine learning. It's a goal pursued now by many other companies, notably Facebook, which is just as gaga about ML and deep learning as Google is. The competition to hire recent graduates in the field is fierce, and Google tries hard to maintain its early lead; for years, the joke in academia was that Google hires top students even when it doesn't need them, just to deny them to the competition. (The joke misses the point that Google does need them.) ""My students, no matter who, always get an offer from Google."" says Domingos. And things are getting tougher: just last week, Google announced it will open a brand new machine-learning research lab in Zurich, with a whole lot of workstations to fill. But since academic programs are not yet producing ML experts in huge numbers, retraining workers is a necessity. And that isn't always easy, especially at a company like Google, with many world-class engineers who have spent a lifetime achieving wizardry through traditional coding. Machine learning requires a different mindset. People who are master coders often become that way because they thrive on the total control that one can have by programming a system. Machine learning also requires a grasp of certain kinds of math and statistics, which many coders, even gonzo hackers who can zip off tight programs of brobdingnagian length, never bothered to learn. It also requires a degree of patience. ""The machine learning model is not a static piece of code - you're constantly feeding it data,"" says Robson. ""We are constantly updating the models and learning, adding more data and like tweaking how we're going to make predictions. It feels like a living, breathing thing. It's a different kind of engineering."" ""It's a discipline really of doing experimentation with the different algorithms, or about which sets of training data work really well for your use case,"" says Giannandrea, who despite his new role as search czar still considers evangelizing machine learning internally as part of his job. ""The computer science part doesn't go away. But there is more of a focus on mathematics and statistics and less of a focus on writing half a million lines of code."" As far as Google is concerned, this hurdle can be leapt over by smart re-training. ""At the end of the day the mathematics used in these models is not that sophisticated,"" says Dean. ""It's achievable for most software engineers we would hire at Google."" To further aid its growing cadre of machine learning experts, Google has built a powerful set of tools to help engineers make the right choices of the models they use to train their algorithms and to expedite the process of training and refining. The most powerful of those is TensorFlow , a system that expedites the process of constructing neural nets. Built out of the Google Brain product, and co-invented by Dean and his colleague Rajat Monga, TensorFlow helped democratize machine learning by standardizing the often tedious and esoteric details involved in building a system - especially since Google made it available to the public in November 2015. While Google takes pains to couch the move as an altruistic boon to the community, it also acknowledges that a new generation of programmers familiar with its in-house machine learning tools is a pretty good thing for Google recruiting. (Skeptics have noted that Google's open-sourcing TensorFlow is a catch-up move with Facebook, which publicly released deep-learning modules for an earlier ML system, Torch, in January 2015.) Still, TensorFlow's features, along with the Google imprimatur, have rapidly made it a favorite in ML programming circles. According to Giannandrea, when Google offered its first online TensorFlow course, 75,000 people signed up. Google still saves plenty of goodies for its own programmers. Internally, the company has a probably unparalleled tool chest of ML prosthetics, not the least of which is an innovation it has been using for years but announced only recently - the Tensor Processing Unit . This is a microprocessor chip optimized for the specific quirks of running machine language programs, similar to the way as Graphics Processing Units are designed with the single purpose of speeding the calculations that throw pixels on a display screen. Many thousands (only God and Larry Page probably know how many) are inside servers in the company's huge data centers. By super-powering its neural net operations, TPU's give Google a tremendous advantage. ""We could not have done Rank Brain without it,"" says Dean. But since Google's biggest need is people to design and refine these systems, just as the company is working feverishly to refine its software-training tools, it's madly honing its experiments in training machine-learning engineers. They range from small to large. The latter category includes quick-and-dirty two-day ""Machine Learning Crash Course with TensorFlow,"" with slides and exercises. Google hopes this is a first taste, and the engineers will subsequently seek out resources to learn more. ""We have thousands of people signed up for the next offering of this one course,"" says Dean. Other, smaller efforts draw outsiders into Google's machine learning maw. Earlier this spring, Google began The Brain Residency program, a program to bring in promising outsiders for a year of intense training from within the Google Brain group. ""We're calling it a jump start in your Deep Learning career,"" says Robson, who helps administer the program. Though it's possible that some of the 27 machine-learning-nauts from different disciplines in the initial program might wind up sticking around Google, the stated purpose of the class is to dispatch them back in the wild, using their superpowers to spread Google's version of machine learning throughout the data-sphere. So, in a sense, what Carson Holgate learns in her ninja program is central to how Google plans to maintain its dominance as an AI-focused company in a world where machine learning is taking center stage. Her program began with a four-week boot camp where the product leads of Google's most advanced AI projects drilled them on the fine points of baking machine learning into projects. ""We throw the ninjas into a conference room and Greg Corrado is there at the white board, explaining LSTM [""Long Short Term Memory,"" a technique that makes powerful neural nets], gesturing wildly, showing how this really works, what the math is, how to use it in production,"" says Robson. ""We basically just do this with every technique we have and every tool in our toolbox for the first four weeks to give them a really immersive dive."" Holgate has survived boot camp and now is using machine learning tools to build a communications feature in Android that will help Googlers communicate with each other. She's tuning hyperparameters. She's cleansing her input data. She's stripping out the stop words. But there's no way she's turning back, because she knows that these artificial intelligence techniques are the present and the future of Google, maybe of all technology. Maybe of everything. ""Machine learning,"" she says, ""is huge here."" Photography by Jason Henry.",en,49
353,2084,1471366025,CONTENT SHARED,-5850390492300666299,7645894863578715801,6107910099552275355,,,,HTML,https://medium.com/@skamille/microservices-real-architectural-patterns-68bd83bbb6cd,microservices: real architectural patterns,"Microservices: Real Architectural Patterns A dissection of our favorite folk architecture Introduction I'm fascinated by the lore and mystery behind microservices. As a concept, microservices feels like one of the most interesting folk architectures of the modern era. It's useful enough to be applied widely across different usage patterns and also vague enough to mean many different things. I've been struggling for a while with understanding what people really mean when they discuss ""microservices."" Despite deploying what I would consider to be a version of that pattern in my last gig, it's quite clear to me that the architecture we used is not the same as the pattern that all other companies use. Recently I finally interrogated someone who has deployed the pattern in a very different way than I have, and so I decided it would be illustrative to compare and contrast the circumstances of our architectures for those in the larger technical audience. This article is going to have two examples. The first is the rough way ""microservices"" was deployed in my last gig, and why I made the decisions I made in the architecture. The second is an example of an architecture that is much closer to the ""beautiful dream"" microservices as I have heard it preached, for architectures that are stream-focused. Microservices Basics I think that microservices as an architecture evolved due to a few factors. A bunch of startups in the late 2000s started on monoliths like rails, scaled their business and team quickly, and hit the wall on what could reasonably be done in that monolith The cloud made it significantly easier to get access to a new server instance to run software We all got much more comfortable with the idea that we were dealing with distributed systems and in particular got comfortable making network calls as part of our systems This combination of factors - scaling woes, easy access to new hardware, distributed systems and network access - played a huge part in what I might call ""microservices for CRUD."" If you have managed to scale a company to a certain level of success on a monolith but you are having trouble scaling the technology and/or the engineering team, breaking the monolith into a services-style architecture makes sense. This is a situation I encountered first-hand. The arguments for microservices here look something like: Services allow for independent axes of scaling. If you have a part of the system with higher load or capacity requirements than other parts, you can scale to meet its needs. This is certainly doable in a monolith, but somewhat more complicated to reason about. Services allow for independent failure domains, to a more limited extent. Insofar as parts of your system are independently operable, you may want to allow for partial availability by splitting them out into services. For example, in a commerce app, if you can serve the checkout flow even when the product search flow is down, that might be considered a good thing. This is much more complicated in practice than it is in theory, and people make many silly claims about microservices that imply that any overlap in services means that they are not valuable. Independent failure domains are sometimes more of a ""nice to have"" than a necessity, and making the architecture truly account for this is not easy. Services allow for teams to work independently on parts of the system. Again, you can do this in a monolith. I have done this in a monolith. But the challenge with monolith (and a related challenge with services in a monorepo (single source repository)) is that humans struggle to tangibly understand domains that are theoretically separate when they are presented as colocated by the source code. If I can see all of the code and it all compiles together and feels like a single thing, my tendency is to want to use it as a single thing. Grab code from here to use there, grab data from there to use here, etc. A few more notes. ""Monolith"" and ""monorepo"" often get tangled up when talking about this world. A monolithic application is one where you have a set of code that compiles into a single main server artifact (possibly with some additional client artifacts produced). You can use configuration to make monoliths do almost anything you can imagine, including all of the services-type things above, but the image produced tends to include most if not all of the code in the repository. This does get fuzzy because sometimes teams evolve their monoliths to compile to a couple of specialized server artifacts via a combination of build tooling and configuration. I would generally still call this a monolithic architecture. Monorepo, or monolith repository, is the model where you have a single repository that holds all of the code for any system you are actively changing (so, possibly excluding the source code for your OSS/external dependencies). The repository itself contains source code that accounts for multiple artifacts that are run as separate applications, and which can be compiled/packaged and tested separately without using the entire repository. Often this is used to enable certain shared libraries to change across all of the services that use those libraries, so that developers who support shared libraries can more easily evolve them instead of having to wait for each dependent team to adopt the newest version. The biggest downside of the monorepo model is that there's not much OSS tooling that supports this, because most OSS is not built this way, so large investments in tooling are usually needed to make this work. Microservices for CRUD-based Applications Before I get to how to evolve a CRUD monolith to microservices, let me further articulate the architecture needed to build your traditional mid-sized CRUD platform. This type of platform covers a use case that is pretty well-trod, that of ""transactions"" and ""metadata."" Transactions: User does an action that you want to persist, consistency of data is very valuable. The ""Create, Update, Delete"" of CRUD. Much less frequent than the ""Read"" actions of CRUD. Metadata: Information that describes things to the users, but is usually only modified by internal content creators, or rarely by external users (reviews, for example). Changes less frequently, often highly cacheable. Even more, can often tolerate a degree of temporary inconsistency (showing stale data). Are there more things that CRUD-heavy companies want to do, especially in the analytical space here? Sure. You may want to adjust results frequently based on user behavior as the user is browsing the site, and other personalization actions. However, that is a hard thing to do real-time and you don't always have the volume of data you need from the user to actually do that well, so it isn't generally the first-order concern of the system. The process for moving off of a monolith in this type of architecture is relatively straightforward: Identify independent entities. This paper by Pat Helland, ""Life Beyond Txns"" , has some useful and interesting definitions there. It's better to go a little bit too big early than to go too small and end up having to implement significant distributed transactions. You probably want data-owning services for the major business objects(products, users, etc), and then sets of integration services that implement aggregations and logic over those objects. Pull out the logic into services entity by entity. Try not to change the data model as much as possible in this process. Redirect the monolith to call APIs in the new services as functionality is moved. That's basically it. You pull pieces out until you have enough to cover a particular set of user functionality in data and integration terms, then you can start to evolve that part of the user functionality to do new things in the services. These services are not classic SOA, but nor are they teeny-tiny microservices. The services that own the data may be fairly sophisticated. You may not want to have too many services because you want to be able to satisfy requests from the user without having to make a ton of network hops, and ideally, without needing to do distributed transactions. You are probably not making new services every day, and especially if you have a sub-50-person engineering team and a long product roadmap, you may not want to invest extensive engineering time into complex orchestration and tooling that enables people to dynamically add new services at the click of a button (nb: the products to support this are getting better all the time, and so at some point this will be worth doing even for that smaller team. It is unclear to me whether that time is now or not.) . The equation to apply for determining how much to invest in tooling is pretty straightforward: how much time does it cost devs to have a less automated process for adding a new service, vs how long does it take to implement and maintain the automation for doing it easily, and how many new services do you expect to want to deploy over time? You're making a guess. Obviously, if you think there is value to enabling people to spin up tiny services fast and frequently, it is better to invest time and tooling into this. As with all engineering process optimization decisions, it's not a matter of getting it perfectly right, but rather, of deciding for the foreseeable future and periodically re-evaluating. There are many microservices ""must-haves"" in this instance that I have found to be anything but. I mentioned extensive orchestration above. Dynamic service discovery is also not needed if you are not automatically spinning up services or moving services around frequently (load balancers are pretty nice for doing this at a basic level). Allowing teams to choose their ideal language, framework, and data store per service is also certainly not a must-have and in fact it's likely to be far more of a headache than a boon to your team. Having independent data stores for the services is also not a must-have, although it does mean that you will have a high-risk SPOF on the shared database. As I was writing this piece I discovered a section of some writing on microservices from 2015: Create a Separate Data Store for Each Microservice Do not use the the same back-end data store across microservices. ... Moreover, with a single data store it's too easy for microservices written by different teams to share database structures, perhaps in the name of reducing duplication of work. You end up with the situation where if one team updates a database structure, other services that also use that structure have to be changed too. This is true, but for smaller teams you can prevent sharing of database structures by convention (process and code review, and automated testing and checking for such access if it is a huge worry). When you carefully define the data-owner services, it's less likely this will happen. And the alternative is the next paragraph: Breaking apart the data can make data management more complicated, because the separate storage systems can more easily get out sync or become inconsistent, and foreign keys can change unexpectedly. You need to add a tool that performs master data management (MDM) by operating in the background to find and fix inconsistencies. For example, it might examine every database that stores subscriber IDs, to verify that the same IDs exist in all of them (there aren't missing or extra IDs in any one database). You can write your own tool or buy one. Many commercial relational database management systems (RDBMSs) do these kinds of checks, but they usually impose too many requirements for coupling, and so don't scale.( original ) This paragraph probably leads to sighs of exhaustion from anyone with experience doing data reconciliation. It's due to this overhead that I encourage those of you in smaller organizations to at least evaluate a convention-based approach before deciding to use entirely independent and individual data stores. This is a decision you can delay as needed. This version of the microservices architecture is very compelling for the scaled CRUD world because it lets you do a rewrite piece by piece. You can do the whole system, or you can simply take out pieces that are most sensitive to scaling. You proactively engage with many of the bits of distributed systems complexity by thinking carefully about the data and where transactions on that data will be needed. You probably don't need a ton of fancy data pipelines floating around. You know where the data will be modified. Do you have to go to microservices to scale this? Probably not, but that doesn't mean using microservices to scale such systems is a bad idea. However, going extreme with the microservices model may be a bad idea, because you really don't want to slice your data up in a way that ends up in distributed transaction land. Microservices For Data Stream Processing Now, let's talk about a very different use case. This use case is not your classic CRUD application, thick with business rules around transactionally-updated objects. Instead, this use case has a large pipeline of data. It has small bits of data flowing into it from many different sources, a very large volume of many bits of data. This large volume of input data sources also has many different services that will consume it, modify it, and pass it along for further processing. The major concern of this application is ingesting large quantities of ever-changing data, processing it in various ways, and showing a view of it to customers. CRUD concerns are secondary to the larger concerns of keeping up with the data stream and recalculating information based on what is happening on that stream. Let's take a metrics-aggregating SaaS application, for example. This application has customers all over the world with various applications, services, and machines that are reporting out metrics to the aggregator. These customers only need to see their data, although the combined total of data for any one customer may be very large. Our aggregator needs to consume these metrics and send them off to the application that is going to show them to the customer. The customer-facing application may be operating on a combination of incoming metrics in real-time plus historical data that comes from cache or a backing storage system. A large part of the value of the data is in the moving-window of what is happening right now/recently. This architecture from the start has considerations of volume that even our scaled CRUD world may not care about for a very, very long time. Additionally, the data itself is mostly a stream of updates over time. The notion of the ""stateful"" data that is transactionally updated is minimal, the most useful data is more like a timeseries or log of events. The transactional data, say, stored user views and user configuration, may be more like the ""metadata"" of our CRUD application in the first example, infrequently changed compared to the updates coming in from the stream. The majority of developer time is most likely spent not in dealing with these transactional changes but rather in managing the streams of inputs, providing new types of inputs, applying new calculations to the stream of inputs, and changing the calculations. In this example, you can imagine a service that wants to run an experiment by doing a different calculation across a particular element on the stream. Instead of modifying the existing code, the experimental service listens to the data stream at the same point as the existing calculation, provides a new calculation value, and pushes that calculation value back into the data pipeline on a different channel. At some point an experiment service pulls this data out for the customers who are assigned to the experimental treatment and shows the results of that calculation instead of the standard calculation. In all of these places you need a record of what happened in order to do analysis of experiment success and debugging, but that record does not need to be strongly, transactionally related to the record of other events in the system at this time, even across related users. In this example, it may very well be much more effective to spin up new services as needed, in order to run quick experiments, rather than changing existing services. Especially in cases where the service can do this without needing to worry about coordinating the data consumption or production with any existing service. This is the world of what I would like to call ""stream-centric microservices."" If there is enormous value to your business to manage real-time data streams, and you are going to have a lot of developers consuming those streams by creating new services to listen to them and produce results, then you absolutely must be willing to commit to the investment in tooling to make the process of creating services and putting them into production as easy as possible. You will probably use this for all of your services over time, once you have it, but realize that the clear value is that you have dynamic data that can be processed and manipulated and experimented on independently. Cron Jobs as Microservices I'd be remiss if I didn't mention this pattern. When it becomes very easy to make anything a microservice, everything becomes a microservice, including things we would traditionally run as cron jobs. But cron jobs are a nice concept, and not everything has to be a ""service."" You can use CloudWatch Events from AWS for this purpose, or scheduled Lambda functions. Use Gearman, a queue and async job runner, to schedule cron jobs. Remember your cron jobs need to be idempotent (can be run twice on the same input without changing the outcome). If you have an easy way to spin up services and it's easy to create tiny services that are basically cron jobs, no problem, but cron jobs in and of themselves are not a great reason to create a large, orchestrated services environment. Conclusion I hope that this has been a useful breakout across a few axes of the wild world of microservices. Going through the thought experiment was very useful for me, personally. It helped me understand how what seems obvious to people at one extreme, say those who spend most of their time focused on stream processing, doesn't make as much sense for people who are more focused on the world of CRUD application scaling.",en,49
354,2650,1477477378,CONTENT SHARED,-6590064508646491981,7645894863578715801,-2649007012474008270,Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:49.0) Gecko/20100101 Firefox/49.0,SP,BR,HTML,http://www.concretesolutions.com.br/2016/08/16/kotlin-no-backend-parte-2/,kotlin no backend (parte 2) :: concrete solutions,"Ontem, começamos aqui no Blog uma série sobre o Kotlin no backend, configurando o ambiente e criando uma função main para mostrar a mensagem. Se você ainda não viu, pode clicar aqui . Hoje, vamos criar algo um pouco mais complexo usando o SpringBoot para levantar um serviço REST simples em Jersey . Início Vamos utilizar o Gradle para fazer build e gerenciar as dependências do SpringBoot no projeto. Para isso, crie uma pasta, execute o gradle e importe no IntelliJ. Criei a pasta ServicoKotlin, mas fique à vontade para escolher a sua =D gradle init -type java-library Ele vai criar a seguinte estrutura: Com o projeto importado no IntelliJ IDEA , vamos configurar o Gradle para resolver dependências, plugins e o build de nosso projeto. Abra o arquivo build.gradle , que se encontra na raiz do projeto, e substitua pelo o seguinte conteúdo: Criando nossas classes Clique com o botão direito nas pastas java em main.java e test.java e as delete. Também com o botão direito, clique na pasta main, crie o diretório kotlin e marque este como Sources Root . A estrutura deve ficar assim: src/main/kotlin, igual ao que está no sourceSets do script build.gradle . Crie um pacote à sua escolha (o meu é org.kotlin) e crie um arquivo kotlin dentro com o nome de App. A extensão dos arquivos kotlin é .kt . Neste arquivo vamos criar nossa classe e a função main para o SpringBoot . Vamos agora detalhar este trecho: @SpringBootApplication é uma anotação de conveniência, que marca a classe com as seguintes anotações com seus atributos default: @Configuration: anota a classe como uma fonte de definições de beans para o contexto da aplicação; @EnableAutoConfiguration: diz ao SpringBoot para começar a adicionar beans com base nas configurações de classpath e várias configurações de propriedade; @ComponentScan: diz ao Spring para procurar outros componentes, configurações e serviços no pacote atual. Modificador open na classe é o oposto do final do Java. Em Kotlin, todas as classes e métodos são final por padrão, e para o Spring elas não podem ser final devido ao uso de proxy CGLIB. Uma função restTemplate com anotação @Bean cria uma instância do RestTemplate como um Bean do Spring . Uma função objectMapperBuilder sem corpo (sim, em Kotlin isto é possível =D) tem anotação @Bean para registrá-lo no Jackson ObjectMapper . Em Java , o método main( ) de um aplicativo é convencionalmente definido dentro da classe de aplicação anotada. Isso ocorre porque Java não suporta métodos de nível superior. Em Kotlin , no entanto, nós temos funções top-level. Assim, podemos tornar o principal ponto de entrada do SpringBoot muito mais simples. Com isto, a função main() é definida em uma classe chamada AppKt e não em uma classe App. A função main() utiliza o método SpringApplication.run() do SpringBoot para iniciar um aplicativo. O método run() retorna um ApplicationContext e este aplicativo, em seguida, recupera todos os beans que foram criados pela aplicação ou foram adicionados automaticamente graças ao SpringBoot . Note que o método run() recebe dois parâmetros: a classe App::class.java e um *args, que detalharei melhor a seguir: App::class.java: (::) logo após o nome da classe indica que iremos pegar uma referência à classe por reflection. O (.java) indica que queremos uma referência Java e não Kotlin ; *args: * operador spread antes do array, insere todo o conteúdo do array no vararg do método run() . Vamos criar agora nossa configuração do Jersey. Crie uma arquivo kotlin com nome de JerseyConfig e configure o Jersey como a seguir: Vamos aos detalhes: JerseyConfig : ResourceConfig(): herança e implementação de interfaces em Kotlin ; init { ... }: Bloco inicializador será criado junto com a instância da classe. Para criar nossa endpoint, crie um pacote chamado api e dentro crie um arquivo kotlin com o nome de JerseyAPI. Veja o exemplo:[ Crie um arquivo chamado application.properties com o conteúdo abaixo dentro da pasta resources do projeto. Aqui diremos ao SpringBoot que o Undertow (WebServer) vai subir na porta configurada. Execute com um duplo click a task bootRun , na visão de Gradle do IntelliJ IDEA . Abra um navegador e digite o seguinte na barra de endereços : Veja o resultado: E aí? Deu certo? Se tiver qualquer dúvida ou quiser dar alguma contribuição, fique à vontade nos comentários. Espero que tenha gostado! Até a próxima. - É desenvolvedor back-end e quer trabalhar com nosso time? Acesse aqui .",pt,49
355,1594,1467292956,CONTENT SHARED,7544768317373280661,-1387464358334758758,8846131487417657746,,,,HTML,http://googlediscovery.com/2016/06/29/google-adiciona-medidor-de-velocidade-da-internet-nos-resultados-de-pesquisa/,google adiciona medidor de velocidade da internet nos resultados de pesquisa | google discovery,"De acordo com um usuário do Twitter, o Google está testando uma nova funcionalidade em seu mecanismo de pesquisa que permite medir a velocidade da internet com apenas uma busca. No caso, o recurso de medição pode ser ativado com o termo [check internet speed] no Google americano. A gigante da internet alerta para a quantidade de dados que será utilizado no processo, a fim de evitar surpresas em operadoras com franquia. Looks like Google is testing their own internet speed test (query = ""check internet speed"") - not seeing it live - pic.twitter.com/wjsPIlEbFv - Dr. Pete Meyers (@dr_pete) June 27, 2016 Atualmente o Bing - também limitado somente aos residentes dos EUA - também já oferece uma solução parecida ao permitir checar a velocidade da conexão diretamente nos resultados. Basta pesquisar por [speed test]. Caso você não queira esperar por uma página do Google capaz de medir a velocidade da sua internet, os brasileiros podem acessar, sem qualquer restrição, o sistema de checagem de velocidade do Google Fiber . Há poucas semanas a Netflix entrou na briga ao lançar o Fast.com , uma ferramenta de verificação de velocidade que não exige nenhuma ação do usuário para funcionar, no qual tem sido bem recebido pelo público.",pt,49
356,1343,1465582475,CONTENT SHARED,-387651900461462767,-1387464358334758758,-3452196695241416722,,,,HTML,http://googlediscovery.com/2016/06/09/android-ira-recomendar-aplicativos-baseado-na-localizacao/,android irá recomendar aplicativos baseado na localização | google discovery,"O Google anunciou hoje que vai disponibilizar uma atualização para o Android que irá utilizar a localização do aparelho para recomentar aplicativos úteis. A novidade foi batizada de Nearby e irá notificar os usuários sempre que aplicativos interessantes forem identificados como relevantes com base na localização do usuário. O buscador prometeu que todos os telefones que rodam Android 4.4 (KitKat) ou superior irão receber a novidade por meio de uma atualização do Google Play Services. Veja um exemplo abaixo: é bacharel em administração de empresas e fundador da FragaNet Networks - empresa especializada em comunicação digital e mídias sociais. Em seu portfólio estão projetos como: Google Discovery, TechCult, AutoBlog e Arquivo UFO. Também foi colunista de tecnologia no TechTudo, da Globo.com.",pt,49
357,1016,1463607275,CONTENT SHARED,-538948733779286129,-1032019229384696495,2055210841846036435,,,,HTML,https://google.github.io/physical-web/,the physical web,"What is this? The Physical Web is an open source approach to unleash the core superpower of the web: interaction on demand. People should be able to walk up to any smart device - a vending machine, a poster, a toy, a bus stop, a rental car - and not have to download an app first. Everything should be just a tap away. We're developing it out in the open as we do all things related to the web. While Google is creating an initial implementation , we hope others join in as well. Why is this important? The number of smart devices is going to explode, and the assumption that each new device will require its own application just isn't realistic. We need a system that lets anyone interact with any device at any time. The Physical Web isn't about replacing native apps: it's about enabling interaction when native apps just aren't practical. How does this change things? Once any smart device can have a web address, the overhead of a dedicated app is no longer required for simple interactions. The Physical Web approach unlocks use cases that would never be practical if a dedicated app were required: A cat collar can let you call to find the owner A bus can tell you its next stop A parking meter can let you pay using your phone and the cloud Any store, no matter how small, can offer an online experience when you walk in A shared car service can broadcast a signup page, allowing you to immediately drive away Industrial equipment can offer diagnostics Each of these examples, taken by itself, is modestly useful. Taken as a whole, however, they imply a vast ""long tail"" where anything can offer information and utility. So many people ask what is the 'killer app' for the Physical Web. That's a bit like asking what is the killer app for the web itself. When any place and object can offer a web page for help, information, configuration, or use, we'll unlock millions of things, rather than a single killer product. What can you build? This Cookbook article shows a wide range of experiences that can be built using the Physical Web. How can I get started? To discover websites associated with objects and locations nearby, try the Physical Web on Chrome for iOS or Android . If you would like to broadcast URLs for others to discover, consult this getting started guide . How does it work? A small utility on the phone scans for URLs that are nearby. We are using the open Eddystone-URL Bluetooth beacon format to find nearby URLs without requiring any centralized registrar. Here is a list of many beacon makers that are already supporting Eddystone. We also support finding URLs through Wifi using mDNS and uPnP. To learn more, visit the project on GitHub .",en,49
358,2432,1474915293,CONTENT SHARED,-6400214860728938634,3061273947166110301,-4294318496550281618,,,,HTML,https://www.oreilly.com/learning/how-to-build-a-robot-that-sees-with-100-and-tensorflow,"how to build a robot that ""sees"" with $100 and tensorflow","Eye of Providence. (source: Bureau of Engraving and Printing on Wikimedia Commons ). Object recognition is one of the most exciting areas in machine learning right now. Computers have been able to recognize objects like faces or cats reliably for quite a while, but recognizing arbitrary objects within a larger image has been the Holy Grail of artificial intelligence. Maybe the real surprise is that human brains recognize objects so well. We effortlessly convert photons bouncing off objects at slightly different frequencies into a spectacularly rich set of information about the world around us. Machine learning still struggles with these simple tasks, but in the past few years, it's gotten much better. Deep learning and a large public training data set called ImageNet has made an impressive amount of progress toward object recognition. TensorFlow is a well-known framework that makes it very easy to implement deep learning algorithms on a variety of architectures. TensorFlow is especially good at taking advantage of GPUs, which in turn are also very good at running deep learning algorithms. Building my robot I wanted to build a robot that could recognize objects. Years of experience building computer programs and doing test-driven development have turned me into a menace working on physical projects. In the real world, testing your buggy device can burn down your house, or at least fry your motor and force you to wait a couple of days for replacement parts to arrive. The new third generation Raspberry Pi is perfect for this kind of project. It costs $36 on Amazon.com and has WiFi, a quad core CPU, and a gigabyte of RAM. A $6 microSD card can load Raspberian , which is basically Debian . See Figure 1 for an overview of how all the components worked together, and see Figure 2 for a photo of the Pi. I love the cheap robot chassis that Sain Smart makes for around $11. The chassis turns by spinning the wheels at different speeds, which works surprisingly well (see Figure 3). The one place I spent more money when cheaper options were available is the Adafruit motor hat (see Figure 4). The DC motors run at a higher current than the Raspberry Pi can provide, so a separate controller is necessary, and the Adafruit motor hat is super convenient. Using the motor hat required a tiny bit of soldering, but the hardware is extremely forgiving, and Adafruit provides a nice library and tutorial to control the motors over i2C . Initially, I used cheaper motor controllers, but I accidentally fried my Pi, so I decided to order a better quality replacement. A $15 camera attaches right into the Raspberry Pi and provides a real-time video feed I can use to recognize objects. There are tons of awesome cameras available. I like the infrared cameras that offer night vision. The Raspberry Pi needs about 2 amps of current, but 3 amps is safer with the speaker we're going to plug into it. iPhone battery chargers work awesomely for this task. Small chargers don't actually output enough amps and can cause problems, but the Lumsing power bank works great and costs $18. A couple of HC- SR0 4 sonar sensors help the robot avoid crashing into things-you can buy five for $11. I added the cheapest USB speakers I could find, and used a bunch of zip ties, hot glue, and foam board to keep everything together. As an added bonus, I cut up some of the packaging materials the electronics came with and drew on them to give the robots some personality. I should note here that I actually built two robots (see Figure 5) because I was experimenting with different chassis, cameras, sonar placement, software, and so forth, and ended up buying enough parts for two versions. Once the robot is assembled, it's time to make it smart. There are a million tutorials for getting started with a Raspberry Pi online. If you've used Linux, everything should be very familiar. For streaming the camera, the RPi Cam Web interface works great. It's super configurable and by default puts the latest image from the camera in a RAM disk at /dev/shm/mjpeg/cam.jpg . If you want to stream the camera data to a webpage (very useful for debugging), you can install Nginx , an extremely fast open source webserver/proxy. I configured Nginx to pass requests for the camera image directly to the file location and everything else to my webserver. I then built a simple Python webserver to spin the wheels of the robot based on keyboard commands that made for a nifty remote control car. As a side note, it's fun to play with the sonar and the driving system to build a car that can maneuver around obstacles. Programming my robot Finally, it's time to install TensorFlow. There are a couple of ways to do the installation, but TensorFlow actually comes with a makefile that lets you build it right on the system. The steps take a few hours and have quite a few dependencies, but they worked great for me. TensorFlow comes with a prebuilt model called ""inception"" that performs object recognition. You can follow the tutorial to get it running. Running tensorflow/contrib/pi_examples/label_image/gen/bin/label_image on an image from the camera will output the top five guesses. The model works surprisingly well on a wide range of inputs, but it's clearly missing an accurate ""prior,"" or a sense of what things it's likely to see, and there are quite a lot of objects missing from the training data. For example, it consistently recognizes my laptop, even at funny angles, but if I point it at my basket of loose wires it consistently decides that it's looking at a toaster. If the camera is blocked and it gets a dark or blurry image it usually decides that it's looking at nematodes-clearly an artifact of the data it was trained on. Finally, I connected the output to the Flite open source software package that does text to speech, so the robot can tell everyone what it's seeing (see Figure 6). Testing my robot Here are my two homemade robots running deep learning to do object recognition. Final thoughts From 2003 to 2005, I worked in the Stanford Robotics lab, where the robots cost hundreds of thousands of dollars and couldn't perform object recognition nearly as well as my robots. I'm excited to put this software on my drone and never have to look for my keys again. I'd also like to acknowledge all the people that helped with this fun project. My neighbors, Chris Van Dyke and Shruti Gandhi, helped give the robot a friendly personality. My friend, Ed McCullough, dramatically improved the hardware design and taught me the value of hot glue and foam board. Pete Warden, who works at Google, helped get TensorFlow compiling properly on the Raspberry Pi and provided amazing customer support.",en,49
359,1662,1467748084,CONTENT SHARED,-5912792039759735631,1895326251577378793,-5147778548049361735,,,,HTML,http://www.mckinsey.com/business-functions/business-technology/our-insights/five-questions-boards-should-ask-about-it-in-a-digital-world,five questions boards should ask about it in a digital world,"CIOs, business executives, and board directors need a shared language to discuss IT performance in a fast-changing environment. Here's a framework for those conversations. Historically, boards have had a hard time assessing and discussing information-technology spending and capabilities. They associate IT with endless reviews of mammoth, years-long transformation projects or complex explanations by the CIO of ever-changing technologies and system requirements. And they try to decode the CIO's reports using their first language, which is centered on traditional cost-related metrics, such as head counts and bottom lines. In this era, companies are exploring digital business models, processes, and automation technologies, as well as seeking to hire and retain people with different skill sets-data analysts instead of data programmers, for instance. The IT organization can no longer be considered just a service provider; how it manages the integration of emerging technologies can help determine the success of a company's digital strategy. Therefore, simply relying on cost-related measures will not provide a full picture of IT performance. And the CIO's boardroom presentations will continue to get lost in translation. Boards need to master a second language-one focused on digital themes, such as speed to market, agile product development, platform-based delivery models, and the benefits and challenges of analyzing various forms of corporate data. With a higher degree of digital fluency, boards can help C-suite leaders make better decisions about how to expand a company's most successful technology initiatives and when to pull the plug on lagging ones. In our experience, board directors are more likely to gain such fluency if they routinely ask these five critical questions relating to the IT organization's performance: To what degree does technology permit core business activities to happen? What value is the business getting from its most important IT projects? How long does it take the IT organization to develop and deploy new features and functionality? How efficient is IT at rolling out technologies and achieving desired outcomes? What skills and talent does IT need to achieve desired outcomes? By systematically considering each question, boards can generate practical, detailed conversations about both IT projects and processes. This approach can help directors understand exactly how much value IT is creating for a company and how its IT capabilities stack up against those of competitors. Costs associated with IT performance will remain an important topic in the boardroom-hence the need to monitor returns on critical projects-but should not dominate the conversation, as they do currently. Making these questions a formal part of IT discussions can also help CIOs determine exactly which data relating to IT projects and processes will be most useful to the board (see sidebar, ""Reporting metrics to the board""). Reporting metrics to the board In this article, we explore the five questions and illustrate the benefits boards may gain by asking them. But first, let's consider some of the challenges board directors, CIOs, and other executives face in today's digital environment. Making decisions about digital According to a McKinsey survey, senior business and IT executives plan to increase their investments in new technologies from about 32 percent of overall IT spending today to about 40 percent in 2019. In many instances, spending has been earmarked for critical digital initiatives-for example, building online channels or mobile applications and services. At board meetings, CIOs and IT organizations share detailed data about technology costs, operations, requirements, and outcomes. But board directors and business executives seem less sure than ever about how to identify the right areas for investment. They face new realities about corporate technology usage and IT performance, among them: Higher stakes. Technology is no longer just another business utility, one of many common inputs into operations. It is shaping strategies and business models as companies seek to meet their customers' demand for tech-enabled products and services. Greater complexity. The typical corporate IT landscape no longer comprises collections of ""island"" systems and applications. It is a complicated network of interlinked applications, interfaces, and databases-and many of them must be able to ""speak"" with external systems. More risk. As companies begin to digitize more products and processes, breaches of security are becoming more common-think of the data losses that have occurred, in just the past few years, at large retailers, financial institutions, utilities, and healthcare companies. Cybersecurity has thus become a frequent point of discussion in the board-room. But in a 2015 survey of more than 1,000 board members, only 11 percent of the respondents said they had a high level of knowledge about the topic. Addressing the five IT questions In the wake of these changes, board directors need to gain a broader, more comprehensive understanding of IT strategy and performance (exhibit). The five questions we've identified can provide some clarity by helping to steer boardroom conversations toward not just the costs but also the capabilities and value that IT engenders. 1. To what degree does technology permit core business activities to happen? Compared with even five years ago, companies are investing more in digital initiatives to gain critical process and production efficiencies, to launch new products, and to enter different markets. These initiatives span multiple business units and functional areas. One of corporate IT's most important objectives, then, is to facilitate end-to-end business processes. These might be activities undertaken by administrative support functions, such as finance or human resources, or they might be core business processes, such as facilitating the steps in a bank's mortgage-application process. In this environment, boards must accurately gauge the IT organization's ability to implement technologies that allow core business activities to happen, as well as its ability to act as a true partner with the business rather than just a service provider. Boards can use a number of metrics to evaluate IT performance in this area-for instance, the percentage of business processes that have been automated, the cycle times for critical end-to-end business processes, and customer- and business-satisfaction scores for various production groups. Board directors and executives at a large US bank, for example, reviewed the progress of a project to digitize the customer-registration process. After some discussion, they homed in on three performance metrics: the time needed to register new customers (in minutes) and to activate new customer accounts (in days), and the percentage of automation within the process (compared with the old one). With these key performance indicators (KPIs) in hand, senior leaders approved the launch of digital initiatives that would decrease the time required to register customers and to activate their accounts, while increasing end-user satisfaction. The board and the technology teams recognized that IT's priorities and resource needs would shift as more customers registered and required support for advanced transactions. They decided to revisit the metrics and priorities associated with the registration process after the project had reached certain milestones. 2. What value is the business getting from its most important IT projects? As we mentioned earlier, cost is already a major part of the dialogue in all boardrooms, but assurances of value should be part of the conversation as well. The leadership mostly understands that to go digital or otherwise modernize IT systems, it will need to invest millions, sometimes billions, of dollars in IT over the next five to ten years. It also knows that the capacity and funding required to get IT projects off the ground are typically in short supply. Thus, it is crucial for boards to track the most important projects actively and learn whether they are delivering the outcomes promised-according to our analysis, the act of measuring a project's benefits can reduce the risk of failure by more than ten percentage points. Board directors must have regular access to information such as the percentage of projects that are completed on time and on budget and that provide functionality within designated time frames, as well as the tangible business and IT benefits that an IT solution generated. The board and other stakeholders have a number of ways to gain access to this information. The leadership team at one global travel company made such outcomes more transparent by implementing a portfolio-management tool that compels all project managers to report project status (cost, schedule, and scope) in a standard way. With this information, IT leaders, executives, and the board could more easily track costs and returns for the company's portfolio of important projects. They were able to allocate and reallocate resources as needed, increasing the organization's operational agility. 3. How long does it take the IT organization to develop and deploy new features and functionality? Companies operating in a digital world can no longer afford to delay product launches or upgrades-not when online companies can deploy new features and functionality on their websites several times a day. Amazon, for instance, can release code every ten seconds or so, update 10,000 servers at a time, and roll back website changes with a single system command. Many IT organizations are now shifting toward two speeds of operation. Product-development teams and IT operations staff are charged with rapidly launching innovative customer-facing applications or upgrades while also maintaining slower (but still reliable) back-end transactions-oriented systems. Boards, CIOs, and other IT leaders must come to a common understanding of the infrastructure, resources, and capabilities required to operate at two speeds. One metric they might use is the percentage of groups, within the IT organization, that can develop and deploy ""business consumable"" functionality within four to eight weeks in a secure, repeatable way. The senior leadership at a large European bank, for example, was considering how to release new product features and functionality into the market more quickly. As part of the boardroom discussions, the directors, the CIO, and the CEO sought to understand the company's technology capabilities by assessing the average time a project took to go from initial funding to first production release. With this and other time-based information in hand, senior leaders recognized gaps in the processes for project approval and software development and saw significant lag time in issuing new releases. They determined that they needed to reallocate IT resources in a way that would ensure the IT organization's agility by funding the appropriate technologies, processes, and people to establish a two-speed operating model. How social tools can reshape the organization Read the article 4. How efficient is IT at rolling out technologies and achieving desired outcomes? Infrastructure operations, application development and maintenance, and security-these three areas account for about 90 percent of staffing and spending in typical IT organizations. So it is critical for boards, executives, and IT leaders to agree on not just what IT is achieving (the focus of the first three questions) but also how efficiently it achieves these outcomes. By regularly monitoring metrics relating to execution, boards can ensure that their companies get optimal returns on IT projects, that teams go to market quickly with new products or upgrades, and that overall costs remain low while the technologies used promote long-term reliability. Stakeholders can learn a lot by reviewing metrics on productivity, product quality, and average costs. Already, IT organizations are collecting many of these data-for example, incident data, average defects in code, or the cost of code per function point. CIOs and other technology professionals can use existing tools to collect and present these metrics in the boardroom. At one travel company, for instance, the board got a clear reading on productivity in the IT organization and product quality by tracking maintenance metrics-for instance, incidents and enhancements reported per software application, help-desk staffer, or call-center employee. Managers on the CIO's team, using information already available, compiled targeted reports for directors to review. What next-generation skills and talent are already embedded in IT? A recent McKinsey survey of more than 700 CIOs and other C-suite executives revealed that talent management in IT is among their top three issues. The IT organization, increasingly, is being asked to participate in projects and initiatives that require technology workers to extend themselves beyond their traditional roles-playing a central role in data analytics or mobile-app development, for instance-while simultaneously maintaining legacy systems. The change in expectations is creating more opportunities but also more risks and stresses among IT teams. It is therefore critical for boards and CIOs to have a common understanding of what skills and capabilities the IT organization might need, now and in the future. They can measure their talent-development efforts along three dimensions: how often a company is rotating professionals in and out of different roles within the IT group and the business units, the degree to which the IT organization is hiring outside people, and how effective it has been in developing people in-house to fill pivotal roles. Getting agreement on these topics may require directors, CIOs, and other stakeholders to consider metrics such as employee-satisfaction scores or the percentage of project roles that could not be adequately staffed over a 12-month period because of a lack of internal skills. The board at one large international retail bank undertook such a review and found that less than 5 percent of its IT leadership team had any business-unit experience. It promptly sought to increase that number to 20 percent. The CIO and the rest of the senior leadership agreed to establish cross-unit training and development teams, redefined certain roles and responsibilities to increase business-IT collaboration, and instituted regular checks with employees to ensure that the IT organization was making progress against its goals for improvement. Because of the speed at which IT innovations occur, boards and CIOs may need to introduce new metrics and drop less relevant ones within each of the five areas of inquiry. Regardless, the questions impose discipline on the relationship between the board and the CIO. They prompt directors and technology executives to have productive discussions about the function's strategic direction and the technology initiatives that matter most, to ensure that the overall management structure required to realize those projects is in place, and to monitor and discuss the results in a way that everyone understands. About the author(s) Aditya Pande is a partner in McKinsey's Chicago office, where Christoph Schrey is an associate principal.",en,49
360,1582,1467226275,CONTENT SHARED,2645380860642118790,2416280733544962613,414825035855844142,,,,HTML,http://www.mckinsey.com/business-functions/strategy-and-corporate-finance/our-insights/weve-realized-a-ten-year-strategy-goal-in-one-year,'we've realized a ten-year strategy goal in one year',"Gerard Paulides, who led Shell's $66 billion acquisition of BG, describes the thinking, the process, and the intensity behind the deal. When Royal Dutch Shell announced plans last year to acquire BG Group, the Britain-based oil and gas producer, the deal represented both Shell's largest M&A deal ever and one of the first energy mergers in an era of low oil prices. Although the acquisition came as oil prices continued to fall, investors roundly approved of it. Gerard Paulides, who led the team that planned the acquisition and worked on its completion, says the strategic discontinuity in the energy sector is more fundamental than finding new resources or taking out costs as oil and gas remain volatile and the mix of energy sources changes. He recently sat down with Ivo Bozon and Dumitru Dediu to discuss deal making in the oil and gas sector, the BG transaction, and the challenges of implementing large mergers and acquisitions. What follows is a transcript of that conversation, edited for publication. Gerard Paulides biography McKinsey: The oil and gas sector would seem to be ripe for deal making. What's the historical view of the role of M&A in oil and gas? Gerard Paulides: Historically, the sector has done big M&A deals (rather than just regular asset transactions) when there have been big discontinuities. In the late 1990s, the discontinuity was oil at $10 a barrel, and the focus was on managing costs. In the early 2000s, the discontinuity was the perception that the world was going to run out of oil and gas at some stage. The focus at that point was on finding more oil and gas reserves-both through M&A and organically finding and developing resources to produce. The discontinuity in the current environment is more fundamental than finding new resources or taking out costs. It's about the ability to move in a changing world with highly volatile oil and gas prices-and, possibly, a different mix of future energy sources. Companies in the oil and gas sector typically develop assets, resources, and relationships with governments organically and over the long term. We like to hold onto assets, developing and producing them over three or four decades. Arguably, the industry's integrated model between production upstream, trading, downstream, gas, and chemicals makes it a bit more dynamic than, say, a pure upstream model would. But at the same time, being integrated also makes the industry even more fixed. McKinsey: How does Shell's recent acquisition of BG fit into that? Gerard Paulides: The purpose of acquiring at this moment in time on such a fundamental scale is that it allows us to recycle a meaningful part of our company. It's a purposeful, deliberate move to emphasize the company's strategic goals in certain segments, such as integrated gas and deepwater. We always have a coveted, or target, portfolio, but it's always something of a ten-year outlook. With the BG acquisition, we've realized a ten-year strategy goal in one year. Having done that, the implications of the move for our portfolio are here and now, and not in ten years' time. And we also have to take out the bits that no longer fit, which are a magnitude bigger than normal. We regularly divest assets from our capital employed. If you make a big move like this one, you have to measure that proportionately-so we now need to divest significantly more, probably double the normal level and maintained over a number of years. We'll take our time, but we do need to do it to rebalance the company. McKinsey: You mentioned the volatility in oil prices, but you also talk about an industry that operates over three to four decades. How closely do you watch volatility given that long-term focus? Gerard Paulides: As a deal maker, I watch volatility closely, in specific segments, over the shorter term, and also in the financial markets in general. Because if volatility is high-over a month, three months, six months-risk capital becomes more scarce and your ability to move is affected. If you're committed and you can fill that vacuum, then you can realize a first-mover advantage relative to your competition. And once you complete a deal, you can focus on running your business while your competition is still trying to deal with that volatility-retrenching in terms of cutting back spending, cutting back capex, laying off people, and making defensive moves. Now, that also means that once you've done a deal, you do need to get on with it. You can't continue to behave as if you hadn't placed your money yet. You've been given a license to spend so many billions of dollars, but people are watching you, and they have high expectations. And the bigger the deal, the more fundamentally it will impact the company. McKinsey: How do you put together the best core M&A team? Gerard Paulides: A company doing sizable, world-scale M&A should have a core deal team of about ten people-and you need to be deliberate about who you include. It's not a seniority game; it's a game about having the best people available for an intense activity over a prolonged period. If you have five external team members available, principals from the bankers, the lawyers, the strategic advisers, then you have a good team-but you need to handpick them. The more you can allow them to do their job and mobilize as their point of view drives them, the better off you are. Reporting lines are also important. An M&A team leader should have a direct reporting line to the CEO and CFO and also establish a relationship with the board. The head of strategy, if there is one, should be a part of any dialogue around deals but shouldn't be a conduit for that M&A dialogue between the team and the CEO. If you're talking about big deals on a global scale, you can only work with one decision maker. Black swans and barrels: How to think about the future of oil prices Read the article McKinsey: How does the long-term nature of the oil and gas industry correlate with how you think about short-term market reactions? Does the market often get it right at the start, or does it need to see a deal play out over time? Gerard Paulides: Obviously, financial-market requirements need to be followed during the entire process, ensuring timely and complete disclosure of information. If the market reacts differently than you expect, then either you didn't explain the deal very well or you didn't see an issue that the market does. You need to respond to that. I also think that in oil and gas it's much too easy to say, ""We're a long-term industry, it's a short-term blip. Let's ignore it."" The financial markets are based on ultimate transparency of information and immediate pricing, and its feedback is immediate, brutal, transparent-and free. So you need to know why the markets react the way they do. The financial markets have the luxury of not having all the detail, so they don't come up with all sorts of rationales to explain why a result is not what you think it should be. They step back and look at the big trends, and compare and contrast, and say, based on all this information, ""I get it,"" or ""I don't get it."" On the other hand, the company has the luxury of having all the detail, so it knows how to explain the market's reaction. That can be a good thing or a bad thing, and you have to be honest enough with yourself to tell the difference. McKinsey: How would you compare the level of effort before announcing a deal of this size with the level of effort after? Gerard Paulides: If you manage a company like Shell, 99 percent of the company doesn't know what's happening prior to the announcement, or why-even though you're using your entire day and your entire week to deal with the intensity of the planning. After the deal is announced, the intensity changes, because then 99 percent of the company and the market know what you're doing. They expect you to allocate time to it. In the beginning, that's relatively predictable, because you've programmed it in, you've prepared yourself, and you've allocated half your calendar and agenda to manage the deal and half to running the company. And that's OK. But then you get to the end of the process-in our case, the last 3 months of a 12-month period-and the heat goes up. The scrutiny gets even more intense, as people have to place their bets, the shareholders have to vote, the debt providers have to calibrate their positions, and the other company has to make up its mind considering its own best interests and the latest developments in the market. For a world-class transaction at the scale of our acquisition of BG-if you think you're going to be busy in those last three months, double what you expect, and you'll probably get close to where it will turn out. That's why it's important not to underestimate how grueling these things can be. It's well worth paying extra attention to your own mental and physical fitness-as well as that of your team. McKinsey: Is there a difference between the intensity of a deal and just the amount of time going into it? Gerard Paulides: You can spend a lot of time without being intense. We had about 20 subject-matter-expert work streams in the BG deal. At any moment, any of those work streams might be the most important, whether it's a treasury topic that requires immediate attention, some regulatory discussion for antitrust purposes, or a valuation of an asset in Brazil. So by intensity, I mean the demands of dealing with all those matters at once-when your judgment is consequential at a level you normally don't have. There were certain points in 2015 and 2016 when I couldn't open a newspaper daily without reading some write-up or some subject-matter-expert review-and everyone knows what you're doing, including your entire family and all your friends. You probably cram three years into one. And you almost think, ""What happened in the last 18 months? We were at Easter, and then it was Christmas, and then it was Easter again."" That's intensity. McKinsey: On the BG deal, what was the market's initial reaction? Gerard Paulides: The BG acquisition was a unique fit for Shell, and the timing and opportunity were there. The market's reaction to the deal was complete and wholesome, and investors have embraced it as a good match. The debate was not about strategy or the rationale for the deal or the portfolio opportunity that the deal would create with divestments. All that was quickly understood. That was why we started the whole exercise, because it all makes sense. The debate was ""at what price?"" With oil prices dropping from above $100 a barrel in early 2015 to below $50 a barrel in early 2016, it's difficult to price the opportunity. You need to work your way through that. So you have your base valuation, you have your financial metrics, you have your synergy on top of that, and then you have your reset opportunity for the company. And most of the debate was around the reset opportunity and the pricing. In fact, that's a pretty luxurious position, because it meant we weren't debating strategy. We weren't debating portfolio. Our fundamentals were spot-on. That's where you want to be for any deal. If you don't get over that hurdle, you don't have a hope of discussing financials, and value, and execution, and management quality, and trust, and all of that. McKinsey: What are the biggest risks to the success of a deal like this? Gerard Paulides: Failing to recognize the intensity of the integration needed. Or, if we go back to what used to be business as usual, spending as if we hadn't done this transaction. Market conditions can make it easier or harder. If oil prices go directionally more up than down, life will be easier-but that carries its own risk. An improving market can bail you out too easily, without the intensity of the reset and the portfolio rebalancing. You may forget your original intentions. About the author(s) Ivo Bozon is a senior partner in McKinsey's Amsterdam office, where Dumitru Dediu is an associate principal.",en,49
361,1407,1466011484,CONTENT SHARED,-5363263367015469934,-1443636648652872475,6340331642204025925,,,,HTML,http://techcrunch.com/2016/06/14/scala-is-the-new-golden-child/,scala is the new golden child,"Tooling in the data science community evolves quickly, and picking the right tool for a job - not to mention a career - can often be divisive. Which tools should you try to master? What is the proper balance between difficulty, relevance and potential? If it's not already, Scala deserves a place at or near the top of your to-learn list - something I saw definitive evidence of last fall when I was coming off a post-doc position and considering leaving academia for the tech industry. At the beginning of my job search, I decided to cast my net wide and speak with a large number of companies before making any kind of career decision. My original idea was to work my way through the MIT Technology Review's list of the 50 most innovative companies of 2015 and apply for data scientist positions at each one. I ended up speaking with 54 companies and doing technical interviews with 24 of them. The tech screenings I did generally lasted an hour and involved two to four coding problems, which I implemented in a REPL while the interviewer watched. Probability and statistics problems were either reserved for the on-site interview or dispensed with entirely (though I may have gotten a pass on this because I have a PhD). With a few exceptions, Python was clearly frowned upon. Of course, no one ever explicitly told me I couldn't use Python to solve a problem. The coding environments always had a Python interpreter, but the interviewers would usually suggest that I use ""a compiled language"" (read: Java) and they would pose data structure problems that are hard to solve in Python. For example, one problem I saw a lot was: Keep track of the k-th largest element in a stream of integers. This is a classic heap problem. However, Python doesn't have a proper collections library, so to solve it I would try to import a specialized module like heapq, which usually wasn't included in the interview environments. This made for an awkward situation. Even when I definitively solved a problem, the feedback was lukewarm. You can use a wrench to hammer nails if that's all you've got. But you'd be better off spending some time learning how to use a hammer. The effect was notable enough that after my first week, I decided to try doing my tech screens in Scala. I did this contrary to the advice of several friends, who suggested that I suck it up and use Java. I'd been playing around with a Scala side project ( PageRanking Soundcloud ) and I strongly prefer Scala to Java, especially in a live coding context. I figured that since I wasn't trying to get a job immediately, I could afford to disqualify myself a few times. What I found surprised me. First, the interviewers actually encouraged me to use Scala even when they weren't familiar with the language. They would trust the REPL to audit my code and use the time to address more general points. Second, the tenor of the screenings changed entirely - they became distinctly more upbeat and discursive. The screeners would get chatty about programming languages and later follow me on Twitter. I began to enjoy the process. I added Scala and Spark to my LinkedIn and AngelList profiles. This resulted in a distinct uptick in the number of inbound interest I was receiving, especially from recruiters. The technical screens became more Scala-centric. At first, I thought the effect was an example of Paul Graham's famous point about programming languages (which, interestingly, was referring to Python back in 2004 when he wrote this): The language to learn, if you want to get a good job, is a language that people don't learn merely to get a job. There is plenty of evidence that developer interest in Scala is on the rise. Stack Overflow's recent developer survey listed Scala as one of the most loved languages: However, in many cases, the more proximal reason for their interest turned out to be Spark, which has recently surpassed Hadoop to claim the title of most active open-source data processing project. Spark is essentially distributed Scala; it uses Scala-esque ideas (closures, immutability, lazy evaluation, etc.) throughout. The Java and Python APIs are semantically quite far removed from the core design innovations of the system. Moreover, unlike Java, Scala makes it painless to experiment with Spark. Vladimir Rokhlin once said that your comfort level with your toolchain is the major determining factor in your ability to debug code. The harder it is to check something, the less likely you are to check it. The same goes for exploratory data analysis and modeling: It is very difficult to take a rapid, iterative approach to analyzing and modeling terabyte- and petabyte-scale datasets using Hadoop. On the other hand, the productivity boost you get from working in Spark using the native implementation language is difficult to overstate. You can use the same API - and often the same code - for small tests and large jobs, for batched data or streaming data, for ad hoc analyses or production machine learning models. Tools [for data scientists] come and go relatively quickly in our field; ideas less so. Scala and Spark will also teach you useful abstractions - particularly in regards to modern functional programming, the most powerful programming paradigm in general-purpose distributed data analysis. And in an era of increasing parallelism and abstraction, failing to understand these will put you at a competitive disadvantage. Conversely, if you can wrap your head around relatively deep ideas like monads, you will be well-situated to be solving hard problems in distributed data science for many years to come. You could, in theory, learn these abstractions in Python or R - they are by no means Scala-specific. For example, Hadley Wickham (chief scientist at RStudio and nerve center of the R data science community) has lately taken to experimenting with monads in R . But in practice you probably wouldn't, because monads and other modern functional abstractions are practically non-existent in Python or R - two languages that were not designed for scalability either in regards to SLOC or bytes processed. By contrast, in order to get anything significant done in Scala, you will be forced to absorb a whole host of ideas that are going to help you professionally for years to come, even if you move on to other languages. This helps you avoid getting stuck in local optima - you can use a wrench to hammer nails if that's all you've got. But you'd be better off spending some time learning how to use a hammer. This is a good educational strategy - it's essentially the same one that MIT used when teaching their introductory computer science class in Scheme. Educational strategies like this are important for data scientists because tools come and go relatively quickly in our field; ideas less so. Therefore, the tools you want to learn are the ones that are rich vessels for forward-thinking ideas. Scala and Spark are precisely that; furthermore, they are in a sweet spot now where they have been thoroughly de-risked as best-in-class industrial tools, but are not yet dominant. This actually does create an applicant's market - Scala and Spark are in high demand in the U.S. , both generally and in data science roles specifically. In the end, my Scala knowledge landed me at a startup in Los Angeles where I use it, in addition to Spark, on a daily basis. And I'm not alone: Scala is already in heavy industrial use by the likes of Netflix, LinkedIn and Twitter, and it's being embraced by more businesses every day . So if you're looking to grow your knowledge and job prospects, now is a good time to pick up some new tools - ones designed for scalability. Featured Image: ShutterWorx/Getty Images (IMAGE HAS BEEN MODIFIED)",en,49
362,1758,1468433976,CONTENT SHARED,2633209697262372913,-4125205337625989832,3171707722573027152,,,,HTML,https://www.smashingmagazine.com/2015/09/why-performance-matters-the-perception-of-time/,"why performance matters, part 1: the perception of time - smashing magazine","Those of us who consider ourselves developers, including me, are very task-oriented. We like to be guided towards optimal results , and we find ourselves uncomfortable when there is no clear path to follow. That is why we all want to know how to do things; we like step-by-step tutorials and how-tos. However, such guidelines are based on certain theories, deep knowledge and experience. Further reading on Smashing: Link For this reason, I will not provide you, the reader, with a structured answer to the question of how to make a website faster. Instead, I aim to provide you with the reasons and theories for why things function in certain way . I will use examples that are observable in the offline world and, using principles of psychology, research and analysis in psychophysics and neuroscience, I will try to answer some ""Why?"" questions, like: Why do time and performance matter? Why don't we like to wait? Why does faster not always mean better in the online world? 8 ""Alice: How long is forever? White Rabbit: Sometimes, just one second."" - Lewis Carroll. ""Alice's Adventures in Wonderland."" In addition to these, we will cover psychological aspects of some practical cases, like performance optimization of an existing project, how to deal with the better performance of a competitor's website and how to make users barely notice any waiting for your services. Hopefully, after understanding these ""Why?"" basics, you will be ready to look outside of some structured ""box"" the next time you are involved in an optimization process and will make your very own conscious path towards an optimal result. Basic Concepts Link ""Well, I never heard it before, but it sounds uncommon nonsense,"" says the Mock Turtle in Alice's Adventures in Wonderland . So, first things first. Let us settle on basic terminology and principles. In the following paragraphs, I will define the main concepts that you will find throughout the article. Time can be analyzed from two different points: objective and psychological. When we talk about time that can be measured with a stopwatch, we're talking about objective time or clock time . Objective time, though, is usually different from how users perceive time while waiting for or interacting with a website, app, etc. When we talk about the user's perception of time, we mean psychological time or brain time . This time is of an interest to psychologists, neuroscientists and odd individuals like me. Performance optimization is a process of improving the delivery speed of services, feedback or any other type of response action in order to meet a user's expectation. We will call the form of communication that occurs when users have to wait for a response from a system a human-to-computer style. On the other hand, when a system barely has any delay in response, we will call that a human-to-human style, similar to the regular conversations we have in real life. A performance budget , as Tim Kadlec defines it , ""is just what it sounds like: you set a budget on your page and do not allow the page to exceed that."" For the purpose of this article, we will consider the performance budget to be directly connected to time - for example, loading a page, responding to a user's input, etc. Armed with these definitions, let's go down the rabbit hole. Part 1: Objective Time Link Now, here, you see, it takes all the running you can do, to keep in the same place. If you want to get somewhere else, you must run at least twice as fast as that! The Red Queen in Lewis Carroll's Through the Looking-Glass As we defined it earlier, objective time is time that can be measured with a clock. In this first article in our series, we will discuss how to use this time more efficiently for our projects. ""Remember That Time Is Money"" Link This phrase, credited to Benjamin Franklin in his Advice to a Young Tradesman , still proves true in our contemporary world. According to a recent study , it takes only 3 seconds for a visitor to abandon a website. For every 1 second of improvement, Walmart experienced an increase in conversions of up to a 2% on its website. When auto-parts retailer AutoAnything cut loading times in half, it experienced a 13% increase in sales. Users value their time, and a human-to-human style of communication is increasingly expected. New ways of communication require a new way of thinking in the design and development of online systems. In the web industry, we have been talking about speed and performance for quite a while. We've been emphasizing the importance of fast communication, and we're always trying to reach certain goals in seconds, kilobytes, frames per second and so on. As a result, we know how to make our websites fast using technology. Furthermore, we keep getting advice from leading developers and companies about preferred times in user-to-website interactions. In November 2014, Dimitri Glazkov, Jochen Eisinger and Chris Harrelson, in their "" State of Blink "" keynote about the Blink rendering engine, suggested a platform success model based on interaction times. 20 Platform success model from Google 21 ( View large version 22 ) After quite a few years of the ""2-second loading time"" recommendation, the platform success model technically sets the limit on page-loading time to 1 second. Usually, the average developer or project manager is satisfied with an explanation like, ""Longer times affect rankings in search results."" But what recommendations are these explanations based on? How much freedom do we have in setting certain time-based goals or a performance budget? And, what is even more interesting than simple numbers, can we be creative with these durations, playing with them to deliver better experiences to our users? These are the main questions we will cover in this article. Choosing A Performance Budget Link As defined earlier, a performance budget is a limit that we set on certain time-related operations of an online system and that we do not allow the system to exceed. Usually, human-to-human conversations involve quite short time intervals between a question being asked and a response being given by the interlocutor. In psychology, we should keep four important spans of time in mind when dealing with short time durations: 0.1 to 0.2 seconds Research points to this interval as the range of the maximum acceptable response time to simulate instantaneous behavior - an interval within which users barely, if at all, notice a delay. 0.5 to 1 seconds This is the maximum response time for immediate behavior. This interval is usually the response time from an interlocutor in a human-to-human conversation. Delays within this interval are noticeable but easily tolerated by most users. During this time, the user must receive an indication that their command (such as their click of a button or link) has been accepted, assuming that the signal was not already sent during the previous interval. 2 to 5 seconds Psychologist Mihaly Csikszentmihalyi defines a flow or optimal experience as a state when people experience concentration, absolute absorption in an activity and deep enjoyment. 5 to 10 seconds According to the National Center for Biotechnology Information at the US National Library of Medicine , the average attention span of a human being dropped from 12 seconds in 2000 to 8.25 seconds in 2015. Guess what? That is 1 second less than the attention span of a goldfish! As a simplification - and to emphasize our superiority to the goldfish - we consider 10 seconds as the absolute maximum time of a user's attention span . The user would still be focused on their task but would become easily distracted. This is the time for the system to engage the user in the process; if this is not done, then the user will most probably be lost forever. Now, if we apply these time definitions to the aforementioned platform success model suggested by the Blink team, we can translate its emphasis on a 1-second loading time and a 0.1-second response time into more human-to-human language: The loading page should happen immediately . Users should get feedback from a given action instantly . 34 Load pages immediately , and give feedback on a given action instantly . ( View large version 35 ) Humans have inner stopwatches that have been tuned well since our cave-dwelling ancestors. Hence, you can safely rely on the time spans above when choosing a performance budget for your next web project. But time on its own is useless unless put into some context. Consider an example. What do you think about in response to the statement, ""A process will take 10 seconds to complete."" Take your time. Done? I'll bet the first thing you thought was, ""What process?"" Your brain iterated over possible options based on your life experience, current lines of thinking, mood and so on. Then, once your brain settled on a dominant guess, it evaluated your expectations about that process' duration against the declared 10 seconds. That being said, most often we don't pick a random time duration on its own, but rather choose time in a certain context. First, the context could be set by our own constraints, such as a need for performance optimization in order to meet users' expectation of a human-to-human style of communication. Secondly, the context might also be set by competitors; in this case, we're dealing with a chase-the-leader scenario to reach the performance level set by others. We will look at both scenarios below. The Need for Performance Optimization: The 20% Rule Link Here is a situation that most of us can relate to to some extent. Your customer says that the search function of your website is slow. Your measurements show an average of 5 seconds to render search results. After employing some optimization techniques, you bring it down to 4.5 seconds. Happily (well, as happily as one could possibly be with a 4.5-second rendering time), you send an email to the customer with this information. All you get in return is, ""Doesn't look any faster to me."" That usually hurts. Let's try to understand why the customer did not notice any difference. In 1834, one of the founders of experimental psychology, Ernst Heinrich Weber , postulated a law that defines difference threshold or a just noticeable difference (JND) as the minimum difference in stimulation (weight, light, etc.) that a person can detect most of the time. Later, Weber's student Gustav Theodor Fechner applied the law to the measurement of sensations, setting the basis for the science of psychophysics . This work by Weber and Fechner is known to us as the Weber-Fechner Law . The law is still regarded by many scientists as arguably the most important tool in understanding perception. Time is not an exception to the Weber-Fechner Law. Practical experiments in psychophysics show that time intervals are prone to a JND of between 7% to 18% on average for shorter periods (with a duration of less than 30 seconds). Based on this, a good rule of thumb is to simplify the Weber-Fechner Law into a 20% rule . That is, in order for users to barely see a difference in time duration, it has to be changed by a minimum of 20%. The Weber-Fechner Law can be simplified into a 20% rule for short time durations. Let's go back to our example of a client noticing no difference in the rendering of search results after our optimizations. Originally, the search results were returned in 5 seconds. Using the 20% rule, we can tell that, for the user to even notice the difference, the new search results should be at least 1 second faster: That is why the client simply did not notice the 0.5-second improvement. We're talking about performance optimization, but this technique works in the opposite direction. If, for example, you are developing a feature that slows down your web page, you could apply the 20% rule to determine whether the performance decrease will be noticed by users at all. Allowing our code to be a bit slower without harming the user experience is called regression allowance . Note: When we talk about 20%, we are talking about a ""just noticeable"" difference. "" Noticeable "" does not mean "" meaningful ."" In order for users to appreciate your performance optimization, you should go well beyond this threshold. But what do you do when a competitor comes to market with significantly better performance for a comparable feature? What if a 20% regression allowance relative to the competitor's time is not even technically approachable for us? In this case, we should at least aim for what G. Moore, in his book Dealing with Darwin , calls neutralization . Neutralization, Or Chasing The Leader Link Time neutralization occurs when the time difference between two services is noticeable but does not influence the user's preference of one service over another. In this case, reliability, usability and other quality -related factors of the service play a much more significant role. In other words, time neutralization is a good position to be in when you have better services than competitors. Again, getting back to our example of search results taking 5 seconds to appear, assume that a competitor comes to the market with a very similar search service: the same functionality, the same results and the same feel. The problem is that their search results are revealed in 2 seconds, not 5. Applying the 20% rule to our 5 seconds would not make sense in this scenario. We're not competing with our own time; rather, we have to match the competitor's. We can make a couple of logical assumptions here: We cannot decrease the search response time to 2 seconds. (If it were that easy, we probably would have done it already.) If 2 seconds is out of reach, the next best solution would be to use the 20% rule of regression allowance relative to the competitor's time: 2 seconds + 20% = 2.4 seconds. At 2.4 seconds, users will not notice any difference between our search results and the competitor's. But if we cannot achieve 2 seconds, then 2.4 seconds is probably also out of reach. When comparing two durations (2 and 5 seconds, in our case), we'll find a ""magical"" psychological threshold between them. Time durations longer than this threshold will be perceived by the user as being closer to 5 seconds. Time durations shorter than that threshold will be perceived as being closer to 2 seconds. Surprisingly, studies in animal timing , particularly those conducted by R. Church, M. MacInnis, P. Guilhardi and others at Brown University, show that this threshold is not simply the average of the two durations. This threshold proved to be predictable and is found at the geometric mean , instead of an arithmetical one. Research with human subjects confirm the same finding. From mathematics, we might remember that the geometric bisection between two numbers can be found using the following formula: Applying this formula to our numbers, we get: For our 2- and 5-second results, 3.2 seconds is the threshold of neutralization. At this point, users will notice a difference, but the difference will be perceived as being unimportant to their choice of service. Factors such as the quality of the service and reliability will play a much more important role, but managing these is beyond the scope of this article. With this, we finish our basic analysis of objective time. By now, we should have a good understanding of where some widely adopted industry standards , such as page-loading time and the response time for a system, come from. We also have a guideline for setting a performance budget , and we understand how to deal with time when we have to improve the performance of a website or match that of a competitor's . But as we have mentioned earlier, another kind of time usually matters much more to users than the kind measured with a stopwatch. Ladies and gentlemen, let's meet in part 2 and talk about the psychological time. 55 Endnotes Link 1 This is the reaction time of the average person to a simple stimulus, such as catching a falling pen or jerking one's hand away from a hot cup. This is probably the most important time we should keep in mind. 2 According to G.A. Miller, most adults can store five to nine simple items in their short-term memory . Research - confirmed by a number of experiments, in particular the one by M. Greene and A. Oliva of MIT - shows that ""the processing of complex natural images and visual object recognition"" takes no more than a tenth of a second. This means that short-term memory processing of five to nine items such as this should take about 0.5 to 0.9 seconds. Hence, 1 second is considered to be the maximum time for the user's flow of thought to stay uninterrupted. 3 While optimal experience time fluctuates according to subjective parameters, for most tasks that the average user faces on the web, the time of concentration falls between 2 to 5 seconds. This is why, for some years, we operated with 2 seconds as the optimal page-loading time. 4 Of course, the state of focused attention in general spans far beyond 10 seconds and can reach 20 to 30 minutes. (This long-term attention is called ""selective sustained attention."") However, first, this time span is much more exposed to fluctuation due to different uncontrolled circumstances; secondly, not many website owners out there can brag about their website being actively used by the average user for more than a couple of minutes. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #endnote1 28 #endnote2 29 30 #endnote3 31 32 #endnote4 33 #success-model 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 #success-model 52 #performance-budget 53 #20-percent 54 #neutralization 55 56 57 58 59 ↑ Back to top Tweet it Share on Facebook",en,49
363,1944,1470021191,CONTENT SHARED,-7564569805639737618,2653698047369148236,4009496569130359022,,,,HTML,https://www.infoq.com/br/presentations/programacao-reativa-funcional-com-rxjava,programação reativa funcional com rxjava,"Resumo ​Nessa palestra, serão exploradas as principais técnicas para conquistar a API RxJava, desde a criação de Observables e sua manipulação, até o tratamento de erros durante o pipeline de operações. Será visto como lidar com Backpressure de forma efetiva, e como abstrair seu framework assíncrono preferido para se integrar em fluxos reativos, entre outras técnicas. Minibiografia Ubiratan Soares é Engenheiro de Software, formado pela USP. Trabalha com mobilidade e desenvolvimento para Android, participou diretamente em alguns dos maiores aplicativos nacionais. Também é instrutor da Globalcode, co-organizador do GDG São Paulo, palestrante, pai e filósofo wanna-be. Faz parte do programa Google Developer Experts para Android e do Android Software Innovator da Intel. Entre 28 de março e 1° de abril, São Paulo recebeu a nona edição brasileira do QCon. Organizado pelo InfoQ Brasil e com palestras selecionadas por um comitê independente, esta edição contou com 3 keynotes, 100 palestras e 13 workshops, totalizando 130 horas de conteúdo, o que levou o QCon São Paulo ao patamar dos maiores QCons mundiais.",pt,49
364,3033,1485795031,CONTENT SHARED,2321412363020240814,6013226412048763966,8310906696171015235,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36",SP,BR,HTML,https://www.linkedin.com/pulse/lean-all-people-o-na-ti-%C3%A9-focado-em-pessoas-luz-pmp?trk=prof-post,lean it is all about people - o lean na ti é focado em pessoas,"Published on Rodolfo Luz, PMP Follow Following Unfollow Rodolfo Luz, PMP Sign in to follow this author Project Manager at CPqD & Lean Enthusiastic Se você chegou agora e deseja obter uma boa introdução sobre Lean, recomendo a leitura deste artigo Um dos motivadores para eu escrever esta série de artigos, foi o fato deste conhecimento ainda estar pouco difundido no Brasil e ao público da língua portuguesa justamente pelo conteúdo da LITA estar disponível apenas na língua inglesa. Além disso, no Brasil, poucos centros de treinamento estão credenciados para aplicar o treinamento inicial da LITA, o Lean IT Foundation e suas agendas ainda são muito restritas. onde falo um pouco de Kaizen, de Lean, sua origem e como este pode transformar uma organização. Se se interessar mais nas ferramentas Lean e Kaizen, tenho um segundo artigo onde falo da ferramenta Lean A3 para melhoria contínua, resolução de problemas e questões do dia-a-dia organizacional e pessoal. Lean IT is all about people - o Lean na TI é focado em pessoas É com esta frase que que dou início à série de artigos sobre Lean para área de Tecnologia da Informação . O Lean tradicional, da área de manufaturam, lida com pessoas, mas muito com produtos, processos e equipamentos tangíveis. Por outro lado, a área de TI lida com serviços, softwares, consultoria e outros aspectos que são intangíveis, além da infraestrutura de TI, o aspecto mais tangível são as pessoas que rodam, alimentam e produzem na área de TI. Portanto, o Lean IT trata principalmente comportamento dos profissionais da área de TI, seus costumes, seus processos, seus valores, sua cultura e sua colaboração. Então, torna-se uma área ímpar para o Lean atuar, onde o foco é nas pessoas. Apesar de haver uma bibliografia vasta de conhecimento sobre Lean aplicada a área de TI, minha fonte de conhecimento em Lean para área de TI e a base para este e próximos artigos é a Lean IT Association, ou simplesmente LITA . Trata-se de uma associação fundada por três Organizações de Treinamento Credenciadas (ITpreneurs, Pink Elephant, Quint Wellington Redwood) e três Institutos de Exame (APMG, EXIN, PEOPLECERT International Ltd) que reuniu diversos gurus do Lean aplicado à área de TI (diretores e executivos de TI, autores de livros e publicações sobre o assunto) que conseguiu reunir as melhores práticas de diversas destas fontes em um corpo de conhecimento unificado e organizado sobre Lean IT. Foco nas pessoas No guia Lean IT Foundation, é relatada a grande diferença do Lean IT em relação ao Lean Manufactoring, pois enquanto que na manufatura o Lean tenta otimizar o uso dos recursos, principalmente o uso eficiente da matéria prima e gestão de estoque just-in-time , na TI, temos recursos tangíveis na área de infraestrutura mas em todo o resto lidamos com recursos intangíveis e o principal recurso na TI é o Tempo e as Pessoas (justificando o título deste artigo). Dado que os serviços de TI são principalmente produzidos por trabalho intelectual, produtividade e conhecimento são críticos e complexos de se compreender. Na TI, parece que sempre ""estamos muito ocupados resolvendo problemas ao invés de documentarmos as coisas"", ""não temos tempo para documentar projetos e completar o formulário da GMUD"", "" não temos tempo para fazer melhorias"" e ""não temos tempo para testar mais antes da entrega"". Portanto, o Lean atuará bastante na otimização do uso do fator Tempo pelos nossos Recursos Humanos na TI. Assim como comentei em um artigo anterior , o Lean pode ser combinado com diversas outras técnicas e boas práticas, portanto não seria diferente com o Lean IT, que foca no comportamento e atitude dos profissionais de TI, desde requisitos à operações. Atuando desta forma, é possível cobrir a lacuna de comportamento e atitude das boas práticas que enfocam muito em processos muito bem estruturados. O Lean IT enxerga que os processos instituídos pelas boas práticas de mercado (exemplo: Gerenciamento de Serviços de TI) são necessários para um bom funcionamento da TI e estruturam uma primeira linha de base para os Kaizens mas acima de tudo, com a Visão Lean será possível direcionar os esforços e os Kaizens para processos que realmente agreguem valor aos clientes e ao negócio. Os Guias Lean IT A LITA possui 3 guias-chave em língua inglesa que constituem seu corpo conhecimento em Lean IT, cada um focado em determinado estágio de aprendizado e aplicação do conhecimento na organização - da fundação à prática e à profissionalização. Além disso, desenvolveu um esquema de certificação baseado nos 3 guias e está desenvolvendo também um 4º guia e certificação voltado para o Coach em Lean IT: Referências: www.leanitassociation.com Artigos de minha autoria relacionados ao assunto: Lean Turnaround -Transformação Lean e sua relação com Agile e Design Thinking Lean A3 Gerenciando para o Aprendizado em Projetos Lean IT Foundation - A fundação para o Lean IT, contém os fundamentos básicos para você conhecer e compreender o Lean para área de TI. Tem o objetivo de ajudar as organizações de TI a garantir que forneçam aos seus clientes os melhores serviços possíveis. Através da compreensão do valor do cliente, os processos que fornecem este valor, a forma de gerir o desempenho, a forma de organizar e a atitude e comportamento necessários e desta forma, as organizações de TI são ajudadas a desenvolver uma mentalidade de melhoria contínua. Lean IT Kaizen - Kaizen é a palavra japonesa para melhoria contínua usando pequenas mudanças incrementais (Kai - mudança; Zen - para melhor). Kaizen é uma abordagem para a resolução de problemas e questões, constitui a base da melhoria contínua contínua nas organizações. Quando aplicado ao local de trabalho, o Kaizen significa melhoria contínua envolvendo todos, gerentes e trabalhadores, todos os dias e em qualquer lugar, proporcionando estrutura para a melhoria do processo. Este guia detalha, as ferramentas de melhoria contínua e Kaizen na TI . Pertence à categoria Practitioner da LITA, ou seja, Praticante de Lean IT, que conhece e aplica a filosofia. Lean IT Leadership - Baseia-se no conhecimento básico adquirido através da Lean IT Foundation. Lean IT Leadership centra-se em garantir que as pessoas que cumprem um papel de liderança dentro da organização saibam o que precisam fazer para ajudar a desenvolvê-lo para uma organização Lean IT. O papel pode ser desempenhado de forma formal ou informal. As pessoas que seguem o currículo Lean IT Leadership podem se beneficiar substancialmente desde a primeira compreensão do papel do Lean IT Kaizen. Uma das principais tarefas da Lean IT Leadership é conduzir e ensinar melhoria contínua. Este guia pertence à categoria Professional , focada em Implementar, Estrategiar e Mobilizar, ou seja, além de implementar Lean, deve torná-lo parte da estratégia organizacional, assim como preconiza Art Byrne em Lean Turnaround . Lean IT Coach (em desenvolvimento) - Também pertencente à categoria Professional , o conteúdo deste guia e certificação ainda não está disponível, pois encontra-se em desenvolvimento. Contudo, acredito que este deverá formar o Coach em Lean IT , sendo responsável por disseminar, capacitar e orientar as empresas e os colaboradores das organizações a aplicar a cultura e filosofia do Lean IT, podendo este profissional atuar internamente em sua empresa ou externamente como consultor para outras empresas criando um novo nicho de mercado, assim como os Agile Coaches para as Metodologias Ágeis . Cada um dos guias acima, encontra-se disponível gratuitamente no site www.leanitassociation.com . Segundo a LITA, os guias concentram todos o conteúdo necessário para conquistar as suas respectivas certificações, contudo, exigem como pré-requisito para prestar o exame de certificação a realização de treinamento em algum centro de treinamento autorizado. Nos próximos artigos falarei um pouco de cada um dos guias Lean IT disponíveis, a começar pela fundação e alicerce dos outros guias, o Lean IT Foundation . Espero que gostem da série. Vejo vocês em breve no próximo artigo. Lean IT Association . Disponível em: <www.leanitassociation.com>. Acesso em 26 de janeiro de 2017. Byrne, Art. Lean Turnaround: A Grande Virada . Lean Institute Brasil. 2014. Artigo original: www.rodolfoluz.net Project Manager at CPqD & Lean Enthusiastic",pt,49
365,2685,1477920217,CONTENT SHARED,-78667914647336721,301435144665447655,-3334567622700890709,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36",MG,BR,RICH,https://leankit.com/blog/2016/10/build-quality-key-continuous-delivery/,build quality in: the key to continuous delivery in kanban,"The technology world is changing fast, faster than ever. A few years ago, we thought that only the Amazons or Facebooks of this world would be able to do continuous delivery. Now, the phenomenon is getting traction, starting to become mainstream and before we know it, it will be commoditized. Everybody will do it. Why? Because the ability to release often gives organizations a great competitive advantage over their competitors with slower turnaround times. In order to practice continuous delivery, teams have to build quality into everything they do. This means that the actual product not only reaches the customers faster - it's a better product, too. Learning to build quality in with Kanban helped my development team reduce waste, deliver faster, and communicate better with the organization around us. Keep reading to learn how my development team changed our infrastructure, development practices, and culture to enable continuous delivery. How Quality and Continuous Delivery Relate I've had the luck of being part of a team working on improving our organization's ability to deliver fast, so I've experienced the turmoil that comes with going from one release a month to multiple releases per day. Applying traditional testing approaches might make the challenge seem impossible, and in fact, without completely changing the way we build and test software, we can't expect to aim at reaching sustainable continuous delivery. Our adoption of Kanban and the Kanban method was fundamental for the direction we were going. By adopting the method, we were able to identify our bottlenecks, make wait time visible and maximize the benefits of small batches and continuous flow. We soon realized that if we wanted to have any chance of being able to release continuously, we needed to reduce rework. In the old world, testers and business users would take two weeks of the iteration for their quality control activities. A lot of the time was not spent testing, but identifying and logging large numbers of defects and dealing with unstable environments. We attacked this problem on three fronts: Infrastructure, development practices, and culture. Infrastructure: Reduce Human Errors To reduce rework in our infrastructure, we developed a brand new delivery pipeline that carried both code and context. We also created on-demand test environments, to reduce the amount of time wasted setting them up manually and reworking incorrect configurations. This meant a drastic reduction of manual steps in build and deployment. While initially this was implemented only on our test environments, it gave us immediate benefits. The ability to test on reliable environments, created with a repeatable process, reduced our test time significantly. Development Practices: Better Communication, Smaller User Stories Behavior Driven Development (BDD) We adopted Behavior Driven Development (BDD) to reduce the misunderstandings between business stakeholders and the development team and as a consequence, reduce rework caused by miscommunications. BDD gave excellent results very early. We soon realized how much we had left to individual interpretation - and the extent to which this had caused most of the defects we found in quality control activities. Thanks to BDD, defect rates went down dramatically. The more we improved the collaboration within the teams, the more the business knowledge grew within its members. This allowed for easier decision making; decisions that had previously required long, frustrating meetings now became quick conversations. Smaller User Stories Another approach we found extremely beneficial in reducing rework was vertically slicing our user stories, hence reducing their complexity. This often meant that the logic in the new code would include a single code path. Lower complexity meant fewer defects and reduced rework. Balanced Test Automation Strategy At the same time, we defined a strategy that involved high usage of automation tests at the unit level, a consistent layer of API level automated tests, and a very thin layer of UI-driven tests in line with Mike Cohn's automated test pyramid . This allowed our regression suite to stay fast and stable, something that's absolutely fundamental if you want to be able to deliver customer value each time your development team pushes some new code. Using BDD and implementing test automation didn't mean that we stopped executing exploratory testing, but that the activity became less and less time consuming due to the reduction of defects and the smaller size of the stories. Building Quality In I worked, for a couple of months, on a product that involved the transformation of a stream of data into a different data structure to be consumed by a client, I remember that I never exploratory tested anything for more than 10 minutes, and I never felt as if I was leaving something behind. This was liberating for me, to feel as though I could produce quality work at a fast pace without sacrificing anything. Our customers didn't encounter problems either. We were inspecting the application an order of magnitude less than before, but the quality of our output was higher. We were building quality in. Culture: Encouraging Collaboration This type of transformation is not too difficult from the technical point of view. It's far more complicated from a cultural point of view. When I joined the company I am referring to, I found extremely skilled developers that were not used to testing. They had been spoiled by the testers running after their messy code and never grew any interest in the discipline. Culture is hard to change. Our strategy was to encourage healthy behaviors rather than punish bad ones. For example, we encouraged and praised developers that were willing to test, and we also praised people who worked in pairs. We encouraged and recognized every sort of collaboration. We knew the team had to become a unit of people who trusted each other and not only a group of people with all the skills necessary to deliver a complex solution. Sharing Knowledge and the Creation of a Test Tribe We needed to reach a work cohesion that brought the ability to collaborate and removed bottlenecks while keeping work flowing. One key to success was the focus we put on building t-shaped skills by learning from each other and offering help at all times. One successful initiative was the creation of the Test Tribe. The Tribe was a voluntary and fully autonomous group of people that cared about software quality. After a while, this group of people, which met every week to have Lean Coffee-style conversations, was becoming large, with the majority of its members being developers. I don't have enough space to mention a number of experiments and improvements that started at this level. One notable success was the design of a tool that greatly improved testability of our main app, which reduced the time for exploration from hours to minutes. The tribe also contributed to changing our process, removing unnecessary steps, and in one specific instance, identifying a change that reduced lead time by 30% with little effort. The tribe was thriving and its successes quickly trickled down to the different teams where tribe members existed. Teams started experimenting more and more and improving their systems stepping beyond quality only improvements, it was great to observe. Build Quality In: Final Thoughts Any organization can practice continuous delivery - and they should, if they hope to compete in the modern marketplace. Continuous delivery helps organizations not only deliver faster than their competitors, but with higher quality, too. I've seen this work on my own development team, through a series of incremental changes that resulted in a major shift in how we operated - helping us transition from one release a month to multiple releases per day: Infrastructure: We changed our infrastructure to reduce the number of manual steps in build and deployment. This allowed us to have more reliable testing environments with significantly shorter test times, which meant we could deliver faster with fewer bugs. Development Practices: We changed our development practices to improve communication up and across the organization. This helped us reduce rework due to miscommunications. We also made user stories smaller and simpler, which reduced complexity and thereby made it easier to deliver frequently. We implemented a balanced test automation strategy, which allowed our regression suite to stay fast and stable. Culture: Finally, we changed our culture, in the same way we changed everything else - quickly, but incrementally, so that the changes were sustainable. We encouraged healthy behaviors, ones that promoted testing and collaboration between teams. We created a Test Tribe, a group of testing enthusiasts, whose influence spread throughout the organization and helped solidify our new culture: one that was built around collaboration, communication, and quality. All of these efforts resulted in a system that was designed to deliver fast, with quality built into every step of the process.",en,48
366,1866,1469460909,CONTENT SHARED,-559964548932224920,-6895155480127642372,909304340885309843,,,,HTML,http://jeffsegurosonline.blogspot.com.br/2016/07/seguros-e-games.html,jeff seguros: seguros e games,"LANÇADO em apenas três países até agora, já virou fenômeno mundial. Aguardado com enorme expectativa, o Pokémon Go, novíssima versão para celular do videogame criado há vinte anos pela Nintendo, chegou na quarta-feira 6 aos Estados Unidos, à Austrália e à Nova Zelândia. Horas depois, só se falava nele. Ainda não há dados oficiais, mas, segundo sites especializados, somente nos Estados Unidos foram 21 milhões de usuários diários, 1 milhão a mais que os do campeoníssimo Candy Crush. Baixar e jogar é gratuito, porém apetrechos para ganhar pontos na brincadeira custam dinheiro. Daí que já se fala em mais de 14 milhões de dólares gastos nas lojas de aplicativos, o que faz do jogo um dos mais lucrativos de todos os tempos. No dia 13, desembarcou na Alemanha! Um resumo para os que não estão entendendo nada: Pokémon Go é um videogame que usa recursos da chamada realidade aumentada para situar os simpáticos bichinhos, os mesmos de vinte anos atrás, em cenários de verdade. É muito legal, é só apontar a câmera do celular para algum lugar real como a mesa do computador, a sala, calçada, o parque, onde você estiver - e lá se encontra um dos bichinhos. A graça é ""caçá-los"", com a ajuda dos tais apetrechos que custam dinheiro, claro). O sucesso foi tão estrondoso que, ainda mesmo restrito aos países que mencionamos, usuários de outras regiões conseguiram instalá-lo em versões piratas e fizeram apelo para que a versão oficial não demorasse a chegar. Foi até o caso do prefeito do Rio de Janeiro, Eduardo Paes. Isso porque a brincadeira incentiva os fãs a saírem pelas cidades em busca de Pokémons virtuais que, olhando pela tela do celular, aparecem em cenários reais. É provável que boa parte do sucesso do Pokémon Go seja pelo fato de ele transitar entre o mundo da fantasia e o mundo real. Não que a ideia seja nova. Mas pegou como nunca. Aí você me pergunta: e o seguro? Calma, vamos chegar lá! Desde o dia 10, as ruas dos três países onde o aplicativo está disponível se encheram de jogadores - treinadores, no jargão dos entendidos - com olhar fixo no telefone, estimulados a sair de casa na captura dos personagens. O fenômeno atingiu uma tal dimensão que despertou interesse até de larápios que viram nele uma oportunidade de se dar bem. Em uma cidade do estado americano do Missouri, quatro ladrões usaram o jogo para atrair jovens a locais ermos e roubá-los. ""Se você está na rua de madrugada, levante a cabeça e olhe para onde vai"", recomendou a polícia. Em alguns lugares, a caça terminou mal. Uma menina do Wyoming, no Oeste americano, seguindo a pista dos bichinhos, deparou com um cadáver à beira de um rio. Em outros, a brincadeira foi tão longe que chegou a perder a graça - um Pokémon que libera gás venenoso foi localizado no Museu do Holocausto, em Washington. A instituição pediu para ser removida das opções de cenário.Como o jogo é o jogo das cifras milionárias, A seguradora portuguesa Tranquilidade, anunciou o lançamento de um produto, o ""AP GO"", um seguro de acidentes pessoais para cobrir os riscos que os caçadores de Pokémons enfrentam como os ja citados. O jogo Pokémon Go da Nintendo é um sucesso em escala mundial. Só em Portugal mais de um milhão de pessoas baixaram o jogo e por isso, na quinta-feira, 21 de Julho, a tranquilidade anunciou o produto. A seguradora, em comunicado, explica que vai oferecer ""um seguro de acidentes pessoais para caçadores de Pokémons, ou outros jogos de realidade aumentada, a quem contratar um novo seguro. O cliente pode aproveitar a oferta para ele ou ou indicar algum familiar"". A oferta está disponível até 15 de Agosto e o seguro é válido até final do ano. ""O seguro 'AP GO' está desenhado para cobrir os danos sofridos pelos jogadores durante a prática"" do jogo, garante a seguradora. No referido comunicado, a Tranquilidade reclama para si o estutato de ""primeira seguradora na Europa a lançar um produto que garante os riscos associados à prática de jogos com o Pokémon Go, criando uma solução pensada exclusivamente para esta nova realidade"". Acho a ideia ótima, afinal, por todo o mundo têm sido relatados acidentes envolvendo caçadores de Pokémons, parabéns a seguradora que enxergou a oportunidade de ganhar dinheiro e proporcionar segurança aos jogadores. Aqui no Brasil, aguardamos o jogo e iniciativas semelhantes. E você? Ja ouviu falar do jogo? O que acha? Texto escrito com inspiração em textos da revista Veja e Isto É desta semana.. Para saber mais sobre acidentes pessoais, clique",pt,48
367,1617,1467391602,CONTENT SHARED,7434270606888620096,3891637997717104548,-2108423950476348958,,,,HTML,https://cloudplatform.googleblog.com/2016/06/Stackdriver-Debugger-now-works-with-source-code-directly-in-GitHub-and-Bitbucket.html,stackdriver debugger now works with source code directly in github and bitbucket,"Stackdriver Debugger has always worked with source code stored in the Google Cloud Source Repository , or even source in local files, without having to upload it to Google servers. Recently, we've also heard from you that you want to use Debugger with code stored in other source repositories. Today, we're happy to announce that Stackdriver Debugger can use source directly from Github or Bitbucket. No need to copy or replicate the source to Google Cloud Source Repository. Simply authorize access the first time you connect your repositories to display and view source. The debugger will automatically display the correct version of the source code for your application when you follow the debugger deployment steps . Note that your source is not uploaded to Google when you connect your Github or Bitbucket repository. Permissions to access the repository can be revoked at any time from the Github or Bitbucket admin pages. For more information, see Stackdriver Debugger setup documentation for App Engine and Compute Engine . And to learn more about Stackdriver Debugger, please visit the Debugger page . Give it a whirl and let us know if you like it.",en,48
368,1303,1465386620,CONTENT SHARED,6521856301289868251,7645894863578715801,1174411516635136277,,,,HTML,https://spring.io/blog/2016/06/07/notes-on-reactive-programming-part-i-the-reactive-landscape,notes on reactive programming part i: the reactive landscape,"Reactive Programming is interesting (again) and there is a lot of noise about it at the moment, not all of which is very easy to understand for an outsider and simple enterprise Java developer. This article (the first in a series) might help to clarify your understanding of what the fuss is about. The approach is as concrete as possible, and there is no mention of ""denotational semantics"". If you are looking for a more academic approach and loads of code samples in Haskell, the internet is full of them, but you probably don't want to be here. Reactive Programming is often conflated with concurrent programming and high performance to such an extent that it's hard to separate those concepts, when actually they are in principle completely different. This inevitably leads to confusion. Reactive Programming is also often referred to as or conflated with Functional Reactive Programming, or FRP (and we use the two interchangeably here). Some people think Reactive is nothing new, and it's what they do all day anyway (mostly they use JavaScript). Others seem to think that it's a gift from Microsoft (who made a big splash about it when they released some C# extensions a while ago). In the Enterprise Java space there has been something of a buzz recently (e.g. see the Reactive Streams initiative ), and as with anything shiny and new, there are a lot of easy mistakes to make out there, about when and where it can and should be used. What Is It? Reactive Programming is a style of micro-architecture involving intelligent routing and consumption of events, all combining to change behaviour. That's a bit abstract, and so are many of the other definitions you will come across online. We attempt build up some more concrete notions of what it means to be reactive, or why it might be important in what follows. The origins of Reactive Programming can probably be traced to the 1970s or even earlier, so there's nothing new about the idea, but they are really resonating with something in the modern enterprise. This resonance has arrived (not accidentally) at the same time as the rise of microservices, and the ubiquity of multi-core processors. Some of the reasons for that will hopefully become clear. Here are some useful potted definitions from other sources: FRP has a strong affinity with high-performance, concurrency, asynchronous operations and non-blocking IO. However, it might be helpful to start with a suspicion that FRP has nothing to do with any of them. It is certainly the case that such concerns can be naturally handled, often transparently to the caller, when using a Reactive model. But the actual benefit, in terms of handling those concerns effectively or efficiently is entirely up to the implementation in question (and therefore should be subject to a high degree of scrutiny). It is also possible to implement a perfectly sane and useful FRP framework in a synchronous, single-threaded way, but that isn't really likely to be helpful in trying to use any of the new tools and libraries. Reactive Use Cases The hardest question to get an answer to as a newbie seems to be ""what is it good for?"" Here are some examples from an enterprise setting that illustrate general patterns of use: External Service Calls Many backend services these days are REST-ful (i.e. they operate over HTTP) so the underlying protocol is fundamentally blocking and synchronous. Not obvious territory for FRP maybe, but actually it's quite fertile ground because the implementation of such services often involves calling other services, and then yet more services depending on the results from the first calls. With so much IO going on if you were to wait for one call to complete before sending the next request, your poor client would give up in frustration before you managed to assemble a reply. So external service calls, especially complex orchestrations of dependencies between calls, are a good thing to optimize. FRP offers the promise of ""composability"" of the logic driving those operations, so that it is easier to write for the developer of the calling service. Highly Concurrent Message Consumers Message processing, in particular when it is highly concurrent, is a common enterprise use case. Reactive frameworks like to measure micro benchmarks, and brag about how many messages per second you can process in the JVM. The results are truly staggering (tens of millions of messages per second are easy to achieve), but possibly somewhat artificial - you wouldn't be so impressed if they said they were benchmarking a simple ""for"" loop. However, we should not be too quick to write off such work, and it's easy to see that when performance matters, all contributions should be gratefully accepted. Reactive patterns fit naturally with message processing (since an event translates nicely into a message), so if there is a way to process more messages faster we should pay attention. Spreadsheets Perhaps not really an enterprise use case, but one that everyone in the enterprise can easily relate to, and it nicely captures the philosophy of, and difficulty of implementing FRP. If cell B depends on cell A, and cell C depends on both cells A and B, then how do you propagate changes in A, ensuring that C is updated before any change events are sent to B? If you have a truly active framework to build on, then the answer is ""you don't care, you just declare the dependencies,"" and that is really the power of a spreadsheet in a nutshell. It also highlights the difference between FRP and simple event-driven programming - it puts the ""intelligent"" in ""intelligent routing"". Abstraction Over (A)synchronous Processing This is more of an abstract use case, so straying into the territory we should perhaps be avoiding. There is also some (a lot) of overlap between this and the more concrete use cases already mentioned, but hopefully it is still worth some discussion. The basic claim is a familiar (and justifiable) one, that as long as developers are willing to accept an extra layer of abstraction, they can forget about whether the code they are calling is synchronous or asynchronous. Since it costs precious brain cells to deal with asynchronous programming, there could be some useful ideas there. Reactive Programming is not the only approach to this issue, but some of the implementaters of FRP have thought hard enough about this problem that their tools are useful. Comparisons If you haven't been living in a cave since 1970 you will have come across some other concepts that are relevant to Reactive Programming and the kinds of problems people try and solve with it. Here are a few of them with my personal take on their relevance: Ruby Event-Machine The Event Machine is an abstraction over concurrent programming (usually involving non-blocking IO). Rubyists struggled for a long time to turn a language that was designed for single-threaded scripting into something that you could use to write a server application that a) worked, b) performed well, and c) stayed alive under load. Ruby has had threads for quite some time, but they aren't used much and have a bad reputation because they don't always perform very well. The alternative, which is ubiquitous now that it has been promoted (in Ruby 1.9) to the core of the language, is Fibers (sic). The Fiber programming model is sort of a flavour of coroutines (see below), where a single native thread is used to process large numbers of concurrent requests (usually involving IO). The programming model itself is a bit abstract and hard to reason about, so most people use a wrapper, and the Event Machine is the most common. Event Machine doesn't necessarily use Fibers (it abstracts those concerns), but it is easy to find examples of code using Event Machine with Fibers in Ruby web apps (e.g. see this article by Ilya Grigorik , or the fibered example from em-http-request ). People do this a lot to get the benefit of scalability that comes from using Event Machine in an I/O intensive application, without the ugly programming model that you get with lots of nested callbacks. Actor Model Similar to Object Oriented Programming, the Actor Model is a deep thread of Computer Science going back to the 1970s. Actors provide an abstraction over computation (as opposed to data and behaviour) that allows for concurrency as a natural consequence, so in practical terms they can form the basis of a concurrent system. Actors send each other messages, so they are reactive in some sense, and there is a lot of overlap between systems that style themselves as Actors or Reactive. Often the distinction is at the level of their implementation (e.g. Actors in Akka can be distributed across processes, and that is a distinguishing feature of that framework). Deferred results (Futures) Java 1.5 introduced a rich new set of libraries including Doug Lea's ""java.util.concurrent"", and part of that is the concept of a deferred result, encapsulated in a Future . It's a good example of a simple abstraction over an asynchronous pattern, without forcing the implementation to be asynchronous, or use any particular model of asynchronous processing. As the Netflix Tech Blog: Functional Reactive in the Netflix API with RxJava shows nicely, Futures are great when all you need is concurrent processing of a set of similar tasks, but as soon as any of them want to depend on each other or execute conditionally you get into a form of ""nested callback hell"". Reactive Programming provides an antidote to that. Map-reduce and fork-join Abstractions over parallel processing are useful and there are many examples to choose from. Map-reduce and fork-join that have evolved recently in the Java world, driven by massively parallel distributed processing ( MapReduce and Hadoop ) and by the JDK itself in version 1.7 ( Fork-Join ). These are useful abstractions but (like deferred results) they are shallow compared to FRP, which can be used as an abstraction over simple parallel processing, but which reaches beyond that into composability and declarative communication. Coroutines A ""coroutine"" is a generalization of a ""subroutine"" - it has an entry point, and exit point(s) like a subroutine, but when it exits it passes control to another coroutine (not necessarily to its caller), and whatever state it accumulated is kept and remembered for the next time it is called. Coroutines can be used as a building block for higher level features like Actors and Streams. One of the goals of Reactive Programming is to provide the same kind of abstraction over communicating parallel processing agents, so coroutines (if they are available) are a useful building block. There are various flavours of coroutines, some of which are more restrictive than the general case, but more flexible than vanilla subroutines. Fibers (see the discussion on Event Machine) are one flavour, and Generators (familiar in Scala and Python) are another. Reactive Programming in Java Java is not a ""reactive language"" in the sense that it doesn't support coroutines natively. There are other languages on the JVM (Scala and Clojure) that support reactive models more natively, but Java itself does not until version 9. Java, however, is a powerhouse of enterprise development, and there has been a lot of activity recently in providing Reactive layers on top of the JDK. We only take a very brief look at a few of them here. Reactive Streams is a very low level contract, expressed as a handful of Java interfaces (plus a TCK), but also applicable to other languages. The interfaces express the basic building blocks of Publisher and Subscriber with explicit back pressure, forming a common language for interoperable libraries. Reactive Streams have been incorporated into the JDK as java.util.concurrent.Flow in version 9. The project is a collaboration between engineers from Kaazing, Netflix, Pivotal, Red Hat, Twitter, Typesafe and many others. RxJava : Netflix were using reactive patterns internally for some time and then they released the tools they were using under an open source license as Netflix/RxJava (subsequently re-branded as ""ReactiveX/RxJava""). Netflix does a lot of programming in Groovy on top of RxJava, but it is open to Java usage and quite well suited to Java 8 through the use of Lambdas. There is a bridge to Reactive Streams . RxJava is a ""2nd Generation"" library according to David Karnok's Generations of Reactive classification. Reactor is a Java framework from the Pivotal open source team (the one that created Spring). It builds directly on Reactive Streams, so there is no need for a bridge. The Reactor IO project provides wrappers around low-level network runtimes like Netty and Aeron. Reactor is a ""4th Generation"" library according to David Karnok's Generations of Reactive classification. Spring Framework 5.0 (first milestone June 2016) has reactive features built into it, including tools for building HTTP servers and clients. Existing users of Spring in the web tier will find a very familiar programming model using annotations to decorate controller methods to handle HTTP requests, for the most part handing off the dispatching of reactive requests and back pressure concerns to the framework. Spring builds on Reactor, but also exposes APIs that allow its features to be expressed using a choice of libraries (e.g. Reactor or RxJava). Users can choose from Tomcat, Jetty, Netty (via Reactor IO) and Undertow for the server side network stack. Ratpack is a set of libraries for building high performance services over HTTP. It builds on Netty and implements Reactive Streams for interoperability (so you can use other Reactive Streams implementations higher up the stack, for instance). Spring is supported as a native component, and can be used to provide dependency injection using some simple utility classes. There is also some autoconfiguration so that Spring Boot users can embed Ratpack inside a Spring application, bringing up an HTTP endpoint and listening there instead of using one of the embedded servers supplied directly by Spring Boot. Akka is a toolkit for building applications using the Actor pattern in Scala or Java, with interprocess communication using Akka Streams, and Reactive Streams contracts are built in. Akka is a ""3rd Generation"" library according to David Karnok's Generations of Reactive classification. Why Now? What is driving the rise of Reactive in Enterprise Java? Well, it's not (all) just a technology fad - people jumping on the bandwagon with the shiny new toys. The driver is efficient resource utilization, or in other words, spending less money on servers and data centres. The promise of Reactive is that you can do more with less, specifically you can process higher loads with fewer threads. This is where the intersection of Reactive and non-blocking, asynchronous I/O comes to the foreground.For the right problem, the effects are dramatic. For the wrong problem, the effects might go into reverse (you actually make things worse). ALso remember, even if you pick the right problem, there is no such thing as a free lunch, and Reactive doesn't solve the problems for you, it just gives you a toolbox that you can use to implement solutions. Conclusion In this article we have taken a very broad and high level look at the Reactive movement, setting it in context in the modern enterprise. There are a number of Reactive libraries or frameworks for the JVM, all under active development. To a large extent they provide similar features, but increasingly, thanks to Reactive Streams, they are interoperable. In the next article in the series we will get down to brass tacks and have a look at some actual code samples, to get a better picture of the specifics of what it means to be Reactive and why it matters. We will also devote some time to understanding why the ""F"" in FRP is important, and how the concepts of back pressure and non-blocking code have a profound impact on programming style. And most importantly, we will help you to make the important decision about when and how to go Reactive, and when to stay put on the older styles and stacks. Please enable JavaScript to view the comments powered by Disqus. comments powered by",en,48
369,1130,1464388061,CONTENT SHARED,511891900317865118,-709287718034731589,-7521446275214227001,,,,HTML,http://martiancraft.com/blog/2016/05/porting-ios-design-to-andoid/,why porting an ios design to android will not work,"During the four years I have been doing Android development, on projects both big and small, there's been one common problem: design. Even on teams which have dedicated UX/UI designers, or on teams where iOS drives the Android design, I find myself making the same case over and over: advocating to do Android the right way. It's important to make an app look, feel, and function natively on Android. More times than not, when I relay this issue to project leads, I am met with the same response: ""It doesn't matter, no big deal."" I am here to tell you it does matter - and I want to walk you through some examples why. From a user perspective Hardware Back Buttons Are a Thing Navigation on iOS is done in the app using UI buttons, whereas navigation on Android is done with both UI controls and the hardware buttons on the device. Many times the app must be aware of the user hitting the back button on Android. Pressing the back button normally takes the user back to the previous page. However, in abnormal situations there must be a discussion of the UI and app flow. For example, if the app has several on-boarding screens, and upon completion of the on-boarding, the user does not need or want to go back to the start of on-boarding, a discussion is in order to determine what happens if an Android user hits the back button in this situation. Where should the app take the user? This is not an issue iOS users face - so it is a new design challenge for Android navigational flows. In order to navigate back in an iOS app, the user normally looks for a back arrow to hit, or can use an edge swipe. That's where pressing the soft/hardware back button serves the same purpose as the software back buttons do in iOS. Some app designs call for an ""X"" button to leave a page or modal. While this is visually appealing, and even necessary for iOS, this button simply should not be on the Android UI. Android users naturally and intuitively hit the hardware back button to exit a dialog without buttons, so there is no need to introduce an ""X"" or close button in the design. Examples of an X button to exit a page from some well known social Android apps (Twitter and Instagram) which are not needed. Slack gets it right by modifying the design to stay true to the platform. iOS (left) vs Android (right) It is very important for designers, developers, and product owners to consider that iOS and Android have different native standards when it comes to navigation patterns and screen transitions, and to be aware of the most current information available on these things. Google's Material Design documentation does a fantastic job of detailing screen transition use, and applying proper navigation patterns to your app. Android users are accustomed to certain navigation and UI patterns. Most apps adhere and keep the user's experience consistent with Android's UI patterns. iOS navigation often uses the bottom tab bar for navigating throughout the app. For Android users this is inconsistent with the standard design language and may frustrate users at first glance. It is better not to utilize the bottom tab bar options and present the navigation options under the hamburger icon which is standard on Android. A great example is how Yelp did this for both to its mobile apps. ( Starting in Android N, Google is introducing bottom navigation. However there is no release date on when it will be available to the public. ) Yelp iOS with bottom nav options (left) , Android with nav option under the hamburger icon (right). If Android is a serious platform for your company, you need to hire designers who are familiar with iPhone and Android designs languages. Platform differences are not only the way things look, but also differences in the structure and flow of the app. Designers need to spend some time getting familiar with the Android platform through hands on experience with Android devices. From a developer perspective Custom views for standard iOS controls Often times, designers expect each element to look the same across platforms, and this requires additional development effort to achieve the same default design as iOS. The more complicated use cases are default controls such as radio buttons, checkboxes, toggles, etc which require a custom view implementation in order to achieve the right ""iOS"" look. An example would be date pickers. Android users typically are not familiar with slot machine reel style date selections. Custom views can get complex which increases work load, complexity, and alienates your app design on Android - all with a negative benefit for Android users. Standard Android Controls Standard iOS Controls iOS/Android Animations are not created equal iOS animation frameworks seem like the platform was built with animation in mind. In contrast, Android animation frameworks seem to be built as an afterthought. Android's animation framework is not nearly as robust as iOS. A very similar comparison would be iOS AV Foundation, which is leaps and bounds to Android's camera and bitmap apis. If animation is required in the design, the UX designer needs to sit down with the developer to see what is achievable in the timeframe of the project. One of my previous projects had a complex profile page which included your profile pic and other information. The page required a sticky header on a list view along squishing/scaling/fading your profile pic (see gif for visual). The animation could be done at a somewhat high level using the built-in iOS framework. Android requires a deeper or lower level approach by extending/customize core view components provided. Not sure how long it took my iOS counterpart, but I am positive it took me much longer to implement this effect on Android. I enjoyed the challenge and everything turned out, however it wasn't as simple and quick as the iOS development side. Sharing experience iOS uses share sheet which allows the developer to customize the share sheet menu to a degree. Often times, designers will want this share experience to port exactly over to the Android app as much as possible. Android handles the share experience by calling out to any app that can share the content type. It would be unnecessary to spend time creating unique share experience for your Android users. Custom experiences have their place in the world, however, the standards are almost always the way to go. Sticking with the standards creates time savings for the product team, by not having to toil over a custom share experience, and for your app users not having to spend extra time learning your unique share experience. Android the right way The message is not that Android development is harder - Android is simply different in design and development when compared to iOS, and needs to be considered as such from the beginning. I encourage product owners and designers to lean on experienced Android developers for recommendations which will help avoid needless development cycles during the project. Project managers and stakeholders must take into consideration timelines for Android production may or may not match the iOS timelines feature-for-feature when things are done correctly. Doing things correctly fares better for your customers than simply trying to mirror what was done on iOS. Investing in Android requires patience and a good understanding of the platform. Remember Android has at least 50% of the U.S smartphone market , if done correctly you will not only gain the attention of your Android customer base, but you can win their affection. Don't leave half the smartphone market behind just to save money on design. Android users always notice.",en,48
370,1197,1464860514,CONTENT SHARED,5875183869203898283,-1387464358334758758,7170482234777071965,,,,HTML,http://www.independent.co.uk/life-style/gadgets-and-tech/news/google-voice-search-records-and-stores-conversation-people-have-around-their-phones-but-files-can-be-a7059376.html,google is quietly recording everything you say. here's how to hear it,"Google could have a record of everything you have said around it for years, and you can listen to it yourself. The company quietly records many of the conversations that people have around its products. The feature works as a way of letting people search with their voice, and storing those recordings presumably lets Google improve its language recognition tools as well as the results that it gives to people. But it also comes with an easy way of listening to and deleting all of the information that it collects. That's done through a special page that brings together the information that Google has on you. It's found by heading to Google's history page and looking at the long list of recordings . The company has a specific audio page and another for activity on the web, which will show you everywhere Google has a record of you being on the internet . The new portal was introduced in June last year and so has been active for the last year - meaning that it is now probably full of various things you have said, which you thought might have been in private. The recordings can function as a kind of diary, reminding you of the various places and situations that you and your phone have been in. But it's also a reminder of just how much information is collected about you, and how intimate that information can be. You'll see more if you've an Android phone, which can be activated at any time just by saying ""OK, Google"". But you may well also have recordings on there whatever devices you've interacted with Google using. On the page, you can listen through all of the recordings. You can also see information about how the sound was recorded - whether it was through the Google app or elsewhere - as well as any transcription of what was said if Google has turned it into text successfully. But perhaps the most useful - and least cringe-inducing - reason to visit the page is to delete everything from there, should you so wish. That can be done either by selecting specific recordings or To delete particular files, you can click the check box on the left and then move back to the top of the page and select ""delete"". To get rid of everything, you can press the ""More"" button, select ""Delete options"" and then ""Advanced"" and click through. The easiest way to stop Google recording everything is to turn off the virtual assistant and never to use voice search. But that solution also gets at the central problem of much privacy and data use today - doing so cuts off one of the most useful things about having an Android phone or using Google search.",en,48
371,718,1462214812,CONTENT SHARED,-5952729859839521828,6013226412048763966,6945824758161114491,,,,HTML,http://vocerh.uol.com.br/noticias/entrevista/investimentos-em-coaching-e-mentoring-aumentam-nas-empresas.phtml,investimentos em coaching e mentoring aumentam nas empresas,"Mais de 60% dos entrevistados em pesquisa da Hays disseram ter esse tipo de programa no local de trabalho Por Redação Você RH Parceria no trabalho | Crédito: Pixabay São Paulo - Uma pesquisa realizada pela empresa de recrutamento Hays, em parceria com a ESPM, mostra que uma das tendências nas empresas em 2016 é um investimento maior em programas de coaching e mentoring. A estratégia tem sido utilizada para aumentar a performance dos colaboradores e trazer bons resultados econômicos para as instituições. A medida tem sido cada vez mais colocada em prática: enquanto apenas 45% dos entrevistados em 2014 disseram ter esse tipo de atividade no local de trabalho, este ano, 62% responderam o mesmo, o que representa um aumento de 17% no requerimento dos programas. Foram ouvidos 3 600 profissionais de 23 estados brasileiros na pesquisa, com a maioria das respostas provindas da região Sudeste. Ainda segundo a maioria dos profissionais, as companhias que não têm programas de coaching ou mentoring apresentam maior rotatividade, o que pode prejudicar o crescimento do negócio. Outros benefícios da adoção da medida, segundo a pesquisadora da ESPM Célia Marcondes Ferraz, variam de um programa para o outro. Em relação aos de mentoring, ela acredita ver vantagem no desenvolvimento de competências mais específicas de determinadas empresas. Já os programas de coaching ajudam desde a inserir jovens profissionais na empresa até apoiar pessoas recém promovidas. ""O coaching ocorre de forma mais ampla, não necessariamente relacionado com o saber fazer. É uma forma de apoio extremamente útil"", diz Célia.",pt,48
372,968,1463310537,CONTENT SHARED,-4233177915193302509,1895326251577378793,5829707946669210619,,,,HTML,http://computerworld.com.br/governo-brasileiro-cria-manual-para-contratacao-de-cloud,governo brasileiro cria manual para contratação de cloud,"O Ministério do Planejamento, Orçamento e Gestão (MP) divulgou um manual de ""boas práticas, orientações e vedações"" para contratação de serviços de cloud computing por órgãos do governo brasileiro. O documento define, por exemplo, que dados e informações devem ficar hospedados em data centers no Brasil para resguardar-se em eventuais questões legais. ""A nossa expectativa é que os próprios fornecedores vão ter o total interesse em resguardar o sigilo das informações quando assim for exigido. Se, eventualmente, houver problemas de judicialização, é importante que isso seja feito na jurisdição brasileira"", justifica Cristiano Heckert, secretário de Tecnologia da Informação da pasta. O documento veda ainda a contratação de salas-cofre e salas seguras por órgãos do Sistema de Administração dos Recursos de Tecnologia da Informação (SISP) visando reduzir gastos. Heckert acredita que as orientações para a contratação de serviços de computação em nuvem vão otimizar os recursos de infraestrutura, pois o modelo é mais efetivo. O manual recomenda aos órgãos a utilização do modelo da 'Nuvem Híbrida', possibilitando a contratação de serviços que não comprometam a segurança nacional de fornecedores privados. Ainda, se o serviço exigir algum tipo de resguardo, as aquisições devem ser realizadas com entidades da Administração Pública Federal ou diretamente pelo órgão.",pt,48
373,803,1462451336,CONTENT SHARED,2589533162305407436,-1443636648652872475,4236721063116312900,,,,HTML,http://radar.oreilly.com/2015/07/6-reasons-why-i-like-keystoneml.html,6 reasons why i like keystoneml,"As we put the finishing touches on what promises to be another outstanding Hardcore Data Science Day at Strata + Hadoop World in New York , I sat down with my co-organizer Ben Recht for the the latest episode of the O'Reilly Data Show Podcast . Recht is a UC Berkeley faculty member and member of AMPLab , and his research spans many areas of interest to data scientists including optimization, compressed sensing, statistics, and machine learning. At the 2014 Strata + Hadoop World in NYC, Recht gave an overview of a nascent AMPLab research initiative into machine learning pipelines. The research team behind the project recently released an alpha version of a new software framework called KeystoneML , which gives developers a chance to test out some of the ideas that Recht outlined in his talk last year. We devoted a portion of this Data Show episode to machine learning pipelines in general, and a discussion of KeystoneML in particular. Since its release in May, I've had a chance to play around with KeystoneML and while it's quite new, there are several things I already like about it: KeystoneML opens up new data types Most data scientists don't normally play around with images or audio files. KeystoneML ships with easy to use sample pipelines for computer vision and speech. As more data loaders get created , KeystoneML will enable data scientists to leverage many more new data types and tackle new problems. It's built on top of Apache Spark: Community, scale, speed, results Spark is the hot new processing framework that many data scientists and data engineers are already using (and judging from recent announcements , more enterprises will start paying attention to it as well). By targeting Spark developers, the creators of KeystoneML can tap into a rapidly growing pool of contributors. As a distributed computing framework, Spark's ability to comfortably scale out to large clusters can significantly speed up computations. Early experiments using KeystoneML tackle some computer vision and speech recognition tasks on modestly-sized Spark clusters - resulting in training times that are much faster than other approaches used. And, while the project is still in its early stages, early pipelines that ship with KeystoneML actually match some state-of-the-art results in speech recognition . Emphasis on understanding and reproducing end-to-end machine learning pipelines Rather than the simplistic approach frequently used to teach machine learning ( input data -> train model -> use model ), KeystoneML's API reinforces the importance of thinking in terms of end-to-end pipelines. As I and many others have pointed out , model building is actually just one component in data science workflows. There are many ways to contribute to KeystoneML As a fairly new project, KeystoneML's codebase is still relatively small and accessible to potential contributors. A typical pipeline includes data loaders, featurizers , models, and many other components. You need not be an algorithm whiz or a machine learning enthusiast to contribute. In fact, I think many important future contributions to KeystoneML will likely be pipeline components that aren't advanced modeling primitives. Moreover, if you have access to, or have already created well-tuned components, the creators of KeystoneML provide examples of how to quickly integrate external libraries (including tools written in C) into your pipelines. A platform for large-scale experiments: Benchmarking and reproducibility As I noted in an earlier post, the project's longer term goal is to produce error bounds for end-to-end pipelines . Another important objective is to build a framework and accompanying components that make it easy for data scientists to run experiments and make comparisons. Recht explained: There are benchmarks, and you get thousands of papers written about the same benchmark and it's completely impossible to know how people are comparing. They'd say, ""Algorithm A is better than algorithm B."" You don't actually get to see how exactly they're running algorithm A, or what they did to the default parameters in algorithm B. It's very hard to actually to make comparisons to make them reproducible. Then [someone will] come along after a bunch of these [and] try to reproduce all of the results and [write a] survey paper comparing a bunch of results. ... What we'd like to be able to do is have this framework where you can actually do those kinds of comparisons. (Automatic) Tuning Data scientists and data engineers who work with big data tools often struggle to configure and tune complex distributed systems. The designers of KeystoneML are building automation and optimization tools to address these issues. Recht noted: The other thing that we're hoping to be able to do are systems optimizations : meaning that we don't want you to load a thousand-node cluster because you made an incorrect decision in caching. We'd like to be able to actually make these things where we can smartly allocate memory and other resources to be able to run to more compact clusters. You can listen to our entire interview in the SoundCloud player above, or subscribe through Stitcher , SoundCloud , TuneIn , or iTunes . Cropped public domain image on article and category pages via Wikipedia . Ben Recht will give an update on the state of KeystoneML at Strata + Hadoop World NYC this September. Meet Recht and many other outstanding speakers at Hardcore Data Science day. Explore emerging topics such as deep learning, statistical decision theory, interpretable machine learning, and tensor methods with this O'Reilly video collection, taken from the Hardcore Data Science sessions at Strata + Hadoop World 2015 in San Jose, California. Save 50% on data video training collections . Push the envelope of data science by exploring emerging topics such as data management, machine learning, natural language processing, crowdsourcing, and algorithm design with this O'Reilly video collection, taken from the Hardcore Data Science sessions at Strata + Hadoop World 2014 in New York. Save 50% on data video training collections .",en,48
374,1242,1464959729,CONTENT SHARED,3598164602856140626,7645894863578715801,7699833899292651472,,,,HTML,http://blog.christianposta.com/microservices/netflix-oss-or-kubernetes-how-about-both/,"netflix oss, spring cloud, or kubernetes? how about all of them!","Some of this I cover in my book ""Microservices for Java Developers"" O'Reilly June 2016 (launching soon!), but I want to give a more specific treatment of it here. I get questions from folks about NetflixOSS (it's awesome) and how it might run in Kubernetes (it's awesome too!) and where the overlap is for some of the components. Let me try to explain some of it. Netflix OSS is a set of frameworks and libraries that Netflix wrote to solve some interesting distributed-systems problems at scale. Today, for Java developers, it's pretty synonymous with developing microservices in a cloud environment . Patterns for service discovery , load balancing , fault-tolerance , etc are incredibly important concepts for scalable distributed systems and Netflix brings nice solutions for these. A quick ""thank-you"" as well that Netflix decided to contribute to the broader open-source community with these libraries and frameworks. Other internet companies do this as well, so thank you. Some other large internet companies patent their findings and keep them closed source. And that's just too bad. Anyway, a lot of Netflix OSS was written at a time where things ran on an AWS cloud and there were no alternatives. A lot of assumptions about this heritage are baked into the Netflix libraries that may no longer apply based on where you're running (ie linux containers, etc) today. With the emergence of linux containers , Docker , container management systems , etc we're starting to see a lot of value running our microservices in linux containers (in a cloud, in a private cloud, both, etc). Additionally, because containers are basically opaque packaging of services, we tend not to care as much about what technology is really running inside them (is it Java? is it Node.js? is it Go?). Netflix OSS is for Java developers mostly. They're sets of libraries/frameworks/configurations that need to be included in your Java application/service code. So that brings us to point #1. Microservices can be implemented in a variety of frameworks/languages but things like service discovery, load balancing, fault-tolerance, etc are still quite important. If we run these services in containers, we can take advantage of powerful language-agnostic infrastructure to do things like builds , packaging , deployments , health checks , rolling upgrades , blue-green deployments , security , and other things. For example, OpenShift which is a flavor of Kubernetes built with enterprises in mind, does all of these things for you: there's nothing you have to force into the application layer that is aware or cares about these things. Keep your apps and services simple and focused on what they're supposed to do. So can the infrastructure help out with service discovery, load balancing and fault tolerance also? Why should this be an application-level thing? If you use Kubernetes (or some variant), the answer is: yes. Service discovery the Kubernetes way With Netflix OSS you typically need to set up a service-discovery server that acts as a registry of endpoints that can be discovered with various clients. For example, maybe you use Netflix Ribbon to communicate with other services and need to discover where they are running. Services can be taken down, they can die of their own volition, or we can add more services to a cluster to help scale up. This central service-discovery registry basically keeps track of what services are available in the cluster. One of the problems: you as a developer need to do these things: decide do I want a AP system ( consul , eureka , etc) or an CP system ( zookeeper , etcd , etc) figure out how to run, manage, and monitor these systems at scale (which is no trivial pet project) Moreover, you need to find clients for the programming language you use that can understand how to speak to the service-discovery mechanism. As we've already said, microservices can be implemented in many different types of languages, so a perfectly fine Java client may be available, but if a Go or NodeJS client doesn't exist, you have to write it yourself. Each permutation of language and developer may end up with their own idea of how to implement this kind of client and now you're stuck maintaining multiple clients that try to do the same thing but in possibly semantically different ways. Or maybe each language uses its own service discovery server and has it's own out-of-the box clients? So now you manage and maintain infrastructure for service discovery servers per language type? Either way... Yuck. What if we just use DNS? Well, that solves our client-library problem right? DNS is baked into any system that uses TCP/UDP and is readily available whether you deploy on premise, cloud, containers, Windows, Solaris, etc, everywhere. Your clients just point to a domain name (ie, and the infrastructure knows how to route to services that DNS points to (can either be a VIP with load balancer or round-robin DNS, etc). Yay! now we don't have to figure out what client I need to use to discover a service, I just use whatever TCP client I want. I also don't have to figure out how to manage a DNS cluster, it's baked into network routers and is pretty well understood. Maybe DNS sucks for elastic discovery The drawback: DNS kinda sucks for elastic, dynamic clusters of services. What happens when services are added to your cluster? Or taken away? The IP addresses for the services may be cached in DNS servers/routers (even some you don't own) and even your own IP stack. What if your services/applications are listening on non-port-80 ports? DNS was made with standard ports like port 80 in mind. To use non-standard ports with DNS, you could use DNS SRV records but then you're back to needing special clients at the application layer to discover these entries. Kubernetes Services Let's just use Kubernetes . We're gonna run things in Docker /linux containers containers anyway and Kubernetes is the best place to run your Docker containers. Or Rocket containers . Or Hyper.sh ""containers"" . (Note, I'm a sucker for technology, and even more so technology that appears ""simple""... because you cannot build complex systems with complex parts. You want simple parts. But making simple parts is in-itself complex. What Google and Red Hat have done with Kubernetes to simplify distributed systems deployment/management/etc is simply awesome to me :) ) With Kubernetes we just create and use Kubernetes Services and we're done! We don't have to waste time setting up a discovery server, writing custom clients, screwing with DNS, etc. It just works and we move on to the next part of our microservice that provides business value. So how does it work? Here are the simple abstractions that Kubernetes brings to the table to enable this: Pods are simple. They're basically your linux containers. Labels are simple. They're basically key-value strings that you can use to label your pods (eg, Pod A has labels app=cassandra , tier=backend , version=1.0 , language=java ). These labels can be anything your heart desires. The last concept is Services . Also simple. A service is a fixed cluster IP address . This IP address is a virtual IP address that can be used to discover/call the actual endpoints in your Pods/containers. How does this IP address know which pods/containers are eligible to be discovered? It uses a _Label Selector__ that picks pods that have labels that you define. For example, let's say we want a Kubernetes Service with a selector of ""app=cassandra AND tier=backend"". This would give me a service with a virtual IP that discovers any pods that match that label (have both app=cassandra AND tier=backend). This selector is actively evaluated so that any pods that leave the cluster or any pods that join the cluster (based on what labels they have) will automatically start to participate in the service discovery. Another added benefit of using Kubernetes Services for selecting pods that belog to a service is that Kubernetes is smart about which pods belong to a service with respect to its liveness and health. Kubernetes can use built-in liveness and health checking to determine whether or not a pod should be included in the cluster of pods for a specific service based on whether or not it's alive and/or functioning properly. It can evict those that are not. Note an instance of a Kubernetes Service is not a ""thing"" or an appliance or a docker container or anything..it's a virtual ""thing""... so there are no single points of failure. It's an IP address that gets routed by Kubernetes. This is incredibly powerful and simple for developers. Now an application that wishes to use a Cassandra backend just uses this one fixed IP address to talk to the Cassandra databases. But hard-coding a fixed IP is usually not a good idea because what if you want to move your app/service to a different environment (ie, QA, PROD, etc). Now you have to change that IP (or inject some configuration) and now you've added to your configuration burden. So let's just use DNS :) Using cluster DNS within Kubernetes is the right answer. Since the IP is fixed for a given environment (Dev, QA, etc) we don't care about caching it: it'll never change. Now if we use DNS, our app can be configured to talk to services at and even when we move from Dev to QA to Prod, insofar we have those Kubernetes Services in each environment, our app doesn't need to change. See the above image for a visual We don't need additional configuration, we don't need to worry about DNS caching/SRV records, custom library clients and managing additional service-discovery infrastructure. Pods can now be added to the cluster (or taken out of the cluster) and the Kubernetes Service's label-selector will actively group the cluster based on the labels. Your app just talks to whether you're a Java app, Python, Node.js, Perl, Go, .NET, Ruby, C++, Scala, Groovy, whatever. This service discovery mechanism doesn't impose specific clients. Just use it. Service discovery just got a lot easier. What about client-side load balancing? This one is interesting. Netflix wrote Eureka and Ribbon and with those combinations you can enable client-side load balancing. Basically what happens is the service registry (Eureka/Consul/Zookeeper/etc) is keeping track of what services exist in a cluster AND sending that data to clients that are interested in this. Then, since the client has information about what nodes are in the cluster, it can pick one (randomly, sticky, or some custom algorithm) and then call it. On it's next call, it can pick a different service in the cluster if it so desires. The advantages here are we don't need physical/soft loadbalancers which could quickly become a bottleneck. Another important aspect: since the client knows where the service is, the client can contact the service provider directly without additional hops. IMHO client-side load balancing is the 5% use case. Let me explain. What we want is a way to do scalable load-balancing ideally without any additional appliances and client libraries. In most cases we probably don't care about the extra hop with a load balancer in the middle (think about it.. probably 99% of your apps are probably deployed this way right now). We could get into a situation where a call to service A calls service B which calls service C, and D, and E, and you get the picture. In this case if each one took an extra hop we'd incur lots of additional latency. So a possible solution could be ""remove the extra hops"".. and it is... but not just in the hops to load balancers: in the number of calls you have to make to downstream services :) Follow my blog on event-driven systems and the discussion around Autonomy vs Authority to see where I'm going with that :) Using Kubernetes Services as we did in the Service Discovery section above, we accomplish proper load balancing (again, without all of the overhead of the service registries, custom clients, DNS drawbacks, etc). When we interact with a Kubernetes Service via its DNS (or IP), Kubernetes will by default load balance across the pods in the cluster (remember, the cluster is defined by the labels and label selectors). If you don't want the extra hops in the load balancing, no worries; this virtual IP is routed directly to the Pods, it does not hit a physical network Yay! Easy for the 95% use case. And chances are, you're in the 95% distribution of use cases, so don't need to over-engineer things. Keep it simple. What about that 5% case? You may have a case where you have to make some business decision at runtime about which exact backend endpoint within a cluster you really want to call. Basically you want to use some custom algorithm that's more complicated than just ""round robin"", ""random"", ""sticky-session"" and is specific to your application. Use client-side load balancing for that. In this model, you can still leverage Kubernetes' service discovery to find out which pods are in the cluster and then use your own code to decide which pod to call directly (based on labels, etc). The Kubeflix project from the fabric8.io community has discovery plugins for using Ribbon , for example, to get a list of all the pods for a service from the Kubernetes REST API and then let users use Java code to make business decisions about which pod to call . The same thing can be done for other languages and just use the Kubernetes REST API to query the pods, etc. For these use-cases it can make sense to invest in client-specific discovery libraries. Even more appropriate is to break this custom logic into its own module so its dependencies are separate from the application. With Kubernetes, you can deploy this separate module as a sidecar to your application/service and keep the custom load balancing logic there. Again, IMHO, this is the 5% use case and comes with additional complexity. For the 95% use case, just use what's built in without any fancy language-specific clients. What about fault-tolerance? Systems with dependencies should always be built with promises in mind . This means, they should always be aware of what they're obligation is even when dependent systems are not available or crash. Should we ask Kubernetes what it has in the way of fault tolerance? Well, Kubernetes does have self-healing capabilities. If a pod or container within a pod goes down, Kubernetes can bring it back up to maintain its ReplicaSet invariants (basically, if you tell Kubernetes I want 10 pods of ""foo"" it will always try to maintain that state, even if pods go down; it will bring them back upt to maintain its replica count, in this case, 10). Self-healing infrastructure is awesome, and comes out of the box with Kubernetes, but what we're interested in for this discussions is what happens to an application when its dependencies (database, other services, etc) go down? Well, it's really up to the application to figure out how it deals with that. For example, at Netflix if you try to watch a particular movie a service call is made to a ""authorizations"" service that knows what privileges you have for watching movies. If that service goes down should we block the user from watching that movie? Should we show exception stack traces? The netflix approach is to just let the user watch the movie. It's better to let a subscriber watch a movie that they're not entitled to every once in a while during a service dependency error than just blow up or say ""no"" when maybe they do have entitlements to that movie. What we want is a way to gracefully degrade or look for alternative methods for keeping our promise about the contract of a service. Netflix Hystrix is a great solution for Java developers. Netflix Hystrix implements a way to do ""bulkheading"" , ""circuit breaking"" , and ""fallbacks"" . Each of these are application-specific impementations, so it's justified to have specific client libraries for different languages in this case. Can Kubernetes help with this at all? Yes! Again, looking at the awesome Kubeflix project, you can use the Netflix Turbine project to aggregate and visualize all of the circuit breakers running in your cluster. Hystrix can expose Server Side Events as a stream that can be consumed by Turbine. But how does Turbine discover which pods have hystrix in them? Great question :) We can use Kubernetes labels for this. If we label our pods that use Hystrix with hystrix.enabled=true then the Kubeflix Turbine engine can automatically discover the SSE streams for each hystrix circuit breaker and display them on the Turbine web page. Thank you Kubernetes! Credit to Joseph Wilk for the image above. What about configuration? Netflix Archaius was written to handle distributed configuration management of services in the cloud. To do this, just like with Eureka and Ribbon, you set up a configuration server and use a Java library to lookup configuration values. It also supports dynamic configuration changes, etc (remember, Netflix built this for AWS.. and they're netflix; as part of their CI/CD pipeline they would build AMIs and deploy those. Building AMIs or any VM images can be time consuming and have lots of unnecessary overhead for most cases... with Docker/linux containers, things are a bit more nimble as I'll explain in a sec from a configuration perspective). What about the 95% use case again? We want to have our environment-specific configurations (this is an important distinction... not ""every"" config is an environment-specific configuration that needs to change depending on the environment we're running) stored outside of our applications and inject them in based on the environment (DEV, QA, PROD, etc) in which we're running. But we really would like to have a language-agnostic way to look up the configuration and not force everyone to use Java and/or complicate their classpaths with additional Java libraries for configuration. In Kubernetes we have three constructs for injecting environment-based configuration. Basically we can inject configuration data to the linux containers via environment variables (which Java, NodeJS, Go, ruby, python, etc can easily read) which most languages can read. We may store our configuration in git (this is a good idea) and we could bind the configuration repo directly to our pod (as files on the file system) that can then be consumed using whatever framework facility there is for consuming application config files. Lastly, we can decouple the git repo a bit by using Kubernetes ConfigMaps to store our versioned configuration into a ConfigMap which then gets mounted as a file system into the pod. Again, you'd consume configuration files just the same way you'd consume any config file from the file-system in your respective language/framework. What about the 5% use case? In the 5% use case you may wish to dynamically change configuration at runtime. Kubernetes helps out with this. You can change configuration files within a ConfigMap and have those changes dynamically propogate to the pods that mount them. In this case, you'd need to have a client side library that's capable of detecting these configuration changes and exposing them to your application. Netflix Archais has a client that can do this. Spring Cloud Kubernetes for Java makes this even easier to do within Kubernetes (using ConfigMaps). So what about Spring Cloud? Java folks developing microservices using Spring often equate Spring Cloud with Netflix OSS since a lot of it is based on Netflix OSS. The fabrjc8.io community also has lots of goodies from running Spring Cloud on Kubernetes. Please check out the for more. A lot of these patterns (including configuration, tracing, etc) can run great on Kubernetes without additional, complex, infrastructure (like service-discovery engines, config engines, etc). Summary If you're looking at building microservices and you've gravitated toward Netflix OSS/Java/Spring/Spring Cloud, be aware that you're not Netflix, you don't need to use AWS EC2 primitives directly and complicate your applications by doing so. If you're looking to use Docker, adopting Kubernetes is a wise choice and comes with lots of these ""distributed-systems features"" out of the box. Layering the appropriate application-level libraries where needed and avoiding over complicating your services from the beginning just because Netflix came up with this approach 5 years ago (because they had to! bet if they had Kubernetes 5 years ago, their Netflix OSS stack would look quite a bit different :) ) is a wise choice :)",en,48
375,1587,1467246438,CONTENT SHARED,4485571547865864859,881856221521045800,-1609125166981192463,,,,HTML,http://gizmodo.com/boses-new-speaker-only-costs-150-but-you-have-to-build-1782743534,bose's new speaker only costs $150 but you have to build it yourself,"Every company wants kids to build stuff-not in an illegal child labor kind of way but in a fun, educational kind of way. Ball robots are teaching kids code and Google's new modular blocks work toward a similar goal . But Bose's BOSEbuild speaker is more interested in teaching the ins and outs of sound and speaker design. It also looks cool as hell. The $150 speaker comes with its own companion app (iOS only) with step-by-step instructions that explore frequency, waveforms, magnets, electromagnets, and the general physics of sound. The included lights and faceplates are completely customizable, and assuming everything is put together correctly, the BOSEbuild speaker is also a fully functional Bluetooth speaker that can connect to whatever device for music-streaming goodness.",en,48
376,1933,1469814927,CONTENT SHARED,980458131533897249,-6786856227257648356,6488968056846413968,,,,HTML,https://minewhat.com/blog/elasticsearch-csv-exporter-for-kibana-discover/,elasticsearch: csv exporter for kibana discover,"Why? We use Elasticsearch + Kibana for data analysis, and we love it. Very often, we share the results of our analysis with the team. Kibana expects us to visualize data in the form of graphs and charts, but some use cases are more tuned for data-table with raw filtered query results. Surprisingly, there is no way to export CSV or create a data table in Kibana. A quick Google search gave these results, not only us; many are waiting for this feature. Solution: We did a quick hack to GTD in the startup way; we have written a chrome plugin, it injects the CSV export functionality in Kibana Discover tab. Business Users: Go ahead install this plugin from the chrome web store. Developers: Here is the Github link, and it is an open source project under MIT license. Please go ahead and tweak for you need. It would be great if you can provide a link back to this blog from the places you refer. Future: We can write an elasticsearch plugin and pair it with this chrome plugin to make it powerful. We know Kibana team is trying to solve this problem in a holistic way, it takes time, so we thought this might help users like us. Let us know, what you think?",en,48
377,1563,1467139108,CONTENT SHARED,7521700660190108859,-7118575739764684077,-2901786489893223302,,,,HTML,http://skdesu.com/risadas-japonesas-na-internet/,como são escrita as risadas em japonês? - suki desu,"Todos conhecem as universais formas de risadas kkkkkkkkkk , rsrsrsrs e até o famoso huehuehue . Já ficou curioso pra saber como é as risadas da internet e na mídia que os japoneses usam? Neste artigo vamos ver alguma delas e seus significados. As risadas que iremos mostrar abaixo podem ser chamadas de onomatopeias . E costumam ser usadas não apenas na internet, mas principalmente em mangas, animes e em todo o Japão, através da mídia e no dia a dia. ははははは Ha! ha! ha! O riso mais comum, usado especialmente por homens. ひひひひひ hihihihihi Risada de uma pessoa que pretende fazer algo de errado ou armar uma trama. ふふふふふ fufufufufu Riso assustador. Também pode representar riso feminino ou maléfico. へへへへへ hehehehehehe Riso envergonhado de uma pessoa que não consegue fazer algo ou está tentando esconder sua timidez. ほほほほほ hohohohoho Riso feminino, quando uma mulher da risadas com bocão aberto, usado frequentemente por senhoras. Observação : Costuma se usar as vogais (あ a / い i / う u / え e / お o) na frente das onomatopeias acima, soa mais coloquial: あはははは ahahahaha / いひひひひ ihihihihi / うふふふふ ufufufufu / えへへへへ ehehehehe / おほほほほ ohohohoho Abaixo vamos ver algumas variações e outros tipos de risadas que costumam ser derivadas das 5 principais. Kya! ha! ha! Forma Infantil de ""Ha! Ha! Ha! Wa! ha! ha! Gwa! ha! ha! Gargalhadas. Usado para deixar a risada um pouco mais maléfica. Kara-kara Tem pessoas que riem assim ""kara-kara"". Não é nada raro. Confie em mim! Gera-gera Geta-geta Risos, sem modéstia, muitas vezes contêm o significado de desprezo. Eles riem ""gera-gera"" de sua piada sem graça. Quando a pessoa esta lendo um manga engraçado, ou lembrou de uma piada, costuma a rir sozinho e um pouco baixinho... I! hi hi hi! ""Hi hi hi"" Eu sei a cor de sua calcinha. Um riso vulgar. Kya! Kya! Riso usado por bebes e crianças. Keta Keta- Riso indecente. Ele ri ""keta-keta"" não levando em consideração a situação. Koro-koro Um riso decente e agradável. Os machos nunca riem ""koro-koro"". Ladies costumam rir koro-koro como uma bola rolando. Bolas rolam e fazem""koro-koro"". Ke! ke! ke! Um riso de escárnio visto principalmente em quadrinhos. Niko niko ニコニコ é um sorriso alegre, e silencioso. O Kanji 笑 o Kanji 笑 - Wara é muito usado na internet para referir aos risos, tanto quando o lol é usado em inglês. Ele costuma ser encontrado entre os parenteses （笑）este kanji significa literalmente riso ou rindo. Na internet ainda é possível encontrar japoneses escrevendo www , mas não confunda com World Wide Web, www pode ser a abreviação de Warai ou Warau que é uma das pronuncias do kanji 笑 . É muito normal encontrar em chats japoneses, milhares de wwwwwwwwww... Essa pode ser a maneira mais simples e mais usada de se escrever risos na internet. E você? Qual risada costuma usar?",pt,48
378,2367,1474057066,CONTENT SHARED,8886002764685330098,-9016528795238256703,1379310357369544249,,,,HTML,https://realm.io/news/360andev-jake-wharton-java-hidden-costs-android/,exploring java's hidden costs,"As Java 8 features come to Android, it's important to remember that every standard library API and language feature comes with an associated cost. Even as devices get faster and have more memory, concerns of code size and performance overhead are still very relevant. This 360AnDev talk will be an exploration of hidden costs associated with some of Java's features. We'll focus on optimizations relevant for both library and application developers and on the tools that can be used to measure their impact . Introduction (0:00) In this talk, I'm going to talk about things I've been looking at for the last six months, and I want to disseminate some of that information. As we're going through this, you might not get these tangible things that you can apply to your application. But wait until the end, and I'll have some concrete tidbits of how to avoid all the stuff that we're going to see. I'll also be showing a lot of command-line tools that I use, and the links to all these resources will be at the end of the post. We'll start with a multiple choice question. How many methods are in this piece of code? Zero, one or two? You probably immediately have a gut reaction. Maybe it's zero, maybe it's one, and maybe it's two. Let's take a look and see if we can answer this question. First of all, there are zero methods inside this class. I've written no methods in the source file, so that's perfectly valid to say. Of course, that would be a really uninteresting answer to this problem. Let's start taking our class through the build pipeline of Android and see what comes up: We take our contents here and write it to a file, then use the Java compiler to take the source code and turn it into a class file. And then we can use this other tool from the Java development kit called javap . That is going to allow us to see inside the class file about what was compiled. If we run this on our compiled class, we see that there's a constructor inside our example class. We didn't write one in the source file, but Java C has taken the liberty of adding that constructor automatically. That means that there's zero methods in source file but one method in the class. But that is not where Android builds stop: In the Android SDK, there's a tool called dx , which does dexing , which takes the Java class files, and turns it into Android's Dalvik bytecode. We can run our example class through dex , and there's another tool in the Android SDK called dexdump , which will give us a little information about what's inside these dex files. You run it, and it's going to print out a bunch of stuff. It's like offsets into the file and counts and its various tables. If we look closely, one of these things stands out, and that's the method table inside the dex file: It says that there are two methods in our class. That makes no sense. Unfortunately, dexdump doesn't give us an easy way to look at what the methods are. In response to that, I wrote a little tool which I'll use to dump methods inside of a dex file: If we do this, we see that it does return two methods. It returns our constructor, which we know the Java compiler created, even though we didn't write one. But it also says that there is an object constructor. Certainly, our code is not calling new object everywhere, so where does this method come from that winds up being referenced in the dex file? If we go back to the javap tool that prints class file information, you can give it some additional flags to look more deeply inside the classes. I'm going to pass the -c flag, which is going to decompile the bytecode into something that's a little more human readable. At index one, it's our generated constructor that is calling the super class constructor. That is because, even though it didn't declare it, Example extends from Object . Every constructor has to call the super class constructor. It gets inserted automatically. That means there are two methods from our class flow. All of these answers to my initial question were correct. The difference between these though is terminology. It's where they live. We didn't declare any methods. But only humans care about this. As humans, we read and write these files. We're the only ones that really care what goes in and out of them. The other two are the ones that are more important, the number of methods that are actually compiled in the class files. These are the methods that declared or not, are inside that class. The two methods are the number of methods that are referenced. That is inclusive in the sense that our own methods that we wrote are counted, and also all of the others that are referenced from within those methods from calling out to Android's logger. That Log.d method that I'm referring to counts against this reference method panel, which is what's in the dex file. That is what people traditionally refer to when you talk about method count on Android, because dex has the infamous limit for the number of methods that can be referenced. We saw a constructor being created without us having declared one, so let's look for some other hidden things that are generated that we might not have known were there. Nested classes are a useful construct: They're not in Java 1.0. They came in a later version. You see stuff like this for when you're defining an adapter inside of a view or presenter: If we can compile this class, this is one file with two classes. One nested inside the other. If we compile this, we see that two separate class files end up in the file system. If Java had truly nested classes, we would only get one class file. We would get ItemsView.class . But there's no true nesting in Java, so what are in these classes? In the ItemsView , the outer class, all we have is the constructor. There's no reference, no hint that there was an inner class: If we look at the contents of the nested class, you see that it has its implicit constructor, and you know that it was inside an outer class because its name got mangled. Another important thing to note if I go back, we see that this ItemsView class is public, which is what we declared in the source file. But the inner class, the nested class, even though it's declared as private, it's not private in the class file. It's package scoped. That makes sense because we had two class files that were generated, in the same package. Again, that just further proves that there's no true nesting in Java. Even though you're nesting those two class definitions, you're effectively just creating two classes that are next to each other in the same package. You could do this if you wanted to. You can use that naming scheme as two separate source files: The dollar sign is a valid character for names in Java. Methods or plus names. However, this is really interesting because I know that I can do stuff to find a private static method in the outer class, and I can refer to that private method in the inner class: Now that we know that there's no such thing as true nesting, though, how does this work in our hypothetical separated system where our ItemsAdapter class needs to refer to the private method in ItemsView ? That will not compile, however this will: What's going on here? When you go back to our tools, we can use javac again. I'm referencing TextView , so I had to add the Android APIs to Java. Now I'm going to print out the contents of the nested class to see what method it has called. If you look at index two, it's not calling the displayText method. It's calling access$000 , which we didn't define. Is that in the ItemsView class? If we look, it is. We see our private static method is still there, but we now have this additional method that we didn't write automatically being added. If we look at the contents of that method, all it does is forward the call to our original displayText method. That makes sense because we need some way to call this private method from a package scoped to a class. Java's going to synthesize a package scoped method to facilitate that method call. If we go back to our separated example, our manual example, we can make this compile in the exact same way. We can add that method, and we can update the other class to refer to it. The dex file has this limit of methods, and so when you have these methods that are going to be added based on the ways that you're writing your source file, those can add up. It's important to understand this is happening because we're trying to access a private member somewhere that it can't work. So you might say, ""Well, you only did Java C. Maybe the dex tool can see that and automatically eliminate that method for us."" If we compile our two classes that are generated and list them out, you can see that that's not the case. The dex tool compiles it like it was any other method. It does end up in your dex file. You might say, ""Well, I heard about this new Jack compiler. And the Jack compiler takes the source file directly and directly produces dex files, so maybe it can do something where it doesn't need to generate this extra method."" There's certainly no access method. However, there's this -wrap0 method, which is effectively the same thing: There's also a tool called ProGuard , which probably a lot of people are using. You might say, ""Well ProGuard should be able to take care of this, right?"" I can write a quick ProGuard key. I can run ProGuard on our class files and print out the methods. And I get this: The constructors were removed because they weren't being used. I'm going to add them back in because normally they would stick around: You'll see that the access method is still there. But if you look closely, we retain the access method, but displayText disappeared. What the hell's going on here? You can unzip the jar that ProGuard produced, and then we can go back to our javap tool and look inside the ProGuarded class file: If we look at the access method, it no longer is calling displayText . ProGuard has essentially taken the contents of displayText and moved them into the access method and deleted the displayText method. This access method was the only one referring to that private method, so it just inlined it because no one else was using it. Yes, ProGuard can kind of help. But it's also not guaranteed to be able to do this. We got lucky because this is such a trivial example, but it is certainly not guaranteed. You might think, ""Well, I don't really nest classes that much, maybe it's a handful. So if I'm only getting a handful of these extra methods, it's not a big deal, right?"" Anonymous class (13:06) Let me introduce you to our friend the anonymous class: Anonymous classes behave the exact same way. They are effectively the same thing. It's a nested class, but it has no name. If in these listeners, which we use all too frequently, you reference a private method in the imposing class, that's going to generate a synthetic access method. That is also true for fields: I think this is the one that's probably a lot more common. We have fields in these outer classes, these activities that we're modifying the state of, inside of these listeners. I've done a completely awful implementation here, but what I'm doing is setting a value. I'm using a pre-increment, pre-decrement, post-increment, and a post-decrement, and then the log message has to read the value from the field. How many methods are we going to get from this? Maybe only two. Maybe one for reading, one for writing, and then the increment and decrement get turned into reads increments and writes. If that was only the case: We compile this, and one method is being generated for each of those types. So if you think that in an activity or fragment or whatever, you have four or five listeners, you have maybe ten fields that are private in the outer class. You're getting a nice combination of explosion of access methods. You still might be unconvinced that this is a problem. Say, ""Well, maybe it's 50, maybe it's 100. Is that really a big deal?"" It turns out we can find out. You can find out how prevalent these are in the wild. These commands will pull every APK from your phone. Every app that you've installed, that's a third party app: We can write a script which uses this dex method list and then greps all the different numbers: Then we can run this crazy command, which is going to go over every single APK that I've pulled off my phone, run this script, and then make a nice pretty table: You can see the table on slide 77, it sorts it by the most accessor methods by package name. And for my phone, we're in the multiple thousands. Amazon makes up five of the top six here. The top ones are in 5,000 synthetic accessor methods. 5,000 methods, that's an entire library. That's like an apken pad. You have an entire apken pad of useless methods that only exist to be jumping points to some other method. Also because we looked at ProGuard, obfuscation is going to screw with these numbers. Inlining is going to screw with them. Don't take them as the exact number. It just puts you in the ballpark of realizing how many methods are made up. How many methods are in your application that are better than these useless accessor inputs? By the way, Twitter at the bottom here? They have 1,000. They're ProGuarding their apps, so there's probably a lot more. But I thought it's interesting because they're the lowest number reported methods for the highest number of actual methods. They have 171,000 methods in their app and only 1,000 synthetic accessors that we use. That was pretty impressive. We can fix this. That is easy to do. We just have to not make something private. We have to make it package scoped when we're referencing it across those boundaries. IntelliGate has an inspection for this. It's not enabled by default, but you can go in and search for a private member. Which is fun to search for, and it will take our example, and it will highlight it yellow. You can option enter on it, and it will give you an intention action to just take that private member that you're accessing and make it package scoped. When you think about these nested classes, try to think of them as siblings instead of with having a parent-child relationship. You can't really access a private member from an outer class. You have to make it package scoped. That is not going to cause problems because even if you're a library, hopefully, these people aren't putting classes in the same packages as you so that they can access these things that you're making more visible. I also put in a feature request for a link check for this. Hopefully, in the future, you could potentially fail your build if you're doing this. I have been going through a ton of open source libraries and fixing these visibility problems so that the libraries themselves don't impose hundreds of extra methods. That is super important for libraries that do code generation. We were able to reduce the number of methods in our application by 2700 just by changing one of our code generation steps. 2700 methods for free for generating something that was package scope instead of private scope. Synthetic methods (18:45) These synthetic methods are called synthetic because you didn't write them. They were generated automatically for you. These accessor ones are not the only ones. Generics is another thing that came post-Java 1.0, and so it had to be retrofitted into how Java works. We see this a lot in libraries and even in our own applications. We're using these generic interfaces because they're extremely convenient, and they allow us to retain type safety. If we just have a method that accepts a generic value, when you compile this, you're going to find that every method that accepts that generic is going to get turned into two methods. One that accepts a string, whatever the generic type that you're using is, and one that's just object. That is like what erasure is. You hear a lot of people talk about erasure, and you might not understand what's happening. This is like the manifestation of erasure. We have to generate code that only uses object because that's what is going to end up being called when you access this generic method. If we look at what goes in that extra method, it's just a cast. It casts to that year type and then it calls the real implementation that accepts the generic. Anyone that's calling this call method is going to dispatch to this object method. Their calling code is going to pass in whatever their object is and then this code has to cast it and call into the real implementation. Every method that uses a generic, winds up turning into two methods. That is also true of return values. If you have a method that's returning a generic, you basically see the exact same thing. Two methods get generated. One that returns. In this case, our view. Then at the bottom, one that returns objects. This one's a lot more simple because it doesn't have to really do anything, it just accepts the view and then re-returns it as an object type. Another interesting thing to note here, which a lot of people don't realize, is that this is a feature of the Java language. If you have a method that you're overriding, you can change its return value to be a more specific version of that type. That is called a covariant return type. It doesn't even have to be, like in this case, we're implementing an interface. The base class doesn't even have to be an interface or anything. You can just be overriding a method from a base class. You can change its return type to be something more specific. The reason that you would do something like this is if you had other methods in our second class, which had to call into this get method, and they want the implementation type. Not the more broad, in this case, View type. It will allow them to do customizations that were only specific to TextView because they're in that class already. Covariant return type (21:58) Covariant return type. I'm sure you can guess what happens here. We get another method. In this case, it's both a generic and a covariant return type. We've taken our one method and turned it into three by basically not doing anything. This is a python script that detects these: That took a long time to figure out, but I wanted to know how prevalent this was in applications. We can go through the same process and run it over every app that I had installed. It's in the low thousands. There's not a whole lot you can do here. I showed that ProGuard can kind of help. The advantage of this case is that if ProGuard can detect that no one is referring to the generic version of the method, the method that takes an object or returns object. It can eliminate that, so you will see hundreds or thousands of these can be removed by ProGuard. But some of them can't because you are using them in the generic sense where you're calling the method on the interface. The last example that I want to look at for methods deals with something that's new and upcoming in Android which is the the Java 8 language features. We've had retro-lamina for awhile. But now the Jack compiler is also implementing these in the same spirit, which is allowing them to be used in a way that's back portable. But is there an associated cost to these new language features? I have a simple class which says, Hi when we call its method. What I want to do is take my Greeter and make it say hello on this Executor . Executor has a single method called run , and it accepts a Runnable . In the current world, we would make that creator type final . Then we would create a new runnable and call the method directly. In Lambda World it's just reducing the verbosity of doing that. That will still create a Runnable , but it does so implicitly. You don't have to specify the type. You don't have to know the actual method name or the argument types. The last one is method references. These are a little more interesting because it infers that this is a method that returns nothing and accepts no arguments. I can turn that into a Runnable automatically because I know all I have to do is call that method. How much do these cost? (24:45) That's fun, but how much do these cost? What are the costs of applying these language features? I set up a Retrolambda toolchain here and a toolchain using Jack. Neither of these is using ProGuard, or minification in Jack's case, because it doesn't affect the results. In the anonymous class case, the case that we've been using for so long, it's always two methods. If we compile our example, we see that this is our anonymous class, given assigned a monotonically increasing number here. We see the constructor. The constructor's going to take in the Greeter class for us and then it has a run method, which if we decompile it, all it does is call the method. That is exactly what we expect, very straightforward. When we do a lambda, if you're using an old version of retrolambda, this is really expensive. That one little tiny line of code can be six or seven methods to produce that functionality. Thankfully, the current version's down to four. And Jack even bests it with three, so only one worse than the anonymous class. But what's the difference? Why is there that extra method? We know how to figure that out. This is retrolambda, which has two additional methods: The one that's new is the top one. What's happening here is, you're defining a block of code inside the lambda, and that code has to go somewhere. It's not encoded in the method that defines the lambda because that would be weird. It doesn't belong to that method. It has to be stored somewhere so that it can be passed into whatever you're calling. That's what this top method is. It just copies and pastes whatever you write into the lambda into method in that same class. You can see the implementation here. All it does is delegate to the sayHi method. Very similar to what our runnable implementation did. We still have our constructor. We still have our run method. Except the repair of the run method is different. Instead of calling Greeter directly, it's going to call back into the original class and call that lambda method. That is the extra method. Here's where retrolambda screws it up. Instead of calling the constructor directly to create this generated class, it generates another method, which is a static factory method that calls the constructor. Jack's basically the same thing except it doesn't have that additional static method. We still get the lambda one, although it's named kind of crazy, and then the generated class is named very creatively with the entire type signature in the name. So that's it. Three methods. Lambdas cause that additional method at the top to be created. That's why they cost one additional method. Method references are also really interesting. Retrolambda and Jack are essentially tied. Retrolambda sometimes has to generate an extra method, and that's because you might be referring to a private method, and so it's not retrolambda generating it. Java generates one of those synthetic accessor methods because if you're passing a reference to a private method to another class, it's going to have no way of calling that. That's what the fourth one is. Jack, interestingly enough, generates three for every single method reference. It should be generating two for every single one except for that private case. In that case, it should only be generating three. Currently, it generates an accessor method for every single method reference. There's a bug filed for that too, so hopefully we'll see Jack get down to two methods. That's really important because that puts the method reference on the same par as the anonymous class. It makes it a zero cost abstraction from switching from the anonymous class to the method reference, which will be nice. Lambda's, unfortunately, will likely never get down to the same amount. The reason that that will never happen is that you also could be referring to private members or private methods inside of the lambda. It can't copy that into the generated runnable. Because again, it doesn't have any way of accessing those things. We can count these as well. Lambdas in the wild (30:05) Let's look and see how many lambdas are being used in the wild. Same deal. By the way, this takes a long time. It takes around 10 minutes. It also depends on how many apps that you end up pulling. If you wind up doing this for whatever reason, don't get impatient. It will take a long time, but you will get a result. Unfortunately, apparently not a lot of people are using lambdas, and I was excited to see that we pay the most. We have 826 lambdas. That's the number of lambdas and not the number of methods. The number of methods that we have from our lambdas is 826 times three or maybe four. No one's using Jack yet, or at least, of the applications I installed, no one's using Jack with lambdas. They might be using Jack and not using lambdas, which would be weird. Or additionally, they could be ProGuarding. So again, this ProGuard completely hides these lambda classes and names of the methods. If you're a popular application using lambdas, and you ProGuard, that's probably why you're not on this list. Or I just don't like your app. That was all on methods. The reason I was looking into this was one, to stave off hitting the 65K limit. But those methods have a run time cost as well. There are costs for loading additional bytecode. There's also costs for the extra trampoline that you have to go through when you're executing them. The private field one is my favorite because a lot of times you see that happen inside these anonymous listeners. Those are usually running as the result of the UI interaction on the main thread. What you don't want is a computationally expensive piece of code that you have to run on the main thread, whether it's for animation, size calculation, whatever. You don't want those, every single time you're referencing those fields; you don't want to be jumping through that extra method. Looking up a field is fairly fast. Invoking a method and then looking up a field is still going to be fast, but it's 100% of the time going to be slower than just looking up the field. You're introducing these indirections which you're not going to be dropping frames all of a sudden because these accessor methods exist. But they're useless methods that the only thing they serve to do is bloat your APK and very, very subtly slow down your application. I want to change gears a tiny bit and talk about something a little more run-time focused. That has to deal with collections. If you have anything that resembles the following in your application, you're probably wasting more resources than you should. Android has these specialized collections which I think most people nowadays know about. The implementation varies, but they're specialized to these cases which are very common. For example, when you have an integer index into a map referring to some value. There's a specialized collection for that. A lot of times people talk about this on the context of autoboxing. What autoboxing is, if you're not aware, is if we have this HashMap, which accepts an integer key, so you have an integer of some value, and you want to put a value in this map. Or conversely, you're iterating over the entries, and you want to get the value from a key, this conversion isn't straightforward. It has to go through an additional step called autoboxing, which is where it takes the primitive value and turns it into the class version of that, which in this case is an integer. The bottom one's not that expensive. It's unwrapping a type. The top one is the one that's bad. For small numbers, there's a cache, so it's not bad, but if you had a random slew of integers, it's going to be allocating objects every single time you call a method. Which is generic and accepts the big I integer. That is what most people talk about as the big advantage. It's very true that this is a significant advantage, but it turns out there are two other big advantages which aren't talked about a lot. The first one is data indirection. If you look at how HashMap is implemented, it has an array of these nodes, and that keeps its own size. When you're either putting a value or looking up a value, it has to jump into this array. That is the hashing step. This is the cost and time operation where it finds the hash and then does the offset into the array. However, this is an array of nodes. The node type has both the key and the value. It also has the hash, and it has a pointer to an additional node. We got to the array; we found the reference to the node, and we have to jump to the node now. Then we have to look inside the node if we want the value. We get the reference to the value, and we have to jump to it. We have to go through these indirections. They're in all different spaces in memory. You have to make these jumps in memory to get a value for a single key. Or to put a value at a single key. And then it can get worse. This is the hash collision problem, where if two items hash to the same bucket, HashMap will degrade into what is a linked list inside that bucket. If we happen to hit this case, we now have to walk the link list and find the matching exact hash. I'm going to look at sparse array. Sparse array's the replacement for this one here. We're going to look at that in a second, but before we get there, the other advantage just comes around to overhead. The overhead in memory of these collections. We have two classes here. We can use a tool called Java object layout, and it's going to tell us what the overhead of creating one of these objects is in memory. Running on HashMap, it's going to print out a bunch of stuff. It is showing you the cost per field. The important number is here at the bottom, where every instance of a HashMap, just the HashMap, not the nodes, not the keys or values or anything. Just the HashMap object itself is 48 bytes. It's not bad. That's small. We're running on the node object. Same stuff. We see our four fields. This one's 32 bytes. It's 32 bytes for every single node in that map. Every item in that map, every key and value pair is going to be inside one of these nodes. It's 32 times the number of entries. We can use a formula like this to figure out what the actual size of an object is at runtime when I start putting values into it. That is not fully correct. You also have to account for things like the array. There's an array that has to hold the nodes, so we need to account for the space that that array has to occupy. That is complicated. It's four, which is a single integer, which is the reference to each array, multiplied by the size of the array. The problem is, HashMap has this thing called a load factor. The eight at the end is a constant overhead for every array. But the load factor never tries to be full. It tries to maintain a certain percentage of fullness, so when it reaches that percentage, it's going to grow as an array list would. The HashMap is also going to grow so that it can maintain some empty spaces. The reason for that is because, if that doesn't happen, you start getting a lot of collisions and a lot of degradation performance. That would be approximately the value that a HashMap with however many entries and whatever the load factor is. We could figure out how many bytes in memory this is going to occupy. By the way, the default load factor is 75%. HashMap tries to stay 3/4 full at all times. Sparse array is what you should be using if you want to replace HashMaps like this. Let's look at the two cases we looked at in HashMap, which is the indirection, the levels of indirection, and the size of memory. Sparse array stores two arrays that are siblings. One is just the keys; the other is just the values. If we're looking up a value in this map, or putting a value on this map, the first thing we do is we go into this integer array, and unlike HashMap, it's not constant time. It has to do a binary search inside this array. We can then jump to the values array, and in this case, the binary search gave us the exact cell in the values array to look up the value. Because that array is the values, we can then return, jump directly to that reference and return that back to you. There's a lot less indirection here regarding memory. The integer array is continuous; there's no linked list. We can jump directly inside the values array. There's no node object that we have to unwrap and then get to the value. There's a lot less indirection. However, it does come with that slow down of not being perfectly constant time. This is why you want to keep them for relatively small maps. And by small, it's in the order of hundreds of entries. If you get into the thousands, the performance of the binary search is going to cause it to be so slow that the overhead of HashMap probably starts looking pretty appealing regarding performance. In this case, because the classes are exactly the same and the JVM 64 bit, and Android's now 64 bit, the number should be translatable. If that bothers you, treat them as an approximation and allow for some 20% variance. It's certainly a lot easier than figuring out how to get the object sizes on Android itself. Which is not impossible, it's just a lot easier this way. The object itself for sparse array is a little bit smaller, 32 bytes. That size doesn't matter. What does matter, however, is that you know these aren't just 32 plates. We have the entry problem again. We have to account for the array, that is, the integers for the keys, and that's four times the number of entries, plus eight. Then there's also the array for the values, which is the same thing, four times the number of entries, plus eight. What's different here is that sparse array doesn't have a load factor, but it does implicitly because these arrays are trees. They're binary trees. They're not filled. They're not contiguous. There are spaces inside of them that are unoccupied. We have to account for that with a little fudge factor, which is similar to the load factor. In this case, and it totally depends on what data you're putting into the map, I'm using the same value. I'm assuming that these arrays are going to be about 75% full, which is a pretty safe assumption. Now we can make a direct comparison against these things. We know that we can count the indirection jumps. That's easy. Now we can use these equations to compare the actual values of memory that these instances would occupy. I'm going to use .75 again for HashMap, which is the default. We can pick a value for number of entries. In this case, I picked 50. Maybe it's a map of states in the United States. Then we can run these computations. What you see is that the sparse array is 1/3 of the size. You have to remember; there's a slight performance overhead to this because every operation is no longer constant time. It's 50 elements, and so the number of searches in the binary tree is going to be very fast. That was a lot of stuff. It turns out that there are some extremely trivial to-dos to avoid these overheads, both at compile time with methods, and run-time for the overhead of various objects, and the indirection that they may afford. The first one I already showed. Turn on that private member inspection and listen to it. Don't ignore it. You don't have to do it for every single type. You don't need to go through and do it all at once. It's something you can do as you go in your app. Conversely, for libraries, it's a little more important. As a library, you should want to minimize your impact both in APK size and run-time performance. Or deck size and run-time performance. If you have a library, you might want to go through and find all these. There is no reason for the synthetic accessor methods to exist in libraries. It's a waste of space in the dex file. It's a waste of time in the run-time. Report them as bugs. If you are using retrolambda, please, please, please make sure you're upgraded to the latest version because you are wasting probably thousands of methods. If you're writing an open-source library, maybe just suck it up and deal with the anonymous classes. It's not that big of a deal. But again, it goes back to the fact that you want to have the minimal impact for your application developers, so that you're never the problem as the library. Try out Jack. It's a big deal. It's still undergoing rapid development. It's missing a lot of things that are going to make it applicable to everyone here, but certainly, there's going to be applications which are more trivial, or not doing anything exotic in build time. Don't ignore bugs. Don't switch, find it crashes and be like, ""Oh, whatever, I'll try in two years."" You can do that, just report the bug before you switch back to Java C index. This is the future, and you can cover your ears and close your eyes, but it's happening. It's better to find these things sooner rather than later. Then just turn on ProGuard. ProGuard helps out here. Whether you're using ProGuard or there's the new shrinker which is incremental and still works with instant run and presumably will be what Jack uses as well. Don't use overly matchy rules. If you ever see * * in a ProGuard rules file, that is wrong 100% of the time. That is never what you should be using because you've effectively just nuked any advantages that you were getting from ProGuard. You're saying that ""Yeah, I'm pulling in OkHttp, but I don't want all of these methods from HTTP/2, which I'm not even using. I don't want them to be stripped. I want to retain them just in case."" It's not smart. If you see these on open-source libraries, that is a bug. Report that as a bug. If you have those in your own app, try and figure out why they were initially there. Take them out, and think and look at the failure of ProGuard and see if there's something more specific that you can change your rule too. If this stuff is really tickling you and interesting you, there are a few other presentations that you can take a look at. Resources",en,48
379,1095,1464135876,CONTENT SHARED,-21036008762564671,2416280733544962613,6543088479402480909,,,,HTML,http://www.mckinsey.com/business-functions/marketing-and-sales/our-insights/the-sales-secrets-of-high-growth-companies,the sales secrets of high-growth companies,"The authors of Sales Growth reveal five actions that distinguish sales organizations at fast-growing companies. What distinguishes sales organizations at fast-growing companies from their lagging peers? In a wide-ranging survey of more than 1,000 companies, we unearthed five meaningful differences: 1. Commitment to the future That the world is changing ever more quickly may be a cliché, but that makes it no less true: all sales leaders know that they need to anticipate changes that could turn into opportunities or threats. Yet the best leaders move beyond acknowledgement to commitment. They make trend analysis a formal part of the sales process through systematic investments of time, money, and people. Building and sustaining the capability to take a forward-looking view of the market is not easy. In discussions with more than 200 sales leaders while researching our new book, Sales Growth , two common characteristics emerged: the mind-set of sales leadership and resource commitment. Sales leaders must consistently monitor the macro-environment in search of sales opportunities, no easy task given the relentless pressure to hit near-term targets. Forward planning must be part of someone's job description-not just part of top management's lengthy to-do list-with sufficient resources to take advantage of the best opportunities. Companies have to be willing to take risks now to create sales capacity long before the revenue will materialize. More than half of the fast-growing companies we analyzed look at least one year out, and 10 percent look more than three years out. After planning, sales leaders aren't afraid to put their money where they think the growth will be: 45 percent of fast-growing companies invest more than 6 percent of their sales budget on activities supporting goals that are at least a year out-a significant commitment in an environment where sales leaders fight for each dollar of investment. 2. Focus on key aspects of digital Successful brands don't just ""do digital""; they use their full arsenal of capabilities to massively increase the effectiveness of their sales force and to transform the customer buying experience to be ""digital first."" It pays off: digital channels provided at least a fifth of 2015 revenues for 41 percent of the fast-growing companies we surveyed-both business-to- business and business-to-consumer-compared with just 31 percent at slow-growing companies. This trend is only becoming more important, as almost two-thirds of all US retail sales by 2017 will involve some form of online research, consideration, or purchase. When it comes to customer experience, leading organizations are building out digital routes to market or augmenting traditional direct or indirect sales with digital. For traditional software companies, the focus on SaaS-based products is driving a change toward a digital sales experience where they discover, demo, and trial, all within a few clicks online. Many industrial companies are seeing their products also sold in external marketplaces, which is prompting them to build out their own e-commerce platforms to directly shape the customer experience. Sales leaders are especially strong at harnessing digital tools and capacities to support the sales organization. Fast-growing companies are more effective than slower-growing ones at using digital tools and capabilities to support the sales organization (43 percent versus 30 percent). They tend to focus on three fronts: First, they arm sales teams with digital tools that can quickly deliver relevant and usable insights. Second, they treat partners as an extension of the sales force and invest in collaboration tools to improve the flow of data between organizations. Third, they recognize the potential for big micromarket or macrotrend analyses to improve planning and capture opportunities most effectively. As the technology emerges, they are making targeted investments in tools, technologies, and talent to make the most of these opportunities. Success in digital comes from fanatical optimization-not as a one-off project, but as a continuous process. It comes from harnessing mobile technologies to drive growth, understanding how customers use and switch between the mobile channel and other channels. And it comes from integrating digital into a great omnichannel experience that spans marketing to post-purchase. 3. Harnessing of the full range of sales analytics Only now is the promise of advanced analytics catching up to the hype. Take customer analytics. Companies that use it extensively see profit improvements 126 percent higher than competitors who don't. And when it comes to sales improvements through the extensive use of advanced analytics, the difference is even larger: 131 percent. The value of advanced analytics is wide ranging, but where sales leaders excel against their peers is in making better decisions, managing accounts, uncovering insights into sales and deal opportunities, and sales strategy. In particular, they are shifting from analysis of historical data to being more predictive. They use sophisticated analytics to decide not only what the best opportunities are but also which ones will help minimize risk. In fact, in these areas three quarters of fast-growing companies believe themselves to be above average, while between 53 and 61 percent of slow-growing companies hold the same view. The new world of sales growth Read the article But even among fast-growing companies, only just over half-53 percent-claim to be moderately or extremely effective in using analytics to make decisions. For slow-growing companies, it drops to a little over a third. This indicates that there remains significant untapped potential in sales analytics. 4. Investment in people A rigorous focus on sales-force training is a clear differentiator between the fast- and slow-growing companies we surveyed. Just under half the fast growers spend significant time and money on sales-force training, compared to 29 percent of slow growers. There's room for improvement, though. Among fast growers, just over half believe their organization has the sales capabilities it will need in the future, while a third of the slow growers feel similarly equipped. As few as 18 percent of fast growers think they excel at pipeline management, and even in the most successful area-understanding specific customer needs-only 29 percent claimed to be outstanding. What is notable from our research, however, is that fast growers are committed to improving sales talent and performance. The head of sales at a North American consumer-services company, for example, tried a new approach to improving sales performance after years of fruitless initiatives. Instead of focusing solely on what the sales force had to do, the program also devoted significant attention to building the talents and capabilities to enable them to do it, making a substantial investment in teaching skills and enforcing their use with specific goals. The result? A 25 percent improvement in rep productivity across all regions within 18 months. More impressive still, the gains stuck, and two years later performance was still improving. 5. Marriage of clear vision with leadership action Two-thirds of fast-growing companies undertook a major performance improvement over the previous three years, and 84 percent considered it successful or very successful. Sales leaders at these organizations said the two most important factors that contributed to that success were management articulation of a clear and consistent vision and strategy, followed by leadership commitment. Articulating the vision should be simple. The chief executive officer of an emerging-markets telecommunications firm, for example, announced a ""3 × 3 × 3"" growth aspiration: three years to expand beyond its home country, three years to expand beyond its region, and three years to become a leading global brand. Besides being simple, the aspiration was bold, specific, and easily measurable. No sales transformation will work without steadfast support from the very top. Only a committed leader can override internal politics, see the big picture, and focus on the best solution regardless of past practices. Sometimes, the commitment can be very personal. For example, the head of sales at another telecom firm recognized how fundamental customer experience was for success. At the same time that he controversially clamped down on aggressive sales techniques that had a negative effect on customer experience, he proposed to his CEO that customer satisfaction ratings should determine 25 percent of his variable pay. Sales leaders face a dizzying array of issues and opportunities to manage, often at speeds that seemed unimaginable even a few years ago. But by focusing on what really matters, sales leaders can break away from their competitors. Click here for more information about the second edition of Sales Growth: Five Proven Strategies from the World's Sales Leaders . About the author(s) Homayoun Hatami is a director in the Paris office, Mitra Mahdavian is an associate principal in the Silicon Valley office, Maria Valdivieso is a senior expert in the Miami office, and Lareina Yee is a senior partner in the San Francisco office. Article - McKinsey Quarterly",en,48
380,2522,1475771143,CONTENT SHARED,3467834993860080188,3609194402293569455,5681278989312189918,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36",SP,BR,HTML,http://tutorialzine.com/2016/10/15-interesting-javascript-and-css-libraries-for-october-2016/,15 interesting javascript and css libraries for october 2016,"15 Interesting JavaScript and CSS Libraries for October 2016 Danny Markov Our mission at Tutorialzine is to keep you up to date with the latest and coolest trends in web development. That's why every month we release a handpicked collection of some of the best resources that we've stumbled upon and deemed worthy of your attention. Below is October's list, packed with awesome free libraries including vanilla JS plug-ins, React components, an icon font, and much more! Enjoy :) Leaflet is one of the most popular open-source solutions for creating interactive maps. It offers everything you'd expect out of a map library - markers, layers, zooming, and many more basic and advanced features. The project recently got it's official 1.0 release with many improvements to the API, accessibility and performance. Flexbox grid system for React that allows developers to quickly build page layouts using a simple API of 3 React components - Grid, Flex, and Box. Since it utilizes the flexbox model, all grids with Reflexbox are responsive and can be easily adjusted via the various flexbox properties. Lightweight no-dependencies JavaScript carousel with excellent gesture controls for touch-screen devices. The library provides lots of customization options for controlling the transition speed, frequency, animations, and other details. Excellent browser-support, a clean API, and lots of examples. Tiny, very simple to use JavaScript library for creating beautiful gradient animations. Go to the examples page to see all the features Granim provides, including linear and radial animations, image masks, and interactive change of gradient colors. A UI toolkit for React Native that combines a number of useful components into one package. It includes elements such as menus, various buttons, and form inputs, with new components coming in the future. Very well documented and easy to implement. JavaScript plugin that makes unresposnive elements responsive. Reframe is especially useful for <iframe> and <video> element as it preserves their original aspect ratio. Like most modern libraries, this one doesn't rely on jQuery but can work with $('selectors') if it is included in the page. Awesome library for creating responsive animated progress bars. ProgressBar.js provides a number of built in shapes, and also allows developers to create their own paths using SVG. The progress bars can be further customized through a rich JS options object. Headroom is a vanilla JavaScript library that allows you to easily set up event listeners for user scroll. You can bind different functions for when the scroll moves up or down, the bottom/top of the page is reached, and other events. One of the best available solutions for creating self-toggling headers. Sass mixins for loading indicators with fluid CSS-only animations. The spinners are extremely easy to implement as they require just one HTML element per spinner. To customize the mixins themselves you can simply change the provided SCSS variables. Vanilla JavaScript library that utilizes the History API to make a robust routing system with proper history on single page apps. In browsers where the History API isn't available, Navigo automatically falls back to a hash (#) based routing. Pretty theme for documents that have been generated from HTML or Markdown. It styles all elements on the page, providing them with proper typography, colors and paddings, greatly improving readability. Implementing the theme is as easy as adding a single class to the root element. Customization is done through Sass. This is a library for easier handling of complex CSS animations. Choreographer makes more manageable the transition of multiple properties at once, animations that start off after different delays, and syncing with user gestures such as scrolling. Color pallette created especially for designing user interfaces. It contains 12 colors, each having 10 variations with different intensity. Open color is available in CSS, Sass, Less, and as a plug-in for Photoshop, Illustrator, and Sketch. No dependencies JavaScript library that enables you to integrate fuzzy-search ( approximate string matching ) to your web app. Fuse is very powerful and has lots of options for you to tinker and play with to get the exact search functionality you need. Ionicons is the icon font from the Ionic framework, but it can be freely used as a standalone library. The collection consists of over 500 premium-quality symbols and logos, all drawn in a modern and very appealing style. One of our favorite icon fonts at the moment. by Danny Markov Danny is Tutorialzine's Bootstrap and HTML5 expert. When he is not in the office, you can usually find him riding his bike and coding on his laptop in the park.",en,48
381,758,1462301684,CONTENT SHARED,8589361101754366293,-1387464358334758758,-6201805413546761219,,,,HTML,http://googlediscovery.com/2016/05/03/acesse-versao-material-design-do-youtube/,acesse a versão material design do youtube | google discovery,"Além dos aplicativos para Android e sua própria plataforma móvel, outros produtos do Google também serão incorporados a linguagem de design desenvolvida pela empresa, como o Chrome e o YouTube. Enquanto o Chrome deve receber o novo layout durante as próximas semanas, o buscador ainda não forneceu datas para que seu portal de vídeos receba a nova identidade visual. No entanto, um usuário do Reddit parece ter encontrado um truque que adianta o processo - pelo menos para aqueles que utilizam o YouTube sem o uso de login. Veja como fazer: 1) Vá para https:www.youtube.com/?gl=US 2) Abra o ""Inspecionar"" do Chrome (ctrl + shift + i) 3) Acesse a guia `Resources´ e apague o cookie ""VISITOR_INFO1_LIVE"" dentro do domínio do YouTube 4) Entre na guia 'Console' e defina o cookie ""VISITOR_INFO1_LIVE"" com o seguinte comando: document.cookie=""VISITOR_INFO1_LIVE=Qa1hUZu3gtk;path=/;domain=.youtube.com""; 5) Recarregue a página Feito o processo, você irá visualizar o YouTube com este novo layout: é bacharel em administração de empresas e fundador da FragaNet Networks - empresa especializada em comunicação digital e mídias sociais. Em seu portfólio estão projetos como: Google Discovery, TechCult, AutoBlog e Arquivo UFO. Também foi colunista de tecnologia no TechTudo, da Globo.com.",pt,48
382,3046,1486034520,CONTENT SHARED,-8717945432162716060,361614354650880527,688811580614557176,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36",MG,BR,HTML,http://www.digitalcurrent.com/seo-engine-optimization/2017-vital-seo-trends/,7 vital seo trends for google rankings in 2017 | digital current,"Use these seven SEO trends to improve your Google search rankings in 2017. Let's face it: Developing an SEO strategy today isn't easy. With an ever-evolving search interface, real-time Penguin algorithm updates, and also Panda now introduced as part of its core algorithm - Google changes fast. The artificial intelligence system RankBrain is also shaking things up. Businesses not up-to-date with SEO can virtually lose their Google rankings overnight if they fail to stay on top of new SEO trends. That's a lot of money down the drain as you lose visibility, and a great deal of untapped potential if you are not yet ranking in the first place. I'm going to reveal the top seven SEO trends you need to know in 2017 to ensure you achieve and maintain elevated Google rankings. Note: Backlinks are still 1 of the top 2 Google ranking factors. That's widely covered elsewhere so I haven't included it as a new trend here. Modern link building should always be a priority! In this article you will notice a constant over-arching theme: The key to pleasing Google is to create thought-leading, conversational content littered with semantic and related keywords. Exceptional user experience must be first in mind. Let's break that down into the most important, specific trends: RankBrain is Google's name for its innovative, machine-learning artificial intelligence algorithm that's used to help process search results. It's been officially named Google's third-most important ranking factor. This algorithm uses machine learning - a computer's ability to teach itself and improve without human input. Yes, AI as we know it...and albeit a little scary, but it's also endlessly fascinating. One of RankBrain's primary purposes is to sift through what it deems to be relevant search results when a user enters a never-before-seen or otherwise unusual query (and learn from the user's consequent actions) - something Google has found difficult to interpret in the past. Not only that, expect it to be constantly learning how to best serve visitors with the most relevant, authoritative, and trustworthy results available - minus the public updates of algorithm changes we've received in the past. That means Google is becoming increasingly difficult to game these days. Simply put: You are better off focusing on publishing truly remarkable content that aligns with your target audience (while ensuring your website is similarly outstanding to navigate) and distributing this content heavily, rather than attempting to use, for example, isolated black hat tactics. This all places great value on superior content - which is why you need to be producing material of such caliber in 2017. Yes, this has been the case for a few years now and we've all had it drilled into us, but RankBrain really hits it home and the ""golden age"" of SEO dark arts on Google is truly dead. Search Engine Optimization on Google is evolving faster than ever before and you must shift your strategy to accommodate progressively smart machine intelligence that seeks integrity. Remember to: Research content topics thoroughly and cater for users' pain points. Apply semantic keyword strategies. Produce thought-leading content. Distribute your content heavily across all mediums. Ensure your website is easy to navigate. According to studies, up to 60 percent of Google searches now come via mobile . You no longer can afford to ignore mobile users. Accelerated Mobile Pages are quick to load and demand a set of technical criteria websites need to follow to provide for this increase in speed. As of the publishing date, AMP is not yet a ranking factor...but it makes sense to implement, for numerous reasons. Google has already revealed that mobile speed and usability are primary indexing metrics and this will only become increasingly prevalent as more people use mobile devices for search. Not only that; research shows slow sites correspond with lower engagement (up to 40 percent of visitors will leave if a site doesn't load after three seconds). Yet, only 23 percent of SEOs report implementing AMP! Developing AMP is a huge opportunity to jump ahead of your competition. Seriously - take advantage and become an early adopter. We've sure come a long way since 10 blue links on page 1 of Google. The Knowledge Graph and Rich Snippets have filled the SERPs with attractive content that includes imagery, review stars, bullet points and accordion-style elements (to name a few) that can be clicked and expanded right there on Google itself. This type of result can be achieved by implementing Structured Data markup on your website . Structured Data is often collectively referred to as Schema or Microdata, although Rich Snippets and Schema actually have different meanings . Despite what could be deemed as unnecessary to click due to the already-visible details, if you were going to click-through...what would you select? The below result or a standard blue link? I know what I'd choose. The number of search results showing rich snippets has already doubled in the last two years, and it doesn't look to be slowing down any time soon. There have been many studies indicating a solid increase in CTR after implementing Schema for Rich Snippets. Not only that, when you apply Structured Data you are helping Google understand the deeper meaning and context of your content so it can better accommodate relevant search queries. Here is a brilliant guide on Schema implementation you can pass to your team of developers. Mark up your site as much as possible and keep your eye on developments, including new definitions. You can technically describe a lot of varied attributes to Google - no matter what your sector. Rich Snippets are here to stay. As voice recognition software like Siri, Cortana, Google Voice, and Amazon Echo becomes more sophisticated, more of us will begin to use voice search to find information. In fact, the speech recognition error rate has decreased from 25 percent to 8 percent and the use of voice search has increased sevenfold since 2010 . Needless to say: Voice search is on the rise, particularly among millennials . Paired with RankBrain, the most fascinating side to this trend is that Google will begin to display results based more on user intent rather than the face-value of words that are entered into the search bar. That means keyword optimization will become more conversational in 2017. How can you capitalize on this trend? Prioritize long-tail keywords (as people are more likely to speak a lengthy search question than type one). Use Structured Data to provide additional context. Create FAQs that answer who, what, when, where, and why questions. Apply video as part of your SEO strategy . Research conversational queries using Google's related searches and the suggestions in its autosuggest feature. Listen on social media and actually take the time to interact with your customers to learn what they are actively searching for. Trust is essential online. People like to feel secure when visiting a new site and many visual indications encourage that feeling. One good visual indication a site is generally considered safe to visit is the green padlock in the URL bar. Google recognizes this and gives priority to websites that apply an SSL or the newer TLS encryption (aka HTTPS sites). And that's not all. As of January 2017, the world's most popular web browser (Google Chrome) will begin labeling non-HTTPs sites that transmit passwords and/or ask for credit card details as ""Not Secure."" The signs from Google are all there and shouldn't be ignored. As the constant focus on trust intensifies, HTTPS is expected to become ever-more important moving forward. Remember: Thoroughly plan your site-wide redirect from HTTP to HTTPS - it can be a very complex process with many pitfalls. Apply Google's UTM tracking for marketing campaigns and consequently accurate incoming referral data. Maintain outgoing referral data using the meta referrer tag in the HTML of your site. Recently, ""epic"" and insanely long blog posts have become the go-to for achieving more organic real-estate. It was found that the majority of articles in the top 10 results of Google were 2,000 words or longer. But while there long-form content continues to perform well, there are a couple of caveats: People want more value per word and Google's new mobile index naturally prefers fast-loading pages. Our attention spans are becoming shorter and Google is shifting to accommodate. We're tired of reading 2,000 words of fluff. In an informational age, we want our articles to be well organized and get to the point. Yet, there is still room for long-form content - if it's remarkable. We'll begin to see shorter-form content with much more ""density""; meaning more information in fewer words. To take advantage of this trend, be ruthless in editing and cut the fat from your articles. Always consider Google's tendency to rank speedy pages ; don't be afraid to trim a 3,000-word post to 2,000 or less and remove images or fancy elements - as long as it makes sense to do so. Last but not least, let's talk about the absolutely essential practice of personal branding. Personal branding is highlighting a real person as a (or the ) face of your company. If you want to beat Google's algorithms - personal branding is exactly how to do it. How many people in your industry have your name? And if you make it big, who will appear in Google search? It will be you. Each and every time. Not only that: People want to do business with people, not corporations! I cannot stress this enough. It all loops back to trust. Personal branding is so immensely powerful and instrumental in securing guest posts, brand mentions, links, webinars, podcasts, partnerships, and, of course, customers. I can 100 percent confirm it from experience. Additionally, social media platforms like Facebook continually refine their algorithms to favor profile posts over page posts, meaning less visibility for social updates that derive from brand accounts. The solution? Elect your CEO (or multiple employees willing to become long-standing faces of your corporation) to begin continually pushing themselves hard through blog posts, social media, and elsewhere. These people must build many industry relationships and dedicate their time to creating a strong presence where they are seen to be a thought-leader. Once they become ever-more known and sought-after - so does your brand. It's a no-brainer - personal branding should be one of your top SEO strategies for 2017. For a long time, brands have ignored Bing (and other search engines) because Google held the vast majority of search traffic. Well, times are changing - Bing now accounts for 21 percent of all search engine traffic and interestingly, 25 percent of all Bing's search traffic is voice-driven. Even Yahoo! is rising up the ranks with a 12 percent share of search traffic. This is quite simply, exciting news for all. Google's tight grip on the search industry has meant failure for many businesses that couldn't keep up in the past. Bing is much more straight-forward to optimize for. It leans toward analysis of traditional metrics, including: Inbound anchor text Click-through-rate (CTR) Visitor engagement Keywords in domains Content and keyword density Site structure and code Additionally, Bing cares more about social media sharing for rankings than Google does - so start getting those social shares in conjunction with your personal brand! Still need another reason to focus on Bing? Experts have found that a well-optimized website has a higher conversion rate when visited through Bing or Yahoo! than on Google. Get started today by creating a Bing Webmaster Tools account which also accommodates Yahoo! data. You can't afford to neglect a combined 33 percent search volume share, after all. Whether you're a search engine optimization expert or in charge of overall digital strategy, knowing the top SEO trends for 2017 is critical for the improvement and maintenance of your brand's Google rankings now and in future. As mentioned at the beginning, it all boils down to creating remarkable content that contains semantic and related keywords (in an informal, conversational tone). Your website must also provide an equally exceptional user-experience. Note: Be done with any intrusive pop-ups. They will negatively impact your Google rankings as of January 2017. Really listen to your audience to discover their pain points and any questions they may have. Then, focus on providing helpful answers with your content. In the end, as always, what matters most is providing value to your visitors. Value generates attention, trust, social shares, backlinks, and ultimately top Google rankings. Remember: Backlinks, content, and RankBrain are Google's top three ranking signals, likely in that order. Vastly distribute your content, utilizing personal brands. Ensure your website is technically sound in terms of Structured Data, AMP, and SSL / TLS. Don't put all your eggs in one basket - optimize for Bing and Yahoo! Too.",en,48
383,1284,1465274456,CONTENT SHARED,8962537427807366481,-1032019229384696495,-7696592431575292648,,,,HTML,http://www.wsj.com/articles/why-its-time-to-take-googles-pc-operating-system-seriously-1465185662,why it's time to take google's pc operating system seriously,"For most of my working life, I've used Macs at home and Windows PCs at work. But a couple of years ago, intrigued by a change in the winds of computing-from the desktop to the cloud-I decided to give Google's Chromebooks a chance. Now, to my surprise, I use the Chrome operating system for all my nonmobile computing. Thanks to continuous...",en,48
384,1456,1466432377,CONTENT SHARED,-5253644367331262405,-2626634673110551643,-2494564909080724082,,,,HTML,https://www.oreilly.com/learning/hello-tensorflow,"hello, tensorflow!","Braided river. (source: National Park Service, Alaska Region on Flickr ). For more on basic techniques and coding your own machine learning algorithms, check out our O'Reilly Learning Path, ""Machine Learning."" The TensorFlow project is bigger than you might realize. The fact that it's a library for deep learning, and its connection to Google, has helped TensorFlow attract a lot of attention. But beyond the hype, there are unique elements to the project that are worthy of closer inspection: The core library is suited to a broad family of machine learning techniques, not ""just"" deep learning. Linear algebra and other internals are prominently exposed. In addition to the core machine learning functionality, TensorFlow also includes its own logging system, its own interactive log visualizer, and even its own heavily engineered serving architecture. The execution model for TensorFlow differs from Python's scikit-learn, or most tools in R. Cool stuff, but-especially for someone hoping to explore machine learning for the first time-TensorFlow can be a lot to take in. How does TensorFlow work? Let's break it down so we can see and understand every moving part. We'll explore the data flow graph that defines the computations your data will undergo, how to train models with gradient descent using TensorFlow, and how TensorBoard can visualize your TensorFlow work. The examples here won't solve industrial machine learning problems, but they'll help you understand the components underlying everything built with TensorFlow, including whatever you build next! Names and execution in Python and TensorFlow The way TensorFlow manages computation is not totally different from the way Python usually does. With both, it's important to remember, to paraphrase Hadley Wickham , that an object has no name (see Figure 1). In order to see the similarities (and differences) between how Python and TensorFlow work, let's look at how they refer to objects and handle evaluation. Figure 1. Names ""have"" objects, rather than the reverse. Image courtesy of Hadley Wickham, used with permission. The variable names in Python code aren't what they represent; they're just pointing at objects. So, when you say in Python that foo = [] and bar = foo , it isn't just that foo equals bar ; foo is bar , in the sense that they both point at the same list object. You can also see that id(foo) and id(bar) are the same. This identity, especially with mutable data structures like lists, can lead to surprising bugs when it's misunderstood. Internally, Python manages all your objects and keeps track of your variable names and which objects they refer to. The TensorFlow graph represents another layer of this kind of management; as we'll see, Python names will refer to objects that connect to more granular and managed TensorFlow graph operations. When you enter a Python expression, for example at an interactive interpreter or Read Evaluate Print Loop (REPL), whatever is read is almost always evaluated right away. Python is eager to do what you tell it. So, if I tell Python to foo.append(bar) , it appends right away, even if I never use foo again. A lazier alternative would be to just remember that I said foo.append(bar) , and if I ever evaluate foo at some point in the future, Python could do the append then. This would be closer to how TensorFlow behaves, where defining relationships is entirely separate from evaluating what the results are. TensorFlow separates the definition of computations from their execution even further by having them happen in separate places: a graph defines the operations, but the operations only happen within a session. Graphs and sessions are created independently. A graph is like a blueprint, and a session is like a construction site. Back to our plain Python example, recall that foo and bar refer to the same list. By appending bar into foo , we've put a list inside itself. You could think of this structure as a graph with one node, pointing to itself. Nesting lists is one way to represent a graph structure like a TensorFlow computation graph. Real TensorFlow graphs will be more interesting than this! The simplest TensorFlow graph To start getting our hands dirty, let's create the simplest TensorFlow graph we can, from the ground up. TensorFlow is admirably easier to install than some other frameworks. The examples here work with either Python 2.7 or 3.3+, and the TensorFlow version used is 0.8. At this point TensorFlow has already started managing a lot of state for us. There's already an implicit default graph, for example. Internally , the default graph lives in the _default_graph_stack , but we don't have access to that directly. We use tf.get_default_graph() . The nodes of the TensorFlow graph are called ""operations,"" or ""ops."" We can see what operations are in the graph with graph.get_operations() . Currently, there isn't anything in the graph. We'll need to put everything we want TensorFlow to compute into that graph. Let's start with a simple constant input value of one. That constant now lives as a node, an operation, in the graph. The Python variable name input_value refers indirectly to that operation, but we can also find the operation in the default graph. TensorFlow uses protocol buffers internally. ( Protocol buffers are sort of like a Google-strength JSON .) Printing the node_def for the constant operation above shows what's in TensorFlow's protocol buffer representation for the number one. People new to TensorFlow sometimes wonder why there's all this fuss about making ""TensorFlow versions"" of things. Why can't we just use a normal Python variable without also defining a TensorFlow object? One of the TensorFlow tutorials has an explanation: To do efficient numerical computing in Python, we typically use libraries like NumPy that do expensive operations such as matrix multiplication outside Python, using highly efficient code implemented in another language. Unfortunately, there can still be a lot of overhead from switching back to Python every operation. This overhead is especially bad if you want to run computations on GPUs or in a distributed manner, where there can be a high cost to transferring data. TensorFlow also does its heavy lifting outside Python, but it takes things a step further to avoid this overhead. Instead of running a single expensive operation independently from Python, TensorFlow lets us describe a graph of interacting operations that run entirely outside Python. This approach is similar to that used in Theano or Torch. TensorFlow can do a lot of great things, but it can only work with what's been explicitly given to it. This is true even for a single constant. If we inspect our input_value , we see it is a constant 32-bit float tensor of no dimension: just one number. Note that this doesn't tell us what that number is . To evaluate input_value and get a numerical value out, we need to create a ""session"" where graph operations can be evaluated and then explicitly ask to evaluate or ""run"" input_value . (The session picks up the default graph by default.) It may feel a little strange to ""run"" a constant. But it isn't so different from evaluating an expression as usual in Python; it's just that TensorFlow is managing its own space of things-the computational graph-and it has its own method of evaluation. The simplest TensorFlow neuron Now that we have a session with a simple graph, let's build a neuron with just one parameter, or weight. Often, even simple neurons also have a bias term and a non-identity activation function, but we'll leave these out. The neuron's weight isn't going to be constant; we expect it to change in order to learn based on the ""true"" input and output we use for training. The weight will be a TensorFlow variable . We'll give that variable a starting value of 0.8. You might expect that adding a variable would add one operation to the graph, but in fact that one line adds four operations. We can check all the operation names: We won't want to follow every operation individually for long, but it will be nice to see at least one that feels like a real computation. Now there are six operations in the graph, and the last one is that multiplication. This shows how the multiplication operation tracks where its inputs come from: they come from other operations in the graph. To understand a whole graph, following references this way quickly becomes tedious for humans. TensorBoard graph visualization is designed to help. How do we find out what the product is? We have to ""run"" the output_value operation. But that operation depends on a variable: weight . We told TensorFlow that the initial value of weight should be 0.8, but the value hasn't yet been set in the current session. The tf.initialize_all_variables() function generates an operation which will initialize all our variables (in this case just one) and then we can run that operation. The result of tf.initialize_all_variables() will include initializers for all the variables currently in the graph, so if you add more variables you'll want to use tf.initialize_all_variables() again; a stale init wouldn't include the new variables. Now we're ready to run the output_value operation. Recall that's 0.8 * 1.0 with 32-bit floats, and 32-bit floats have a hard time with 0.8; 0.80000001 is as close as they can get. See your graph in TensorBoard Up to this point, the graph has been simple, but it would already be nice to see it represented in a diagram. We'll use TensorBoard to generate that diagram. TensorBoard reads the name field that is stored inside each operation (quite distinct from Python variable names). We can use these TensorFlow names and switch to more conventional Python variable names. Using tf.mul here is equivalent to our earlier use of just * for multiplication, but it lets us set the name for the operation. TensorBoard works by looking at a directory of output created from TensorFlow sessions. We can write this output with a SummaryWriter , and if we do nothing aside from creating one with a graph, it will just write out that graph. The first argument when creating the SummaryWriter is an output directory name, which will be created if it doesn't exist. Now, at the command line, we can start up TensorBoard. TensorBoard runs as a local web app, on port 6006. (""6006"" is ""goog"" upside-down.) If you go in a browser to localhost:6006/#graphs you should see a diagram of the graph you created in TensorFlow, which looks something like Figure 2. Figure 2. A TensorBoard visualization of the simplest TensorFlow neuron. Making the neuron learn Now that we've built our neuron, how does it learn? We set up an input value of 1.0. Let's say the correct output value is zero. That is, we have a very simple ""training set"" of just one example with one feature, which has the value one, and one label, which is zero. We want the neuron to learn the function taking one to zero. Currently, the system takes the input one and returns 0.8, which is not correct. We need a way to measure how wrong the system is. We'll call that measure of wrongness the ""loss"" and give our system the goal of minimizing the loss. If the loss can be negative, then minimizing it could be silly, so let's make the loss the square of the difference between the current output and the desired output. So far, nothing in the graph does any learning. For that, we need an optimizer. We'll use a gradient descent optimizer so that we can update the weight based on the derivative of the loss. The optimizer takes a learning rate to moderate the size of the updates, which we'll set at 0.025. The optimizer is remarkably clever. It can automatically work out and apply the appropriate gradients through a whole network, carrying out the backward step for learning. Let's see what the gradient looks like for our simple example. Why is the value of the gradient 1.6? Our loss is error squared, and the derivative of that is two times the error. Currently the system says 0.8 instead of 0, so the error is 0.8, and two times 0.8 is 1.6. It's working! For more complex systems, it will be very nice indeed that TensorFlow calculates and then applies these gradients for us automatically. Let's apply the gradient, finishing the backpropagation. The weight decreased by 0.04 because the optimizer subtracted the gradient times the learning rate, 1.6 * 0.025, pushing the weight in the right direction. Instead of hand-holding the optimizer like this, we can make one operation that calculates and applies the gradients: the train_step . Running the training step many times, the weight and the output value are now very close to zero. The neuron has learned! Training diagnostics in TensorBoard We may be interested in what's happening during training. Say we want to follow what our system is predicting at every training step. We could print from inside the training loop. This works, but there are some problems. It's hard to understand a list of numbers. A plot would be better. And even with only one value to monitor, there's too much output to read. We're likely to want to monitor many things. It would be nice to record everything in some organized way. Luckily, the same system that we used earlier to visualize the graph also has just the mechanisms we need. We instrument the computation graph by adding operations that summarize its state. Here, we'll create an operation that reports the current value of y , the neuron's current output. When you run a summary operation, it returns a string of protocol buffer text that can be written to a log directory with a SummaryWriter . Now after running tensorboard --logdir=log_simple_stat , you get an interactive plot at localhost:6006/#events (Figure 3). Figure 3. A TensorBoard visualization of a neuron's output against training iteration number. Flowing onward Here's a final version of the code. It's fairly minimal, with every part showing useful (and understandable) TensorFlow functionality. The example we just ran through is even simpler than the ones that inspired it in Michael Nielsen's Neural Networks and Deep Learning . For myself, seeing details like these helps with understanding and building more complex systems that use and extend from simple building blocks. Part of the beauty of TensorFlow is how flexibly you can build complex systems from simpler components. If you want to continue experimenting with TensorFlow, it might be fun to start making more interesting neurons, perhaps with different activation functions . You could train with more interesting data. You could add more neurons. You could add more layers. You could dive into more complex pre-built models , or spend more time with TensorFlow's own tutorials and how-to guides . Go for it!",en,47
385,2527,1475783563,CONTENT SHARED,-5658245291907121574,-1032019229384696495,916944361140684324,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.50 Safari/537.36",NY,US,HTML,https://medium.com/@srobtweets/machine-learning-and-the-vp-debate-36dd6e1862b7,machine learning and the vp debate,"Using a similar approach to my Twitter analysis here , I analyzed tweets from last nights VP debate with the Cloud Natural Language API, BigQuery, and Exploratory for visualization. This time, in addition to running syntax analysis on every tweet I also used the NL API's sentiment analysis feature: Twitter sentiment during the debate The NL API returns two values for sentiment: polarity and magnitude . polarity is a number from -1 to 1 indicating how positive or negative the text is. magnitude indicates the overall strength of the statement regardless of whether it is positive or negative, and is a number ranging from 0 to infinity. A good way to gauge sentiment is to multiply the two values so that statements with a stronger sentiment (higher magnitude) are weighted accordingly. I used the Twitter npm package to stream tweets, filtering on the following search terms: With all the tweets I collected (299k total) as rows in a BigQuery table, I wrote the following query to get the average sentiment for all tweets in a given minute: Then I graphed it with Exploratory (thanks Felipe Hoffa for showing me this awesome new data viz tool!): The graph starts at 8pm Eastern (an hour before the debate) and ends at 5am the day after (October 5). We can see that sentiment fluctuated during the debate (from 9 - 10:30pm), but was largely negative in the hours that followed, specifically from 1 to 3am. This is around the same time news outlets started publishing articles analyzing the outcome of the debate. While I haven't yet come across a timed transcript of the debates, bonus points for anyone who wants to feed an audio file of the debate to the Speech API , run it through NL for sentiment analysis and compare it to the graph above. We can also get the average sentiment for all tweets collected during the debates (including a few hours before and after): And here's the result: We can compare this to the sentiment for tweets mentioning tax by adding the following to our query: Syntactic analysis Using the NL API's text annotation method, we can break down a tweet by parts of speech and use BigQuery to find linguistic trends. For each sentence, the NL API will tell us which word is the subject (labeled as NSUBJ). Since I've got the JSON response from the NL API saved in BigQuery, I can write a user-defined function to find the top subjects in tweets about the VP Debate: And graph the results: Interestingly, there were many more tweets about Pence than Kaine (48k vs 34k). Top debate emojis Last but not least, how did people express their feelings about the debate in emojis? Here are the results in an emoji tag cloud: I'm not sure how the taco emoji snuck in there, but I'm guessing it has something to do with October 4th being National Taco Day . What's next Have questions or more ideas for natural language processing? Find me on Twitter @SRobTweets or let me know what you think in the comments. And here's are the tools I used:",en,47
386,2565,1476455711,CONTENT SHARED,-1452340812018195881,3609194402293569455,-1767084608010459028,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36",SP,BR,HTML,http://www.diolinux.com.br/2016/10/globo-g1-lanca-bot-para-o-telegram-enem.html,globo g1 lança bot para o telegram que te ajuda a estudar para o enem,"Quem está começando a ficar preocupado com as provas do Enem deste ano e está revirando os livros para se dar bem nas provas pode usar a tecnologia para deixar os seus estudos mais eficientes e mais divertidos. Estudar não precisa ser algo complicado, aplicativos e sites voltados para passar conhecimento existem aos montes, recentemente até nós lançamos o sistema de educação à distância do Diolinux , então nada mais natural do que usar a tecnologia a nosso favor para melhorarmos as nossas habilidades, especialmente em períodos de prova como o Enem e vestibulares. Eu conheci hoje um Bot do Telegram que te ajuda a fazer isso, ele foi produzido pelo pessoal do portal G1 da Globo e é muito interessante. O que é um Bot do Telegram? Os Bots são um dos recursos mais legais do Telegram, o aplicativo de mensagens concorrente do WhatsApp, nós já falamos sobre ele diversas vezes aqui no blog e uma das grandes vantagens é poder utilizá-lo também no computador sem necessidade da ligação com o Smartphone, como o WhatsApp faz, - você pode aprender a instalar o Telegram Desktop aqui - além disso, ele possui vários recursos interessantes, um deles são os bots. Como o nome sugere, os bots são ""robôs"", programas de computador que podem automatizar várias tarefas diferentes, este Bot do G1 funciona praticamente como um jogo onde você conversa com ele como se ele fosse um simples usuários pautando os assuntos que você quer estudar. Como você pode ver na imagem acima, o funciona é simples o ""papo"" que você tem com ele é bem descontraído, as perguntas são interativas e você tem a resposta na hora. Caso você erre, ele vai te informar do equívoco e te dizer qual é a resposta correta: Para adicionar este Bot ao seu Telegram é muito simples, basta clicar no botão abaixo, lembrando que você precisa já ter o Telegram instalado para começar a utilizá-lo. _____________________________________________________________________________ Viu algum erro ou gostaria de adicionar alguma sugestão a essa matéria? Colabore, clique aqui.",pt,47
387,1093,1464111011,CONTENT SHARED,-3750879736572068916,1895326251577378793,4037412057375119343,,,,HTML,http://www.businessinsider.com/apple-is-the-next-microsoft-2016-5,"no, apple isn't the next blackberry - it's the next microsoft","Business Insider Over the weekend, long-time iOS developer and serial entrepreneur Marco Arment made a splash when he aired his concerns that without a serious investment in artificial intelligence, Apple could go the way of BlackBerry . ""Today, Amazon, Facebook, and Google are placing large bets on advanced AI, ubiquitous assistants, and voice interfaces, hoping that these will become the next thing that our devices are for ,"" Arment wrote. ""If they're right - and that's a big 'if' - I'm worried for Apple."" After Arment's piece, the general consensus from Apple's legion of armchair quarterbacks seems to be that yeah, maybe Apple isn't investing in artificial intelligence today to the same degree as its peers, and that it's not playing a role in the larger AI community . But with $200-billion-plus in cash reserves, goes the sentiment, Apple can buy its way to relevance when they need to - a luxury that RIM, the company that later renamed itself BlackBerry, didn't have in 2007 when the iPhone launched. It's a nice idea on paper - Apple is the most valuable company in the world. And while Apple may have whiffed on iPhone sales this past quarter, it's still an incredible, and incredibly profitable business. Just remember that you can't always buy yourself into the next big thing. Which is to say, instead of looking at BlackBerry for historical context, try looking at Microsoft. When the iPhone launched in 2007, Microsoft was also caught flatfooted. While Microsoft had made some early investments in smartphones with its Windows Mobile business, and had much of the technology , the company's obsessive focus on the Windows PC kept it from pulling these mobile efforts together into a cohesive business. And so, in 2014, realizing that it had fallen far behind, former Microsoft CEO Steve Ballmer made what we now recognize as a tremendous mistake : He championed the purchase of Nokia's phone business for $7.2 billion in a Hail Mary play to make Windows on smartphones a thing. Microsoft It didn't work. Microsoft took a $7.6 billion writedown in 2015. And on Monday, research firm Gartner issued a smartphone market report indicating that Microsoft has less than one percent of overall share of the market. That's bad. It was also, in hindsight, totally avoidable if Microsoft had gotten its act together, sooner. The lesson for Apple, and Apple shareholders, is that it's not enough to have a ton of money and just throw around cash to solve every problem. It takes real vision, strategy, and execution, and a first-mover advantage is nice, too. It's important to remember, too, that while Microsoft is no longer the dominant company it once was, it hasn't just rolled over and died during Apple's march to the top. In fact, under new CEO Satya Nadella, Microsoft has rethought how it does business and is now going through a kind of critical renaissance across every product line (except smartphones). YouTube Similarly, it seems extremely unlikely that Apple is doomed. As much as we're all talking about voice assistants and virtual reality as the next big thing, we'll still need smartphones for a long time, and Apple has that market pretty well locked up. But as Arment notes, if Google et al are right about artificial intelligence, Apple will have to follow in Microsoft's footsteps and totally change its its modus operandi to embrace the future - not just with its checkbook, but with its culture and its vision. And that may prove more difficult than anybody gives it credit for. EXCLUSIVE FREE REPORT: 25 Big Tech Predictions by BI Intelligence. Get the Report Now """,en,47
388,2051,1471007990,CONTENT SHARED,-7387185690512762935,6971525809430309144,-2898569632633959942,,,,HTML,https://medium.freecodecamp.com/angular-2-versus-react-there-will-be-blood-66595faafd51?gi=8a3b4fcebe45,angular 2 versus react: there will be blood - free code camp,"Angular 2 versus React: There Will Be Blood Angular 2 has reached Beta and appears poised to become the hot new framework of 2016. It's time for a showdown. Let's see how it stacks up against 2015's darling: React . Disclaimer: I enjoyed working in Angular 1 but switched to React in 2015. I've published Pluralsight courses on React and Flux and React and Redux in ES6 ( free trial ). So yes, I'm biased. But I'm attacking both sides. Alright, let's do this. There will be blood. You're Comparing Apples and Orangutans! Sigh. Yes, Angular is a framework, React is a library. Some say this difference makes comparing them illogical. Not at all! Choosing between Angular and React is like choosing between buying an off-the-shelf computer and building your own with off-the-shelf parts. This post considers the merits of these two approaches. I compare React's syntax and component model to Angular's syntax and component model. This is like comparing an off-the-shelf computer's CPU to a raw CPU. Apples to apples. Angular 2 Advantages Let's start by considering Angular 2's advantages over React. Low Decision Fatigue Since Angular is a framework, it provides significantly more opinions and functionality out of the box. With React, you typically pull a number of other libraries off the shelf to build a real app. You'll likely want libraries for routing, enforcing unidirectional flows, web API calls, testing, dependency management, and so on. The number of decisions is pretty overwhelming. This is why React has so many starter kits (I've published two ). Angular offers more opinions out of the box, which helps you get started more quickly without feeling intimidated by decisions. This enforced consistency also helps new hires feel at home more quickly and makes switching developers between teams more practical. I admire how the Angular core team has embraced TypeScript, which leads to the next advantage... TypeScript = Clear Path Sure, TypeScript isn't loved by all, but Angular 2's opinionated take on which flavor of JavaScript to use is a big win. React examples across the web are frustratingly inconsistent - it's presented in ES5 and ES6 in roughly equal numbers, and it currently offers three different ways to declare components . This creates confusion for newcomers. (Angular also embraces decorators instead of extends - many would consider this a plus as well). While Angular 2 doesn't require TypeScript, the Angular core team certainly embraces it and defaults to using TypeScript in documentation. This means related examples and open source projects are more likely to feel familiar and consistent. Angular already provides clear examples that show how to utilize the TypeScript compiler . (though admittedly, not everyone is embracing TypeScript yet, but I suspect shortly after launch it'll become the de facto standard). This consistency should help avoid the confusion and decision overload that comes with getting started with React. Reduced Churn 2015 was the year of JavaScript fatigue . Although React itself is expected to be quite stable with version 15 coming soon , React's ecosystem has churned at a rapid pace, particularly around the long list of Flux flavors and routing . So anything you write in React today may feel out of date or require breaking changes in the future if you lean on one of many related libraries. In contrast, Angular 2 is a careful, methodical reinvention of a mature, comprehensive framework. So Angular is less likely to churn in painful ways after release. And as a full framework, when you choose Angular, you can trust a single team to make careful decisions about the future. In React, it's your responsibility to herd a bunch of disparate, fast-moving, open-source libraries into a comprehensive whole that plays well together. It's time-consuming, frustrating, and a never-ending job. Broad Tooling Support As you'll see below, I consider React's JSX a big win. However, you need to select tooling that supports JSX. React has become so popular that tooling support is rarely a problem today, but new tooling such as IDEs and linters are unlikely to support JSX on day one. Angular 2's templates store markup in a string or in separate HTML files, so it doesn't require special tooling support (though it appears tooling to intelligently parse Angular's string templates is on the way). Web Component Friendly Angular 2's design embraces the web component's standard. Sheesh, I'm embarrassed I forgot to mention this initially - I recently published a course on web components ! In short, the components that you build in Angular 2 should be much easier to convert into plain, native web components than React's components. Sure, browser support is still weak , but this could be a big win in the long-term. Angular's approach comes with its own set of gotchas, which is a good segue for discussing React's advantages... React Advantages Alright, let's consider what sets React apart. JSX JSX is an HTML-like syntax that compiles down to JavaScript. Markup and code are composed in the same file. This means code completion gives you a hand as you type references to your component's functions and variables. In contrast, Angular's string-based templates come with the usual downsides: No code coloring in many editors, limited code completion support, and run-time failures. You'd normally expect poor error messaging as well, but the Angular team created their own HTML parser to fix that . (Bravo!) If you don't like Angular string-based templates, you can move the templates to a separate file, but then you're back to what I call ""the old days:"" wiring the two files together in your head, with no code completion support or compile-time checking to assist. That doesn't seem like a big deal until you've enjoyed life in React. Composing components in a single compile-time checked file is one of the big reasons JSX is so special. For more on why JSX is such a big win, see JSX: The Other Side of the Coin . React Fails Fast and Explicitly When you make a typo in React's JSX, it won't compile. That's a beautiful thing. It means you know immediately exactly which line has an error. It tells you immediately when you forget to close a tag or reference a property that doesn't exist. In fact, the JSX compiler specifies the line number where the typo occurred . This behavior radically speeds development. In contrast, when you mistype a variable reference in Angular 2, nothing happens at all. Angular 2 fails quietly at run time instead of compile-time . It fails slowly . I load the app and wonder why my data isn't displaying. Not fun. React is JavaScript-Centric Here it is. This is the fundamental difference between React and Angular. Unfortunately, Angular 2 remains HTML-centric rather than JavaScript-centric . Angular 2 failed to solve its most fundamental design problem: Angular 2 continues to put ""JS"" into HTML. React puts ""HTML"" into JS. I can't emphasize the impact of this schism enough. It fundamentally impacts the development experience. Angular's HTML-centric design remains its greatest weakness. As I cover in "" JSX: The Other Side of the Coin "", JavaScript is far more powerful than HTML. Thus, it's more logical to enhance JavaScript to support markup than to enhance HTML to support logic . HTML and JavaScript need to be glued together somehow, and React's JavaScript-centric approach is fundamentally superior to Angular, Ember, and Knockout's HTML-centric approach. Here's why... React's JavaScript-centric design = simplicity Angular 2 continues Angular 1's approach of trying to make HTML more powerful. So you have to utilize Angular 2's unique syntax for simple tasks like looping and conditionals. For example, Angular 2 offers both one and two way binding via two syntaxes that are unfortunately quite different: In React, binding markup doesn't change based on this decision (it's handled elsewhere, as I'd argue it should be). In either case, it looks like this: Angular 2 supports inline master templates using this syntax: The above snippet loops over an array of heroes. I have multiple concerns: Declaring a ""master template"" via a preceeding asterisk is cryptic. The pound sign in front of hero declares a local template variable. This key concept looks like needless noise (if preferred, you can use `var`). The ngFor adds looping semantics to HTML via an Angular-specific attribute. Contrast Angular 2's syntax above with React's pure JS*: (admittedly the key property below is React-specific) Since JS supports looping natively, React's JSX can simply leverage all the power of JS for such things and do much more with map, filter, etc. Just read the Angular 2 Cheat Sheet . That's not HTML. That's not JavaScript. It's . To read Angular: Learn a long list of Angular-specific syntax. To read React: Learn JavaScript. React is unique in its syntactic and conceptual simplicity. Consider iterating in today's popular JS frameworks/libraries: All except React use framework specific replacements for something that is native and trivial in JavaScript: a loop . That's the beauty of React. It embraces the power of JavaScript to handle markup, so no odd new syntax is required. Angular 2's syntactic oddities continue with click binding: In contrast, React again uses plain 'ol JavaScript: And since React includes a synthetic event system (as does Angular 2), you don't have to worry about the performance implications of declaring event handlers inline like this. Why fill your head with a framework's unique syntax if you don't have to? Why not simply embrace the power of JS? Luxurious Development Experience JSX's code completion support, compile-time checks, and rich error messaging already create an excellent development experience that saves both typing and time. But combine all that with hot reloading with time travel and you have a rapid development experience like no other. Size Concerns Here's the sizes of some popular frameworks and libraries, minified ( source ): Edit : Sorry, I had incorrect numbers earlier that were for simple ToDoMVC apps instead of the raw frameworks. Also, the Angular 2 number is expected to drop for the final release. The sizes listed are for the framework, minified, in the browser (no gzip is factored in here). To make a real comparison, I built Angular 2's Tour of Heroes app in both Angular 2 and React (I used the new React Slingshot starter kit). The result? So Angular 2 is currently over four times the size of a React + Redux app of comparable simplicity . (Again, Angular 2 is expected to lose some weight before the final release). Now that said, I admit that concerns about the size of frameworks may be overblown: Large apps tend to have a minimum of several hundred kilobytes of code - often more - whether they're built with a framework or not. Developers need abstractions to build complex software, and whether they come from a framework or are hand-written, they negatively impact the performance of apps. Even if you were to eliminate frameworks entirely, many apps would still have hundreds of kilobytes of JavaScript. - Tom Dale in JavaScript Frameworks and Mobile Performance Tom is right. Frameworks like Angular and Ember are bigger because they offer many more features out of the box. However, my concern is this: many apps don't need everything these large frameworks put in the box. In a world that's increasingly embracing microservices, microapps, and single-responsibility packages , React gives you the power to ""right-size"" your application by carefully selecting only what is necessary . In a world with over 200,000 npm modules , that's a powerful place to be. React is a library. It's precisely the opposite philosophy of large, comprehensive frameworks like Angular and Ember. So when you select React, you're free to choose modern best-of-breed libraries that solve your problem best. JavaScript moves fast, and React allows you to swap out small pieces of your application for better libraries instead of waiting around and hoping your framework will innovate. Unix has stood the test of time. Here's why: The philosophy of small, composable, single-purpose tools never goes out of style. React is a focused, composable, single-purpose tool used by many of the largest websites in the world . That bodes well for its future (That said, Angular is used by many big names too). Showdown Summary Angular 2 is a huge improvement over version 1. The new component model is simpler to grasp than v1's directives, it supports isomorphic/universal rendering, and it uses a virtual DOM offering 3-10x improvements in performance. These changes make Angular 2 very competitive with React. There's no denying that its full-featured, opinionated nature offers some clear benefits by reducing ""JavaScript fatigue"". However, Angular 2's size and syntax give me pause. Angular's commitment to HTML-centric design makes it complex compared to React's simpler JavaScript-centric model. In React, you don't learn framework-specific HTML shims like ngWhatever. You spend your time writing plain 'ol JavaScript. That's the future I believe in.",en,47
389,2853,1481544116,CONTENT SHARED,-3020916840917845519,801895594717772308,-5831772257995241402,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.98 Safari/537.36",MG,BR,HTML,https://blog.daftcode.pl/hype-driven-development-3469fc2e9b22?gi=227fe4fcd5a1,hype driven development,"Software development teams often make decisions about software architecture or technological stack based on inaccurate opinions, social media, and in general on what is considered to be ""hot"", rather than solid research and any serious consideration of expected impact on their projects. I call this trend Hype Driven Development, perceive it harmful and advocate for a more professional approach I call ""Solid Software Engineering"". Learn more about how it works and find out what you can do instead. New technology - new hope Have you seen it? A team picking newest, hottest technology to apply in the project. Someone reads a blog post, it's trending on Twitter and we just came back from a conference where there was a great talk about it. Soon after, the team starts using this new shiny technology (or software architecture design paradigm), but instead of going faster (as promised) and building a better product they get into trouble. They slow down, get demotivated, have problems delivering next working version to production. Some teams even keep fixing bugs instead of delivering new features. They need 'just a few more days' to sort it all out. Hype Driven Development Hype Driven Development (HDD) has many flavors and touches your project in many different ways: Reddit driven development  - when a team or individual decide on technology/architecture/design based on what popular blogger wrote or what is hot on reddit, hackernews, blogs twitter, facebook, GitHub or other social media. Conference driven development  - watch carefully what happens after people are back from conference. People get inspired. And that's a two-edged sword. Starting to use newest hottest lib/framework/architecture paradigm without enough research might be a highway to hell. Loudest guy driven decisions  - is when one guy is talking all the time about this new framework/lib/tech's that he has no experience with, but talks about it all the time and finally the team decides to use it. Gem/lib/plugin driven development  - a specially strong in Ruby On Rails community, where occasionally I can see a Gemfile so long that the only thing longer is the time it takes to load the app. It comes from the idea that every problem in rails should be solved with a gem. Sometimes it would take a couple of lines to build solution ourselves. But we're just solving problems by adding libs, plugins, gems or frameworks. I would also mention here behavior popular among hype driven developers -  Stack Overflow driven development  - when developers copy-paste solutions from Stackoverflow (or in general from the internet) without really understanding them. HDD is how teams bring doom on themselves The problem with hype is that it easily leads to bad decisions. Both bad architectural decisions and technological stack decisions often hunt a team months or even years later . In worst case they may lead to another very problematic situation in software engineering: The Big Rewrite . Which almost never works out. The root of all evil seems to be social media  - where new ideas spread much faster than they get tested.Muchfaster than people are able to understand their pros and cons. The Anatomy of Hype Most hypes have similar structure. Here it goes: Step 1: Real problem and solution They start in some company with a problem. A team within some company decides that solution to the problem is beyond the current technological stack, process or architecture. The company creates a new framework, library or paradigm and soon the problem is solved. Step 2: Announcement, buzz and keywords The team is excited to show their work to the rest of the world and soon they write blog posts and do talks on conferences. The problem oftentimes is non-trivial, so they are proud to present the impressive results of a non-trivial solution. People get excited about the new technology. The only problem is not everybody who gets excited is able to fully understand what the exact problem was and all the details of the solution. It was a non-trivial problem with a non-trivial solution after all. Takes more than a tweet, chit-chat or even blog post to explain. With communication tools like social media, blog posts and conference lightning talks the message gets blurred along the way. Step 3: Mania starts All shades of hype driven developers read blog posts and attend conferences. Soon the teams all over the world start using the new technology. Due to the blurred message - some of them make hasty decision to use framework even though it does not solve any of their actual problems. Yet the team does have expectation that this new technology will help. Step 4: Disappointment As the sprints go by, the technology does not improve the team's life as much as people hoped, but brings a lot of extra work. There's a lot of rewriting the code and extra learning for the team. Teams slow down, management gets pissed off. People feel cheated. Step 5: Realisation! Finally the team does retrospection and realizes what are the tradeoffs of the new technology and for what purpose it would be more relevant. They get wiser... till the next hype shows up. Examples of Hype: Let's examine some examples of hypes and see how those went through. Example 1: React.js Step 1: Facebook has a problem - advanced one page apps like Facebook itselves have, so many state changing events that it is hard to keep track what's going on and keep the application state consistent. Step 2: Facebook promotes new paradigm with buzzwords: functional, virtual DOM, components. Step 3: Mania: Facebook has created the front-end framework of the future! Let's write everything in react from now on! Step 4: Wait there is a lot of work, but no quick return on investment! Step 5: React is great for advanced one page app with lots of real-time notifications, but does not necessarily pay off for simpler applications. Example 2: TDD is dead by DHH Step 1: David Heinemeier Hansson (DHH, creator of Ruby on Rails framework) realises that it is hard to do TDD with Rails as this framework doesn't have architecture supporting good OOP. Makes a pragmatic choice - not to write tests upfront. Step 2: Hype starts with DHH blog post and conference talk . Hype keywords: TDD is DEAD. Step 3: Let's skip tests! Our Guru says so. We didn't write them anyway. Now we're at least not pretending. We're finally honest. Step 4: Wait! Even fewer things work now than before. We've built a buggy code. Step 5: ""TDD is not dead or alive. TDD is subject to tradeoffs, including risk of API changes, skill of practitioner and existing design"" - Kent Beck. Example 3: Microservices Step 1: Big monolith application scales hard. There is a point when we can break them down into services. It will be easier to scale in terms of req/sec and easier to scale across multiple teams. Step 2: Hype keywords: scalability, loose coupling, monolith. Step 3: Let's rewrite all to services! We have a 'spaghetti code' because we have a monolith architecture! We need to rewrite everything to microservices! Step 4: Shit! It is now way slower to develop the app, difficult to deploy and we spend a lot of time tracking bugs across multiple systems. Step 5: Microservices require a lot of devops skills in the team and with right investment might pay off as a way to scale the system and team. Before you reach serious scale issues it's an overinvestment. Microservices are extracted not written. You must be this tall to use microservices. Example 4: NoSQL Step 1: SQL databases have problems with high loads and unstructured data. Teams around the world start developing new generation of databases. Step 2: Hype keywords: Scalability, BigData, High Performance. Step 3: Our database is too slow and not big enough! We need NoSql! Step 4: We need to join tables? That is a no go. Simple SQL operations are becoming increasingly challenging. Development is slow and our core problems are not solved. Step 5: NoSql are tools to solve very specific problems (either extremely high volumes of data, unstructured data or very high load). SQL is actually a great tool and handles high load and huge data volumes well if used skillfully. The case for NoSql is still pretty rare in 2016. Example 5: Elixir and Phoenix (or put your favorite lang/framework pair here) Step 1: Web frameworks like Ruby On Rails don't deal well with high performance applications, distributed applications and websockets. Step 2: Hype keywords: Scalability, High Performance, Distributed, Fault-tolerant. Step 3: Oh my good, our application is slow and our chat is not scalable! Step 4: Wow, learning functional programming and distributed approach is not that easy. We are now really slow. Step 5: Elixir and Phoenix is great framework, but takes significant effort to learn. It will pay back in a long run if you need specifically high performance app. The list goes on and on: In this crowded space of computer engineering we have a lot of areas where hypes are common. In JavaScript world new frameworks are born everyday. Node.js (keywords: event programming), reactive programming, Meteor.js (keywords: shared state), front-end MVC, React.js. You name it. In software engineering new architectures are born: Domain Driven Development, Hexagon, DCI. What is your favorite hype?",en,47
390,2207,1472235024,CONTENT SHARED,8626761683919164251,6735372008307093370,1984623151613671672,,,,HTML,http://www.brasilpost.com.br/2016/08/25/como-sair-armario-amigos_n_11668956.html,tem hora certa pra sair do armário?,"Publicado: Para qualquer LGBT, viver dentro do armário sufoca. Seja aos 15 ou aos 30, a sensação de ter uma parte da sua vida não compartilhada com quem você gosta e admira não faz bem a ninguém. Dica #1: saiba que é algo recorrente No curta 'Reencontro' da série 'Renografias' da Renault, três amigos refazem uma viagem da juventude, mas apesar de todas as boas lembranças, um deles não consegue mais se divertir como antes, pois guarda em segredo sua orientação sexual. A viagem foi o momento ideal para o personagem contar aos amigos que é gay. Mas, será que tem uma hora certa pra isso? Fomos investigar e descobrirmos três dicas que podem ajudar nesta decisão. Eu sou gay! Eu sou gay! Eu sou gay! Eu sou gay! Eu sou gay! Para muitas pessoas, sair do armário é tão difícil porque significa enfrentar o medo de ser apontado como ""anormal"". Por isso, entender mais sobre sexualidade ajuda a perceber que não existe nada de errado com os seus desejos. Dica #2: Inspire-se nas experiências de outras pessoas Entender o assunto também faz você se preparar para inseguranças que surgirão no caminho. Para a professora de antropologia Larissa Pelúcio, que pesquisa gênero e sexualidade na Unesp de Bauru (SP), um dos primeiros obstáculos está logo ali no seu círculo de convívio. ""A dificuldade de sair do armário vem da percepção de que isso te coloca em vários campos de risco. O primeiro deles é nas suas relações de afeto. É sentir que você está frustrando pessoas que não poderia frustrar"", ela explica. Conhecer melhor o jeito como a sociedade funciona também faz você perceber fatos que nunca tinha pensado. Por exemplo, que sair do armário é um processo que se repetirá várias vezes na vida. ""Você vai administrar esta informação a cada nova relação que estabelecer, seja profissional ou pessoal. Então é uma 'economia do segredo', você vai continuar considerando perdas e ganhos para sair menos machucado"". ""Como sair do armário"". Pesquisar. Dica #3: Conte com um psicólogo Ler outros texto na internet pode ajudar - e muito! Um dos maiores endereços em português sobre o tema, o blog Saindo do Armário , tem exemplos disso. ""Eu queria uma forma de ajudar outras pessoas que estavam passando por situações semelhantes"", conta Leandro Guedes, 31 anos, criador do site. ""As pessoas procuram conforto no blog, querem se sentir aliviadas lendo a história de alguém e vendo que pode dar certo"". Aqui no HuffPost Brasil, dois dos textos de maior audiência sobre o assunto também são boas fontes de informação e histórias. Em "" 7 coisas que eu gostaria de saber antes de sair do armário "", o blogueiro relata situações comumente enfrentadas por gays. Já na reportagem "" 5 lições para sair bem do armário "", o primeiro tópico é uma dica certeira: ""Antes para você mesmo, depois para os outros"". ""Pode deitar que eu já tô anotando tudo"" Fazer terapia é outra atitude que pode ajudar no processo, principalmente se você ainda tem dificuldade de lidar com seus desejos. ""Noventa e cinco por cento das experiências que acompanhei estavam nesta fase de se sentir bem na própria pele, de reconhecer o próprio desejo sem que isso seja algo destrutivo"", revela o psicólogo Raoni Rodrigues. Nesse aspecto, parte do trabalho é mostrar que se assumir LGBT traz mudanças na sua identidade, mas que isso não afeta a essência da pessoa. ""Essas questões que ameaçam a nossa identidade nos deixam muito assustados sobre o que vamos continuar sendo, sobre como continuarão as nossas relações. As pessoas ficam paralisadas com medo de sofrer perdas a partir dessas mudanças"", explica Raoni. Mas tem como a psicologia fazer essa revelação acontecer da forma mais suave possível? ""É uma ilusão achar que vamos encontrar a hora certa, o jeito certo... É como encontrar um jeito de mudar menos as coisas. Vai trazer mudança, mas não necessariamente será uma mudança caótica"", argumenta o psicólogo. A série Renografias da Renault, aborda a relação do carro com as pessoas. A marca acredita que, assim como a sua vida, o seu carro também precisa ter espaço para aventuras. Assista às outras histórias no site.",pt,47
391,3101,1487345705,CONTENT SHARED,2277184202014276839,-5706287032724665714,1536755170726159771,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36",SP,BR,HTML,https://medium.freecodecamp.com/how-to-build-cross-platform-mobile-apps-using-nothing-more-than-a-json-markup-f493abec1873?gi=65b1783a740,how to build cross-platform mobile apps using nothing more than a json markup,"For the past few months, I've been working on a new way to build cross-platform, native iOS and Android apps called Jasonette . It lets you describe an entire app with nothing but a single JSON markup. If your app consists entirely of JSON, it can be treated like any other data. And it can be served remotely from the cloud on-demand. The app logic no longer needs to be hardcoded on the device, and you can update it as much as you want just by updating your JSON on the server-side. Your app will be freshly loaded from the server every time you open it. Check out the video below for a quick intro: Jasonette has many different parts. You can express functions, templates, styles, and more all by using a JSON Markup . And as a result, you can write a super-sophisticated native mobile app in a fully Model - View - Controller manner. In this post I'll show you specifically the ""View"" part: How Jasonette expresses various cross-platform UI patterns in JSON. How it implements these JSON-to-Native mappings internally. Basic Structure Under the hood, Jasonette works similarly to a web browser. But instead of interpreting an HTML markup and drawing a web view, Jasonette fetches a JSON markup and constructs a native view, on-the-fly. The markup is just a JSON file that follows some predefined conventions . First of all, it starts with a $jason key, which has two children: head and body , and looks like this: Design Philosophy When I first started designing the JSON syntax for describing native views, I had a couple of constraints in mind: Native  - There's a reason why iOS and Android came up with their own native layout systems. The layout systems designed for the desktop era don't always translate well to the small device world. The syntax should express the underlying layout in as mobile native manner as possible. Cross platform  - Yet it needs to be cross platform. For example, iOS has something called autolayout and visual format language but these are not implemented natively on Android, so not the right solution. Simple yet expressive  - It should be easily expressed in a simple JSON format and easy to compose into a sophisticated structure. When you take a look at how most mobile apps are built, they all boil down to a small number of common interface patterns: Vertically scrolling list Horizontally scrolling list Absolute positioning Grid Let's take a look at the first three, since they are most widely used. 1. Sections - Describing scrolling lists The most frequently used UI pattern is scrolling lists . On Jasonette we call them sections . There are two types of sections: Vertical and Horizontal . Vertical sections scroll vertically, and horizontal sections horizontally. Implementation - Vertical Sections This is probably the most frequently used UI for displaying data on mobile devices. On iOS, Jasonette implements this with UITableView . On Android it's implemented with RecyclerView . On iOS, above JSON markup creates a UITableView with three UITableViewCells , each of which contains a UILabel , with corresponding text attributes. On Android, it creates a RecyclerView with three items, each of which is a TextView that displays the corresponding text attributes. All of these are constructed programmatically without any use of Storyboards (iOS) or XML layout files (Android) in order to make sure every detail is programmable dynamically. Implementation - Horizontal Sections Syntax-wise, horizontal sections are not much different, all you need to do is set the type as ""horizontal"" , and the items flow horizontally. Note: The syntax for the horizontal section is simple, but internally it's actually quite complex. Horizontal sections on iOS were implemented with UICollectionView . It's a well-known technique, but basically a horizontally scrolling UICollectionView is embedded into its parent UITableView (which scrolls vertically). And on Android, it's implemented in a similar manner, but using nested RecyclerViews instead. 2. Items - Describing layout within each scrolling unit Now that we understand how the top level view is structured, let's look at items . Each section is made up of multiple units of scrollable items . Note that each item has a fixed dimension and nothing inside the item itself scrolls. An item can be: Just a single component like a label , image , button , textarea , etc. A combination of all of those components Implementing this part was not as straight-forward as the sections implementation, because I had to choose a cross-platform, native, simple, and expressive way to form a super-sophisticated layout. Thankfully, iOS and Android have very similar native layout systems called UIStackView and LinearLayout , respectively. And these layout schemes in turn are similar to CSS Flexbox , so I would say it's as cross-platform as it can get. Lastly, these layout systems are infinitely composable. As seen below, you can create a vertical layout, a horizontal layout, or nest a vertical layout within a horizontal layout, and so forth, recursively. To create a vertical layout, you would set the type as vertical , and then set its components : Same thing with horizontal layout. Just set the type as horizontal instead: Nesting layouts is as simple as specifying a layout as another layout's component. I have not talked about the styling feature here for the sake of brevity, but you can style each individual component as well as the layout itself to make sure the layout looks exactly like you wanted. All you need to do is add style objects describing font , size , width , height , color , background , corner_radius , opacity , etc. 3. Layers - AKA ""absolute positioning"" Sometimes you may want to position items at exactly certain parts of the screen without scrolling. In CSS-terms we would call this ""absolute positioning"" . Jasonette supports this through what's called layers . Currently layer supports two types of child components: image and label . You can place these components anywhere you desire on the screen this way. Here's an example: In this example, we have two labels (the temperature and the weather messages) and an image (the camera icon) on the screen, whose coordinates have been explicitly set to make sure they stay in place without scrolling. The markup would look something like this: Amazingly enough, this is all you need to know in order to build any kind of sophisticated view you can imagine on mobile devices. Just like you can build anything with simple lego blocks, you can compose these basic components and layouts in different ways to create any sophisticated view. Here are some examples, 100% built by composing aforementioned UI elements: Beyond Views If you read this far, you may be either thinking: ""Wow cool! I wanna try this!"", or ""Yeah you can probably build a toy app, but no way you can build a production app using this way!"" Like I briefly mentioned above, this is just the ""View"" part of Jasonette, which is the simplest part. But what's really powerful about Jasonette is that you can actually go much further and write a full declarative program in JSON . You can attach actions to UI elements, which get triggered when a user touches them. You can also trigger these actions one after another via success/error callbacks. You can also listen to certain events and automatically trigger these actions. Just like this, when you can describe not just a ""View"" but also the ""Model"" and the ""Controller"" logic (all in JSON), you can do anything . What is possible? Since all you need is a server that sends JSON, Jasonette is completely platform agnostic. There is no proprietary server technology you need to depend on. All you need is JSON. And JSON can come from anywhere, from local device, to remote servers, to even a raspberry pi ! Have a web app? : If you already have a web app, you can instantly build a mobile native app for your Node.js app, Rails app, Django app, PHP app, or really any web app, justby making requests to your API endpoint. You don't even need a server : Since you can fit an entire model-view-controller in a single, self-contained JSON file, you can pretty much store and serve it from anywhere. You can even create an app from a static JSON file served from a Pastebin or Github! Turn any HTML website into an app : Jasonette has a powerful HTML-to-JSON parser powered by the cheerio library which lets you transform any HTML into a JSON object. And you already know what we can do when we have JSON - you can build a native view from the transformed JSON! This way, you can build a native app from a website that doesn't even have an API. Of course, the recommended way is to use JSON whenever you can, but this is really cool regardless. I can go on forever, but here are some examples: A photo sharing app that lets you take a photo using the device camera and upload it to S3, and then post the entry to your own server, creating a feed: A Node.js powered Eliza Chatbot app for iOS and Android: A Microblog app , complete with session management: An example app that turns an HTML web page into JSON and then turns it into a native app: Conclusion Jasonette is a young project. I open-sourced the iOS version in late 2016, and the Android version a month later. But it has already grown into a vibrant community of contributors and makers and is under active development. I hope this technology will empower anyone (not just developers) to build apps effortlessly. Sounds good? Check out the website here . Last but not least, you can find the Github repositories here: iOS and Android ( Contributions super-welcome! )",en,47
392,1110,1464219941,CONTENT SHARED,1012456848389051365,5206835909720479405,3676775991656629655,,,,HTML,http://epocanegocios.globo.com/Empresa/noticia/2016/05/como-o-e-commerce-brasileiro-pode-enfim-comecar-lucrar.html,"como o e-commerce brasileiro pode, enfim, começar a lucrar","Tie Lima, na sede do Enjoei, em SP: a startup nasceu como um shopping online (marketplace) e prevê lucro em 2017 (Foto: Claus Lehmann) Matéria originalmente publicada na edição de fevereiro de 2016 de Época NEGÓCIOS O casal Tie Lima e Ana Luiza McLaren fundou, em 2012, o site Enjoei. Ele permite a criação de lojinhas virtuais para que qualquer pessoa, enjoada de suas roupas, calçados e gadgets, os venda pela internet. O site não faz outra coisa além de aproximar compradores de vendedores. Por essa mediação, embolsa 20% de cada negócio. O Enjoei atingiu mais de 2 milhões de consumidores, sendo 90% deles mulheres. Sua estratégia de divulgação é baseada em uma curadoria, com uma equipe que dá destaque a peças selecionadas. Distribui também convites para que celebridades vendam suas tranqueiras ali. No ano passado, a cantora Anitta despachou 240 itens em menos de três horas. As expectativas ao redor do Enjoei são altíssimas. Os fundos Monashees e Bessemer Venture Partners investiram mais de R$ 45 milhões no negócio. Por enquanto, ele opera no vermelho, já que todos os ganhos são reinvestidos. ""A gente vai começar a dar lucro em 2017"", diz Lima. A predição parece boa. Cinco anos entre o nascedouro e o lucro é um tempo bastante curto para os padrões do comércio eletrônico. Essa perspectiva soa plausível apenas porque o Enjoei está estruturado a partir de um modelo de negócios peculiar. Ele é chamado de marketplace. Trata-se de sites onde pequenos e médios comerciantes vendem seus produtos. Qual produto? Qualquer um. Esse tipo de espaço é exatamente isso: um mercadão, um empório virtual. Seus donos cobram uma comissão para intermediar as vendas. E é só. No Brasil, os marketplaces deverão movimentar R$ 115 bilhões em 2018, registrando um aumento de 130% no período de cinco anos, segundo relatório da consultoria Mintel. Eles, aliás, vivem uma situação rara no e-commerce nacional. Em geral, são lucrativos. O maior exemplo de marketplace na América Latina é o Mercado Livre. Ao revender produtos de terceiros, a plataforma virtual, fundada pelo argentino Marcos Galperin em 1999, dá lucro há 37 trimestres seguidos. E nada indica que isso mudará. ""O ano passado foi o melhor da nossa história"", diz Helisson Lemos, presidente da empresa. Nos primeiros nove meses de 2015, o último dado disponível, o site lucrou US$ 87,2 milhões. O Brasil é responsável por pouco mais da metade desse valor. Não é essa a situação da maioria das lojas online no Brasil. A maior empresa de e-commerce nacional, a B2W, dona do Submarino e da Americanas.com, apresentou um rombo de R$ 205,5 milhões nos nove primeiros meses de 2015, o pior de sua história. Algo similar deu-se com a Cnova, sua maior rival. Nascida da fusão entre a francesa CDiscount, do Casino, e a brasileira Nova Pontocom, do Grupo Pão de Açúcar, ela registrou perda de US$ 107 milhões no mesmo período. O problema não está restrito aos grandalhões do setor online. Lojas médias, focadas em mercados específicos, sofrem do mesmo mal. São perseguidas pelos números negativos. A Netshoes, especializada em acessórios esportivos, teve R$ 97 milhões de prejuízo em 2014, o dado mais recente. Sua concorrente, a Dafiti faturou R$ 592 milhões, mas, ainda assim, nada de lucro. O entrave é que, ao contrário dos marketplaces, todos esses sites vendem produtos próprios e têm custos enormes. Por exemplo: sustentam grandes centros de distribuição, metem-se em disputas canibais por preço, investem um dinheirão em novas soluções tecnológicas e bancam o frete de toda sorte de produtos enviados para a clientela. Centro de distribuição da Netshoes, em SP: lojas convencionais têm custo muito alto (Foto: Editora Globo) A boa notícia para o setor é que as lojas online têm planos para sair do buraco. Aliás, elas têm o mesmo plano. E não é difícil adivinhar qual o modelo que querem adotar - o marketplace, claro. A tarefa, contudo, não é tão simples. A B2W que o diga. Sua primeira investida na área ocorreu em novembro de 2013, quando inaugurou o marketplace do Submarino. Seis meses depois, foi a vez da Americanas.com. Mas a empresa concluiu que os serviços só avançariam se fossem turbinados. Em fevereiro de 2014, a B2W criou uma nova divisão, a B Seller, a partir de duas startups compradas meses antes, a Uniconsult e a Kanlo. Tudo para oferecer uma plataforma de venda a lojistas de todos os tamanhos. Para aumentar a atratividade do produto, inaugurou três centros de desenvolvimento de softwares, dois no Rio de Janeiro e um em São Paulo, chamados LAB, onde mais de 600 técnicos passam o dia codificando algoritmos. Ao mesmo tempo, comprou outras startups que pudessem acrescentar novas ferramentas ao B Seller. Foi o caso do Sieve, um sistema de monitoramento de preços em tempo real, o Tarkena, um otimizador de busca, e o Shopgram, aplicativo que facilita vendas pelo Instagram e pelo WhatsApp. Em 2014, para arrumar dinheiro em meio à crise que despontava, a B2W, primeiro, atraiu o fundo Tiger Global, tradicional investidor do comércio eletrônico no Brasil. Em janeiro daquele ano, ele comprou 7,2% da empresa e injetou R$ 1,2 bilhão na operação. A participação não durou um ano e meio. A Tiger abandonou a B2W no primeiro semestre de 2015. Não restou alternativa a não ser vender duas unidades de negócios secundárias para levantar o caixa. Em maio, a CVC adquiriu o Submarino Viagens por R$ 80 milhões. Quatro meses depois, a Fandango Media arrematou o Ingresso.com por R$ 280 milhões. Ainda assim, a B2W conseguiu alguns resultados com a investida. No fim de 2014, 1,5% das vendas da companhia foram feitas pelo marketplace. Um ano depois, o número subiu para 10%. Entre 2014 e 2015, o total de itens disponíveis de terceiros cresceu mais de dez vezes, atingindo mais de 500 mil produtos de 2 mil fornecedores. Analistas consultados por NEGÓCIOS estimam que a lucratividade da B2W está a caminho. Ela chegará quando as vendas pelo marketplace empatarem com o site convencional. O negócio da B2W, no entanto, nem sempre patinou. Por quatro anos, ela reinou no mercado online. E com lucro. Em 2010, os ganhos com o Submarino e a Americanas.com foram de R$ 33,6 milhões. A partir do ano seguinte, com a chegada da Nova Pontocom ao mercado, ambas se engalfinharam em uma disputa por preços, cuja canibalização do segmento foi sua principal consequên­cia. Elas ofereciam descontos, fretes gratuitos e parcelamentos a perder de vista aos consumidores. A estratégia sacrificou a rentabilidade. ""Chegou um momento em que os investidores cansaram de colocar dinheiro a fundo perdido"", diz Maurício Salvador, presidente da Associação Brasileira de Comércio Eletrônico (ABComm). Há dois anos, começou um ajuste operacional - o site cortou custos, demitiu, diminuiu os gastos em campanhas na TV e reduziu vantagens, como o frete gratuito e o crédito ilimitado. Ainda assim, o lucro nunca voltou. Já era tarde. No quintal do Mercado Livre A Cnova adotou uma rota diferente para buscar o nirvana do market­place. Desde abril de 2013, permite que lojistas vendam seus produtos pelo Extra.com. Em 2015, replicou o modelo no Ponto Frio e na Casas Bahia. A parte mais controversa da estratégia, porém, foi se aliar ao ""inimigo"". Desde novembro, a Cnova tenta vender uma seleção de produtos, como geladeiras e máquinas de lavar roupa, dentro do Mercado Livre. A vantagem de ser mais uma entre milhares de lojas no maior empório online da América Latina é atingir um público de 26,1 milhões de brasileiros, segundo estimativa da consultoria comScore. Já o perigo é, ao vender por ali, fortalecer o rival, com comissões sobre comissões. Ainda assim, o ousado plano parece estar funcionando. No terceiro trimestre de 2015, quase 13% das vendas da Cnova no Brasil vieram do marketplace, mais do que os 10,2% da B2W. Há, contudo, um problema de adaptação, tanto para a Cnova quanto para a B2W, a esse mundo dos sites que reúnem produtos de diversos vendedores. Ambas nasceram varejistas e, ao criar um shopping virtual para terceiros, terão de escolher o que priorizar. Outra dificuldade da transição é contar com um bom exemplo para seguir. Quando precisam de inspiração, todos os sites de e-commerce que buscam a metamorfose para mercadões olham para a Amazon. Nessa transformação, porém, a empresa de Jeff Bezos não é o melhor parâmetro. Quem ajuda a Amazon a fechar no azul é a divisão de computação em nuvem, a Amazon Web Services, e não a venda de produtos. Um endereço melhor para reproduzir é o JD.com, o site chinês que conseguiu a mudança de loja online para marketplace em cinco anos. Em 2013, as vendas diretas superavam em quatro vezes os negócios do market­place. Dois anos depois, ambos estão muito próximos. As diretas faturaram US$ 61,3 bilhões, e o shopping virtual US$ 49,7 bilhões no terceiro trimestre de 2015. Tudo indica que, este ano, o jogo vai virar. O desafio de uma guinada para o modelo de marketplace se aprofunda porque a competição virá de todos os lados. No ar desde o começo de 2014, o shopping virtual do Walmart Brasil tem mais de mil fornecedores. Sites médios também têm seus shoppings virtuais. É o caso de Dafiti e Netshoes. Ambas estrearam seus serviços quase simultaneamente, no começo de 2016, e têm foco especial nos setores onde atuam: roupas, acessórios de moda e sapatos. Nos próximos anos, a Netshoes usará sua estrutura para lançar outras lojas que vendam produtos de alta rotatividade e com margens gordas, diz o diretor financeiro do grupo, Leonardo Dib. Cada nova loja terá seu próprio serviço para fornecedores venderem seus produtos. Há ainda exceções, de lojas online que vendem produtos próprios - não são, portanto, marketplaces -, mas vão bem. Mas elas fazem muito mais do que simplesmente vender. Um exemplo é o site de vinhos Wine. Fundada em 2008, a loja só alcançou o lucro quando lançou o Clube W, um serviço que seleciona e envia garrafas da bebida todo mês à casa dos clientes. Em quatro anos, já são mais de 130 mil consumidores, que pagam uma mensalidade entre R$ 66 e R$ 372 pela comodidade. E quem assina o clube, gasta mais no site - o que garante maior receita. Desde 2013, a Wine faz parte do exclusivo clube das lojas online lucrativas. Mas, como se vê, vai muito além da simples venda de vinhos pela web. Na contramão do e-commerce, o Mercado Livre, de Helisson Lemos, não perde dinheiro há quase dez anos (Foto: Regis Filho/Valor)",pt,47
393,1471,1466518183,CONTENT SHARED,-601952361752157430,3609194402293569455,-7072205224139961274,,,,HTML,http://www.b9.com.br/65606/advertising/cannes-lions-2016-tecido-tecnologico-do-google-leva-gp-de-product-design/,cannes lions 2016: tecido tecnológico do google leva gp de product design,"Em maio de 2015, durante a conferência I/O, o Google apresentou projetos de sua divisão ATAP (Advanced Technologies And Projects). Um dos destaques foi o Project Jacquard , que desenvolva a adição de tecnologia em tecidos, num passo da empresa dentro do conceito de Internet das Coisas. Essa foi a proposta premiada com Grand Prix na categoria Product Design , em Cannes Lions 2016 . Sensores tornam sua roupa sensíveis ao toque, e dessa maneira é possível controlar dispositivos conectados com gestos simples. As aplicações pensadas ainda são simples como acender uma lâmpada, ligar um smartphone ou aumentar o volume da música, mas o importante aqui é o fato do hardware ser flexível, o que deve abrir caminhos para toda uma nova gama de produtos. No mês passado, o Project Jacquard criou uma jaqueta jeans em parceria com a Levi's :",pt,47
394,1530,1466955123,CONTENT SHARED,7823781477664935857,-1032019229384696495,-2318276786697086259,,,,HTML,https://techcrunch.com/2016/06/25/telecoms-open-shop-on-madison-avenue-but-will-brands-buy/,"telecoms open shop on madison avenue, but will brands buy?","Many companies have transformed and realigned their focus with great success. Avon transitioned from peddling books door-to-door to marketing beauty products. Wrigley started as a soap and baking soda company. IBM originally sold massive mainframe computers and calculators. Now, telecom companies are making similar pivots into a lucrative industry. The battle du jour is about customer data and powering the $100 billion global mobile advertising business. Wireless carriers are in a great position to provide mobile ad services because of their intimate connections to hundreds of millions of customers. They might be able to finally fulfill digital advertising's promise of delivering the right ad to the right person at the right time. Wireless carriers have troves of personalized data, powerful distribution platforms and many of them produce their own content. Also, telecom is one of the largest ad spend verticals. The industry understands the challenges that big brands face on mobile because it shares many of them. The next powerhouse digital ad platforms may come from telecoms. The race to vertically integrate will challenge industry players like Apple, Google, Facebook, Microsoft and niche players. However, deep pockets do not guarantee success. Large and bureaucratic companies may find it especially challenging to innovate, pivot their focus and develop new competencies. In fact, history is littered with failed transformations: Saatchi & Saatchi with management consulting, Cosmopolitan with yogurt and Allegis with its foray into travel. Through recent acquisitions, aggressive product development and wider industry partnerships, telecom companies are building sophisticated adtech products. Will brands respond favorably to the new big box shops on Madison Avenue - or shut them out? Why adtech? Competition in wireless is fierce. The U.S. oligopoly is dominated by Verizon and AT&T, with 33 percent and 34 percent market share, respectively. Sprint and T-Mobile each have approximately 16 percent, according to Statista . Market saturation is inevitable with more than 207 million Americans already using a smartphone. Growth rates in the U.S. are expected to be in the low single digits for the rest of the decade, Statista reports . As the industry has matured, innovation has withered. Wireless carriers have launched, but have yet to realize runaway success in connected home products, smart watches or virtual reality devices. While some players like AT&T look for success in the content distribution space (e.g. through its streaming content service U-Verse and by purchasing DirectTV), others are making a play for an even loftier prize - monetizing the customer data profile. Traditional adtech companies are presumptively in the best position to satisfy brand demands for better mobile ads. Google, Yahoo and Microsoft have all been in the adtech space since the inception of online advertising. However, according to a recent Accenture study, more than 73 percent of respondents said that digital ads do not match their personal interests or preferences. This finding highlights a big opportunity for wireless carriers because the data they possess is hyperpersonal, available in real time and geo-tagged. The mobile phone has become life's remote control. The mobile phone has become life's remote control. From mobile banking, appointment scheduling and bill paying to meal ordering, video watching, online shopping and photo sharing, no single device in a person's life knows as much about us as the smartphone. But carriers have traditionally been more of a utility and not an application innovator - they did not have inherent competencies to monetize content. If a wireless carrier can capture just a fraction of the U.S. mobile ad market, it could outperform the competition. The opportunity is massive and brands are taking notice. Investments Many global wireless players have already invested in mobile adtech. SingTel from Singapore, Telstra from Australia and Telefonica from Europe have been building adtech capabilities for some time. Global brands have been responding favorably. Brands can utilize SingTel's mobile ad solutions to connect with their global audience. ""The digital advertising business is about scale,"" said Mark Strecker, chief executive officer of Amobee , a SingTel company, in a WSJ interview. Through acquisitions, the company is enlarging and expanding its mobile advertising footprint. However, there is potential for conflict of interest in the industry. Competing brands may need to transfer customer information and other data to telecom platforms in order to run campaigns. As a result, telecom ad platforms would control the distribution and messaging for another brand. American telecom companies have now entered the fray. Verizon recently started building its own mobile adtech stack with AOL as the centerpiece. AOL and mobile ad network Millennial Media provide Verizon with a robust mobile video ad solution. This is synergistic, as Verizon makes a large proportion of its revenue from data plans, and video is the largest source of data usage. According to a Juniper study , mobile video currently accounts for about 60 percent of IP traffic in the U.S., with this figure expected to catapult over 70 percent in the next few years. Verizon will benefit from an increase in mobile video consumption regardless, but it will also benefit from the ads it runs and sells alongside video content. There is a clear business case for Verizon to monetize mobile video. Verizon also has top-quality user data that brands can leverage for enhanced mobile engagement. But will brands be open to running ads on a network that is controlled by another brand? Specifically, telecom and related industries might be wary because, unlike other media, on mobile, vast amounts of customer data is transferred between platforms. As a result, some advertisers might proceed with increased caution to ensure that their customer messaging and anonymized data is fully controlled. Verizon has already started walling off its garden in a bet to go head-to-head with Google and Facebook. According to a Digiday source, ""Verizon has been notifying partners that they are cutting off agreements for their precision insights product."" By restricting third parties from its data trove, and instead using the data to power AOL's ad platform, Verizon will funnel more dollars into its coffers. Will brands be open to running ads on a network that is controlled by another brand? Despite the promising outlook, telecom companies could still face an uphill challenge. For example, Verizon was recently reprimanded for using ""supercookies"" without customer consent. Supercookies can be used to track and store personal data even if a consumer has deleted their traditional, browser-based cookies. Many e-commerce publishers use supercookie data packages and uniquely tailor messaging, onsite ads and content to wireless carrier customers. This recent occurrence brings to light some privacy challenges that telecom companies will face and issues that brands must consider when working with them. Sprint also has made an important play in mobile with its acquisition of mobile ad platform Pinsight Media . The company focuses on processing and uncovering consumers' insights and behaviors in order to provide enhanced branded experiences. Sprint is now able to monetize hundreds of apps while layering on data about Sprint subscribers to propel performance. The company combines demographic data with social and mobile interests and behavioral insights. Product development More proof that wireless carriers are advancing in mobile adtech can be seen in their launch of various content services. Wireless carriers are vertically integrating to control not only data and ad platforms, but content and content distribution, as well. These content services provide an entry point for wireless carriers to enhance their clout in the mobile ecosystem. Verizon launched ""go90,"" an ad-supported app geared toward millennials. T-Mobile launched ""T-Mobile TV,"" a subscription service content portal. Sprint launched ""SprintTV,"" a free premium content service focused on mainstream entertainment properties. These services highlight new ways that wireless carriers are attempting to create new content offerings to then monetize. Other examples of wireless carriers expanding into mobile advertising include the emergence of online/offline programmatic video buying . AT&T, together with its DirectTV unit, partnered with Videology to offer cross-screen programmatic video ad solutions. AT&T's goal is to take some of the precision from online advertising and inject it into the traditional television market. It remains to be seen if cross-platform programmatic commercial buying will make the leap to the mainstream. Some agencies have just started to purchase certain types of television inventory programmatically, but if agencies can achieve scale with online and offline bundled solutions, this approach may prove to be more appealing than siloed ad buying platforms. Industry partnerships In addition to investments and services development, wireless carriers are forming major global partnerships with each other in order to own a piece of the mobile ad ecosystem. Sprint and Telefonica started a joint venture to provide advanced ad solutions for global brands. The partnership is unique in that it spans across Europe, the U.S. and Brazil. The venture can be leveraged by brands for large-scale global access to customers. Many global telecom partnerships are fluid and subject to change. This is especially true given industry consolidation, and the complexity of global data usage and associated privacy laws. Many big brands have shied away from partnering with companies that have the potential to generate controversy regarding data privacy. Brands see mobile adtech as the key to closing the loop on mobile data and targeting. Other mobile-centric industry partnerships focus on content acquisition and ad distribution. Verizon has been acquiring new, high-quality video inventory to bolster its ""go90"" app. Through its partnership with Hearst, Verizon has added content from AwesomenessTV, Rated.com, SeriouslyTV and recently acquired Complex Media. Despite a recent deal with Publicis Groupe to run $50 million of go90 inventory, it is unclear if brands will consider this new digital-first content as premium. It also remains to be seen if go90 will attract enough users to make the app viable in the long term. Verizon is also expanding its mobile footprint through its global partnerships with Microsoft. The telecom titan will monetize much of the tech giant's display and mobile inventory across top destination sites, including the MSN Homepage, Outlook Mail and Skype. Outcomes Given dwindling opportunities for further industry consolidation in mature markets, many telecom companies in these markets are looking for new revenue streams to bolster growth. Brands see mobile adtech as the key to closing the loop on mobile data and targeting, and Google and Apple already benefit from cross-device capabilities. Wireless carriers may have an even greater capacity to provide value because of their intimate relationship with customers. For brands and agencies, more competition in the space may mean better pricing and more advanced targeting, but perhaps less brand control. How comfortable will brands like Comcast, Disney and Dish be, running their brand campaigns on a wireless carrier's ad network? This remains to be seen. Brands and advertisers are cautiously optimistic. Adtech heavyweights, have, for some time, promised an environment in which the right person is targeted with the right ad at the right time. So far, the industry has not achieved its goal - but there is a big opportunity for wireless carriers to improve the state of mobile advertising. If they do, this could be a win-win-win. Consumers win because they get ads that are better suited to their interests and are hopefully less intrusive. Brands win because they can connect with customers in a more individualized and organic way. Finally, wireless carriers win because the revenue potential is massive, and they can drive further rapid innovation and potentially provide better inventory at varied pricing. Telecommunication companies may be opening up shop on Madison Avenue, but it remains to be seen if their presence will improve the industry and propel brands on mobile. Featured Image: Thiago Leite / Shutterstock",en,47
395,1550,1467116330,CONTENT SHARED,3205047378909988692,7645894863578715801,-1632521130887615017,,,,HTML,http://blog.takipi.com/java-vs-net-vs-python-vs-ruby-vs-node-js-who-reigns-the-job-market/,java vs .net vs python vs ruby vs node.js: who reigns the job market? | takipi blog,"We crunched 351,799 job openings - Here are the languages you need to master to get them Every now and then a new article/blog post/opinion piece/twitter rant is published wondering about the future of Java. However, the cold facts state that it's still the most popular language in the programming world, but what does it actually mean? In the following post we'll dig into the buzz around Java through analysing the current job market, focusing on server side languages. Who knows, it might even help you find your next position. New Post: Java vs .NET vs Python vs Ruby vs Node.JS: Who Reigns the Job Market? pic.twitter.com/bBPc5IcHUg - Takipi (@takipid) June 16, 2016 Java rules the domain For some Java might mean a hot cup of coffee, but for us it's one of the most practical programming languages; it's concurrent, class-based, object-oriented and designed to have as few implementation dependencies as possible. If you're not into that factual lingo, the main selling feature here is ""write once, run anywhere"", allowing us to run our code on every Java supported platform without having to recompile it. There's no doubt Java is popular. It's the second most popular programming language, and it is the base for pretty much any other object oriented principles language. If you ask Oracle, they can show you some numbers that go along with these statements: 97% of Enterprise Desktops Run Java 89% of Desktops (or Computers) in the U.S. Run Java 9 Million Java Developers Worldwide 3 Billion Mobile Phones Run Java In order for us to see how popular Java really is, we decided to look where it counts: the job market. We focused on the 2 popular job hunting sites: Glassdoor and Indeed, inside the US market. Looking for a job? Look for Java Just like any other person looking for a job, we searched the term ""Java"" in Glassdoor , limiting the result to the US, and got 60,322 results. Among the different positions available, you can apply for Java/J2EE Developer, Test Automation Engineer, Software Engineering Team Leader, Full Stack Software Engineer and even be a Java teacher for an online class. We ran a similar search on Indeed, focusing on developers and engineers. This search landed us with 62,249 available positions. There's a variety of positions available here as well, such as Back End Services Engineer, Test Automation Engineer, Applications Developer and so on. Programming and StackOverflow go hand in hand, and that's why we decided to look on it as well. When searching for Java we found 1,088,646 tagged questions, which indicate the overall popularity of this language. These numbers mean that if you're looking for a job in the Java field, you have a lot of options to choose from. But how do they reflect Java popularity? For that, we'll have to look for open positions in other languages. First, we chose those who are relevant to us: C++ and .NET. While the numbers are still high, they're lower than Java. For C++, you'll find 28,879 jobs on Glassdoor, and 34,451 jobs on Indeed with our developers and engineers filter. If you're more of a .NET programmer, you can choose from 23,509 jobs on Glassdoor and 32,801 on Indeed, and you should be aware of the decrease in demand, as the following chart shows: What about ""younger"" languages you ask? When looking for Python the numbers slim down to 29,800 jobs on Glassdoor and 32,434 on Indeed, and Ruby offers you 12,928 jobs on Glassdoor and 14,303 jobs on Indeed. The demand for Node.js seems lower, with 4,805 jobs on Glassdoor and 6,277 jobs on Indeed. FYI: Indeed indicates that the average Java salaries for job postings nationwide are 77% higher than average salaries for all job postings nationwide. Java is the prom queen TIOBE, the programming community index, show similar results in its June report . Java is the most popular language, and it has been holding this title for a few long years. C++ and .NET are not far behind, holding places third and ninth respectively. It's interesting to see that Python hold the honorable fourth place and Ruby is placed at number 10 - indicating that they are on the rise, but apparently it's still not enough to make a significant impact on the job market. Not just for Java We know what you're thinking, there are other JVM languages beside Java - what about them? Right now, while the job market does offer a handful of positions, it's not likely they'll take over soon. When searching for Scala positions, the numbers slim down to 2,618 jobs on Glassdoor and 4,242 jobs for developers and engineers on Indeed. When it comes to Clojure, we found 461 jobs on Glassdoor and 575 on Indeed. We decided to look if there are any open positions in the Reactive Programming Framework, used by Java and Scala developers. The numbers grew a bit, with 2,224 jobs on Glassdoor but only 404 jobs on Indeed. It also seems that Akka, which is also used by Java and Scala developers, is less popular, with 283 jobs on Glassdoor and 427 jobs on Indeed. What about something a little less popular? When searching for Jython we got 228 jobs on Glassdoor and 178 jobs on Indeed, JRuby has 104 jobs on Glassdoor and 107 jobs on Indeed and Kotlin has a low number of 10 open jobs on Glassdoor, and 34 jobs on Indeed. We also ran a search on ""Groovy"", and though Glassdoor showed 1,318 jobs, Indeed had no less than 14,663 jobs. We thought it was a little weird, and an in-depth look taught us that people still use ""Groovy"" as an adjective to describe what kind of people they're looking for. We know, it's embarrassing. In order to get some clue about open positions for Groovy developers, we narrowed down the search term to developers and engineers, and got a reasonable number of 1,795 jobs available. Rise of the Devops One of the new buzzwords everyone has been looking for lately is Devops. While the actual meaning of this word is to emphasize collaboration and communication of software developers and IT professionals, it doesn't apply ( get it? ) to the job market. One Devops description doesn't match another; some companies require high programming skills, while other are looking for someone who knows how to create basic scripts and handle APM tools (shameless plug: like Takipi ). When searching for Devops we found 22,506 jobs Glassdoor and 11,487 jobs on indeed. But all you really need is one look at the following chart to get the picture. Devops are the new orange. We also tried breaking it down and search for the 2 major tools every Devops engineer should know: Chef and Puppet. Puppet shows 2,685 jobs on Glassdoor and 3,809 jobs on Indeed. Just like Groovy, we got a lot of irrelevant results when searching for ""Chef"" (unless you'd like to change your life and become a catering chef). However, searching for Chef and limiting the results to Devops, we got 2,862 jobs on Glassdoor and 4,048 jobs on Indeed. Final Thoughts Numbers are great and we love them. Just the other week We Crunched 1 Billion Java Logged Errors and then found the The Top 10 Exception Types in Production Java Applications . But all of the numbers included in this post say something about the state of Java: it's still going strong and it's here to stay. At least for the next decade.",en,47
396,2520,1475754252,CONTENT SHARED,-7527881206365368,-4585796377251906117,7302477188747402041,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.97 Safari/537.36",SP,BR,HTML,http://economia.uol.com.br/empregos-e-carreiras/noticias/redacao/2016/10/06/de-piada-sobre-tpm-a-servir-cafezinho-como-lidar-com-machismo-no-trabalho.htm,de piada sobre tpm a servir cafezinho: como lidar com machismo no trabalho?,"Getty Images Nem sempre o machismo no ambiente de trabalho é escancarado, como em um caso de assédio moral ou sexual. Às vezes, pode acontecer de forma sutil em situações do cotidiano no escritório. É sobre isso que fala a jornalista norte-americana Jessica Bennett, autora do livro ""Feminist Fight Club - An Office Survivor Manual For a Sexist Workplace"" (Clube da Luta Feminista - Manual de Sobrevivência no Escritório para Ambientes de Trabalho Sexistas), inédito no Brasil. ""Durante muito tempo eu imaginei que o problema era comigo, mas conversando com outras mulheres eu percebi que o problema não era individual, mas sistêmico, coletivo"", diz Bennett. ""E, se é sistêmico, nós podemos lutar contra ele juntas."" O UOL conversou com Bennett e com a coordenadora do Grupo de Estudos de Gênero e Raça da FEA/USP, Itali Collini; com Lyanna Bittencourt, diretora executiva da consultoria grupo Bittencourt; e a juíza do trabalho Elinay Melo. A partir das entrevistas, foram identificadas as situações mais frequentes de machismo no trabalho e como as mulheres devem enfrentá-las. 1) Ser interrompida ao falar As mulheres tendem a ser interrompidas desnecessariamente mais vezes que os homens quando estão falando, tanto por homens quanto por outras mulheres, segundo um estudo feito pela pesquisadora Adrienne Hancock , da Universidade George Washington. Nem personalidades famosas escapam desse comportamento, conhecido como ""manterrupting"". Por exemplo, a candidata democrata à Presidência dos Estados Unidos, Hillary Clinton, foi interrompida 51 vezes pelo candidato republicano, Donald Trump, no debate realizado em 26 de setembro, segundo levantamento do site de notícias ""Vox"". Trump foi interrompido 17 vezes. O que fazer? Quando for interrompida, a mulher deve continuar falando com o mesmo tom de voz, como se não tivesse percebido, ou dizer frases como ""me deixe terminar minha fala"" ou ""espere que eu conclua meu raciocínio"", diz Collini. Se outras mulheres do mesmo grupo notarem o mesmo problema, é possível criar uma ""rede de apoio"", sugerem Bittencourt e Bennett. Quando uma for interrompida, outra pode chamar a atenção, dizendo ""Ela ainda não terminou de falar"", por exemplo. 2) Receber explicações óbvias Outra situação típica de machismo no trabalho, segundo Bennett: um homem parte do pressuposto que a mulher não entende de um tema simples e óbvio e decide explicar --sem que ela tenha pedido. Pode acontecer também no caso de um leigo tentando ensinar a uma mulher um assunto sobre o qual ela é especialista. Conhecido como ""mansplaining"", o comportamento é descrito no livro ""Men Explain Things to me"" (Homens Explicam Coisas para Mim), de Rebecca Solnit. Solnit conta que um homem passou uma festa inteira falando sobre um livro que ""ela deveria ler"", sem dar-lhe a chance de dizer que, na verdade, ela era a autora. O que fazer? A mulher deve deixar claro que já conhece o assunto, sugere Collini. Se for especialista naquele tema, deve dizer o que estudou e mostrar os resultados de sua pesquisa, ""para não ficar apenas no argumento de autoridade"", sugere a especialista. 3) Ouvir piadas sobre TPM Fazer piadas sobre TPM (tensão pré-menstrual) quando uma mulher age de forma mais assertiva no trabalho também pode ser considerado machismo. Frases como ""ela deve estar naqueles dias"" embutem a ideia de que os hormônios fazem as mulheres serem menos racionais que os homens, segundo as especialistas. ""Não é justo, mas acontece: homens ganham status quando agem de forma agressiva. São vistos como apaixonados pelo trabalho. Já mulheres que agem da mesma maneira perdem status e são vistas como loucas"", diz Bennett. O que fazer? Uma saída, segundo Bennett, é a profissional enfatizar por que está brava: porque fulano deixou de entregar um relatório no prazo ou beltrano não cumpriu uma obrigação, por exemplo. ""A ideia é mostrar que não está dando um 'chilique'. Alguém fez besteira e isso está afetando seu trabalho"", diz a jornalista. 4) Tratá-la como assistente (se você não é) Outra situação que pode ser considerada machista é pedir a uma mulher, por exemplo, para servir café ou tirar xerox, mesmo isso não estando nas atribuições de seu cargo. Segundo as especialistas, alguns homens presumem que cabem às mulheres funções de assistente ou de secretária, ou que o trabalho delas é menos importante que o deles. O que fazer? A mulher deve dizer que aquela tarefa não faz parte de seu trabalho, sugere Collini. ""Mesmo que não se importe de tirar o xerox, deixe claro que o faz por gentileza, não por obrigação"". Bennett menciona em seu livro um exemplo dado pela psicóloga organizacional Katharine O'Brien: ""Se pedirem para você ficar responsável por tomar notas em uma reunião, por exemplo, diga que não o fará porque essa tarefa coloca as mulheres numa posição de subordinação --por terem que tomar notas, e não falar."" 5) Ouvir comentários negativos sobre gravidez e maternidade Outra situação machista que pode acontecer no ambiente de trabalho é colegas ou chefes dizerem que mulheres grávidas ou mães são preguiçosas e/ou trabalham menos. Segundo a juíza do trabalho Elinay Melo, se esse tipo de comentário for feito repetidamente na presença de funcionárias gestantes ou mães pode até ser caracterizado como assédio moral. ""Elas podem procurar a Justiça do Trabalho, além do sindicato da categoria."" O que fazer? Ainda que o comentário negativo seja feito uma única vez, a profissional pode responder que, mesmo sendo mãe, continua comprometida com o trabalho. Se estiver grávida, pode pedir para marcar uma reunião com seu chefe para tratar de seus objetivos na carreira quando voltar da licença-maternidade, sugere Bennett. 6) Ter ideias 'roubadas' Outra situação comum no escritório, segundo Collini: uma profissional apresenta uma ideia em uma reunião, mas é ignorada; depois, um colega é aplaudido ao apresentar a mesma ideia --e não diz que, na verdade, a autoria foi de sua colega. O que fazer? A mulher deve apontar que foi ela quem deu a ideia pela primeira vez, usando frases como ""Obrigada por apoiar a minha a ideia"" ou ""Que bom que você concorda comigo"". Uma reportagem publicada pelo jornal ""The Washington Post"" conta que mulheres da equipe do presidente dos Estados Unidos, Barack Obama, adotaram uma estratégia chamada de ""amplificação"" . Quando uma profissional dá uma ideia durante uma reunião, as outras repetem-na dando crédito à autora. Essa prática, segundo a reportagem, força os homens presentes a reconhecerem a contribuição feita pelas mulheres à discussão e reduz as chances de que eles tentem se apropriar das ideias delas. Obrigado pela sua inscrição",pt,47
397,1233,1464907229,CONTENT SHARED,-348408475077850711,-8020832670974472349,2348433893919184737,,,,HTML,https://www.oreilly.com/ideas/introduction-to-release-engineering?utm_medium=social&utm_source=twitter.com&utm_campaign=lgen&utm_content=mcnuttsre-excerpt-jj&cmp=tw-webops-books-videos-article-vlca16_mcnuttsre_excerpt_jj,introduction to release engineering,"Curious about release engineering? Some would call release engineering an art. Here's how Google goes about it. Wide-angle Optical Multi-channel Probe (WOMP) (source: Energy.gov ). This is an excerpt from Site Reliability Engineering , edited by Niall Richard Murphy, Jennifer Petoff, Chris Jones, Betsy Beyer. The Role of a Release Engineer Google is a data-driven company and release engineering follows suit. We have tools that report on a host of metrics, such as how much time it takes for a code change to be deployed into production (in other words, release velocity) and statistics on what features are being used in build configuration files. Most of these tools were envisioned and developed by release engineers. Release engineers define best practices for using our tools in order to make sure projects are released using consistent and repeatable methodologies. Our best practices cover all elements of the release process. Examples include compiler flags, formats for build identification tags, and required steps during a build. Making sure that our tools behave correctly by default and are adequately documented makes it easy for teams to stay focused on features and users, rather than spending time reinventing the wheel (poorly) when it comes to releasing software. Google has a large number of SREs who are charged with safely deploying products and keeping Google services up and running. In order to make sure our release processes meet business requirements, release engineers and SREs work together to develop strategies for canarying changes, pushing out new releases without interrupting services, and rolling back features that demonstrate problems. Philosophy Release engineering is guided by an engineering and service philosophy that's expressed through four major principles, detailed in the following sections. Self-Service Model In order to work at scale, teams must be self-sufficient. Release engineering has developed best practices and tools that allow our product development teams to control and run their own release processes. Although we have thousands of engineers and products, we can achieve a high release velocity because individual teams can decide how often and when to release new versions of their products. Release processes can be automated to the point that they require minimal involvement by the engineers, and many projects are automatically built and released using a combination of our automated build system and our deployment tools. Releases are truly automatic, and only require engineer involvement if and when problems arise. High Velocity User-facing software (such as many components of Google Search) is rebuilt frequently, as we aim to roll out customer-facing features as quickly as possible. We have embraced the philosophy that frequent releases result in fewer changes between versions. This approach makes testing and troubleshooting easier. Some teams perform hourly builds and then select the version to actually deploy to production from the resulting pool of builds. Selection is based upon the test results and the features contained in a given build. Other teams have adopted a ""Push on Green"" release model and deploy every build that passes all tests. Hermetic Builds Build tools must allow us to ensure consistency and repeatability. If two people attempt to build the same product at the same revision number in the source code repository on different machines, we expect identical results. Our builds are hermetic, meaning that they are insensitive to the libraries and other software installed on the build machine. Instead, builds depend on known versions of build tools, such as compilers, and dependencies, such as libraries. The build process is self-contained and must not rely on services that are external to the build environment. Rebuilding older releases when we need to fix a bug in software that's running in production can be a challenge. We accomplish this task by rebuilding at the same revision as the original build and including specific changes that were submitted after that point in time. We call this tactic cherry picking . Our build tools are themselves versioned based on the revision in the source code repository for the project being built. Therefore, a project built last month won't use this month's version of the compiler if a cherry pick is required, because that version may contain incompatible or undesired features. Enforcement of Policies and Procedures Several layers of security and access control determine who can perform specific operations when releasing a project. Gated operations include: Approving source code changes-this operation is managed through configuration files scattered throughout the codebase Specifying the actions to be performed during the release process Creating a new release Approving the initial integration proposal (which is a request to perform a build at a specific revision number in the source code repository) and subsequent cherry picks Deploying a new release Making changes to a project's build configuration Almost all changes to the codebase require a code review, which is a streamlined action integrated into our normal developer workflow. Our automated release system produces a report of all changes contained in a release, which is archived with other build artifacts. By allowing SREs to understand what changes are included in a new release of a project, this report can expedite troubleshooting when there are problems with a release. Google uses a monolithic unified source code repository. Article image: Wide-angle Optical Multi-channel Probe (WOMP) (source: Energy.gov ). Dinah McNutt is a member of the Google Release Team. She assists teams in Google's Technical Infrastructure organization with automating their build and deployment processes. She was the program chair for USENIX Release Engineering Workshop '14, chaired LISA VIII, sat on several program committees, and gave talks and tutorials at numerous conferences including the Sun User Group, LISA, and Usenix Configuration Management Conference. Building functionality that really delivers the expected customer value The secret to successful infrastructure automation is people. Designing, building, and operating services from the perspective of customer goals helps improve quality. Learn about key microservices principles like Last Responsible Moment, risk sliders to assess benefit versus risk, heatlhchecks, and metrics.",en,47
398,1879,1469534467,CONTENT SHARED,623421554979899217,7774613525190730745,-2746681480439347704,,,,HTML,https://blog.bitbucket.org/2016/07/18/git-large-file-storage-now-in-bitbucket-cloud/,git large file storage (git lfs) now in bitbucket cloud - bitbucket,"By Szilard Szasz-Toth on July 18, 2016 In recent years software teams across all industries have adopted Git thanks to its raw speed, distributed nature and powerful workflows. Additionally, modern software teams are increasingly cross-functional and consist not only of developers but designers, QA engineers, tech writers, and more. In order to be successful these teams need to collaborate not just on raw source code but on rich media and large data. It's no secret though that Git doesn't handle large files very well and quickly bloats your repositories. We are therefore excited to announce that, following Bitbucket Server's lead earlier this year, to improve the handling of your large assets. So even if your files are really, really large, Bitbucket Cloud allows your team to efficiently store, version and Why should you care about Git LFS? Git was optimized for source - it's easily merged and compressed and is relatively small, so it is perfectly feasible to store all history everywhere, but this makes it inefficient and slow when trying to track large binary files. For example, If your designer stores a 100 MB image in your Git repository and modifies it nine times, your repository could bloat to almost 1 GB in size, since binary deltas often compress poorly. Every developer, build agent, and deploy script cloning that repository would then have to download the full 1 GB history of changes, which may lead to drastically longer clone imagine what would have happened if your designer made 99 changes to that file. A common solution to this inherent flaw in Git is to track these large files outside of Git, in local storage systems or cloud storage providers, which leads to a whole new set of problems. Separating your large files from your repository will require your team to manually sync and communicate all changes to keep your code working. With the addition of Git LFS support, you can say goodbye to all these problems track all your files in one place in Bitbucket Cloud . Instead of bloating your Git repository, large files are kept in , and only lightweight references are stored making your repositories smaller and . The next time your team clones a repository with files stored in Git LFS, only the references and relevant large files that are part of your checked out revision will get downloaded, not the entire change history. For those interested in a longer explanation of how Git LFS works and how to migrate your existing repository, watch this presentation by Tim Pettersen, Atlassian Developer Advocate, on Tracking huge files with Git LFS . Generally, if you want to use Git to easily version your large files, Git LFS is the right choice. To call out just a few cases in which Git LFS will make your life easier, here's a short list: Sign up for Bitbucket",en,47
399,1338,1465568090,CONTENT SHARED,-2855910583928922225,-709287718034731589,3285541426833936337,,,,HTML,http://daringfireball.net/2016/06/the_new_app_store,"the new app store: subscription pricing, faster approvals, and search ads","""We're doing something a little different this year. We've got a bunch of App Store/developer-related announcements for WWDC next week, but frankly, we've got a busy enough keynote that we decided we're not going to cover those in the keynote. And rather, just cover them in the afternoon and throughout the week. We're talking to people today for news tomorrow about those things, in advance of WWDC, and then developers can come and be ready for sessions about these things, with knowledge about them before the conference. We haven't done this before, but we figured, what the heck, let's give it a try."" So started my phone call with Phil Schiller yesterday. Doing something a little different, indeed. These changes and improvements to the App Store - or App Store s , if you prefer, because they all apply to iOS (and thus WatchOS), Mac OS X, and tvOS - are no little things. These changes fundamentally change the App Store, for users and especially for developers. A quick summary: App Store review times are now much shorter. These changes are already in place, and have been widely noted in recent weeks. Apple is today confirming they're not a fluke - they're the result of systemic changes to how App Store review works. Subscription-based pricing was heretofore limited to specific app categories. Now, subscription-based pricing will be an option for any sort of app, including productivity apps and games. This is an entirely new business model for app developers - one that I think will make indie app development far more sustainable. Changes to app discovery, including a smarter ""Featured"" tab, the return of the ""Categories"" tab, and, yes, as rumored , paid search ads. Subscription-Based Pricing for All Until now, subscription pricing was reserved for apps that served media content: streaming audio and video, news, etc. Apple is now opening it to apps from any category, which effectively solves the problems of recurring revenue and free trials. Even better, Apple is changing the revenue split for all subscriptions: for the first year of any subscription, the revenue split remains 70/30; after the first year, the revenue split changes to 85/15. In Schiller's words, this is ""in recognition that the developer is doing most of the work"" with any app that is so good that the user remains a paid subscriber for over a year. This change is effective starting this Monday - any app that already has subscribers will start splitting revenue with Apple 85/15 on subscriptions that are at least a year old. This dramatically changes the economics of the App Store. Until now, productivity apps could charge up front as paid downloads and that was it. Updates had to be free, or, to charge for major new versions, developers would have to play confusing games by making the new version an entirely new SKU in the app store. Twitter clients like Tweetbot and Twitterrific, for example, did this, to justify years of ongoing development. Now, apps like this can instead charge an annual/monthly/etc. subscription fee. This could be the change that makes the market for professional-caliber iPad apps possible. On the Mac, there has long been a tradition of paying a large amount of money for a pro app, then paying a smaller amount of money for major updates. The App Store has never allowed for that sort of upgrade pricing - but upgrade pricing is what enabled ongoing continuous development of pro software. Paying for each major new version, however, is arguably a relic of the age when software came in physical boxes. Subscription-based pricing - ""software as a service (SaaS)"" - is the modern equivalent. That's the route both Microsoft and Adobe have taken. In the old world of boxed software and installation disks, the natural interval was the version. In today's world where everything is a download, months or years are more natural payment intervals. Now, it's available to all app categories in the App Store. I think this is terrific news both for developers and users. Other changes: There are now more than 200 price points available to subscription-based apps. Apps can now change subscription prices easily. This was a huge pain in the ass previously. When an app changes subscription pricing, existing subscribers will be notified automatically and given the option to agree to the new pricing or unsubscribe. Subscription-based apps can now offer multiple tiers. Think bronze/silver/gold. Again, previously, Apple's app subscription APIs made this difficult if not impossible. Now, it should be easy. And it should be easy for users to change tiers on the fly. Territory-based pricing is now possible. Developers can use this to charge lower prices in countries like China and India. This was not possible before. Most subscription-based apps use either monthly or annual renewal intervals. But apps have the option of renewing every two months, three months, or six months as well. Developers have been asking for a way to do free trials and to sustain long-term ongoing development ever since the App Store opened in 2008. This is Apple's answer. I think all serious productivity apps in the App Store should and will switch to subscription pricing. You might argue that people don't want to subscribe to a slew of different apps. But the truth is most people don't want to pay for apps, period. Nothing will change that. But for those people willing to pay for high quality apps, subscriptions make sustainable-for-developer pricing more palatable, and more predictable. App Store Review Times This is mostly a benefit to developers, who get a quicker turnaround time. Users don't really see the review process - it's hidden from them. Users just see when updated apps are available to them in the App Store. But there is a benefit to users of faster turnaround times - when developers fix bugs, users will get those bug fixes much sooner. According to Schiller, 50 percent of submitted apps are now approved within 24 hours; 90 percent in 48 hours. For years, turnaround time was measured in days, not hours, and was generally in the range of a week. Schiller told me this change can be attributed to three things: Tool improvements internal to Apple. Staffing changes. Policy changes. I asked for details regarding those ""policy changes"", and Schiller demurred. One thing he emphasized, however, is that the rules for apps haven't changed at all. If anything, Schiller claimed, with the new tooling at the disposal of reviewers, reviews are even better at identifying apps with quality problems than before. The impression I'm left with is that reviewers are now given more discretion to fast track apps from long-time trusted developers, once their binaries have passed Apple's automated tests. One can argue that this is long overdue, but better late than never. Ads in Search Results I was against this when it first leaked, and I remain skeptical. But now that I've seen the details , I'm OK with it. Here's what Schiller told me: One and only one ad will be shown at a time. Ads are clearly marked . They're always at the top, have a light blue background, and a blue ""Ad"" tag. Ads will be relevant to the search terms from the user. We shall see if Apple succeeds here, but the intention is good. Ads are only shown in search results. There is no pay-for-placement in the Featured section. Ads are paid for in a second-price auction system. This means the winning bid only pays ""just enough"" more than the second-place bid for a keyword. No ads will be shown to children 13 or younger. Ads are pay-for-click. (Or pay-for- tap , on iOS.) Developers only pay when actual users actually tap their paid placement. Apple has been leaving a lot of money on the table for Google and Facebook, both of which have been raking in money hand over fist from developers paying for ads that lead to app downloads. Schiller only mentioned ""search engines"" and ""social networks"", but the truth is they should have been singulars, not plurals. It's Google and Facebook. Apple's system does not use tracking. No profiles are kept of users, and no user-identifying information is sent to advertisers. And users can opt out of things like location-based ads with the system level preferences for location privacy. Downloads are already being driven by paid ads, so they might as well be in the App Store itself, where Apple can take some of the money and deliver ads that live up to its standards for privacy. It's also important to note that the ads are the same as the regular App Store listings. What you'll see is the same exact content, vetted through the same approval process, as the regular app store listing. Developers are not paying to get a discrete ""ad"" displayed, per se, but paying to get their regular App Store listing displayed as an ad. My concern when word first leaked about this is that the system would be geared toward large developers. Schiller emphasized to me that Apple designed this system with small indie developers in mind. 65 percent of app downloads start with searches in the App Store, according to Schiller - effectively two out of every three downloads. App Store search is absolutely essential to discovery. These ads should help small developers gain exposure - and they should be more efficient than ad spends at Google or Facebook, because they only appear in the context of a user who is already searching for an app. The Big Picture Collectively, these changes should put to rest any notion that the App Store is neglected or in any way an afterthought for Apple. I'm certainly glad about the vastly improved turnaround time for App Store submission reviews, and I have a certain ""wait and see"" ambivalence regarding the App Store search ads. But I am genuinely excited about subscription pricing being an option for all apps. I think it's truly win-win-win, for Apple, its users, and most importantly for developers. For Apple, Ben Thompson nailed it back in 2013 : What makes monetizing productivity apps so tricky is that they are indispensable to some consumers, yet overwhelming to others. It's that indispensable part, though, that should matter to platform owners. If a user comes to depend on certain productivity apps that are only available on one platform - and, in general, mobile productivity apps are much more likely to be monogamous - then that user is effectively bound to the platform, and won't even consider another platform when it comes time to upgrade. The opportunity for growth in smartphones is increasingly previous-smartphone owners (as opposed to new smartphone owners). Keeping those owners around should be a top priority for every platform, and one of the best ways to do so is fully supporting a subscription model for productivity apps. It will make them more successful and thus stickier, ultimately to the platform's long-term benefit. For users, the benefit is that they should see more high quality productivity apps. And they'll only have to pay for them as long as they're actually using them. For developers, I think this is the first time the App Store supports a business model that sustains long-term ongoing development of deep applications. The proof of the pudding is in the eating, and so we'll have to wait and see if this actually changes the economic dynamics of developing for the App Store. But Phil Schiller's enthusiasm for small indie developers is undeniable. It's palpable. The new 85/15 revenue split after the first year of a subscription is proof of it. He's trying, and now that the entire App Store is under his leadership , that means Apple is trying. I would argue that the App Store has changed more in the last six months than it did in the previous eight years. Throw in a ""finally"" if you want, but again, I say better late than never.",en,46
400,1468,1466507526,CONTENT SHARED,-7342604649736473979,-2979881261169775358,-4081266306360200792,,,,HTML,https://blog.docker.com/2016/06/docker-1-12-built-in-orchestration/,docker 1.12: now with built-in orchestration!,"Three years ago, Docker made an esoteric Linux kernel technology called containerization simple and accessible to everyone. Today, we are doing the same for container orchestration. Container orchestration is what is needed to transition from deploying containers individually on a single host, to deploying complex multi-container apps on many machines. It requires a distributed platform, independent from infrastructure, that stays online through the entire lifetime of your application, surviving hardware failure and software updates. Orchestration is at the same stage today as containerization was 3 years ago. There are two options: either you need an army of technology experts to cobble together a complex ad hoc system, or you have to rely on a company with a lot of experts to take care of everything for you as long as you buy all hardware, services, support, software from them. There is a word for that, it's called lock-in. Docker users have been sharing with us that neither option is acceptable. Instead, you need a platform that makes orchestration usable by everyone, without locking you in. Container orchestration would be easier to implement, more portable, secure, resilient, and faster if it was built into the platform. Starting with Docker 1.12, we have added features to the core Docker Engine to make multi-host and multi-container orchestration easy. We've added new API objects, like Service and Node, that will let you use the Docker API to deploy and manage apps on a group of Docker Engines called a swarm. With Docker 1.12, the best way to orchestrate Docker is Docker! The Docker 1.12 design is based on four principles: Simple Yet Powerful - Orchestration is a central part of modern distributed applications; it's so central that we have seamlessly built it into our core Docker Engine. Our approach to orchestration follows our philosophy about containers: no setup, only a small number of simple concepts to learn, and an ""it just works"" user experience. Resilient - Machines fail all the time. Modern systems should expect these failures to occur regularly and adapt without any application downtime that's why azero single-point-of-failure design is a must. Secure - Security should be the default. Barriers to strong security - certificate generation, having to understand PKI - should be removed. But advanced users should still be able to control and audit every aspect of certificate signing and issuance. Optional Features and Backward Compatibility - With millions of users, preserving backwards compatibility is a must for Docker Engine. All new features are optional, and you don't incur any overhead (memory, cpu) if you don't use them. Orchestration in Docker Engine aligns with our platform's batteries included but swappable approach allowing users to continue using any third-party orchestrator that is built on Docker Engine. Let's take a look at how the new features in Docker 1.12 work. Creating Swarms with One Decentralized Building Block It all starts with creating a swarm-a self-organizing and self-healing group of engines-which for the bootstrap node is as simple as: Under the hood this creates a Raft consensus group of one node. This first node has the role of manager, meaning it accepts commands and schedule tasks. As you join more nodes to the swarm, they will by default be workers, which simply execute containers dispatched by the manager. You can optionally add additional manager nodes. The manager nodes will be part of the Raft consensus group. We use an optimized Raft store in which reads are serviced directly from memory which makes scheduling performance fast. Creating and Scaling Services Just as you run a single container with docker run, you can now start a replicated, distributed, load balanced process on a swarm of Engines with docker service: This command declares a desired state on your swarm of 5 nginx containers, reachable as a single, internally load balanced service on port 80 of any node in your swarm. Internally, we make this work using Linux IPVS , an in-kernel Layer 4 multi-protocol load balancer that's been in the Linux kernel for more than 15 years. With IPVS routing packets inside the kernel, swarm's routing mesh delivers high performance container-aware load-balancing. When you create services, can optionally create replicated or global services. Replicated services mean any number of containers that you define will be spread across the available hosts. Global services, by contrast, schedule one instance the same container on every host in the swarm. Let's turn to how Docker provides resiliency. Swarm mode enabled engines are self organizing and self healing, meaning that they are aware of the application you defined and will continuously check and reconcile the environment when things go awry. For example, if you unplug one of the machines running an nginx instance, a new container will come up on another node. Unplug the network switch for half the machines in your swarm, and the other half will take over, redistributing the containers amongst themselves. For updates, you now have flexibility in how you re-deploy services once you make a change. You can set a rolling or parallel update of the containers on your swarm. Want to scale up to 100 instances? It's as simple as: A typical two-tier (web+db) application would be created like this: This is the basic architecture of this application: Security A core principle for Docker 1.12 is creating a zero configuration, secure-by-default, out of the box experience for the Docker platform. One of the major hurdles that administrators often face with deploying applications into production is running them securely, Docker 1.12 allows an administrator to follow the exact same steps setting up a demo cluster that they would to setup a secure production cluster. Security is not something you can bolt-on after the fact. That is why Docker 1.12 comes with mutually authenticated TLS, providing authentication, authorization and encryption to the communications of every node participating in the swarm, out of the box. When starting your first manager, Docker Engine will generate a new Certificate Authority (CA) and a set of initial certificates for you. After this initial step, every node joining the swarm will automatically be issued a new certificate with a randomly generated ID, and their current role in the swarm (manager or worker). These certificates will be used as their cryptographically secure node identity for the lifetime of their participation in this swarm, and will be used by the managers to ensure secure dissemination of tasks and other updates. One of the biggest barriers of adoption of TLS has always been the difficulty of creating, configuring and maintaining the necessary Public Key Infrastructure (PKI). With Docker 1.12, everything not only gets setup and configured with safe defaults for you, but we also automated one of the most painful parts of dealing with TLS certificates: certificate rotation. Under the hood, every node participating in the swarm is constantly refreshing its certificates, ensuring that potentially leaked or compromised certificates are no longer valid. The frequency with which certificates are rotated can be configured by the user, and set as low as every 30 minutes. If you would like to use your own Certificate Authority, we also support an external-CA mode, where the managers in the swarm simply relay the Certificate Signing Requests of the nodes attempting to join the cluster to a remote URL. Bundles Docker 1.12 introduces a new file format called a Distributed Application Bundle (experimental build only). Bundle is a new abstraction on top of service focused on the full stack application. A Docker Bundle file is a declarative specification of a set of services that mandates: What specific image revision to run What networks to create How containers in those services must be networked to run Bundle files are fully portable and are perfect deployment artifacts for software delivery pipelines because they let you ship fully spec'ed and versioned multi-container Docker apps. The bundle file spec is simple and open, and you can create bundles however you want. To get you started, Docker Compose has experimental support for creating bundle files and with Docker 1.12 and swarm mode enabled, you can deploy the bundle files. Bundles are an efficient mechanism for moving multi-service apps from developer laptops through CI to production. It's experimental, and we're looking for feedback from the community. Under the hood of Docker 1.12 When you take a look under the hood, Docker 1.12 uses a number of other interesting technologies. Inter-node communication is done using gRPC , which gives us HTTP/2 benefits like connection multiplexing and header compression. Our data structures are transmitted efficiently thanks to protobufs . Check out these additional resources on Docker 1.12: Learn More about Docker docker 1.12 , Docker orchestration , dockercon , orchestration",en,46
401,1406,1466007477,CONTENT SHARED,18930987176089166,-1578287561410088674,241349347866220587,,,,HTML,http://materialdesignblog.com/10-multipurpose-material-design-themes-to-make-your-projects-10-times-better/,10 multipurpose material design themes to make your projects 10 times better,"The Web trends tend to change with a lightning speed, and there are 2 latest trends which we are going to combine in this blog post. The first one is a material design trend and the second one is a multipurpose theme usage trend. Thanks to Google and their launch of the material design as a concept, it became one of the leading trends in the last years. Not surprisingly, as it definitely has its benefits - it improves user engagement, makes a better UI/UX, it's cost effective etc. Nevertheless, at the very launch of it, hundreds of people disagreed with the new trend as it eliminated any trace of skeuomorphism and any resemblance of the design to the live objects. Artificially plain colors were unusual to a viewer's eye. Meanwhile, hundreds of designers' faces turned towards the new material design trend, feeling a huge potential in it. That was the dawn of material design as a concept when hundreds of material design websites appeared. Right now there are dozens of beautiful material design templates and free material design resources available for any project related to material design. At the same time there are separate templates for blogs, online shops, agency websites and so on. However, there is a different option - using a multipurpose theme . And here comes the second trend! What are the benefits of using multipurpose themes? Firstly, it lets you choose the design which you like. Secondly, even if you decide to alter the design when your website is already made, normally you'd have to change horses in midstream and change the template itself, but not if you have a multipurpose template. You may simply switch the design and enjoy the result. On top of all that, multipurpose templates fit almost all project types - eCommerce, blogs, agencies etc. In this blog post we've gathered 10 greatest multipurpose material design templates from different CMS (the best ones among WordPress, PrestaShop and the best of HTML). Please note that we don't name the templates of a specific type (like WordPress themes or Joomla templates only), but we name the best multipurpose templates of many CMS which you may use for your project. Monstroid is more than just a WordPress theme, it rather reminds us of a WordPress theme club. Being a multipurpose template, it lets you change the design, move the design items, customize it up to your needs, while you have 40+ of child themes just inside of it. This is obviously very handy to most of the customers, as Monstroid can transform into a blog, an agency or a portfolio website, and, on top of that, it can also become an eCommerce store. That is pretty impressive. Intense if great multipurpose HTML template which also has its home skins and child themes. How do you benefit from it? You can choose either a totally different design of the whole website, or pick a home page design, leaving the rest as it is. There's been a full review of Intense published recently , by the way. Wegy is King among Joomla templates. It involves advanced theme options, sliced PSD, alternative module layouts, it has a neat modern design which makes it perfect for many types of projects. Pay attention to the drop-down menu which has a banner, that usually raises a website conversion a lot. Styler is one of the most downloadable Prestashop themes. When creating a website, you may choose a home, product and product listing pages which you like. It's a decent option for an eCommerce website, build on Prestashop. And, just like all usual themes, it has all the common features like social media buttons, hover effects, Parallax effect and a blog. Get your customers hooked at first sight! Good material design HTML templates are not hard to find, however, good multipurpose material design HTML templates are. Take a look at this template which is crafted for a transportation agency or any other project which could benefit from clean design, Parallax effect, Google maps, Google fonts in it and so on. Visual presentation of the data and bright colors will attract the visitors and gain customers for the business. Modicate Multipurpose Website Template Undoubtedly, material design makes some things easier. For instance, the widgets or the categories sections look much cleaner when performed in material design. Modicate is an example of how a corporate material design HTML template may look. See for yourself, check out the design options, choose the ones you like and keep in mind that this is how your project could look like. Do you like the overall impression? Then maybe it's the one! Construction Multipurpose Website Template Any engineering-related project like construction, for instance, need to look trustworthy in colors, in design, even in an intuitive layout (no one who'd experience any problems with navigation on the website would entrust something to this company in reality). Take a look at the widgets, they are simply irresistible to click on. Bakery Multipurpose Website Template Testimonials are one of those sure ways to boost conversion of the website. This material design HTML template has them. Moreover, its features like contact form, material design social media buttons, back-to-top buttons and seamless image slider make the design attractive, welcoming and cozy, which are is perfect description of a bakery website. Moderno Marketing Agency Responsive Website Template It's a multipurpose material design HTML template for corporate projects which may become a decent presentation of an online business presence. Thanks to its responsiveness, it'll look well on any screen. Clean code, a grid layout, Parallax effect, special widgets with hover effects will win the website a competitive place in the professional niche. Fashion Clothes Store Magento Theme An impressive design is vital for any online store as it is one of those factors which influence the conversion. This is the template which could become a great basis for a Magento online store. The material design elements implied in it improve the user experience. Subconsciously, the customers will compare your online store to the other which they have visited, and a design like this will make your website be one of the top websites they have been to. If you liked the roundup please don't forget to spread a word about it. And if you didn't find the design you were looking for remember that these are multipurpose themes with tons of designs and skins built in them. Just follow any Demo link and you'll see how many pages on different topics does each of this themes have! Don't forget to share and good luck with your future projects!",en,46
402,1322,1465496331,CONTENT SHARED,-1101832997669013246,5660542693104786364,-7990858727359702237,,,,HTML,http://www.businesswire.com/news/home/20160602006259/en/Visa-Introduces-NFC-Enabled-Payment-Ring-Team-Visa,visa introduces nfc-enabled payment ring for team visa sponsored athletes to use at rio 2016 olympic games,"SAN FRANCISCO--()--Visa Inc. (NYSE:V), the exclusive payment provider at the Olympic and Paralympic Games, today introduced a new innovation for use at the Rio 2016 Games - the first payment wearable ring backed by a Visa account. The Visa payment ring will be given to all Team Visa athletes in Rio, a group of 45 Olympic hopefuls from around the world who embody Visa's values of acceptance, partnership and innovation. The Visa payment ring is NFC-enabled, allowing Team Visa athletes to make purchases by simply tapping their ring at any NFC-capable payment terminal. Key features of the ring make this a unique payment experience. The ring uses the patented NFC Ring ® design of McLear & Co. that includes a secure microchip made by Gemalto, with an embedded NFC-enabled antenna, enabling contactless payment capabilities. Unlike many other payment wearables, the ring does not require use of a battery or recharging. It is also water resistant to a depth of 50 meters, meaning Team Visa athlete and Olympic gold medalist Missy Franklin can go from the pool to payment all with the tap of her ring. In addition, at an event in New York City today, Visa demonstrated an advanced prototype version of the Visa payment ring, which uses token technology provided through Visa Token Service , making it the first tokenized payment ring. Visa's token technology replaces sensitive payment information, such as the 16-digit account number, with a unique digital identifier that can be used to process payments without exposing actual account details. ""Visa's first payment ring puts smart payment technology right on the hands of our athletes for convenient and easy payments,"" said Jim McCarthy, executive vice president of innovation and strategic partnerships at Visa Inc. ""This ring is the latest example of how Visa is continuously innovating to deliver on its goal of universal acceptance at the games and across the world."" Athletes and fans will also be able to use their Visa accounts leading up to, and at the Rio Games, using various new form factors. From booking and planning their trip using Visa Checkout or making purchases at the games on their mobile phone, both experiences are first-time offerings for Rio 2016, enabling athletes and fans to swipe, tap, dip or click to pay during their Olympic experience. ""As an Olympian, rings have a special meaning to me,"" said Missy Franklin, a four-time Olympic gold medalist and Team Visa athlete. ""The Visa ring is a great innovation that I know all the athletes competing in Rio will enjoy as it will be great to go from a competition to purchase without having to carry a wallet or card."" As the exclusive payment provider of the Olympic Games, Visa is creating and managing the entire payment system infrastructure and network throughout all venues including stadiums, press centers, point-of-sale (POS), the Olympic Village and Olympic Superstores. In Rio, Visa will implement approximately 4,000 NFC-enabled POS terminals capable of accepting mobile and wearable payments across key Olympic venues, the US Olympic Committee's USA House and Copacabana Megastore. As the payments industry increasingly shifts from plastic to digital, new technology advances from Visa and its partners are bringing consumers a simple and more secure purchasing experience. About Visa Inc. : Visa Inc. (NYSE: V) is a global payments technology company that connects consumers, businesses, financial institutions and governments in more than 200 countries and territories to fast, secure and reliable electronic payments. We operate one of the world's most advanced processing networks - VisaNet - that is capable of handling more than 65,000 transaction messages a second, with fraud protection for consumers and assured payment for merchants. Visa is not a bank and does not issue cards, extend credit or set rates and fees for consumers. Visa's innovations, however, enable its financial institution customers to offer consumers more choices: pay now with debit, pay ahead of time with prepaid or pay later with credit products. For more information, visit usa.visa.com/about-visa , visacorporate.tumblr.com and @VisaNews .",en,46
403,2112,1471571375,CONTENT SHARED,4508362112182399467,-1443636648652872475,5370317555391394665,,,,HTML,https://techcrunch.com/2016/08/18/facebooks-artificial-intelligence-research-lab-releases-open-source-fasttext-on-github/,facebook's artificial intelligence research lab releases open source fasttext on github,"Every day, billions of pieces of content are shared on Facebook. To keep up with the data, Facebook has been using a variety of tools to classify text. Traditional methods of classification, like deep neural networks, are accurate but have serious training requirements. In an effort to classify both accurately and easily, Facebook's Artificial Intelligence Research (FAIR) lab developed fastText. Today, fastText is going open source so developers can implement its libraries anywhere. FastText supports both text classification and learning word vector representations through techniques like bag of words and subword information . Based on the skip-gram model, words are represented as bag of character n-grams with vectors representing each character n-gram. ""In order to be efficient on datasets with very large number of categories, fastText uses a hierarchical classifier, in which the different categories are organized in a tree, instead of a flat structure (think binary tree instead of list),"" said Facebook authors Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov in a post. For those less artificially intelligent, the bag of words process is fast because it essentially ignores word order and instead focuses on the occurrences of a word. ""Words"" are represented in a multidimensional space and linear algebra is used to calculate the relationship between a query and a categorized set of words. Remember that when we feed a computer text, we are starting from scratch. To adults, grammar is intuitive - we know what words are, where they end and where they begin. Computers can handle the most complex computational challenges, but can struggle to differentiate ""I love TechCrunch"" from ""CrunchLove iTech."" Methods like this essentially take a qualitative analysis problem and force it to be quantitative through the addition of statistics. These techniques enable fastText to be faster than traditional deep learning methods. Facebook created this nifty comparison chart to show us side-by-side accuracy. FastText is not restricted to English and can work with other languages including German, Spanish, French, and Czech. Earlier this month, Facebook implemented an anti-clickbait algorithm into its Newsfeed. While the algo is quite complicated and focuses on both behavioral identifiers and language, fastText enables developers to create similar tools themselves. Not to brag but Facebook says that the new open source technology can be ""trained on more than 1 billion words in less than 10 minutes using a standard multicore CPU. fastText can also classify a half-million sentences among more than 300,000 categories in less than five minutes."" #HumbleBrag Starting today, Facebook's fastText will be available from their GitHub .",en,46
404,1771,1468503361,CONTENT SHARED,667643281369196267,2754566407772265068,6939571413154315416,,,,HTML,http://zeroturnaround.com/rebellabs/java-tools-and-technologies-landscape-2016/,java tools and technologies landscape report 2016 | zeroturnaround.com,"Welcome to the Java Tools and Technologies Landscape Report 2016 . This is a comprehensive report that is focused on analyzing the data about the tools and technologies Java developers use. There are three main parts to this report and they can be found through the links below: Alternatively, you can download a single pdf version of the report and enjoy it all at once: Editor's note Overall, we received 2040 completed survey responses. This gave us plenty of data with which I've been able to heavily improve my Excel knowledge. We also managed to raise $1000 for Devoxx4Kids , a wonderful charity, because we reached our 2000 response target. Great job to all those who took part and answered all of our questions! If you're interested in the previous editions of our survey reports, you can find them here: Java, the language and the platform, owes much of its fame and longevity to the libraries, frameworks and tools which together make up its ecosystem. No other programming language has been able to match the support that a rich ecosystem like the JVM has achieved. It's extremely important to understand how the ecosystem exists today, as well as its history. This way we can extract trends and patterns to understand the direction in which we are heading. In fact, all ecosystems are living organisms. They're born, they grow and become established, live their lives and one day will be replaced by a better, more suitable technology that solves the problem better. Of course the problems we deal with are changing too. Some ecosystems survive in care homes, refusing to pass away, like FORTRAN and COBOL. This report sets out to understand the current landscape in Java, the JVM and its ecosystem, including JVM languages, application servers, build tools, Java and Java EE adoption, microservices architectures, web frameworks, performance tooling, agile processes and much more. Our goal is to show you the raw data as well as profile which respondents are more likely to prefer certain tech over others based on their organization and other preferences. We'll also look forward, making predictions based on the data for what the landscape might look like in a couple more years. The Data The survey itself was open between March and April 2016 and profiled application types, processes, tools and more. Overall we received 2044 responses and we had to eliminate 4 based on their responses to the years of experience question. While it would be amazing to speak to someone who states they have over 10,000 years of experience, we feared the entry could well have been bogus! Please take the findings in this report at face value, and use at your own risk. If we assume that there are around 9M Java developers in the world the sampling error is around 2.2%. But since the sample is not truly random, it can contain some biases. Although you will see that the respondents description seems quite representative of the industry. This report The report is split into 3 sections. The first is a representation of the raw answers given to the survey questions. No fluff, no pivoting, just pure data and answers! Part 2 provides a more in-depth analysis, with pivoting on the data points to understand trends. Pivoting? Wasn't that a thing in my old physics class with a see-saw, some forces and some kind of formula? Well, maybe. But in this instance, we're asking more detailed questions about our data, based on the answers given to other questions. For instance, do early adopters that believe they're better than the average person in their role actually use technologies such as J2EE and Java 1.4? Read on to find out, the results may just surprise you! Part 3 looks at the data we collected in this survey and compares it to previous reports we've created - to see trends in adoption for tools and frameworks. But for now, let's start at the beginning with Part 1. Part 1: Raw data Welcome to the real start of the report! And congratulations for getting past the fluff that precedes the actual fun part. Let's look at the core results. The raw data. In this section,our headings are the questions that were asked during the survey. There's only one question that's missing: ""What tool, technology or library are you super excited or proud about having used or planning to use in 2016?"" . Don't worry, we didn't forget it! We thought that question is better suited to Part 3's future trends section. So, let's begin. How many years of experience do you have in software engineering? This was a question we've not asked in past surveys. This year, we wanted to understand more about the type of person who responds to the survey as well as their technology preferences. We actually had a pretty good spread of results as you can see from Figure 1.1 . Around half of the respondents exist in the 5-15 years of experience bracket. The overall median is 10 years of experience and the average is 12.2 years. What is your job description ? This is a common question that we ask every year. Our respondent split was similar to previous years, with the majority being made up of Software Developers (54%), followed by Architects (18%) and Team Leads (12%) as you can see in Figure 1.2 . The remaining split was fairly equal across consultants, management, C levels, and Dev advocates. The 'Other' category included operations and QA, among others. This was sad as it would have been great to get some of the opinions from operations teams on our microservices questions later. Overall, this spread of roles is what we would expect as the communities are heavily made up of developers and it's those communities which RebelLabs associates heavily with. Which description best fits the company you work for ? Here's another new question where we try to understand the style of company our respondents work for. Figure 1.3 shows us that almost one in three respondents work on the USS Enterprise, taking 32% of the share. The next most common response was a mid-sized company, at 31%. Other interesting responses included 5% of respondents working for a startup and 8% working as a contractor. Let's spare a moment to think about the 1% of respondents who are sadly out of work at the time of the survey. Hopefully they've found work since then, without needing to resort to PHP or JavaScript. What kind of person would you say you are? Many people or companies are opposed to trying something new, considering it wasteful to spend time on a technology that might not even exist in five years time. Or perhaps putting off an upgrade or version migration till they are sure all bugs/issues had been ironed out. This question was therefore designed to understand how likely someone was to try out a new tool or technology. We refer to respondents as early adopters or technology sheep based on their adoption preferences. This question will also be key in understanding which technologies or versions are considered established among respondents and which are still considered new and upcoming. Figure 1.4 shows that the majority of people claimed they are technology sheep at 53%. A very strong proportion were early adopters with 44%. A mere 3% of people say that change is all bad and that they'd prefer to do real work rather than change. Be sure to check out Part 2 of this report to see which tools and technologies the early adopters are looking at! Would you say you are better than the average person in your role? Now for my favorite question in this survey! We asked participants whether or not they thought they were better than the average person in their role. Now, all our RebelLabs readers are likely to be better than average in their role as they're trying to constantly better themselves by reading technical content. However, it's always amusing to see the results of possible overconfidence, or maybe arrogance, bubble up? Either way, we'll see just how good the 74% of people are, those who consider themselves to be better than average, when we study their tools, technologies and behaviours in Part 2. Tell me they use J2EE, please tell me they do! Which type of application describes the main project you work on? We're always keen to get a breakdown of the applications that our respondents work with. As we can see from Figure 1.6 , two thirds of respondents work on full stack web applications, as you might expect, with a further 18% working on backend code. The remaining votes are split between batch, mobile and libraries, with 6% working on desktop applications. The 'other' option contained middleware developers, tools developers, programming languages and more. Have you adopted a microservices architecture ? Microservices has been a popular buzzword in recent times, so we felt we had to include a specific question about it in this year's survey to better understand its adoption rate. This question is fairly simple on the face of it. However, I think that if we sat a few respondents next to each other, they may question whether each other has indeed implemented a microservices architecture or just adopted a couple of best practices. As we can see from Figure 1.7 , two thirds of people (66%) have not adopted a microservices architecture whereas the remaining third (34%) have adopted a microservices architecture. Next, we'll investigate this in more depth by asking two further questions. To those who have not adopted microservices, we'd like to know whether they plan to in the future. To those who have adopted microservices, does the new architecture make their job easier or harder? Are you planning to move to a microservices architecture? For those not using microservices, we asked if they were planning to move to that architecture. Looking at the yellow portion of the outer ring in Figure 1.8 , we see just 12% of respondents said yes, they were planning to adopt them. Almost three times that number (32%) stated they did not want to move to a microservices architecture with 56% still making their minds up. Has moving to a microservice architecture made your job easier or harder ? We asked those who stated they have already adopted a microservices architecture whether it made their job easier or harder. On the green portion of the outer ring in Figure 1.8 we see that almost half of those who responded (42%) stated it made little difference to their job, with 40% saying their jobs became easier and 18% claiming their jobs were harder as a result. Put in another way, we can say that approximately one in five people (18%) say their job is harder whereas four in five people (82%) state that their job is about the same or easier while using microservices. We could also state that three in five people state their roles are either the same or harder as a result of adopting microservices. Aren't statistics a wonderful thing! Luckily in this report, we show you all the numbers so you can make up your own minds! Which JVM language do you use most often? Very often we get a good spread to this question in our survey, but this year shows Java in an extremely dominant lead over other JVM languages. Notice that we didn't ask which JVM languages people have used, rather which JVM language people use most often. It's clear that while people flirt with other languages in the ecosystem, Java is still monopolizing the JVM with 93% of the total vote. Groovy and Scala, as you'd expect, are next in line - eating the leftovers from Java's table with 3% and 2% respectively. Kotlin, Ceylon, Clojure, JRuby and many others all contribute to the remaining 2%. Which Java version do you use to run your main app in production? One of the most common questions people ask in the Java community concerns Java version adoption. This is mostly fueled by the exciting release of Java 8 back in March 2014 (I wanted to say recent, but it's already been over 2 years now!), combined with the aggressive removal of public updates on older versions. It's important to upgrade for a number of reasons, including performance, security and features. From a library or tooling vendor point of view, it's incredibly frustrating to support older versions of Java as it's harder to keep backward compatibility and make use of newer language features. An important item to note at this point is that we're asking about the respondent's main application in production - rather than a proof of concept application running a service nobody really cares about. We can see from Figure 1.9 how conclusive the results are, showing the success of the Java 8 release. 62% of respondents state that Java 8 is used to run their main production applications. 28% of respondents are on Java 7 with less than one in ten using Java 6. The 1% sitting in the other category consist of Java 5 users and some brave Java 9 power users! Needless to say, the success of Java 8 is very apparent in this one image of coffee cups. Now, it's time to grab an espresso and carry on reading! Next up is Java EE. Which version (if any) of Java EE do you tend to use most? See, I told you it's Java EE next! Now that we've covered Java SE, let's take a look at how life looks in the enterprise world. Java EE 7 was released in June 2013 so it's no surprise it is the version which most have since migrated up to. Four out of ten respondents don't use Java EE. If we normalize the data, to only look at Java EE users, we can say 58% of them are on the current version, Java EE 7. It makes me very emotional to see almost one in ten people spending their working lives on a J2EE project, a specification that was released 13 years ago. Maybe we should have a minute's silence and finish the espresso we made in the last question. Which IDE do you use most often? Next up is a real doozy of a question! The eternal battle between good and evil is nothing compared to the IDE flame war! This year marks a very special milestone for this question, as all previous surveys have seen Eclipse take the top spot. This year, even after combining the different Eclipse flavors, such as STS and JBoss tool suite etc, IntelliJ is the number one IDE among Java developers for the first time in our survey. Not only is it number one, but with 46% of the share it beats Eclipse by a clear 5% of the vote! In part 3 we'll see the trend of previous years to determine whether this result was expected or not. SPOILER: It was! This will please JetBrains, the company behind IntelliJ IDEA, given the recent controversy over the new subscription based model that JetBrains introduced in 2015 (which is a great model by the way!). It will be interesting to see if they can hold the top spot next time we run the survey! NetBeans still has a solid 10% of the votes with 3% existing in the other category which includes hardcore devs, who use vi, vim, Emacs or a magnet, needle and steady hand, as well as JDeveloper, RAD and TextMate users (we're looking at you, @venkat_s !). Which build tool do you use most often? Another source of much frustration in a developer's job is their choice of build tool. Of course, unlike IDE choice, build tools are decided by the project rather than the individual developer. You wouldn't see a team of developers working on the same project using different build tools, which you quite often see with IDEs. The big question this year around build tools is whether Gradle has further eaten away at the Maven monopoly or not. Well, looking at the responses, Maven still has a very strong dominance in the build space with almost seven in ten developers (68%) downloading the internet every day using Maven. The Gradle team will be upset to see such a poor uptake of just 16% of the vote, with Ant just behind on 11%. There's always one using SBT, and that's covered in the other category along with the hardcore makefile users. #Respect. Note that the question here wasn't multiple choice, so as we've seen with other surveys, Gradle is used by many more people, but clearly not as the primary build tool. For more information about this, be sure to check out the trends section in Part 3 of this report. Which Application Server do you use for your main application? Application servers - the technology we love to hate! Whether we're waiting for yet another server restart (Wat! You should be using JRebel ! OK, cheap shot!), or fighting with a configuration model that could only have been dreamt up by an alien race, they often take the brunt of our bad language and frustrations. We asked a couple of questions about application server usage; one pertaining to production environments, the other about development preferences. While production environments are designed, in principle, by decision makers, a developer could always opt to use a lighter weight application server in their development environment. This of course brings other challenges, like migration issues between development and production, but for now we'll just look at what people are actually using. We can see from Figure 1.13 on the following page that Tomcat is once again the standout leader in application server usage with 42% of respondents using the application server in both development and production. So, almost one in two of those who state they use an application server, pick Tomcat. Everyone else takes the scraps that are left. In fact, Tomcat's biggest competitor among production applications servers is ""We don't use an application server in production"". That pretty much sums it up. However, I would have expected to see Tomcat's development percentage to be higher than production as I might have predicted that many using the heavyweight app servers in production would switch to Tomcat for ease of use in their local development environments. Perhaps Jetty has taken the mantle for this as the second most popular development application server at 12%. In fact, there are 50% more respondents using Jetty in development than production. Where does this additional 50% come from? One possibility is Spring Boot users, picking Jetty as their embedded server of choice. Looking down the graph in Figure 1.13, we see WAS and WebLogic lose the same percentage in development. Almost half of those using WebSphere in production switch application servers in development, which should be very concerning for IBM. A quarter of those using WebLogic in production also switch to a different server in development. WildFly and JBoss EAP share similar numbers as you'd expect with a minor percentage exchange where you'd expect those developing for a JBoss EAP production environment are using WildFly in development. Not really much of a migration as they're essentially the same codebase. It's nice to see GlassFish at 4% particularly after they received the cold shoulder treatment from Oracle. Perhaps this server has remained significant due to the support of the team at Payara - great job and community support, folks! Which databases do you use in production? This multiple choice question (hint - percentage values won't add up to 100%) is another we've used in the past. Databases are like bugs - every application has them! The question is, which is the most popular? Well, Oracle DB takes first place with 39% of respondents using it, but that's a mere 1% more than MySQL which sits pretty at 38%. MongoDB, the leading NoSQL provider is used by 15% of respondents in production. Interestingly, while the majority (56%) use just one database in their production environment, over four in ten respondents (42%) state they use 2 or more databases in production. Only 2% of respondents don't use any database in production. Not even little bobby tables could derail their applications! How long does it take you to compile, package and redeploy your application once you've made a code change, so that you can actually see your change at runtime? (in minutes) As the company behind JRebel, a tool that eliminates time spent during compile, package and redeploy stages, we're always interested in understanding how long it takes for developers to go from changing code in their IDE to seeing their code changes in a live runtime. We typically ask a question about how long a redeploy takes. But a more realistic question is to ask how long it takes a developer to go from making a change in their IDE to seeing their change in their runtime. This includes compilation, build, redeploy or server restart, state creation, including logging back into your app and navigating to the page with your changed code. We can see varying results from Figure 1.15 , and this is because different environmental factors will have contributing factors to this overall number, including application size, application state, build process, application server and so on. The majority sit in the 2-5 minute bracket, which is unacceptable for any developer to suffer through. In fact, over 70% of respondents were 2 minutes or over, with the median being 3 minutes and the mean an incredible 8.7 minutes. 18% of developers, that's almost one in five, have to wait over 10 minutes to see their code changes in a live runtime. For ideas of how you and your team can eliminate this wasted time and stay sane, visit . Which Web Frameworks do you use? Another topic that developers love to argue over is web frameworks. Which one is best overall? Which is more suitable for their needs? And even, which they need to learn to be best placed for their next job! Now, before you email in, note that this question could be answered with multiple selections, so numbers won't add up to 100, number-nerds. We can clearly see how dominant Spring has remained in 2016, with 43% of respondents stating they use Spring MVC. Spring Boot was also extremely strong (29%), particularly as a technology that was only created in 2014. This must be very pleasing for the teams at Pivotal. The closest competitor to Spring technologies is JSF at 19%, followed by Vaadin with 13%. Other things to note is the drop-off between Play 2 and Play 1 usage being quite significant, as users clearly choosing to migrate up to Play 2. Typically, applications use just one web framework. In fact, from our responses, the median number of web frameworks used is 1 with a mean of 1.4. This takes into account all responses, also factoring in all those who said they don't use web frameworks as well as those who do. If we remove the responses for who don't use web frameworks (17%), we're left with 41% of respondents who use one web framework and 42% of respondents who use two or more web frameworks. Therefore, if you're going to take advantage of web frameworks, you'll likely end up using more than just one in your application. Which Continuous Integration Server do you use? As one might expect from a survey that asks which CI servers people use, Jenkins is the runaway winner. The 10-year-old CI server is used by almost two in every three (60%) respondents. The biggest competition by votes is ironically the ""We don't do CI"" category, which makes Jenkins' dominance even more impressive. Bamboo is the next most popular CI server with 9% of the votes, with TeamCity, Hudson and Travis CI taking up the rest of the share. In the 'other' category, Circle CI is certainly a server worth mentioning that had a good portion of the remaining votes. Before we go, we should mention Hudson, the CI server from which Jenkins was forked back in 2011, with overwhelming support from the community. Note that Hudson was transferred from Oracle to the Eclipse foundation in 2012. Since Jenkins owns the market in the CI space and with 20 (twenty) times the number of users (using our sample data) compared to Hudson, it's a mystery why Hudson is still being developed or supported today. The game has long been won by Jenkins and perhaps it's time to unplug Hudson altogether. Which VCS do you use most often? It's time for you to cast your eyes upon another technology monopoly! This time, we're talking about VCS tools. Needless to say, saving our files in a directory using a format like: [filname]_[version] is the most popular solution... Oh no, that's incorrect! Git is actually the most popular solution, and by a wide margin! Almost 7 in 10 respondents state they use Git most often, seeing the infamous subversion (SVN) fall back quite a distance behind on 23%. Mercurial and CVS deserve having their names mentioned, taking 3% and 2% of the vote respectively, with many more tools being collected in the 'other' category. If anyone was in doubt or asking which VCS system won the fight, you're a bit late to the party. Git is most definitely the last VCS standing! Which Java Profilers do you use? VisualVM tops this category with 38% of the votes, but sadly 35% of respondents claim they don't use profilers at all. This is unfortunately a typical representation of how performance testing takes a back seat, particularly with developers. Years ago, the same could be said about quality testing during development time. With the introduction of developer focused profilers like XRebel, with 6% of the votes, we see a glimmer of hope for early application profiling that can be performed as developers write their code. JProfiler is another popular profiler, shown by the 16% share it got from respondents. It's very pleasing to see 15% of respondents using Java Mission Control. As a tool that's provided with the Oracle JDK and is free for use by developers on non-production data, it's a key tool to have in any performance tester's utility belt. YourKit and NetBeans profiler also perform well with 12% and 7% of the votes respectively. Which APMs do you use? Hang on, APMs? That's not a development thing - perhaps that's why almost one in three respondents had no clue which APMs are used to monitor their applications. This is even worse considering 47% of respondents state they don't use APMs, or perhaps they do but just don't know that they do! As a result, the most popular APM at just 11% of the votes is New Relic. In fact, it's more than twice as popular as the next APM, AppDynamics, with DynaTrace following closely behind with 4%. Which Virtualization Environments and tools do you use? As you would expect, the Docker hype manifests itself into the highest share in our survey among people who use virtualization environments, taking 32% of the votes. AWS ECS lags behind on 13% and there are few others posing any kind of competition beyond this. Interestingly, only 4% state they use Kubernetes, which is surprising given the amount of great press and conference sessions it gets. Perhaps we'll see increased adoption over the coming years. Virtualization is still a practice performed by the minority, with 54% of respondents stating they don't use virtualization environments at all. I expect this will switch in the near future, particularly if Docker's strong growth continues. Is your team agile ? The agile party mark a landslide victory, with 71% of respondents stating their teams are agile. This doesn't mean they are agile, of course. Rather, it means it's the individual's belief their team works in an agile way. Next, we'll look at which practices our respondents follow, and be sure to check out Part 2 when we break it down further to see which practices our agile developers follow! The results are popcorn worthy. Do you practice the following processes ? With only 4% of respondents working in a certified agile team, this likely says more about the number of teams who care about being certified agile than the practices most teams follow. One might argue that what works for one team is very different to another, so working in a prescribed way could be counter-intuitive for a team. Almost two in three respondents (59%) have daily standups - which is reasonable - with almost half of respondents (47%) using Kanban boards to manage their tasks. One in five of our respondents (20%) don't care too much for any of these practices, stating they don't follow any of them whatsoever. 39% of respondents state they assign tasks to their team at the beginning of a sprint which I would have expected to have been higher, given it's a core part of agility. Only one in five respondents (19%) claim they have well written specifications for their projects, so not much love for documentation there. Does your project have formal requirements for the following areas? Almost two in three respondents have formal functional requirements in their project, which used to be an anti-pattern of agile that encouraged people to ignore documentation altogether. I think this is a pretty outdated view of what it means to be agile and this is backed up with almost two thirds of respondents producing formal documentation. We can see everything else takes a lower priority to function, with security, rather surprisingly, next most popular with one in three stating they have formal requirements in place for their project. Performance is next with 29% of people caring enough to enforce formal requirements for their application. The same number (29%) don't care about any of these formalities, claiming they do not have formal requirements for any of these areas. Are you (or your company) likely to pay for development tooling ? Good tooling invariably comes at a cost. It's important for companies to understand that and empower their developers with the right tools they need to do the job. You wouldn't expect a chef or a builder or in fact any trade to do a job without giving them the right tools to accomplish that task well, but for some reason in software development it's often accepted. First, let's spend a minute thinking about the poor one in every ten developers (11%) whose company will plain not pay for any tooling whatsoever. Done that? Good. It's good to see the remaining nine out of ten respondents at least standing a chance of purchasing the tools they need. In fact, 6% of them will pretty much buy anything, while the vast majority, four out of five, will be able to purchase tools if they are essential or if their value can be proved. Checkpoint 1 In this blogpost we focused on the first part of our Java Tools and Technologies Landscape Report. And we just described the raw answers we got from the survey. No fluff, no pivoting, just pure data and answers. You can draw your conclusions already, the IntelliJ IDEA has overcome Eclipse in popularity, Maven is still the king, Git has won. People don't really understand agile and so on. However, this is not the end ! We've done more data analysis and it awaits you in the second and third parts of the report. And be sure to grab the pdf version of the report. It's beautiful, it's easier to read, we've put quite an effort into it! Check it out: Agreed, I need to check out the pdf version of the report Simon Maple Oleg Shelajev",en,46
405,1955,1470090834,CONTENT SHARED,8215778202715904841,-6067316262393890508,2245546651160341154,,,,HTML,https://hbr.org/2016/07/how-to-create-an-exponential-mindset,how to create an exponential mindset,"Digital business models are a bit of a misnomer. It's not digital technology that defines them; it's their ability to create exponential value. The music and video industries, for example, weren't redefined by converting analog to digital formats. Just ask Sony about Minidisc players and Netflix about their DVD business. To create exponential value , it's imperative to first create an exponential mindset . The incremental mindset focuses on making something better , while the exponential mindset is makes something different . Incremental is satisfied with 10%. Exponential is out for 10X. In the last century, industrial business models were defined by their use of machines to create increasing returns to scale. Digital business models use network effects to create what Ray Kurzweil describes as accelerating returns to scale. The key difference is that industrial models are linear while digital models are exponential, as shown in the chart below. While others have written about how to design exponential strategies and organizations, I want to focus here on how to create an exponential mindset . My work with clients suggests that the incremental mindset is more deeply embedded than we might think. Unless you are conscious and diligent, you can end up with a strategy that looks digital (i.e. uses digital technology) but doesn't actually operate digitally (i.e. achieves accelerating returns). The role of incremental and exponential mindsets vary in each phase of the business journey: launch, grow, and expand. Launch: Vision and Uncertainty In the launch phase of a business, the team needs to develop and refine the business model. The Lean Startup approach of test, iterate, and pivot is the right thing to do. But you also need the right way to think. Are you thinking about your business incrementally or exponentially? The incremental mindset draws a straight line from the present to the future. A ""good"" incremental business plan enables you to see exactly how you will get from here to there. But exponential models are not straight. They are like a bend in the road that prevents you from seeing around the corner, except in this case the curve goes up. Without an exponential mindset, Google would never have created such an ambitious vision as ""organizing the world's information,"" Facebook would never have set out to ""make the world more open and connected,"" and Airbnb to ""create a world where all 7 Billion people can Belong Anywhere."" Similarly, a group of innovative organizations in the public sector are out to solve global social issues by achieving "" transformative scale ."" In Maine they have an expression that ""You can't get there from here."" In the launch phase, you need to realize that an exponential strategy has inherent uncertainty. You can't know what things will look like on the other side of the curve. You can't draw a straight line from where you are to where you are going. There's no step-by-step plan. The exponential mindset helps you become comfortable with uncertainty and more ambitious with your vision. Build: Courage and Patience These days, many companies are able to get through the launch phase with an exponential mindset. They manage their uncertainty, take the leap, and start the journey despite being unable to see around the bend. Fear of disruption and envy of unicorns can be a powerful motivator. But then something happens. Or more precisely, something doesn't happen. Take a look at the chart above. In the first part of the build phase, you don't see a lot of change. It's not until the second part when the line starts to bend. It's simply the nature of exponential change. Things happen very slowly before they happen very quickly. If this was the only world we knew, it wouldn't be a problem. But we were raised with an incremental mindset. So we can't help but compare the exponential path to the incremental path. And this creates a problem. We are accustomed to measuring progress linearly and incrementally. If 30% of the time has gone by, we assume that we should be 30% of the way there. That's how things work in the physical world when we are traveling to a destination. But exponential models don't work that way. What happens is that businesses run into something I call the ""expectation gap,"" where the exponential strategy is at greatest risk from the incremental mindset. It's where many companies abandon the exponential model for the incremental. I see this consistently on a micro scale in my own work. My workshops are designed with an exponential mindset to generate new ways of thinking about marketing, culture and strategy. Somewhere around a third of the way into a workshop, the leader invariably says something like ""so when are we going to get something done?"" The reason is that they are still operating with an incremental mindset. A third of the time has passed, but it seems like they are only 10% of the way to our destination. In fact, most of the progress happens once the curve starts to bend. Invariably by the end of the day the same people are remarking that they can't believe how much we got done in such a short period of time. In your exponential journey, pay attention to when people get the most impatient for results. It's the point in the chart where there is the largest gap between incremental and exponential paths. This expectation gap is a risk to the business strategy because the impatience can be used by opponents or skeptics to convince stakeholders to jump from the exponential to the incremental. You will have the immediate relief of having ""line of sight"" once again and see steady progress. But you will also have given up the possibility of accelerating returns and the opportunity to keep up with customers and competitors. The exponential mindset helps you have the courage to persevere and the patience to see it through. Grow: Agility and Control In the third phase, you have managed the uncertainty of the early days, the impatience of the middle phase, and now you are firmly ""in the curve."" Growth is happening faster than you can handle. At this point, the incremental mindset is to try to rein things in and get things under control. But that would be a mistake. To sustain the accelerating returns, you need to shift your mindset about how to mobilize and manage resources. The incremental mindset assumes that it takes more inputs to produce more outputs. So as growth starts to accelerate, teams start to look for more resources in proportion to the growth. But the addition of too many people or too many resources can ""flood the engine"" of growth. You need an exponential mindset to figure out how 1X additional input can create 10X additional output. You also need to apply an exponential mindset to how you manage the resources you have. The incremental mindset about management is like creating a line of dominos . Everything needs to be highly coordinated with active oversight to make progress one step at a time. The exponential mindset is like this demonstration with ping pong balls in which things happen in parallel with a focus on the interactions among participants. As I've written about separately, there is a way to let go without losing control. In the exponential mindset, managers replace control of people with control of principles. The use of doctrine to guide decision-making generates alignment, consistency and empowerment. But most leaders are accustomed to making decisions rather than empowering decisions. The anxiety from a loss of control can easily push companies off the exponential path back onto the incremental path. The exponential mindset helps to grow output faster than input, and empower teams to achieve both alignment and autonomy. To summarize, digital business models require a shift from incremental to exponential. At the start, it takes vision and a leap of faith to commit to the unknown. In the early days, it takes courage and patience to build the foundation for growth even when results aren't yet apparent. When growth kicks in, agility comes from empowering others and letting go without losing control. In all of the stages, the challenge is to ""unlearn"" familiar ways of thinking and embrace the unfamiliar. But with a shift from the incremental to exponential mindset comes the opportunity for real innovation.",en,46
406,1137,1464575885,CONTENT SHARED,-7809508238092432842,-709287718034731589,-2466032665594402813,,,,HTML,https://medium.com/athena-talks/lena-dunham-bey-the-apology-plague-and-my-rage-against-the-machine-sorrynotsorry-eb6ce9e80f70,"if women in tech stop apologizing, we might just be able to shut the sexist shit down. - athena talks","If women in tech stop apologizing, we might just be able to shut the sexist shit down. Let's talk about an uncomfortable truth, sexual harassment and assault is common in the tech industry. The only reason we don't hear more about it is because the men who perpetrate it the most are also the ones who hold the ""keys to the kingdom""- as investors or powerful industry figures, women don't want to ruin their careers by speaking out. So I have come to a conclusion. I am adding a ""sexual misconduct clause"" into all of my investment agreements. If an investor or employee of the investor/accelerator/incubator makes a sexual advance towards me or anyone in my company , then they are stripped of all of their shares in my company (even the ones that have vested) and there will be a public notice to shareholders as to the reason why. There are plenty of ""bad actor"" clauses that already exist. It is time to add a new one that protects women. Let me walk you through my process: Yesterday I read Lena Dunham's Linkedin post ""Sorry, Not Sorry: My Apology Addiction"" and I thought, WOW- Dunham is so fierce, and Bey is too. Her article lit a spark which fired up some self-awareness, because I heard myself apologizing for the rest of the day. I apologized for having an opinion, I apologized for bumping into someone who actually walked into me. I even said ""I am sorry but I think you are wrong"" (to someone who totally had their facts wrong) So, what the fuck is up with that? I am part of the apology plague. I caught it or maybe always had it. I keep saying it, and just like Lena said, it is just a thin veil over my rage. The reason I am so angry is because I was sexually assaulted at an afterparty during a tech conference. It was the last straw after years of putting up with all kinds of sexism and as a result, I closed my company. I tell people it was ""founder burnout"", but it wasn't. It was a trauma which I couldn't shake or face and the tech events went from fun parties to threatening crowds. I am always outnumbered (by a lot) and there is a frat boy mentality by some of even the most respected investors. I didn't let it stop me, and I launched Civilize because I am on a fucking mission. I believe in what we are doing. (helping people end harassment from debt collectors) Plus, I already built the tech and I couldn't just walk away and let it sit on a server somewhere collecting dust because I was feeling scared or hurt. I pushed forward but the feelings didn't go away. I just suppressed them and they find their way to the surface. As Lena said in her post: ""... the fact is, a lot of the time when I say sorry it's because I'm mad. Really mad. So mad that I'm afraid anything but sorry will cause me to explode and drip my hideous rage juice all over someone I'm simultaneously pissed at and trying to please. And so saying sorry serves as a sort of cork, making sure my emotions are contained and packaged neatly. Sorry is the wrapping paper AND the bow."" I am having a great time as part of Barclays/Techstars accelerator. I love the people, the other companies and my team. In terms of my company, I am ""crushing it"" but demo day is coming. While other CEOs are worried about getting funded, I am shit scared that one of them will invest. Every time I get up to pitch, instead of sharing my vision I am exploding with rage. And then I apologize and promise to do better, and change my pitch to one that is even more antagonizing then the last. I was literally told that I need to work harder on hiding my thinly veiled contempt for the investors. And I am so sorry that I am angry, and I am so sorry that I can't get over it, and I am so sorry that I hate them as a whole instead of being able to remember that there a a few bad actors but most are not like that. And I know that the future of my company depends on me being able to get past this. Because here is my deepest fear- I am afraid that one of these men, these bad actors will end up investing in Civilize . He will have a board seat, he will own part of my life's work. One day, he will offer to take me out to dinner and I will think it is professional but he will have another agenda ( because in his mind lunch is for business and dinner is for lovers but I didn't get the memo ), he will make an advance, I won't know what to do, everything will get awkward and I will be afraid of making an enemy of him because he will have the power to oust me from my company. This is my imagination, but these aren't imaginary fears. This happens. I need to make sure that it doesn't happen to me again, or to any members of my team. I think that I felt more alone in these fears before Ellen Pao spoke up. She spoke for more women than anyone realizes and she didn't apologize. Then more women spoke without apologizing. And with every voice that we add, we get collectively stronger. But we need more than to speak out, we need to do something actionable to make a change and hold men accountable. As women in a male dominated, we deserve to be protected from unwanted sexual advances, and this seems like a good place to start. *if you or someone you love is in debt, check out Civilize and maybe we can help. As always, you can find me here or on twitter @sarahnadav",en,46
407,1512,1466731387,CONTENT SHARED,1379546145237381445,2062670502532932588,-6060396183220286898,,,,HTML,https://www.infoq.com/news/2016/06/eclipse-neon,eclipse foundation releases neon,"Today, the Eclipse Foundation announced the release of Eclipse Neon, the eponymous IDE which provides support for Java, JavaScript, C/C++, PHP and Fortran, amongst others. This release marks the eleventh release of the combined release train, with contributions from 779 developers (of which 331 are committers) and totalling 69 million lines of code. This release is the first to encourage users to use the Eclipse Installer, a new technology powered by Eclipse Oomph , which allows a small installer to be provided that can download and provision additional tools. This reduces the load on the Eclipse servers, which instead of having to host a single large zip can provide a subset of plug-ins that can be downloaded only if needed. The standard packages - such as the Java developers package and the CDT C/C++ developers package - are now available as optional installations from the installer. This allows a custom IDE to be created with a combination of the standard developer tools. A long-awaited feature (over thirteen years ), word wrapping in editors, is now finally available. By default this is switched off but can be enabled in the preferences or with the Alt+Shift+Y keypress. It is now also possible to increase the size of a text editor's font with Ctrl++ and Ctrl+-, along with pinch-to-zoom on supported trackpads. Other general platform features include the IDE now supports high-DPI monitors and has a number of icons created at a higher resolution. Improvements have also been made to the GTK3 support for menus and auto-scaled icons for high-DPI resolutions. Full screen support has been added to Windows and Linux, following from OSX's full screen support. The Java IDE has been upgraded in this release with a number of new features , such as: Null annotation detection has been expanded, allowing both the standard Eclipse @NonNull annotation as well as others (such as FindBugs and CheckerFramework) to be used for annotating null or not-null methods. Null annotated methods in generics now provides more specific recommendations, and will pick up generic method types that have the @NonNull or @Nullable can now be inferred through generic types. Quick fixes for adding nullable annotations are now prompted, though missing entries can be changed based on preferences. JUnit test runs now pass the -ea (enable assertions), to try and detect when assert failures occur as well as the JUnit assertions. This can be disabled in the launch configuration if not required. Content assist in JDT has been improved, allowing searching by substring and displaying partial matches. The terminate and launch feature allows re-running programs quickly, by automatically killing a previous running launch before starting it again. The standard packages now include Maven and Gradle build support out of the box. Java 9 support is not available (since Java 9 has not been finalized yet) but betas are available from the Eclipse marketplace. Running Eclipse on a Java 9 platform requires the use of -addmods java.se.ee on the JVM command line, due to dependencies on annotations such as javax.annotation.PostConstruct that is not present in the default java.se module. The C/C++ developer tools 9.0 have had a number of changes: A Create new Class quick-fix is now available when referring to a C++ class that does not exist (for example, a reference to a local variable of that type). This mirrors a similar quick fix available in JDT previously. Code completion now suggests local expressions of the same type for the required parameter in the same scope. Code analysis warnings can be suppressed through the use of a specially formatted single line comment. The OS Resource View now shows local processes, and can be used to connect remote debuggers by right-clicking on a process and performing a connect . The connect button is now no longer shown in the toolbar, but can be invoked from the pop-up menu or through a keystroke. The PHP tools 4.0 have increased support for PHP 7 by default, with a new set of breakpoints available for exceptions, editor advancements that provide annotated views when classes or methods are deprecated, improved code assist, skipping warnings inside comments, and more . The JavaScript tools have been significantly improved support for ES6 (EcmaScript 2015), a built-in JSON editor, and the ability to run debuggers with Node and NPM support. The Eclipse LinuxTools include new support for Asciidoc editors and Docker file creation. It is now possible to see a list of docker images running locally, and fire up a shell connected to that image from within Eclipse. Docker containers can be run as ordinary or privileged containers or pulled from remote image repository. The Eclipse Neon release includes for the first time the Andmore project , which is the continuation of the Android Developer Tools based on Eclipse. Using this, in conjunction with an Android NDK, allows Android applications to be built on Eclipse. A full list of the new and noteworthy features for the Eclipse Neon release is available from the new and noteworthy page . Eclipse Neon can be downloaded from the Eclipse downloads page , using the Eclipse Installer or one of the pre-defined packages.",en,46
408,2984,1484908994,CONTENT SHARED,-5813211845057621660,7645894863578715801,4609210914782123550,Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:50.0) Gecko/20100101 Firefox/50.0,SP,BR,HTML,https://medium.com/@diogo.lucas/communication-patterns-in-a-microservice-world-af07192b12d3,communication patterns in a microservice world,"Ok, so you drank the microservice kool-aid and that's everything you intend to do from now on. By the time you implement your second service (or even better, refactor an existing one into two smaller pieces), you have to think of communication between MSA modules. At first, things are simple enough, so you pick a weapon of choice (say, REST) and use it. Then again. And yet another time. And before you know it, you have implemented a chatty, reflective and frail set of interfaces. Hardcore fans of a given communication model are probably already shouting (in their heads, I hope, seriously, people, stop it) ""THIS IS A PERFECT CASE FOR REST/EVENT-SOURCING/ACTORS/UNICORN-BASED-MESSAGING!"" when, in reality, it usually isn't. These approaches either preconize a very specific way of setting up interfaces or are broad enough that different, inconsistent patterns may arise if no care is given. And that violates an important rule of thumb: While microservices allow easy disposal of modules, they require careful consideration of cross-cutting protocols. So, let's break that statement into chunks and justify it, shall we? easy disposal of modules : if each service is sufficiently a) small and b) isolated, they should be cheap and fast to create and maintain. This allows a fast-paced delivery of new features while keeping potential technical doubt under control; they require careful consideration of cross-cutting protocols : microservices are no silver bullet agains complexity, pushing it outside of service boundaries. Where does it go, then? To the interaction between these modules. Requests to implement something akin to distributed transactional behavior across systems or simply performing calls over diverse syntaxes (eg: payloads, url routes, HTTP verbs) can easily become nightmarish if teams do not reach a consensus in terms of standards (yeah, that ugly word). Commands A command happens when a client system needs an action to be performed by a given service provider. The client may also need a response (in either a sync or async way) informing about the operation result . The core things to keep in mind are: this promotes some coupling, as client (and potentially provider) are made aware of one another; coupling becomes tighter in case the client needs a response, as this means it will need to a) wait and b) handle all possible outcomes of the operation (including unavailability); coupling becomes tighter in case of synchronous calls, since the provider needs to process the request at the time it is made. For an in-between solution, a common pattern is to provide a simple ""accepted"" answer synchronously and do the heavy-lifting in an async fashion; time-bound operations increase interaction complexity: if a client issues a command and needs an answer in up to a minute, what should it do after the timeout? What should it do if the provider still processed the request after, say, 2 minutes? A cousin to the this pattern is the query . While a different beast in nature (it shouldn't be data altering, for example) and potentially something that can be implemented in a radically different way (see CQRS , which, BTW, proposes similar concepts) the bullet points above are still valid for such operations. A part of the popularity of commands is that, at the end of the day they are very easy - no wonder most of us are introduced to programming by relying exclusively on it (yeah, HelloWorld , I am talking about you). Having said all that, this pattern is frequently unavoidable: you will need an answer from the credit card service in your e-commerce before you move forward in the order process, no matter how much you would like to avoid it. Events An event refers to an occurrence in the past . That simple statement carries a number of consequences: events are immutable . You can't say ""nonono, that order wasn't submitted, I won't have that!"" . It's a done deal, there's nothing you can change about it. You can, at most, react to it; the emitter can be completely oblivious of the event listeners. Just publish it somewhere and let consumers react to it as they see fit; the listeners may also be oblivious of the emitter - in case the event messages flow through a shared channel (eg: a message queue), this is the only address they need; the overall process is async by nature; therefore, this pattern allows fairly decoupled communication. The event emitter is saying ""look, buddy, this is what happened, do what you will"" . If well implemented, the provider does not even know who are the clients listening to that event - it simply doesn't care. Think of it as a news outlet: broadcasting to a large audience is fairly the same process, no matter the audience size. This pattern has been around for ages, especially in UI-focused stacks: old-time languages such as Delphi had events as a first-class citizen and newer, trendy libs such as redux are designed around event-based flows. In the big architecture world, buzzwords such as Event-Driven Architecture , Event Sourcing and others revolve around this concept. In case this is starting to sound like a sales pitch, please beware that it's not all flowers in the event world: due to its async nature, developers have to worry about correlation : you ask for something (eg: create an order for me) and things start happening (eg: picking, packing, invoicing), in parallel to other people's requests. To find the events you care about, you need a tracking number of sorts, so you can tie things together; implementing something similar to a distributed transaction becomes a major challenge as each event is a) independent and b) immutable, hence you cannot roll it back. The is no simple answer to that and possible approaches include business process changes, compensating operations and eventual consistency (and quite commonly a mix of them); since processing is so distributed, systems that are sensitive to ordering have a major hard time coping with all the inherent non-determinism of when they receive messages. There are systems out there, such as Kafka, that allow some ordering support, but quite frequently any ordering-bound logic needs some level of revision; in a web world, protocols for this pattern are not as close to consensus as for commands (AKA REST). But in case the server is publishing events, how can they be made easily available for consumers? Web sockets ? SSE ? Polling (yuck)? Web hooks ? The fact that so many methods exist shows that none can be claimed king of the hill; all the points above considered, while this approach may scale and extend better over time, it usually presents a higher ticket to entry, leading teams to defer a push towards events until it's absolutely required (or, unfortunately, even later); In a nutshell Use commands if: The service client needs the operation response in order to move further with its process; The operation needs to be synchronous; The call is a simple query (long-running ones may require scheduling); Consider using this approach if you are just getting started, as it tends to be simpler at first; Use events if: It is a commonly sought-after message (eg: an order has been invoiced), meaning a single message may be read by multiple clients; The producer of the information does not care about extra processing done by whoever is consuming it - those extraneous operations should have no impact on the original results; Consider introducing this as your architecture grows, as a pure command pattern does not escale well over a larger problem set; Further reading None of the concepts above is radically new and can be found in seminal text on topics such as: Event-driven Architecture proposes events as a centerpiece kind of message for decoupled communication between different systems. Event sourcing goes one step further and suggests that application state emanates from events, allowing a) a clear separation of both (each kind of data can be stored with different strategies) and b) the composition of events into multiple consolidated visions (financial and fulfillment aggregations on order data); CQRS uses the difference between commands and events to propose models in which write and read operations are asymmetric to one another, so it is a great match with event sourcing; Reactive programming , in many of its flavors, uses events as a core concept.",en,45
409,1773,1468506180,CONTENT SHARED,-5848755753905481990,8676130229735483748,-2413784802356185288,,,,HTML,http://meiobit.com/347470/microsoft-skype-lanca-novo-client-alpha-e-versao-web-do-messenger-para-linux-e-chrome-os-com-restricoes/,"incrível, skype se lembrou que o linux existe!","Apesar do Linux manter uma comunidade pequena, porém ardorosa de fãs e ser muito forte no mercado corporativo para solução de grande porte, no desktop ele continua dando ponto de audiência. A Microsoft sabia disso, embora corporativamente a empresa ame o Linux ( até porque não é nada pessoal, apenas business ) ela nunca deu muita bola para o usuário final; no que diz respeito ao Skype por exemplo Redmond fingiu por anos que ele não existia. Não mais: hoje a empresa lançou um novo cliente do messenger para distros do pinguim e Chrome OS, só que obviamente algumas ressalvas precisam ser feitas. A Microsoft vinha ignorando o Linux há muito tempo. A última versão estável do Skype para a plataforma foi a 4.3, ficando esta extremamente defasada em relação às demais para Windows, Mac e dispositivos móveis; estas passaram da sétima versão e continuam sendo atualizadas constantemente, ganhando novas funções e melhorias enquanto a versão do pinguim parou no tempo. Ela apenas funcionava e nada mais, era meramente um quebra-galho. Só que em tempos de mundo conectado, alienar uma parcela de usuários ( ainda que mínimos ) não é uma decisão muito sábia, ainda mais com a política atual do CEO Satya Nadella de fazer da Microsoft uma empresa prestadora de serviços, ao invés de uma vendedora de software. Dessa forma o Skype deve sim se fazer presente no Linux e por tabela no Chrome OS, o sistema operacional que roda nos Chromebooks. Só que como atualizar o Skype 4.3 do Linux daria muito mais trabalho do que o aceitável a Microsoft decidiu lançar um cliente totalmente novo, em duas versões: há a tradicional, um pacote instalável pelos repositórios das distribuições e outra web, compatível com o Google Chrome até para funcionar nos Chromebooks. Tanto uma como a outra utilizam a API WebRTC e funcionam totalmente livres de plugins. Claro, há alguns detalhes que precisam ser levados em conta. O cliente ainda está em fase Alpha, portanto bugs são esperados. Ele não é compatível com versões antigas do Skype em outras plataformas, portanto o usuário Linux só conseguirá conversar com seus amigos no Windows, Mac ou plataformas móveis se estes estiverem rodando a última versão do messenger, e ele ainda não realiza chamadas para telefones fixos ou celulares. Por fim, apenas o chat de voz está habilitado num primeiro momento e não há previsão de quando introduzirão as conversas por texto ou outras novidades . Apesar de ainda estar bem cru, como o código-fonte do Skype para Linux será escrutinado pela comunidade e é o mesmo para todas as versões, e dado o evidente interesse do Google em torná-lo verdadeiramente funcional no Chrome OS podemos dizer que ele será atualizado com mais frequência do que a versão anterior, e tal estratégia da Microsoft demonstra que desta vez a plataforma não será abandonada. Os usuários agradecem enquanto continuam aguardando pelo Ano do Linux no Desktop, lembrando que ela foi atualizada: agora é $ano_do_linux = YEAR(NOW())+5 . Fonte: Skype . Relacionados: abandono , abandonware , Alpha , Android e Linux , Ano do Linux , chamada de voz , Chrome , Chrome OS , chromium , compatibilidade , comunidade , google chrome , messenger , skype",pt,45
410,446,1460747075,CONTENT SHARED,-5784228041033890044,3217014177234377440,-3607070907401987200,,,,HTML,https://www.coursera.org/learn/design-thinking-innovation,[e-learning] design thinking for innovation - university of virginia | coursera,"About this Course Today innovation is everyone's business. Whether you are a manager in a global corporation, an entrepreneur starting up, in a government role, or a teacher in an elementary school, everyone is expected to get lean - to do better with less. And that is why we all need design thinking. At every level in every kind of organization, design thinking provides the tools you need to become an innovative thinker and uncover creative opportunities that are there - you're just not seeing them yet. In this course, we provide an overview of design thinking and work with a model containing four key questions and several tools to help you understand design thinking as a problem solving approach. We also look at several stories from different organizations that used design thinking to uncover compelling solutions. 5 weeks of study, 1-2 hours/week",en,45
411,981,1463417770,CONTENT SHARED,-4333056764244640444,881856221521045800,-6874533437743367699,,,,HTML,http://computerworld.com.br/lentes-de-contato-inteligentes-serao-o-wearable-definitivo,lentes de contato inteligentes serão o wearable definitivo,"Lentes de contato inteligentes soam como uma história de ficção científica. Mas já há uma espécie de competição tecnológica para criar as lentes do futuro - aquelas que te darão a visão de um super-humano e oferecerá câmeras, telas, sensores médicos e muito mais. E melhor, tais produtos já estão sendo desenvolvidos. Soa um tanto irreal, é claro. Mas parece que os seus olhos são o lugar perfeito para adicionar tecnologia. Lentes de contato inteligentes são como implantes, mas elas não exigem cirurgia e podem ser removidas e inseridas pelo usuário a qualquer momento. Além disso, elas estão expostas tanto a luz quanto ao movimento mecânico de piscar, logo nada impediria de que elas também armazenassem energia. O que você precisa saber é que lentes de contato inteligentes serão inevitáveis por todas essas razões. E aqui reunimos algumas das coisas que você vai ver e muito antes do que você espera. Verily: anote, o Google quer ser dono da sua íris A companhia que até então tem sido a mais agressiva ou vanguardista sobre a ideia de levar eletrônicos às lentes de contato é a Verily , que integra a divisão de Ciências da Vida da Alphabet, holding criada para abraçar todos os segmentos de pesquisa e negócios do Google, incluindo a divisão de buscas da gigante de tecnologia. A última lente de contato inteligente da Verily é injetada dentro do olho, de acordo com um registro de patente recentemente publicado . Então soa menos como uma lente de contato e mais como uma cirurgia. O processo é de dar um certo frio na espinha: suas lentes naturais são removidas do seu globo ocular. Um fluido então é injetado no olho para depois se solidificar. Dentro dessa nova e artificial lente ficam armazenados bateria, sensores, radio e outros componentes eletrônicos. As lentes artificiais assumiriam o trabalho de focar luz dentro da retina, melhorar a visão em várias formas sem óculos, mas de uma forma flexível e interativa. A Verily é conduzida por Andrew Jason Conrad, que também é o inventor das lentes. A divisão também trabalha com a fabricante de medicamentos suíça Novartis para desenvolver e vender lentes de contato inteligentes para ajudar pessoas com diabetes acompanharem seus níveis de glicose no sangue. A Verily também recebeu um pedido de patente para uma lente de contato alimentada por energia solar. Supervisão Uma das aplicações mais legais para lentes de contato é a melhoria da visão sem óculos. Pesquisadores da Universidade de Winsconsin, Madison, inventaram uma lente de contato inteligente que foca instantaneamente a visão. O produto se encontra em fase de desenvolvimento para uma companhia israelense chamada Deep Optics. A ideia é baseada na função ocular do peixe nariz de elefante. A lente usa circuitos eletrônicos e sensores de luz que são alimentados por uma célula solar, todas construídas dentro da própria lente de contato. Quando os sensores determinam que o olho precisa de foco, o chip envia um comando para uma pequena corrente elétrica, que muda a distância focal da lente em uma fração de segundo. A lente foi projetada para tratar hipermetropia, que afeta hoje cerca de 1 bilhão de pessoas. Os pesquisadores dizem que a tecnologia está a cinco anos de ser concretizada. Cientistas da Universidade de Michigan estão desenvolvendo lentes de contato que possam dar a soldados e outras pessoas a habilidade de ver no escuro usando imagens térmicas. A tecnologia usa grafeno, uma única camada de átomos de carbono, para pegar todo o espectro de luz, incluindo luz ultravioleta. O grafeno foi integrado com sistemas de silício microeletromecânicos (MEMS). Como você deve imaginar, há militares americanos financiando o projeto. O Google Glass sem Google Glass A habilidade para ver conteúdos em realidade aumentada e mista ou tirar fotos e vídeos com o Google Glass foi uma ideia brilhante. Mas todos criticaram o visual nerd e nada prático dos óculos do Google. E se todas as funcionalidades fossem encolhidas em uma lente de contato, hã? A Sony aplicou um pedido de patente para lentes de contato inteligentes que conseguem gravar vídeos. Você as controlaria ao piscar seus olhos. De acordo com a patente da Sony, sensores nas lentes podem dizer a diferença entre piscadas voluntárias das involuntárias. Essa, inclusive, era uma habilidade do protótipo do Google Glass, que poderia tirar uma foto quando você piscasse. Quando a tecnologia detectar uma piscada proposital, ela então gravará um vídeo. As lentes de contato da Sony seriam alimentadas por sensores piezoelétricos que convertem o movimento do olho em energia elétrica. Envolveria ainda versões extremamente pequenas de todas as partes digitais de uma câmera moderna - lentes de foco automático, uma CPU, uma antena e até mesmo armazenamento na lente. A Samsung também foi concedida uma patente para tecnologia que funciona como uma versão pequena de uma lente de contato semelhante ao Google Glass. Conta com uma tela em miniatura que projeta vídeo diretamente nos seus olhos. A experiência oferecia uma experiência de realidade mista, como o óculos HoloLens da Microsoft. Como as lentes da Sony, ela teria uma câmera interna e seria controlada por suas piscadelas. Mas diferente da tecnologia da Sony, a invenção da Samsung armazenaria o conteúdo e o processaria em um smartphone. Lentes de contato para manter a saúde em dia Outras companhias estão trabalhando em lentes de contato inteligentes que são desenhadas para ajudar várias condições médicas. Pesquisadores do Centro Médico da Universidade Columbia estão trabalhando em lentes de contato inteligente que podem dizer se o glaucoma de um paciente está progredindo especialmente rápido. Ao ter um paciente usando a lente por apenas 24 horas e constantemente monitorando sua curvatura durante o período, médicos teriam um quadro mais preciso sobre a progressão da doença. Uma startup chamada Medella está trabalhando em uma lente de contato que mede os níveis de glicose, que usa sensores e chips pequenos em lentes para monitorar os níveis de açúcar e depois transmitir os dados para o smartphone do usuário via Bluetooth, um conceito similar ao projeto da Verily. Lentes de contato com sensores médicos serão provavelmente as primeiras a chegarem no mercado, possivelmente nos próximos dois anos. Nós também podemos ficar ansiosos por lentes de contato que funcionem em conjunto com outros wearables, incluindo aí, e especialmente, uma nova categoria de vestíveis - os hearable, dispositivos audíveis que nos darão o máximo dos benefícios de smartwatches e smartphones, mas sem o peso de um gadget eletrônico visível. E tudo isso chegará antes que você pensa. Não pisque, senão você perderá a revolução das lentes de contato.",pt,45
412,1860,1469394378,CONTENT SHARED,8974280745225397183,-1032019229384696495,8189482671194530524,,,,HTML,https://techcrunch.com/2016/07/24/prisma-for-android/,prisma for android is now live for all in the google play store,"The hits just keep on coming for Prisma. Just over a month after launching on iOS and almost immediately capturing the imagination of the Instagramming masses in the process, AI-fueled photo filtering app is Prisma is now live for all Android users. That's a mere five days after the app first launched in beta on Google's mobile operating system, through a limited invite system. The app certainly isn't the first to promise to turn your smartphone snaps into ""artwork,"" but the output speaks for itself, with a slew of ""art filters"" that rapidly transform photos into a wide swath of different sketch and painting styles. The app quickly made its way up the top of the App Store charts, hitting number 10 in the US around the time we checked in with co-founder and CEO Alexey Moiseenkov a few days back. That number has since settled down a bit, but the app is still at an impressive number 17. This will, no doubt, prove a watershed moment for the latest App Store darling - it surely won't escape the notice of industry insiders that Instagram got snatched up by Facebook for $1 billion a mere week or so after it launched launched its Android version. Acquisition rumors have already been swirling around the Russian company for some time. And, hey, the company's head just happened to be visiting with the social media giant when we spoke with him. Facebook batted away such speculation, but Moiseenkov added, cryptically, ""For now I can't disclose all this information. By the end of the week I think we can discuss more."" Whether that means acquisition, funding or something else remains to be seen. For now, you can see what the fuss is about in the Google Play Store. The Android version of the app contains some three dozen different filters, constituting a large cross section of different styles.",en,45
413,877,1462899171,CONTENT SHARED,279771472506428952,-8845298781299428018,414537565071856330,,,,HTML,http://www.forbes.com/sites/janakirammsv/2016/05/09/five-unique-features-of-google-compute-engine-that-no-iaas-provider-could-match/,5 unique features of google compute engine that no iaas provider could match,"Google Compute Engine (GCE), the infrastructure service of Google Cloud Platform, is a late entrant in the market. Amazon EC2 was announced in 2006 while Microsoft added VMs to Azure in 2012. Google announced the general availability of GCE only in late 2013. Source: Google Despite being the laggard in the IaaS [...]",en,45
414,1321,1465496203,CONTENT SHARED,2048657552251060795,5660542693104786364,-7990858727359702237,,,,HTML,http://kevin-moseri.de/creating-value-with-vlendright/,creating value in auto finance with vlendright | kevin moseri,"Creating value in auto finance is now at the forefront of innovation. vLendRight is partnering up with banks to streamline the customer journey and help financial institutions gain market share in auto finance. ""Our belief is that automotive financial institutions ought to look beyond the traditional direct and indirect delivery mechanism and strive to make experiences better for their customers by offering them what we call a hybrid customer journey platform"" - Sachin Kumar, Platform Strategy vLendRight vLendRight, a start-up incubated and powered by ValueLabs recently won the 3 rd prize at the ENBD FinTech Challenge which had 230 applicants from 89 cities. The ENBD FinTech Challenge is a global competition for financial technology start-ups launched by the Emirates NBD Group and the Open Bank Project. By re-imagining customer's auto purchase journey as a purchase funnel, the vLendRight digital age customer journey platform matches cars to a customer persona and allows financial institutions to effectively use their knowledge of the customer. On the other hand, by integrating partner dealers, data ecosystem and car data, the platform also allows vLendRight to deliver smart advice on financing options and deals Creating value in auto finance for the customer There are a lot of market inequalities which cause a lot of friction when it comes to the auto purchase process. The vLendRight platform facilitates friction removal in the purchase process by: Optimizing the end to end auto purchase cycle into a seamless process that ends up saving time and removing hassle from the purchase process for the end customer Integration and enrichment of the customer journey through data sources that enables customers to move down the purchase funnel Easing the customer interaction with the automobile salesman, a situation that can otherwise involve an adversarial transaction, much unlike a normal dealing with a mortgage broker Enabling bank's dealer partners to expedite sale of their vehicle inventory by way of digital vehicle listings, transaction enablers (lead management, financial tools etc.) and insights emanating out of the customer journey on the platform ""vLendRight can be summed up as a platform that enables financial institutions to remain relevant across their customer's end to end auto purchase journey by creating value for not only the end customers but for all other key stakeholders as well "" - Sushankar Daspal, Product Manager vLendRight Creating value in auto finance for banks and dealers There is an inherent gap in auto finance that vLendRight is looking to fill. By design thinking the customer auto purchase journey, it allows banks to play a key role in the process because: Banks and dealerships have the opportunity to trace the customer journey at any step; and take a proactively positive action Banks can disrupt the marketplace by delivering a contextual pre-approval linked to a vehicle of choice for customers before they enter a dealership Banks strengthens dealer relationships by sending confirmed leads thereby helping dealers improve their sales efficiency and inventory turnover How good is a deal financially- explained by way of the concept of vPrudence score ""Customers generally get emotionally invested in a vehicle on the back of an auto purchase pursuit that is not only aspirationally but also affordability led. Moreover the associated financial deal quoted to the customer initially seems affordable but may turn out to be costly in the long run"" - Sushankar Daspal, Product Manager vLendRight vLendRight offers personalized financial guidance by meshing individual's affordability across all key aspects of the purchase journey and importantly while structuring the loan/ lease repayment deals. The platform goes beyond the concept of a loan calculator and enables customers to leverage vPrudence score by comparing repayment deals on their alignment with their financial attitudes/ goals as ascertained by their journey on the platform. There are a lot of market inequalities which cause a lot of friction when it comes to the auto purchase process. Click To Tweet Further details about the vLendRight platform are available on and for enquiries, please email info@vlendright.com A marketing expert for the first-to-be-licensed E-Money institute in Germany, PayCenter GmbH. He has experience in developing online marketing campaigns, online & mobile product launches, and EU funding regulation. He is an active fintech blogger with interests in online banking, mobile banking, mobile payment, and insurance.",en,45
415,2396,1474494465,CONTENT SHARED,7458850059926549081,693665567872370395,5332149560998295599,,,,HTML,https://realm.io/news/altconf-stephen-barnes-bring-your-app-to-life-calayer/,bring your app to life with calayers,"Many times, the most memorable part of an app is an intricate, animated experience. iOS provides a number of tools to create these experiences, but it can take some work to get the most out of them. The basis for many great experiences lies in Core Animation. Using tools such as CALayer and CATransaction we can create a high resolution, vector-based, animated sequence that delights users . In this talk, we'll use code samples to go through different animations and discuss how you can use these techniques in your app. Let's use the power of CALayers to make your app memorable. Introduction (0:00) My talk has two parts. The first part is taking a complex interaction and getting it into a state where we can animate it . This part is half retrospective. It's how we've done something before, how it worked out, and how you can do it next time. The second part is about animations, in particular, Core Animation , which I think is not as hard as many people might think. It's also a lot of fun. How it all started (0:53) It all started when a different iOS developer at Fitbit, named Aidan, and a designer on our team, Drew Matthews, came up with this really fun interaction for our sleep screen. This little interaction comes up whenever we were introducing sleep goals, but also whenever you start ""sleep"" in the app. It's this really delightful, little, happy moon. When we were working on some of the other features in the app a little bit later, we were thinking, ""Okay, what could we use to really spice this up?"" We wanted to create an onboarding experience around hourly activity and reminders to move, both features that came out alongside the FitBit Alta. It keeps track of how much you've moved every hour during a certain window and also helps remind you when to move so you can get 250 steps to keep active. The last thing we wanted to do was create another set of onboarding screens that nobody wanted to read . The first thing to keep in mind is that at the top-right we added a ""done"" button because even though I would like people to really enjoy this, sometimes you don't want to go through onboarding. Besides that, our illustrators came up with these really cool, little illustrations. The benefit of working with Drew on this is he just came along and said, ""How about we do something cool with it,"" and he put together a movie in After Effects. We now had something to start with, and a goal of what we wanted to make it look like. Our first question was, ""Okay, that's cool, but how in the world do I build this?"" There are a number of different ways. We're going to talk how we did it for this interaction in Fitbit, and I'll also mention a few ways you can explore other solutions as well. Sketch and PaintCode (3:01) The way that we built the animations and interactions was using Sketch , then going into PaintCode , and finally going into Xcode via code . Sketch , as many of you might have worked with, is a vector-based tool. It's very popular in mobile development. We have these illustrations made by an illustrator, we cleaned them up and exported them via SVG, which is a standard vector format that we can take and import into PaintCode. PaintCode is a really cool tool for doing several things. One of the main things that it's great at is moving interactive controls via code. It's a WYSIWYG editor, and you can design all these things in it and then output direct code, no assets required. It also has tools for easily adjusting anchor points, bounds, relative locations, grouping things, etc. This was a really decent solution for what we were looking for, though there are a few cons. First, it's great for controls, but what we wanted was sequential animations. Second, its default implementation relies on drawRect , which doesn't have a whole lot of leeway for state. Another thing is that it imposes a third-party tool for getting your designs into your actual application because you go through this editor which outputs code. Lastly, what I discovered in hindsight, is that there's a chance that if you have really complex shapes and interactions, it can actually increase your compile time just a little bit for each file because they can get rather large. Let's talk a little bit about some of the handy features. First is grouping. It's really helpful when you have a whole bunch of different shapes, particularly ones you might want to animate together, but also separately. This is just a sample of maybe 1% of the generated code that PaintCode will output whenever you're taking your files or your vector input and actually outputting your code. As you can see on drawRect , it takes in a few parameters for variables that we can define in PaintCode. But that brings us to CALayers . What we want to do in this case is not rely on drawRect . We want to take our output from PaintCode, convert it into using CALayers, and animate those sequentially. We'll go through a few steps to do that. The first thing you do, it is crucial to clean up your vectors. When someone's drawing something in Illustrator or Sketch or anything similar, sometimes there are tricks that you can use to hide things or to clips things, etc. When you're outputting code, it might not be useful or performant if you have a whole bunch of hidden shapes in your file. Delete unused shapes and merge contiguous shapes with others so that you have one path as opposed to twenty. You should also name your colors and layers; it will save you a lot of frustration in the end when you're looking at your code. That way it's mostly human readable, and instead of ""color 37"", it might be ""dark blue"". One of the other things that we're going to do is break down and transform our generator code. We're going to create a custom view with a custom CALayer , this custom CALayer is going to expose a number of different animatable groups, and we're going to manipulate our PaintCode output into this form. What does that mean? Let's look at some actual code. Here we have our custom CALayer , and it has a bunch of different sub-shape layers beneath it. In this case, it's the first scene of our onboarding experience. I have a bunch of different elements that I want to animate: a picture frame, a chair, a person, a left and a right arm that I want to animate separately, etc. I'm going to configure these different shape layers using the bounds, the anchors, and the positions that I get from PaintCode. One thing you can do inside PaintCode is move the anchor position, and use that to set the anchor position for your CALayer . It's one of the benefits of doing CALayer animations. The next thing you want to do in this process, and this is a little bit odd; you want to break up your groups. Now this is because we're transforming PaintCode's output. We are doing a few things that it doesn't necessarily, and normally, want to do. Typically it does a whole bunch of offsets in drawRect , but we don't want any of those offsets built in. So, again, what we're going to do with our output is create a CALayer for each shape. Then, we're going to set the path of the CALayer to the vector output for that actual shape. Finally, we're going to place fill, which you typically use for drawRect , the fill color for the shape, and then we're going to add that CALayer to the paint layer. We're slowly building up our scene. Let's talk about the picture frame and how things get built up. First, we're going to create the picture frame layer. We're going to do this for one of the sub-shape layers and set its anchor point, its bounds and its position. You can get that from looking at PaintCode's outputted code and also its GUI. We're going to set the bounds of the custom layer itself and add each of those shape layers that we created. What does that createPictureFrame() layer look like? Simply, we create a shape layer. Then we configure any of our colors, and this is where naming your colors and your layers is helpful. A lot of this code comes from PaintCode, so we have a whole bunch of different colors. For example, maybe a pictureBackgroundBlue . Then we're going to take our output code from PaintCode and convert it into a shape layer, one for each shape within our picture frame. We're going to set its fillColor , set it to path , and then we're going to add it. We're going to do this for each part of our shape layers. Now, this can be a little bit tedious. This is part of the retrospective. If you're going to make PaintCode work in ways that it's not really meant to, you have to be willing to either think about scripting, using other tools, or be willing to put in a little bit of extra tedious work. Where did that code come from? (10:54) Where did that code come from? In the PaintCode UI there is a large panel that has all of your output code. You can select Objective-C or Swift, depending on what your compatibility is, and you take that and start breaking it down. Earlier I mentioned that we wanted to break down our groups. If you don't break down your groups, there's a bunch of stuff in your output code that looks like this: The way that it typically works using drawRect is that it will shift and transform in order to place things that are relative to groups, but because we're breaking things down and building our CALayers that have their own position and bounds, that's going to mess us up a little bit. Just remember, if you're using this technique to break down your groups. The important part in all of this is that we want to rinse and repeat that whole process, which is not the most fun thing in the world, but it is rather effective. What we have at the end is a nice, rendered illustration that could have taken us maybe two minutes to do as an image, but the really fun part is that each one of these is vector rendered, so we don't have to worry about device size. We can animate things individually with different anchor points and it's ready to go. For a little bit of retrospective, however, there's a number of different ways to get to this state: One is that you can experiment with SpriteKit . Another one is that there are several different tools, one of which is QuartzCode . QuartzCode does something very similar to PaintCode, except instead of having to translate things from drawRect into CALayer , QuartzCode will actually output CALayers. Now I haven't worked with QuartzCode much myself, so I can't say if it works well or not, but it's another alternative. Another one is Squall . Squall does a similar thing, except it goes directly from After Effects to code. If you have something that's built in After Effects, that's something to consider, but not many of us are very proficient in After Effects. CATransaction - How to make it move (13:10) Now we get on to the fun part. We've done all of this work, and we have our shapes and our layers all ready to go for animation. Let's talk a little bit about how we're going to make it move. The first technique we're going to talk about is CATransactions . Now CATransactions allow us to adjust properties one after another and have them perform in a batch in an animated fashion. We're going to talk about how we're going to do this first little animation. We have everything come in from the left and the right side. We'll do some of the other animations as we go, but right now we're going to talk about how the chair, the picture frame, and the person come in from the left, and the lamp and the other side table come in from the right. We have our custom view called IntroView and it has our custom CALayer . We're going to configure it at the beginning for our intro animation. To start, we're going to take the bounds of our view, divide it in two, and move things to the left and right. Now, when you apply something to a CALayer , it will directly apply it to that layer. Before we render, we're going to configure it in the center and then immediately move it to the left and the right. You'll note that the chair and picture frame we set it to offset with this handy little method called updatePosition , by just adjusting the position of the CALayer . We're going to do the same thing for the picture frame, the left arm, and the right arm. But for those, we're going to set them at an initial angle because you'll notice they move a little bit. We're going to do that using CATransforms . The fun part of this is animating. For that, we have startIntroAnimation . You can begin a CATransaction , and you can set many properties that you might be more familiar with using UIView Animations on your CATransaction . You could set a duration, so in this case, half a second. You can set timing functions, ease in, ease out, etc. Then within your CATransaction , adjust the properties on your CALayers and commit the transaction. In this case, we're going to set the position of our elements back to the center by making their offset the reserve of what we did in the configureForIntroAnimation . Then when we commit the transaction, everything moves into the center. Basic animations (16:15) Let's move on to CAAnimation . Let's talk a little bit about the picture frame. Notice that it moves, but it also has this fun little wobble to it. We want to pretend like we have some motion or we actually have some physics behind it. By doing that, we can create a CAAnimation class. In this case, it's a lazy variable used to initialize it once, but we're going to use CAKeyframeAnimation s. Keyframe animations give you the ability to set multiple states in a row. In this case, we want that wobble. That means we're going to animate the transform.rotation , which means you're animating the way that this thing rotates along the Y axis. If you look, there is a series of values for your keyframe animation. In this case, zero, negative 30, negative 10, and negative 20. Each one of those is an angular value for how I want it to move. We have a duration for the entire thing. We have keyTimes , which allows you to specify how long different parts of each one of your keyframe animations is going to take. Notice we have four values, four keyTimes , and the range for keyTimes is between zero to one. Each one of these is going to take about a third of our total duration, which is, in this case, about three-fourths of a second. Also, note that you can do timing functions for each part of your keyframe animation. In this case, we're going to do an ease in, ease out between the first and the second, ease out between the second, and the third, and so on. One of the other really fun, handy things is that there is a beginTime property on your basic animations. This allows you to adjust the offset of when your animation is going to start. By using CACurrentMediaTime() , you can add to that and say essentially, ""This is going to start a quarter of a second after the current time."" We have several things happening in our animated sequence. We want our picture frame to move laterally first, and then start swinging. To do that, one way we can do it is by delaying the basic animation for our rotation about a quarter of the second. When we put this all together, we get both lateral movement and rotation. One thing you have to do is add animations to elements. In this case, let's talk a little bit about the left arm, the right arm, and the picture frame. You'll notice that all of them have a little bit of rotation on them, The arms will pop up and have a little bit of a bounce on them and the picture frame moves as well. To do that, we're going to call addAnimation . We have a property called leftIntroAnimation that we'll go into in just a bit, and we give it forKey , which is not required, but I do tend to recommend it. It's a way that you can reference an animation later on. It also helps in debugging because you can print out what animation you might be looking at by name. So for our arm animations, we are going to use some of our same tools. We're going to use CAKeyframeAnimation , using rotation to make it do that little bounce. We're also going to do the same begin time. We're going to do a little bit shorter duration, some of our timing functions, but in this case, for our arm animations, we're going to have a base function that gives us back a standard arm animation. But we need to do it a little bit differently for our left and our right arms. Our left arm animation is going to call into this base implementation and set a few additional properties on it because the left arm and the right arm have different angles because they're on different sides. We're going to construct that basic keyframe animation for our arms, set the values, set the keyTimes , and return it. We could also have put the keyTimes in our constructor method, but in case the left arm or the right arm needed different values or different number values, that seemed a little bit risky. You always want your values and your keyTimes to be the same. Now we've got our bouncing little arms. We've got the picture frame rotating, and we've got everyone coming in from the left and the right. Animation groups (21:39) Let's talk about CAAnimationGroup . When the arms come in from the left and the bottom with a little bit of rotation, we can use a CATransaction , but we have a few new interesting things. There are some fireworks in the background, and those fireworks do a combination of things: they grow, fade in, and then they repeat for a long period. So for this, we're going to need to try animation groups. Animation groups allow you to apply multiple CAAnimation to a single object at the same time. In this case, we have an identityTransformationAnimation , a fadeAnimation , and we're going to put those together under one thing called a largeFireworkAnimationGroup . It's not as complicated as it sounds. For the animation group, create a CAAnimationGroup . I'm going to put the repeat count at HUGE , which pretty much means indefinitely. Then I'm going to assign an array of animations to that group, which in this case is the identity and the fade. Identity animation, in this case, is going to be that growth animation. It doesn't seem like it's a growth animation, but that's because we're going to use some of our tricks from before. What we'll do is transform the fireworks to be very small. Then, we're going to scale them down and animate them to their normal size. Typically in graphics, identity operation is returning something back to its normal state, so we're going to scale them down and then transform them up. Next, we're going to fade our fireworks in. When we scale them down, we're also going to put their alpha at a low or zero value. Then we're going to fade it in. Now when we are going to configure this, we create our animation group. You can set a duration for the entire group, so each animation in the group might have a different one. The fade and transform might be really quick, but the whole process is going to take about three and a half seconds. Then we're going to repeat the whole animation group again. Then all we have to do is say addAnimation , just like our other basic animations. An animation group can be applied, like a CAAnimation . What do we get out of that? Well, we get our nice, fun fireworks where they grow in size, they fade in or they fade out, depending upon how we have it set up. And it repeats. CATransaction - Part 2 (24:41) Let's go back to CATransaction . CATransaction s are a really powerful concept, and it's really handy because they work similar to UIView animations. For this screen, we have many things happening, and they happen all in a row. Animation 1 happens, and when that's done, we add animation 2, etc. CATransaction s have a completionBlock . To do that, you can start your transaction, set your properties that you normally do like last time, duration, timing function, but then you can also set a completion block. This completion block is going to happen when the transaction is completed. At the end of our duration of 0.45, so about half a second again, this CATransaction will happen, and you can chain these together. You want to be careful so that you don't have this chain of doom that goes 20 tabs out to the right. In this case, we want to move the main items into the center like we did with our first intro screen. Everything starts to the left or the right. We move it to the center, but if we notice the way this works is that after the character gets to the center, you'll see the arms and legs do an animation at that point. We want to trigger that only after our first transaction is complete. After we move our character to the center, we're going to put an identity transform onto the left arm, the right arm, and the left leg so that it undoes the initial rotation that we put on them. Next, commit that second transaction within the block. After this completion block, we still have to do our first transaction. One of the main things you want to consider is that you need to set your completion block before you commit your transaction because you can't put a completion block after something's already started. It can get a little counter-intuitive because it's going to look reverse oriented. You're going to have your last operation inside of your second to last, inside your first one. It's going to work similarly to our first screen where we're going to take our female character and move her to the center. We're also going to do that with the trees, where we just transform them into their identity state. We're going to unscale them. When we're done we have our character move to the center. After that's done, we had the arms and leg do their animation, and we have the trees and the buildings scale upwards back to their original state. Bonus: CATransitions! (28:03) One last thing, as a bonus, is we have CATransitions. You'll notice in a number of these animations that the text transitions between states is in a little bit more of an elegant fashion than our normal set text. Set text is very effective, but when you replace the text within about one frame, it's very jarring. When you're fading colors around your text, we really want to consider fading the text as well. What we have right here is a simple category or extension on a CATransaction for a factory method. This one creates a simple fade transition. We can apply this to our UI label. This looks very similar to our basic animations. We have a duration, we have a timing function, but we also have a type. In this case, we're going to call it a fade type. And that's all there is to it. I just return this simple fade. The way that we use this is we get our simpleFadeTransition and then add the animation to our title label before we set our text. As long as you add this and then set text, you get that really nice transition effect. Now if you're curious, and you want to play with this in the Fitbit app, it's the onboarding experience. You can also get it by going to the details for your hourly activity for that day. There's a little ""Learn More"" button in the top-right that'll present this at another time. Wrapping up, I'd like to encourage people to use the asset pipeline that works best for what you're building. In our case, Sketch to PaintCode into code worked fairly well. However, in retrospective, there were a few more tools that I should have checked out such as QuartzCode. One of the other things is that after you get your assets into a state, that's equilibrium, doing animations is really quick and simple, and it's a lot of fun. You can combine core animations, animation groups, and transactions to produce a swath of different experiences. Lastly, transitions are both simple and effective. I really encourage you to consider them for a variety of different UI elements, one of which is just tech labels. Q: With all the timing, and ease in, ease out combinations, there's a lot of build and run. Do you have any tips for how you can converge on 0.45 without having to go through a million iterations? Stephen: There are a couple of things you could do. In our case, we had a designer who's very capable with motion and animation. They mocked everything up in After Effects and we had a movie to go from, which was immensely helpful because it gives you something to go for as opposed to just shooting in the dark. One of the other things is that a number of the prototyping tools the designer might use will have similar types of curves and values that they might be using. So you can get those values from them to start with. But at the end of the day, both the values that they mocked up and the values that you use are going to need tweaking. It's really simple, but just communicating with your designer in person or over a video chat or something and doing a couple of runs, ""What do you think of this? Can you tweak it a little bit?"". Some of the other tools like QuartzCode and Squall might help with that as well. QuartzCode and Squall both directly implement animations in code, so you don't have to do it yourself. One of the downsides of that is it's a little less human readable, and the maintenance pattern is that you have to go back directly to the tool. Q: What are some tips that you would recommend as far as working with motion designers, as opposed to just static designers? Stephen: One thing you can do is set your own animation curves. You can define them based off of a Bezier path. Some programs, I believe After Effects, have the ability to actually export the curve that they're using. So if the motion designer really, really wants a specific curve, you can set that curve, and you can build up a library of factory methods of different curve types that you can apply to your animation. You can also build up your own curve set. It's not the most intuitive thing at all because it's just defining a Bezier path, but if you have a designer with a toolset that is very comfortable with that, that's definitely one option. I personally only go to curves and animations if it really makes sense because it keeps things a little simpler and more consistent, but at the end of the day, they typically do know best. Q: Have you had any issues combining your animations with Auto Layout? Stephen: Yes and no. CALayers don't necessarily respond to Auto Layout. They're hosted within a UIView, so your UIView responds to Auto Layout, but it hosts layers. In this case, because we had a central layer, we resized it. So you have to manually set the bounds or the frame of your layer based upon when the bounds or the frame of your host view changes, if your layer is not a host layer. Because of that, you'll notice that many of the animations we used, for example, when the left and the right items were starting off the screen and moving to the center, those were based off of view.bounds and not Auto Layout. When you're doing UIView animation, I highly recommend using Auto Layout inside of your animation block because that's the recommended course. When you move to the CALayer world, you're hosted within a view, which should have Auto Layout laid out, but then you're using bounds and frame a bit more.",en,45
416,851,1462717594,CONTENT SHARED,-8279505492976427918,-1032019229384696495,-3679495377217154046,,,,HTML,http://techcrunch.com/2016/05/07/bots-messenger-and-the-future-of-customer-service/,"bots, messenger and the future of customer service","In the 1970s, CFOs sat with CEOs and devised ways for upset consumers to not be able to get compensation easily. They put up automated phone systems and arcane and inflexible policies and rejoiced at how little the company had to ""give up"" to complaining customers. And this system worked... until social media gave every average ""Joe"" the same power as society's most prominent citizens to get a company's attention when they weren't happy. Watching F8 last month, one couldn't help but wonder if the future is truly as simple as opening a chat and texting what you need. Order flowers ✔ Get news ✔ Check the weather ✔ Handle customer service issues? How many of us who have called our cable company and been on hold ""forever"" wouldn't love to chat with a rep and get our issue handled in a minute ( Amy Schumer would agree )? How many of us would love to chat with an airline and resolve an issue in real time? While these things sound great in theory, it's not quite so simple. Chat is a better medium than the phone for most people, especially younger people. But it doesn't solve everything. An inarticulate consumer is going to be inarticulate over chat. A rep that's having a bad day is going to be just as inflexible and unsympathetic in a chat. Changing the medium isn't a silver bullet for customer service. Bots can help with customer service. They can gather information for the eventual interaction with a human rep, understand exactly what happened and what the consumer wants and even be empowered to solve basic issues, automatically. Bots have been around a long time -  phone systems ask you to speak your account number, say what you're calling about or push a digit corresponding to what you want. The difference is that, while those kinds of bots typically annoy customers, chatbots have the potential to have the reverse effect. The right balance of ""bot"" and ""human"" is going to be different for each company, and it depends greatly on the quality of the bot - and, of course, the quality of their human customer service reps. I believe in an intelligent mix of humans and bots to get the job done  -  the ""job"" being making a consumer happy when things go awry, while being fair to the business. Over time, AI (""bots"") will get smarter, and brands will trust it more to solve issues for them. Imagine the cable company allowing you to schedule an appointment in 30 seconds via a chat (versus 5-25 minutes on the phone). Imagine a retailer replacing a defective product in seconds. Or a hotel making up for a bad experience by awarding points. Bots can handle all these scenarios automatically, freeing up the brand's valuable (and expensive) reps to handle the issues that truly require a human touch. Facebook's Messenger platform is just one of many future platforms where you'll be able to handle customer service issues. By blending smart software with a light human touch, consumers will have a better experience when they need customer service, and companies will avoid nasty tweet storms, negative online reviews and the kind of brand damage consumers are now capable of inflicting when they're upset. The future of customer service is making it easy for consumers to go through the medium they want (likely not the phone), have an experience that respects their time and have issues resolved quickly, ideally without involving a human at all (via intelligent software). In the future, ""customer service"" won't be something consumers dread having to call, it will be something that builds powerful relationships with consumers. The best marketing is great customer service, and chatbots are a great step forward. Featured Image: Bryce Durbin",en,45
417,2165,1471999492,CONTENT SHARED,621816023396605502,-1443636648652872475,560375642352974408,,,,HTML,http://www.wired.com/2016/08/boomerang-using-ai-help-send-better-email/,ai is here to help you write emails people will actually read,"Looking for an artificial intelligence that can write all your email messages so you don't have to? Too bad. That dream is still years away. But in the meantime, a startup called Boomerang wants to take you at least part of the way there. Boomerang makes a plug-in for Gmail and Outlook that lets you hit the ""snooze button"" on certain messages. They'll disappear from your inbox but then reappear at later time. But today, the company added a new twist to this plug-in: a new AI-powered tool called ""Respondable."" Respondable analyzes your messages as you write them, predicts how likely they are to get a response, and then suggests ways you can improve them. If your subject line it too terse, for instance, or the email's tone seems rude, it will tell you. In theory, this will make life easier for people on both ends on the exchange: The recipient the email will get clearer, more actionable emails, and the sender should be more likely to get a prompt response. The project is in its early days but it does give us a glimpse of how AI might work in concert with humans, not to take our jobs but to make our jobs a bit easier. The tool is part of a much wider effort to push artificial intelligence into our email and calendar services. The startup Crystal scrapes publicly available data about the people you send email to and gives you tips on what style of email you they might prefer based on their personality. Clara and X.ai analyze your email to help you schedule meetings. And Microsoft just acquired a meeting coordination startup called Genee. Meanwhile, Google is already generating simple automatic replies to emails. But Boomerang is in a unique position to offer this particular service. In addition to helping you snooze emails, the plugin can also remind you about emails you're waiting on a response to. That gives the company a nice big set of data about which emails receive responses and which don't. While companies companies like Google and Microsoft obviously have access to much bigger piles of email, Boomerang CEO Alex Moore says his company's approach may be more accurate. ""Gmail is going to see a lot of email that people aren't necessarily expecting a reply to,"" he explains. But users are clearly expecting to receive responses to the messages they ask Boomerang to monitor. The company also relies on pubic data sets to help train its algorithms. Specifically, Moore says, the company drew upon movie reviews to help its AI learn the difference between text with a positive sentiment and text with a negative sentiment. The algorithms are a mix of different AI approaches, including deep learning , a branch of AI benefitting from a wealth of research and open source software released by big name companies, including Google, Facebook, and Microsoft. But Moore says that in many cases, deep learning algorithms are hard to work with because it's hard to tell why they produce the results they do. ""We focused on algorithms that are explainable,"" he says. Moore says the most surprising thing the team learned is that responses drop-off significantly for emails that use language above about a sixth grade reading level, but emails that are too simple tend to be ignored as well. A 3rd grade reading level is optimal. He was also surprised that neutral emails tend to receive fewer responses than positive or negative emails. In short, being too negative is bad, but being slightly negative is better than being boring.",en,45
418,3027,1485533629,CONTENT SHARED,-2314360211211303885,8195788452563155020,8414459452745693932,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36",SP,BR,HTML,http://www.hypeness.com.br/2017/01/o-meme-luiza-voce-esta-atenta-explica-da-melhor-forma-o-machismo-na-area-da-tecnologia/,"o meme 'luiza, você está atenta?' explicou da melhor forma o machismo na área da tecnologia","Que o mercado de trabalho em geral ainda é extremamente machista, a gente já sabe. Mas infelizmente, há diversas áreas onde o machismo é tão evidente que torna a presença feminina praticamente inexistente. É o caso do mercado tecnológico. No Brasil, somente 17% dos programadores de TI são mulheres, por exemplo. E estima-se que mais de 90% dos profissionais da computação são do sexo masculino. Isso sem contar as diferenças salariais. Para combater esses números, existem iniciativas como o PyLadies , um grupo internacional com foco em ajudar as mulheres a se tornarem participantes ativas e líderes da comunidade de código aberto Python. E o PyLadies de Teresina , no Piauí, aproveitou a deixa do meme do momento, o ""Luíza, você está atenta?"" , para debater este problema, mostrando que as mulheres podem fazer sim a diferença na computação. ""Você tá achando que as mulheres vão deixar o machismo na área de tecnologia barato? BANG, Luiza! A gente cria várias redes de mulheres pra ocupar esses espaços!"" , diz o post do projeto, que teve mais de 1 mil curtidas. Todas as imagens © Reprodução Facebook",pt,45
419,1215,1464879671,CONTENT SHARED,7277161137091492767,3891637997717104548,4240699853832706755,,,,HTML,http://buytaert.net/an-overview-of-web-service-solutions-in-drupal-8,an overview of web service solutions in drupal 8,"Today's third-party applications increasingly depend on web services to retrieve and manipulate data, and Drupal offers a range of web services options for API-first content delivery. For example, a robust first-class web services layer is now available out-of-the-box with Drupal 8. But there are also new approaches to expose Drupal data, including Services and newer entrants like RELAXed Web Services and GraphQL. The goal of this blog post is to enable Drupal developers in need of web services to make an educated decision about the right web services solution for their project. This blog post also sets the stage for a future blog post, where I plan to share my thoughts about how I believe we should move Drupal core's web services API forward. Getting aligned on our strengths and weaknesses is an essential first step before we can brainstorm about the future. The Drupal community now has a range of web services modules available in core and as contributed modules sharing overlapping missions but leveraging disparate mechanisms and architectural styles to achieve them. Here is a comparison table of the most notable web services modules in Drupal 8: Core RESTful Web Services Thanks to the Web Services and Context Core Initiative (WSCCI), Drupal 8 is now an out-of-the-box REST server with operations to create, read, update, and delete (CRUD) content entities such as nodes, users, taxonomy terms, and comments. The four primary REST modules in core are: Serialization is able to perform serialization by providing normalizers and encoders. First, it normalizes Drupal data (entities and their fields) into arrays with a particular structure. Any normalization can then be sent to an encoder, which transforms those arrays into data formats such as JSON or XML. RESTful Web Services allows for HTTP methods to be performed on existing resources including but not limited to content entities and views (the latter facilitated through the ""REST export"" display in Views) and custom resources added through REST plugins. builds on top of the Serialization module and adds the Hypertext Application Language normalization, a format that enables you to design an API geared toward clients moving between distinct resources through hyperlinks. allows you to include a username and password with request headers for operations requiring permissions beyond that of an anonymous user. It should only be used with HTTPS. Core REST adheres strictly to REST principles in that resources directly match their URIs (accessible via a query parameter, e.g. ?_format=json for JSON) and in the ability to serialize non-content into JSON or XML representations. By default, core REST also includes two authentication mechanisms: basic authentication and cookie-based authentication. While core REST provides a range of features with only a few steps of configuration there are several reasons why other options, available as contributed modules, may be a better choice. Limitations of core REST include the lack of support for configuration entities as well as the inability to include file attachments and revisions in response payloads. With your help, we can continue to improve and expand core's REST support. RELAXed Web Services As I highlighted in my recent blog post about improving Drupal's content workflow , RELAXed Web Services , is part of a larger suite of modules handling content staging and deployment across environments. It is explicitly tied to the CouchDB API specification , and when enabled, will yield a REST API that operates like the CouchDB REST API. This means that CouchDB integration with client-side libraries such as PouchDB and Hood.ie makes possible offline-enabled Drupal, which synchronizes content once the client regains connectivity. Moreover, people new to Drupal with exposure to CouchDB will immediately understand the API, since there is robust documentation for the endpoints. RELAXed Web Services depends on core's REST modules and extends its functionality by adding support for translations, parent revisions (through the Multiversion module), file attachments, and especially cross-environment UUID references, which make it possible to replicate content to Drupal sites or other CouchDB compatible services. UUID references and revisions are essential to resolving merge conflicts during the content staging process. I believe it would be great to support translations, parent revisions, file attachments, and UUID references in core's RESTful web services - we simply didn't get around to them in time for Drupal 8.0.0. Services Since RESTful Web Services are now incorporated into Drupal 8 core, relevant contributed modules have either been superseded or have gained new missions in the interest of extending existing core REST functionality. In the case of Services , a popular Drupal 7 module for providing Drupal data to external applications, the module has evolved considerably for its upcoming Drupal 8 release. With Services in Drupal 8 you can assign a custom name to your endpoint to distinguish your resources from those provisioned by core and also provision custom resources similar to core's RESTful Web Services. In addition to content entities, Services supports configuration entities such as blocks and menus - this can be important when you want to build a decoupled application that leverages Drupal's menu and blocks system. Moreover, Services is capable of returning renderable objects encoded in JSON, which allows you to use Drupal's server-side rendering of blocks and menus in an entirely distinct application. At the time of this writing, the Drupal 8 version of Services module is not yet feature-complete: there is no test coverage, no content entity validation (when creating or modifying), no field access checking, and no CSRF protection, so caution is important when using Services in its current state, and contributions are greatly appreciated. GraphQL GraphQL, originally created by Facebook to power its data fetching, is a query language that enables fewer queries and limits response bloat. Rather than tightly coupling responses with a predefined schema, GraphQL overturns this common practice by allowing for the client's request to explicitly tailor a response so that the client only receives what it needs: no more and no less. To accomplish this, client requests and server responses have a shared shape . It doesn't fall into the same category as the web services modules that expose a REST API and as such is absent from the table above. GraphQL shifts responsibility from the server to the client: the server publishes its possibilities, and the client publishes its requirements instead of receiving a response dictated solely by the server. In addition, information from related entities (e.g. both a node's body and its author's e-mail address) can be retrieved in a single request rather than successive ones. Typical REST APIs tend to be static (or versioned, in many cases, e.g. /api/v1 ) in order to facilitate backwards compatibility for applications. However, in Drupal's case, when the underlying content model is inevitably augmented or otherwise changed, schema compatibility is no longer guaranteed. For instance, when you remove a field from a content type or modify it, Drupal's core REST API is no longer compatible with those applications expecting that field to be present. With GraphQL's native schema introspection and client-specified queries, the API is much less opaque from the client's perspective in that the client is aware of what response will result according to its own requirements. I'm very bullish on the potential for GraphQL, which I believe makes a lot of sense in core in the long term. I featured the project in my Barcelona keynote ( demo video ), and Acquia also sponsored development of the GraphQL module (Drupal 8 only) following DrupalCon Barcelona. The GraphQL module, created by Sebastian Siemssen , now supports read queries, implements the GraphiQL query testing interface, and can be integrated with Relay (with some limitations ). Conclusion For most simple REST API use cases, core REST is adequate, but core REST can be insufficient for more complex use cases. Depending on your use case, you may need more off-the-shelf functionality without the need to write a resource plugin or custom code, such as support for configuration entity CRUD (Services); for revisions, file attachments, translations, and cross-environment UUIDs (RELAXed); or for client-driven queries (GraphQL). Special thanks to Preston So for contributions to this blog post and to Moshe Weitzman , Kyle Browning , Kris Vanderwater , Wim Leers , Sebastian Siemssen , Tim Millwood and Ted Bowman for their feedback during its writing.",en,45
420,1122,1464351304,CONTENT SHARED,-5515204090203464377,-48161796606086482,-7349077394912525823,,,,HTML,http://marker.to/iL9tJm,a step-by-step guide to agile growth experiments,"After driving growth for the world's largest companies, building automated trading systems like SearchForce and managing over $1 billion media spend while studying high growth companies like AirBnB, Uber and Zenefits, I've consolidated my experience into a single, strategic framework for building and running successful experiments for marketing. Every organization is unique; each with different customers, markets, products, and business models. What is universal, however, is the opportunity for any business to adopt a highly scientific approach to marketing. Therefore, running lean and adopting a data-driven mindset to achieve breakthrough growth is not just a do or die for lean and mean startups - it's for any organization that wants to build its own growth machine. Marketing experiments involve five fundamental steps: 1. GrowthStorm You start by brainstorming all sorts of ways to drive growth. You also generate ideas on how you can test these hypotheses. Put them in a process document called ""GrowthStorm."" This will serve as a central creative repository for your growth efforts. Download the template for experiments pipeline and growth storming here. 2. Prioritize execution Once you have your collection of ideas, it's time to organize them. Create a ""Prioritize Execution"" document that keeps tabs on past, current, and future experiments. This document should also include goals and projections for tests you're running. The doc provides a quick and easy way for team members to understand what tests have been run, when they were run, and their respective outcomes. Note: This document must be available at every level of the company, from the CEO on down to the implementation team. Running experiments in a silo makes it unlikely that insights will drive real action, particularly where skills across departments are required. 3. Apply the scientific method Each test gets its own ""Method"" document stating, in detail: Problem: What is the test attempting to solve for Hypothesis: The specific results you expect to see Resource Estimation: The resources of time and energy you'll need from your team Experiment: The nitty-gritty - implementation, elements, etc. Gather Data: Compiling test data once completed Action Items: Taking action on findings - optimizations and future test planning Download the template for OKR Growth goal and a sample Method document. 4. Standardize learnings and scale Once a test has been created, run, and analyzed, capitalize on your new insights by standardizing key findings back into your product and marketing. The idea is that as you run more tests and gain more data and insights, you then use those data and insights to run smarter tests with more informed predictions. The growth process is cyclical, so the standardization phase isn't the end of the process. Rather, it's an ongoing process of up-leveling by which teams are constantly gaining greater insight into their customers and markets, and translating those insights into better user experience, better products, and more effective marketing. Download the template for sample agile meetings and monthly tracking against goals. Remember to learn from your tests. The last step to this process is one of coming together as a team and taking the time to discuss questions, findings, ideas, and proposed actions related to the tests you've performed. This is a crucial part of the process because it's easy to get so caught up in the implementation and management of current tests that the actual results of completed tests are overlooked. 5. Ask your growth team these key questions What are we learning from the tests we're running? (Just collecting the data isn't enough.) How can we double down on what's working by refining successful tests? (When something works, focus on figuring out why.) How do we implement our findings at scale? (Once you understand the why, how will you bring that learning to other areas of the business?) What are we learning about what NOT to do? Not all tests will be successful and it's crucial to take action on failed experiments. What is the plan of action going forward? (Keep the heat on your team to maintain their testing schedule and continue to generate insights!) Find out about VentureBeat's upcoming agile marketing roadshow. This is a process of focusing on specific tactical actions aimed at improving performance, and then stepping back for a more macro look at what's working and what isn't, and how to take action on key findings. For example, from a 10,000-foot level you are learning to improve your product, UX, functionality, marketing, pricing, etc. This data will teach you how to optimize every one of your channels individually, and also which is most useful to choose from among those channels. You can monitor competitors' data and use that for bench marking your progress. Then you dive back in and actually plan and run new tests, and improve on current tests. You never stop toggling between these two vantage points; constantly communicating, collaborating, learning, and improving. This is your process to achieve hockey stick Growth via Agile. References: Tammy Camp @ 500 Startups & The Scientific Method: How to Design & Track Viral Growth Experiments by Brian Balfour, Hubspot. Samir Patel is president and CEO of Growth Machines and a growth mentor at 500Startups. You can follow him on Twitter: @meetsamir . Get more stories like this on Twitter & Facebook",en,45
421,1466,1466456159,CONTENT SHARED,2809702925357288303,-1443636648652872475,-8854509447672303603,,,,HTML,https://cloudplatform.googleblog.com/2016/06/build-your-own-scalable-location-analysis-platform-with-Google-Cloud-Platform-and-Maps-APIs.html,"build your own scalable, location analysis platform with google cloud platform and maps apis","When our customers work with telemetry data from large fleets of vehicles or big deployments of sensors that move about in the world, they typically have to combine multiple Google APIs to capture, process, store, analyze and visualize their data at scale. We recently built a scalable geolocation telemetry system with just Google Cloud Pub/Sub , Google BigQuery and the Google Maps APIs . The solution comes with a full tutorial, Docker images you can use straight away and some sample data to test it with. We chose Google Cloud Pub/Sub to handle the incoming messages from vehicle or device sensors as it is a serverless system that scales to handle many thousands of messages at once with minimal configuration. Just create a topic and start adding messages to it. Google BigQuery offers petabyte scale, serverless data warehousing and analytics - ideal for large fleets of vehicles that will send thousands of messages a second, year after year. Further, BigQuery can perform simple spatial queries to select by location or do geofencing on vast datasets - all in a few seconds. The Google Maps APIs add an extra dimension to telemetry data by converting raw GPS position into human-readable structured address data, as well as adding other really useful local context such as elevation (great for fuel consumption analysis) and local time-zone (maybe you want to just see locations recorded during working hours for a given location). Google Maps also provides an interface with which the majority of your staff, customers or users are familiar. Finally we packaged our solution using Docker so that you can just take it and start working with it right away. (Of course if you'd rather just run the code on a server or your local machine you can do this as well; it's written in Python and can be run from the command line.) To get started, read the solution document , then head on over to the tutorial to explore the sample application and data. Once you've had a play, fork the code on GitHub and start working with your own telemetry data!",en,45
422,1661,1467745314,CONTENT SHARED,2822049545552366036,8766802480854827422,-2589067721385166046,,,,HTML,http://chiefmartec.com/2016/07/2-terrific-martech-talks-rise-ai-marketing/,2 terrific #martech talks on the rise of ai in marketing - chief marketing technologist,"One of the hottest topics in marketing technology today is the rise of artificial intelligence (AI), machine learning, and cognitive computing in marketing. If you're wondering about the difference between those three terms, well, my feeble attempt at delineating them is this: machine learning is a subset of AI (which broadly applies to computers doing ""smart"" things), and machine learning is used in cognitive computing to enable a software program to ""reason"" like a human, which is one kind of AI. Does that clear it up? ""Uh, not really."" Well, here are two much better explanations of these concepts and how they are being applied to marketing by two of our featured speakers at the MarTech conference in San Francisco this past March . First, Gerry Murray of IDC gave a fantastic talk on Cognitive Marketing: The Rise of the Super-Intelligent Marketer : And second, David Raab of Raab & Associates - and a member of the MarTech Conference advisory board - gave a terrific presentation on How Machine Intelligence Will Really Change Marketing : Thinking marketing technology has been a wild ride so far? We're just beginning. Interested in more talks on this caliber on marketing technology? Join us at an upcoming MarTech conference in Europe or the US.",en,45
423,1997,1470413693,CONTENT SHARED,6615662748793536194,1895326251577378793,4845135633002089199,,,,HTML,http://computerworld.com.br/especial-o-papel-dos-grandes-provedores-de-ti-na-rio-2016,especial: o papel dos grandes provedores de ti na rio 2016,"Durante o mês de agosto, o mundo todo estará com os olhos voltados para o Brasil. Os jogos Rio 2016 receberão mais de 10,5 mil atletas de 206 países, que disputarão medalhas em 28 esportes, que acontecerão em 37 locais distintos. Estima-se, também, que 4,8 bilhões de espectadores no mundo todo assistirão as competições. Embora seja invisível para a maioria das pessoas, a tecnologia desempenha um papel essencial para que os Jogos sejam um sucesso. Grandes fornecedores de TI estão envolvidos para ajudar entregar ao mundo um evento memorável. A seguir, apresentamos algumas das principais ações da indústria durante os jogos. Atos A Atos é responsável por todo projeto de TI da Rio 2016 , integrando soluções de diversos vendors (dentre os quais Omega, Panasonic, Samsung, Embratel/Claro, EMC, Cisco e Symantec ). A empresa é parceira do Comitê Organizador Internacional e ficou encarregada de montar a estrutura e fazer todo o suporte dos recursos computacionais do evento.. A Atos fornece, por exemplo, os sistemas que transmitem os resultados ao mundo inteiro em menos de um segundo, para uso na televisão, na Internet e em múltiplos aparelhos, o portal que possibilita o recrutamento e treinamento de 70 mil voluntários e a solução que processa 400 mil credenciais para que os envolvidos nos Jogos possam ter acesso às áreas necessárias durante os Jogos. América Movil Há quatro anos, os Jogos Olímpicos de Londres atingiram a impressionante marca de geração de 60GB de informações por segundo. Além disso, o equivalente a 2 mil horas de cobertura ao vivo foi transmitida digitalmente para mais de 14,4 mil emissoras de TV em todo o mundo. Este ano, durante a Rio 2016, as empresas do Grupo América Móvil (Embratel, Claro e Net) estimam que o tráfego de dados e de internet pode chegar a superar em 4 vezes todo o tráfego dos Jogos Olímpicos de Londres. As empresas são responsáveis por toda a infraestrutura de rede que servirá aos locais de prova, à Vila Olímpica e ao centro de transmissão (IBC), de onde sairão imagens para aproximadamente 5 bilhões de telespectadores de 200 países. A base de telecomunicações do Rio 2016 será o Backbone Olímpico Embratel, uma rede com mais de 370 quilômetros de fibras ópticas, triplamente redundante, para garantir a alta disponibilidade na transmissão de dados dos jogos. A rede olímpica terá velocidade de 40 Gigabits por segundo, conectando mais de 60 mil pontos de acesso à rede, distribuídos em mais de 100 locais relacionados ao evento, de competição ou apoio. Parte de toda essa rede só será retirada daqueles locais construídos temporariamente para os jogos. Em boa parte das instalações olímpicas, permanecerá. A Embratel fornecerá, ainda, a rede de fibra óptica que captará os sinais de vídeo de todas as competições esportivas do Rio 2016, nos distintos locais onde se realizam, entregando-os ao centro de transmissão IBC - International Broadcast Centre - de onde sairão imagens para aproximadamente 5 bilhões de telespectadores de 200 países. Serão mais de 7 mil horas de transmissão geradas pela Olympic Broadcasting Services, para atender todas as emissoras detentoras de direitos ao redor do mundo. Cisco A companhia é a parceira oficial e fornecerá infraestrutura de rede e servidores para as Olimpíadas. A empresa descarregou mais de 60 toneladas de equipamentos para montar um ambiente nos 37 locais de provas e 183 pontos onde não haverá competição (centro de mídia, Vila Olímpica, etc). Ao todo, serão mais de 100 mil portas de redes, 5 mil pontos de acessos e 150 firewalls nos mais de 400 Cisco UCS Servers. EMC A companhia assinou um contrato com Comitê Organizador dos Jogos Olímpicos Rio 2016 para suportar duas principais torres de tecnologia de informação do evento (Rede Administrativa-ADMIN e Rede Jogos-GAMES), que centralizam os dados obtidos nas 35 instalações de competições, nas quais, mais de 10,5 mil atletas estarão competindo. Segundo a fabricante, esta capacidade promoverá os ambientes de armazenamento e proteção dos dados que forem gerados, permeando todas as fases das competições, desde o planejamento até o período de operações. Dados cruciais como os resultados das competições, fotos e vídeos dos eventos, informações dos atletas e de suas nações, registros das instalações, entre outros, serão armazenados e protegidos por meio das tecnologias EMC: Avamar, VNX e RecoverPoint. O espaço poupado pelo uso das soluções EMC para armazenamento dos dados é o suficiente para dois milhões de fotografias adicionais, por exemplo. Para o evento, será destinado espaço de armazenamento para mais de 55 terabytes de dados, o equivalente a 720 mil fotos ou 4 mil minutos de filmagem. Microsoft O conceito de cloud chegou às Olimpíadas. O portal do evento rodará na plataforma em nuvem Azure integrada à tecnologia desenvolvida e gerenciada pela Atos, parceira mundial de TI dos Jogos Olímpicos, atualiza o site com informações em tempo real. O website exibe a partir de hoje os resultados de mais de 300 provas das 42 disciplinas esportivas disputadas nos 17 dias do evento, além de calendário, regras estabelecidas pelas Federações, dados sobre os mais de 10 mil atletas e equipes, ranking de medalhas, desempenho por país e muito mais. Visa A empresa aproveita as Olimpíadas para impulsionar meios inovadores de pagamento. A companhia apresentou uma solução wearable pré-pago para realizar transações em meios de transporte público, por exemplo. Google A gigante de buscas liberou ferramentas para que as pessoas confiram o cronograma de competições, ver o quadro de medalhas, descobrir o resultado de cada disputa e ver as tendências de busca sobre os atletas, países e modalidades. Ao pesquisar por ""Jogos Olímpicos"" ou sobre algum dos esportes e atletas favoritos, o sistema mostrará um painel detalhado de informações na parte superior da página de resultados. No painel haverá informações sobre os resultados, calendário de cada modalidade e informações sobre as delegações. Se a busca for feita no aplicativo para Android ou iOS, há opção de receber atualizações automáticas dos eventos que queiram seguir durante os jogos. HPE A HPE participa de uma forma não tão direta na Rio 2016. A companhia participou de um projeto de modernização da infraestrutura de rede do RIOgaleão (Aeroporto Internacional Tom Jobim), que será a porta de entrada oficial das Olimpíadas. a iniciativa contemplou, também, o lançamento de um aplicativo para facilitar a vida dos turistas no local. GE A GE fechou acordo com o COI para fornecer um software de gestão da saúde para toda a estrutura oficial de atendimento médico das Olimpíadas de 2016, no Brasil, incluindo a Policlínica, e que atenderá atletas de todos os lugares do mundo. A companhia é uma das patrocinadoras dos Jogos. A tecnologia, chamada Centricity Practice Solution, foi desenvolvida inicialmente para o United States Olympic Committee (USOC), que a tem utilizado com muito sucesso desde os Jogos Olímpicos Londres 2012. A ferramenta, que realiza o registro eletrônico de todas as interações médicas dos atletas, contará com uma versão totalmente em português, atendendo às leis de saúde brasileiras, além de possuir os conteúdos e aplicações em inglês.",pt,45
424,2488,1475419441,CONTENT SHARED,834896074125772354,-48161796606086482,-440734810322943341,Android - Native Mobile App,SP,BR,HTML,http://venturebeat.com/2016/10/02/so-you-want-to-sell-to-banks/,so you want to sell to banks?,"If you ask almost any early-stage B2B FinTech startup selling to banks what their biggest challenge is, the answer you'll likely get is ""sales."" Although interest in FinTech from incumbent institutions has surged over the past few years, getting to initial pilots and contracts with banks is a challenging process for any early FinTech company. Many B2B FinTech companies will experience a 6-12 month sales cycle to get to pilot. After the financial crisis, banks have become more risk-averse, tightening their compliance processes, and startups often must navigate through multiple decision makers and legacy systems. As an investor at Matrix, an early-stage venture capital firm, I spend a lot of time looking at FinTech. I wanted share some best practices from companies both inside and outside our portfolio who have had success in selling to banks, as well as from representatives at regional and national banks who make the purchasing decisions. Find the individual at the right level within the organization who can endorse the product. This individual can help walk a company into places where there may be a back-and-forth conversation (about compliance, for example). At TrueAccord, for example, the full sales team is focused on identifying and reaching the individual who can champion the product. Every Friday, a group of salespeople and executives at the company will do a strategy session on the top accounts, and salespeople will voice the resources they need to move certain accounts forward, from investor introductions to additional collateral. Successful sales teams also maintain a regular cadence with their champions, arming them with new, relevant information and providing details on their progress with other accounts. According to an executive at a national bank, one of the biggest mistakes startups make is not keeping their champion updated on their progress: ""I will sometimes make introductions inside the organization for a company and then never hear from them again. I want to know how the discussions are progressing and how I can help."" 2. Come prepared to educate the client and share your materials FinTech sales often look different from a traditional enterprise software sale. ""You are part consultant, part salesperson,"" says Bill Ervin, the senior VP of business development, at Mirador. ""We look for people with experience in banking or ERP software on our sales team."" Banks will want their vendors to understand their internal and regulatory processes. ""We show our customers that we are not just a technology company but that we know the business processes and the industry,"" says Brian Kneafsey, head of sales at Blend. On topics like blockchain, the conversations with banks can be half academic, half commercial. Furthermore, banks will typically want a substantial amount of detail before moving ahead with a proof of concept (POC). In the initial conversation, coming equipped with collateral that explains the product and contains a detailed appendix can be helpful for a bank representative who will likely be sharing your materials with 6-7 other individuals. Over the course of the sales process, banks will often ask for details about the security of the platform, financials, and compliance. ""You need to have all your ducks in a row. Banks will want to know exactly how you are doing things,"" says Ty Bennion, senior VP of global sales at HyperWallet. ""You need to be able to say here is my backup documentation, and anticipate potential questions about the offering."" 3. Articulate the project plan early A traditional bank will often ask a vendor to talk to a number of departments before proceeding. These conversations may also be spread several months apart. Coming prepared for the initial conversations with a suggested project plan and playbook before POc can significantly expedite the process and increase a client's confidence in the team. ""Try to plan to work as much as possible in parallel paths,"" Joe Gelbard, the Chief Revenue Officer of True Accord, advises. ""When selling to a bank, you may have to speak to operations, vendor risk management, legal, and a number of other divisions, each of which take time. If you can coordinate some of these steps, it can significantly shorten your sales cycle."" It is important to be specific with timelines in a conversation without being pushy. Banks are often significantly more interested once they have seen the startup execute a playbook successfully with another bank. 4. Beware of scope creep Furthermore, it is important to ask questions about decision-makers, processes, and budget early in the conversation. ""You want to ask questions early because it can be hard late in the game,"" says Kneafsey at Blend. ""There can be a lot of nuances inside a bank."" Understanding how products move successfully through the organization and getting the organization's thoughts around the budget (e.g., is it an established budget? Given many banks will plan in advance, what year is the budget for?) can be incredibly helpful for the prioritization and forecasting of your pipeline. As more people from the bank's side become involved in a project, many startups have experienced the problem of scope creep. As an early-stage company with limited resources, it is important to stay as focused on the core product as possible. 5. Align on the goals of the POC and try to limit the use of the bank's resources Often, having representation from legal and compliance as early as possible can help mitigate issues in these areas down the road. Aligning on the base requirements in advance and having the champion involved in the conversations can help prevent scope creep. Banks are unlikely to invest significant time and resources into a POC. The less integration required, the faster the process can typically move. ""We try to place the least amount of burden on the engineering team as possible until there is a firm commitment,"" says Gelbard of TrueAccord. The first POC for an early stage company may not result in revenue. It is more important to find the right partner who is responsive with early feedback and can invest the right time and resources to make the product successful. It is also important to have clear goals from the POC. ""You need to agree on the KPIs before the POC so you can point to delivering them,"" says Bennion of HyperWallet. Furthermore, many successful startups share a clear path with potential customers about getting from POC to live contract. This information can be documented in the contract, and some companies also offer incentives to prompt the bank to go live by a certain date. This strategy has been successful for a number of FinTech companies. Some startups have adopted ""light"" versions of the product that may not have the same credit and compliance requirements. Others will try to get a portion of the bank's business before aiming for a large contract. For example, TrueAccord asks a bank to provide it with a portion of the business it may be giving to other collection agencies to demonstrate the lift their product can provide. Other companies have gotten smaller departments live with their product, which has significantly eased the sales process with other departments within the company. Anoushka Vaswani is an investor at Get more stories like this on Twitter & Facebook Matrix Partners , where she focuses on FinTech and enterprise software. She is involved in the firm's investments in Lever, Inflection, and GOAT and serves as a Board Observer for ACME Technologies and PayRange. If you have any further thoughts or suggestions, please drop me a line at anoushka@matrixpartners.com or find me on Twitter @anoushkavaswani. Thanks to Ty Bennion at HyperWallet, Bill Ervin at Mirador, Joe Gelbard at TrueAccord, and Brian Kneafsey at Blend as well as the banking executives who contributed to this post.",en,45
425,3067,1486571714,CONTENT SHARED,8477804012624580461,-8606737560479590536,1088209812385750316,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36",SP,BR,HTML,http://ofuturodascoisas.com/gartner-em-2020-nao-havera-mais-aplicativos/,gartner: em 2020 não haverá mais aplicativos,"""Em 2020, as pessoas não irão usar aplicativos em seus aparelhos. Na realidade, os apps estarão esquecidos. As pessoas vão contar com os assistentes virtuais pra tudo. A era pós- app está vindo"". Essa foi a declaração de Peter Sondergaard , Vice-Presidente sênior de Pesquisa da Gartner, empresa líder de pesquisa em tecnologia de informação (TI). O trabalho da Gartner também é prever o futuro. Mas, como assim? Os aplicativos deixarão de existir? Segundo Sondergaard, os algoritmos é que farão todo o trabalho. Para ele, num futuro não muito distante, é possível que os algoritmos tomem decisões importantíssimas, ""que podem significar vida ou morte."" No Gartner Symposium/ITxpo , realizado em Outubro do ano passado, a Gartner tinha destacado 15 tendências de tecnologia que serão estratégicas em 2016 para a maioria das empresas, e tinha abordado esse assunto dos assistentes virtuais. O Futuro das Coisas criou esse post , explicando estas 15 tendências, incluindo a tendência dos assistentes virtuais como uma oportunidade das empresas aumentarem a sua produtividade ou desenvolverem novas soluções com base no aprendizado de máquina . Gartner Symposium/ITxpo, realizado em Outubro do ano passado, em Orlando (EUA). Aprendizado de máquina O aprendizado de máquina abre as portas para um mundo de máquinas inteligentes, incluindo robôs, assistentes pessoais virtuais e carros, que agem de forma autônoma. Google Now, Siri e Cortana estão ganhando inteligência e são os precursores dos assistentes virtuais que serão criados para aprender por si mesmos e que se tornarão, no futuro, a única interface com que nós usuários teremos que interagir . Os algoritmos (pelo menos hoje) são um conjunto de regras que nos ajudam a resolver um problema. A informação chega ao algoritmo e, em seguida, com base nas regras, o resultado é determinado e você tem uma resposta para orientar o que precisa ser feito. ""No futuro, os algoritmos serão capazes de ajustarem-se sozinhos, elaborando novas regras. Os algoritmos também, aparentemente, podem se multiplicar."" Peter Sondergaard , Vice-Presidente sênior de Pesquisa da Gartner Os assistentes virtuais irão criar novos assistentes e robôs irão criar novos robôs. Por isso, Sondergaard considera crucial criar algoritmos da forma certa. Aqui, no O Futuro das Coisas , já tínhamos abordado nesse post a questão da necessidade da Inteligência Artificial ser desenvolvida com o máximo de cuidado, de forma que seja empática e segura para a humanidade. Sondergaard prevê que a economia algorítmica impulsionará a próxima revolução econômica na era da máquina-máquina. As empresas serão valorizadas, não só por terem um Big Data, mas pelos algoritmos que transformam esses dados em ações que, em última análise, afetam os clientes. Não podemos subestimar a importância disso. Por exemplo, em relação aos carros autônomos. No futuro, se vários carros estiverem prestes a colidir, os algoritmos serão responsáveis pelas ações que cada carro deve tomar para proteger seus passageiros. Mas - isso pode ser assustador - e se os algoritmos concluírem que, para proteger três passageiros não há outra escolha senão colocar outro passageiro em risco? Apesar das previsões sinistras apresentadas em filmes de ficção científica sobre a inteligência artificial, Sondergaard é otimista em relação à ela. De qualquer forma, empreendedores e empresários, sabem agora que precisam evoluir para a utilização de algorítimos, de forma que possam criar soluções cada vez mais inteligentes e úteis para as pessoas. Deixe seus comentários abaixo Redação O Futuro das Coisas O Futuro das Coisas é dedicado a trazer conteúdo exclusivo em inovação, tecnologia, educação e medicina numa linguagem divertida e acessível.",pt,45
426,2008,1470666100,CONTENT SHARED,-9128741757954228992,-1032019229384696495,8967586490626066216,,,,HTML,https://techcrunch.com/2016/08/08/confirmed-walmart-buys-jet-com-for-3b-in-cash/,confirmed: walmart buys jet.com for $3b in cash,"Walmart today took its biggest step yet in a bid to compete more against Amazon in the world of digital commerce: today the retail giant announced that it would be acquiring Jet.com - an online-only shopping site that has been live for a little over a year - for $3 billion in cash, plus up to $300 million in shares for the founders. ""We're looking for ways to lower prices, broaden our assortment and offer the simplest, easiest shopping experience because that's what our customers want,"" said Doug McMillon, president and CEO, Wal-Mart Stores, Inc in a statement. ""We believe the acquisition of Jet accelerates our progress across these priorities. Walmart.com will grow faster, the seamless shopping experience we're pursuing will happen quicker, and we'll enable the Jet brand to be even more successful in a shorter period of time. Our customers will win. It's another jolt of entrepreneurial spirit being injected into Walmart."" The news caps off a week of reports that Walmart was in talks to buy the company, with several sources telling others (and us directly ) that the price tag would be precisely this amount. Jet.com's ambitions and positioning have in some ways far outstripped what the company has actually achieved in its relatively short life as a startup - with early investor interest, it seems, largely based on the strength and vision of the founding team. That team included and was led by Marc Lore, who had founded Quidsi, the umbrella company for Daipers.com, Soap.com and BeautyBar.com, and then sold the company to Amazon for $545 million in 2010. Founded in 2013, the company had raised hundreds of millions of dollars (upwards of $500 million , but possibly more unreported) at a steadily rising valuation before finally launching in July 2015 . Previous funding rounds included investments from another company that likely keeps Amazon and Walmart up at night: Alibaba . The $3 billion that Jet.com has now sold for, in fact, was one valuation that the company was reportedly floating for a funding round it was trying to raise for the end of last year. But as a startup, Jet.com has had some distinct ups and downs. When it first opened its virtual doors for business, Jet.com was modelled on the model made popular by services like Amazon Prime and Costco, where users had to be members ($50/year in Jet.com's case) in order to shop for discounted goods on the site. That fee was dropped some three months after the site went live in order to drive more users. To increase the number of products on its platform, early on Jet.com ran into trouble with several brands after linking to their own sites without permission . While the membership fee was essentially The company had projected that it would only reach profitability in 2020. More to come.",en,45
427,1653,1467718988,CONTENT SHARED,-969155230116728853,2833428826475063405,-2483383014515077366,,,,HTML,https://www.tofugu.com/japanese/learn-hiragana/,learn hiragana: the ultimate guide,"To learn hiragana is to create a foundation for the rest of your Japanese. By learning hiragana, you will learn the basics of Japanese pronunciation. It will also open doors in terms of the Japanese resources you can use. There are no (good) Japanese textbooks or learning resources that don't require you to know hiragana. In essence, it's the first step to learn Japanese. Many classes and individuals spend months learning hiragana. This is too long. You should be able to learn everything in a couple days. A week, tops. Some people have reported back that they could read all the hiragana after a few hours, using this method. How long it takes depends on you, but if you follow the steps laid out below, you'll come out the other side with the ability to read hiragana. To make this possible, you will employ a few important methods. Mnemonics: Due to hiragana's relative simplicity (at least compared to kanji), image-based mnemonics are a perfect method for memorization. Each hiragana character has a memorable illustration that goes along with it. For a long time I believed that mnemonics were a waste of time. If this is you, I recommend you give it a serious try. It's amazing what you are able to memorize when using a mnemonic method. No Writing: ""WHAT? NO WRITING!?"" you scream. I know what you're thinking. But, think about it for a moment. When's the last time you actually wrote something by hand? Probably the last time you had to sign your name on a receipt at a restaurant. The need to write by hand is going down. Most of your written communication comes in the form of typing. Learning to read can be done very quickly and is very useful. Learning to write doubles or triples how long it takes to learn hiragana, with very little real-life benefit. It will be important to learn eventually, but for now you have more important fish to fry. Exercises: As I mentioned earlier, there are exercises for you to go through. They also happen to be very well thought out, too. If you do them, and you don't cheat (yourself), you will learn hiragana. In these exercises, you should do your best to force yourself recall items, even when you don't think you can come up with the answer. The more effort and strain you put into recalling something, the stronger of a memory your brain will end up building (as long as you actually recall it, that is). For the most part, if you follow along and do everything that this hiragana guide says, you will learn the hiragana. It will be difficult not to. Hiragana Chart To begin, download this hiragana chart . If you have a printer, print it out. If not, you can follow along digitally too. Hiragana Pronunciation Before you begin learning to read the hiragana, you have to learn how to pronounce the hiragana. Since hiragana pronunciation is such a listening and speaking thing, we made a video to cover this topic. Follow along. When you can pronounce the five ""vowel sounds"" of hiragana, move on to the next section, where you'll learn to read them. This is the first (and most important!) column in hiragana. It sets the pronunciation of every other column coming after it, because every other column is basically just the a-i-u-e-o column with consonants attached to them. The same basic sound repeats over and over and over, with a consonant plus these five vowel sounds, so make sure you have the right pronunciation for these five right from the start. Shall we? No, that's okay, after you. あ is pronounced like the ""a"" in ""c a r"" or the ""a"" in "" a wful."" To remember this kana, find the capital "" A "" inside of it. This ""A"" will tell you that this kana is also ""a"" aka あ. There is another similar kana, お, but that one doesn't have an ""A"" in it, which is how you can differentiate them. い is pronounced like the ""ee"" in "" ee l."" To remember this kana, just think of a couple of ee ls (i) hanging out. They're upright because they're trying to mimic the letter ""i"" which also stands upright and also happens to be the way you spell out this character in romaji. う is pronounced like the ""oo"" in "" oooo ... ahhh!"" when you're watching fireworks. It also sounds like the ""ou"" in ""You."" To remember this kana, notice the "" U "" shape right in it! It's sideways but it's there, telling you what this kana is. え is pronounced like the ""e"" in "" e xotic"" or the ""e"" in "" e gg."" To remember this kana, think of it like an e xotic bird. The big feathery thing on its head gives it away that it's exotic and not normal. It also lays exotic e ggs, because it's an exotic bird, after all. お is pronounced like you're saying "" oh ."" It also sounds like the ""o"" in "" o riginal."" Can you see the letter "" o "" in here, two times? This one looks similar to あ, except for its one key difference: there are two letter ""o"" symbols visible in there. Make sure you use this to differentiate this kana (お) and that similar kana (あ). This is one area of hiragana where a lot of people trip up, but by using this mnemonic you will be able to figure them out every time. あいうえお Tasks Now that you've put these kana into your brain (at least somewhat shakily) it's time to pull them out. Recall is the foundation of memory, and you're going to start doing just that. For each ""tasks"" section make sure you follow along perfectly. Skipping these steps may cause you to fail later on in the future. Having a strong base to build off of is important with each section. Head over to the website Drag n' Drop Hiragana . All I want you to do is to find the five kana you just learned (a-i-u-e-o) and drag them to their correct spot. That's it! It's an exercise in recognizing the kana you learned as well as matching them to the correct sounds. When you've done it once hit the refresh button and do it again and again until you're able to get it done in 10 seconds. Print out, copy, or download this worksheet . You'll need to go through it, filling in the boxes with the romaji for the kana. Try your best not to cheat - even if you spend a while trying to remember a kana it will be beneficial to your memory (as long as you're able to recall it on your own). Looking up the answer doesn't help your memory at all, but struggle (with accomplishment) tells your brain that this is a thing worth remembering. Try using the mnemonics when you need to recall something you can't figure out right away. This should be fairly easy with only five kana (and maybe a little boring too), but when you're done move on to the next five hiragana. The next set of hiragana is from the ""k-column."" This is just the ""k"" sound plus the vowel sounds you learned above, making it ka-ki-ku-ke-ko. There are no weird exceptions in this column either, so enjoy it while you can. か is just the ""K"" sound plus あ, making a ""ka"" sound. ♫ Ca nnn, can you do the can can, can you do the can can... ♪ To remember this, think of someone who's doing the "" Ca n- Ca n"" (ka) dance. The か kana even looks like someone doing the Can-Can. き is just the ""K"" sound plus い, making a "" ki "" sound. In fact, it sounds just like the word "" key "" which is the mnemonic we end up using. To remember this, notice how much it resembles a key (ki). く is just the ""K"" sound plus う, making a "" ku "" sound. To remember this, think of this kana being the mouth of a coo-coo/cuckoo (ku) bird popping out saying ""ku ku, ku ku!"" け is just the ""K"" sound plus え, making a "" ke "" sound. You'll have to use your imagination here, but this kana looks a lot like a keg. The three dimensional shape that it makes is somewhat ke g-like, right? こ is just the ""K"" sound plus お, making a "" ko "" sound. Ko is a couple of co -habitation (ko) worms. They're so happy together, co-habitating the same area! Alternatively, you could imagine a couple of short co rds laying on the ground next to each other. かきくけこ Tasks More tasks! This time we'll include the あいうえお column along with this ""K-column"" you just learned. Using Drag n' Drop Hiragana , find the あいうえお and かきくけこ hiragana and drag them to their correct spots. How quickly can you identify and place these ten hiragana characters? When you can do it in under 25 seconds, or you've completed this task at least five times, move on to the next step. Print out, copy, or download this worksheet . Complete it by filling in the blanks with the romaji for each of the kana. This time it will be both of the columns that you've learned (so far) so it should be a little more interesting (and half familiar). Once again, when you get stuck just think back to the mnemonic before you cheat. When you're done you can move on to the next group. Now that you have the ""K-column"" under your belt it's time for the ""S-column."" There is one weird exception in this row, and that's for ""si"" aka ""shi."" It's pronounced just like the word ""she"" in English, and doesn't quite follow the pattern you've seen up until now. You'll want to use ""sa- shi -su-se-so"" for this column. さ is just the ""S"" sound plus あ, making a "" sa "" sound. This kana looks like a weird sign (where the ""si"" of "" si gn"" is pronounced like ""sa"") stuck in the ground. Focus on the pronunciation, not the spelling, from this mnemonic. し is just the ""Sh"" sound plus い, making a "" shi "" sound. Take note that this is the first ""exception"" kana where it doesn't follow the patterns that show up everywhere else. Instead of being ""si"" it's ""shi"" (though you will see it written both ways when dealing with romaji. One more reason why you ought to just learn hiragana already). This kana looks like a giant hook you're dipping into the water. What do you catch? A poor sea l (shi). す is just the ""S"" sound plus う, making a "" su "" sound. See the s wing (su) doing a loop-dee-loop throwing that poor kid off of it? Imagine him screaming ""I'M GONNA SUE SOMEBODY FOR THIIIIIiiiissss"" as he flies off into the distance. せ is just the ""S"" sound plus え, making a "" se "" sound. This kana looks like a mouth with a big fang in it. What would someone like this say (se)? How se xy is that tooth, btw? そ is just the ""S"" sound plus お, making a "" so "" sound. This kana is just a so ngbird (so), flapping its little wings while singing a little tune! "" So so so soooo !"" ♪ さしすせそ Exercises Now that we've done three sets of five, it's time for exercises! As usual, these exercises will help you to practice kana you've previously learned plus the ones you just learned. Back to our best buds Drag n' Drop Hiragana . Identify and place the あ, か, and さ columns into their spots. Do this several times and see if you can do it all in under 30 seconds (or just complete the task 5 times). Once you're able to do either of those, move on. Using this worksheet , print out, copy, or download it and fill out the boxes with the correct romaji. If you can't remember something try to think back to the mnemonic first before cheating. When you're able to do these two tasks move on to the next five kana. P.S. Have you noticed how in the worksheets you're being asked to wait 5 minutes then 10 minutes? Waiting is actually an important part of building memory. By waiting and then recalling something as it's fading away, you're telling your mind that it shouldn't forget that item. But, if you keep bringing it up over and over again in a short period of time your brain will just keep it in its short term memory, meaning you probably won't remember it later. Don't skip the waiting periods! In fact, if you think you can wait longer without forgetting much that's even better! Time for the fourth column, the ""T-column."" Now you have a lot to remember! Hopefully mnemonics and the reasons for using them are starting to make sense now. If not, that should happen soon. Like the さ column, you'll find an exception in the た column. In fact, you'll find two exceptions, them being ち (chi) and つ (tsu). So, for this column we'll have ""ta, chi , tsu , te, to."" た is just the ""T"" sound plus あ, making a "" ta "" sound. This looks just like the romaji that spells it out: "" ta "" ち is just the ""Ch"" sound plus い, making a "" chi "" sound. This is the second ""exception"" hiragana. Instead of a ""ti"" sound, it is a ""chi"" sound. Try not to forget this. This kana looks like a man's face... except it's missing something: the chi n! つ is just the ""Ts"" sound plus う, making a "" tsu "" sound. This is another ""exception"" hiragana. Instead of saying ""tu"" you say ""tsu."" Try not to forget this. Do you remember the kana し? It's a hook that's dipped straight down into the water. This didn't work very well (you caught a poor seal!), so now you're trying a new strategy: pulling the line behind you in a boat. This way the hook is facing sideways. It works, too! You pull up your line and you have two (tsu) fish! て is just the ""T"" sound plus え, making a "" te "" sound. This kana looks like the uppercase letter ""T"" where ""T"" is for "" Te n."" How many kana can you learn at one time? I bet at least ten of them (let's start with the next set!) と is just the ""T"" sound plus お, making a "" to "" sound. This kana looks just like someone's toe (to) with a little nail or splinter in it. Imagine how much this would hurt if it was your toe! たちつてと Exercises Now that we have a few kana under our belt we'll be adding a third resource to our arsenal. Still, we'll start with something familiar. Just follow along. When you've completed everything and feel like you can recall all 20 of these kana, move on to the next section. Now it's time to try 10 at a time. You're getting better at this, after all! This is your first ""more than 5 things to learn"" group. In fact, it's a whole ten things! But you'll be just fine. You're getting better at learning the hiragana with all this practice. Too bad there's not 150 hiragana for you to practice on. な is just the ""N"" sound plus あ, making a "" na "" sound. The na ughty (na) nun is praying in front of the cross, asking for forgiveness of her naughty ways. The cross up in the air like this should be the main giveaway that this is な. に is just the ""N"" sound plus い, making a "" ni "" sound. Do you see the nee dle (ni) pulling the thread? ぬ is just the ""N"" sound plus う, making a "" nu "" sound. This kana looks like some noo dles (nu). There are several other kana that are similar to this one (れ, め, ね, わ) but you know this one is noodles because there are no sharp sides in it. It's 100% smooth and bendable, like noodles! It even has an extra loop at the back, because it is a noodle. ね is just the ""N"" sound plus え, making a "" ne "" sound. This is Ne lly the cat. There are other kana very similar to this one (ぬ, れ, め, わ) but you know this is different because it has a loop at the end for the tail and it's not super bendable like ぬ (noodles) is (see those sharp corners on the left?). To top thins off, Nelly is a ne cromancer. Why? I have no idea, you'd have to ask her. It must have something to do with the undead cat army she's creating. Also, if you know the word ""neko"" (Japanese for ""cat"") you can use that too. This is a ねこ. の is just the ""N"" sound plus お, making a "" no "" sound. See the big pig no se (no) there? You can also think of this as a ""No smoking"" sign (the ones with the cigarette and the big red circle and slash through it). Pick the one that sticks with you the best. Now let's look at the next five in this set. If you're feeling really shaky you can jump over to RealKana or Drag n' Drop Hiragana to practice, but you don't have to (yet)! は is just the ""H"" sound plus あ, making a "" ha "" sound. This kana looks like the uppercase letter ""H"" plus the lowercase letter ""a."" What does that spell? "" Ha !"" Why are you laughing? Stop that. Make sure you can see the H+a in the kana. ひ is just the ""H"" sound plus い, making a "" hi "" sound. He (hi) has a big nose. See that big nose? Now say it out loud. ""He has a big nose."" ふ is just the ""F/H"" sound plus う, making a "" fu/hu "" sound. Usually this kana is pronounced with an ""f"" (fu) in hiragana, so we're going to go with that. However, this kana does look a lot like a hula dancer too, so keep the ""hu"" in mind as well. If you want, you can think of this hula dancer as a "" fu -reaky hu la dancer"" to remember the fu. へ is just the ""H"" sound plus え, making a "" he "" sound. Do you know the famous mountain Mt. Saint He lens? This kana isn't totally flat like Helens is, but it's pretty squat looking. That's why this one is Helens. ほ is just the ""H"" sound plus お, making a "" ho "" sound. The left side line is a chimney. The right side is a mutated Santa Claus. He has four arms, a snake tail, and no head. Out of his neck he's uttering "" ho ho ho... ho ho ho..."" Hopefully he doesn't come down your chimney. なにぬねのはひふへほ Exercises Time to practice ten at a time! It's a lot, but you're getting better at learning these things, right? When you are done with these exercises it's time to move on to the next set of hiragana. Not quite ten in this set (before the exercises), but close enough. Let's start with the ""M-column."" ま is just the ""M"" sound plus あ, making a "" ma "" sound. Removing your head? Doubling your hands and arms? What sort of evil ma gic is this? What makes it weirder is that your mama is the one doing this magic. Imagine your ma looking like this. Aghh! み is just the ""M"" sound plus い, making a "" mi "" sound. Looks like lucky number 21. Who just hit the blackjack? Me (mi)! Who just turned 21 as well? Me (mi)!! む is just the ""M"" sound plus う, making a "" mu "" sound. "" Moooooo "" (mu), says the cow. ""MOOOOOOO."" め is just the ""M"" sound plus え, making a "" me "" sound. Look at that beautiful eye! It's so beautiful because of the ma keup (me) on it. Gotta look pretty in those eyes! If you also happen to know the word for ""eye"" in Japanese, that will help too. The word for ""eye"" in Japanese is just め (me). も is just the ""M"" sound plus お, making a "" mo "" sound. You want to catch mo re (も) fish so you add more worms to your hook. This is the third ""hook"" one, so make sure you can differentiate the mnemonics in your head: し, つ, and now も. This column is a little strange. There are only three items in here, and ""ye"" and ""yi"" are seemingly missing. Actually, they used to exist but now they don't (instead people use い or え, because it sounds pretty similar). Because of that, you only have to learn three kana for this section! や is just the ""Y"" sound plus あ, making a "" ya "" sound. Do you see the ya k in this kana? ゆ is just the ""Y"" sound plus う, making a "" yu "" sound. This kana is a very u nique (yu) looking fish! It looks like a big eyeball swimming in the water. よ is just the ""Y"" sound plus お, making a "" yo "" sound. The hitchhiker has his arm and thumb out. He's yelling "" YO ! yo!"" at all the cars that go past him. Why won't they pick him up? まみむめもやゆよ Exercises Time to practice these eight hiragana (and the previous ones as well). Once again, go through the steps to make sure you know everything well! When you're all done, it's time to tackle the last ""main hiragana"" section. You're almost there! Not so hard, right? Welcome to the last main set! It's only eight characters just like the last set, so hopefully it's not too bad. It does include the infamous ra-ri-ru-re-ro column though, which does tend to give some people trouble pronunciation-wise. Please be sure to check out our "" how to pronounce the Japanese R "" (coming soon!) guide for more information on this. ら is just the ""R"" sound plus あ, making a "" ra "" sound. The ra pper is rapping at the DJ table. り is just the ""R"" sound plus い, making a "" ri "" sound. The ree ds (ri) are swaying in the wind. This kana can also be written without the connection in the middle, too, which makes it more reedlike in that case (I wanted to present the more difficult of the two versions here, though). る is just the ""R"" sound plus う, making a "" ru "" sound. The is like ろ (you'll learn it in a second) except it has a loop at the end. る is a crazier route (ru). There is a loop (ru) at the end. Are there no rules on this road? れ is just the ""R"" sound plus え, making a "" re "" sound. This looks like a guy kneeling on the ground, re tching up his dinner. This kana is similar to め, わ, ぬ, and ね. What makes this one different is the curve at the back. You can identify this as the guy's knees bending, which makes it so you know he's keeled over retching his guts out. ろ is just the ""R"" sound plus お, making a "" ro "" sound. This is the counterpart to る, except this one doesn't have a loop at the end (there are rules here!). So, this kana is just a plain old road (ro). And finally, the last group. This is a weird one. It includes わ (which is quite normal), を (which is pronounced just like お, but is primarily used as a particle), and ん (which is the only consonant-only character in all the kanaa). Let's go through them one by one. わ is just the ""W"" sound plus あ, making a "" wa "" sound. This kana looks like a wa sp flying straight up. It looks similar to れ, ぬ, ね, and め. It looks especially similar to ね. You know ね is Nelly the cat because of the curl of a tail on the end. So, you can imagine the cat chasing this wasp, which is why it's flying straight up to get away. Its but is also a straight sharp line. This is its stinger. を is just the ""W"" sound plus お, though it sounds more like "" oh "" than it does ""wo."" The ""w"" is pretty silent, though it's still a tiny bit there. You can pretty much just pronounce it like お. "" Whoa !"" (wo) yells the guy with no chin (ち). Someone threw a boomerang into his mouth, so of course he's going to yell something. ""WHOA!"" ん is just the "" N "" sound, that's it. It's the only kana that consists of a single consonant. This kana looks just like the lowercase "" n "" in English. They happen to be the same sounds, as well. How convenient! nnnんんん. らりるれろわをん Exercises This is the last of the main hiragana. The exercises will now cover quite a bit (you know quite a bit!), so make sure you understand and know everything before moving on. That will finish out all the main hiragana. From here on out it's just combinations of kana or variations on kana you already know, which makes things both easier and harder. Let's start with the ""variation hiragana,"" also known as... Dakuten Dakuten takes hiragana you already know and adds an additional symbol to it to change their pronunciation. Usually this symbol is something that looks like a quotation mark, though in one instance you'll see this mark as a small circle. Here they are: Luckily for you, there are only five rows of dakuten kana to learn, and all you have to learn is what the sound changes to (since you know the kana already). Let's go over each of those dakuten transformations. か　→　が Every kana in the か column can have dakuten. When this happens, the ""K"" sound becomes a ""G"" sound. か　→　が (ga) き　→　ぎ (gi) く　→　ぐ (gu) け　→　げ (ge) こ　→　ご (go) Because you know the か column already, all you really need to remember is that K → G. Think of it this way: The car (か) runs into the guard (が) rail. Before you move on, make sure you know that ka → ga, ki → gi, etc. さ　→　ざ When something from the さ column gets dakuten, it changes to a ""Z"" sound, with the exception of し (which is already an exception, so this makes sense!). さ　→　ざ (za) し　→　じ (ji) す　→　ず (zu) せ　→　ぜ (ze) そ　→　ぞ (zo) All you have to remember is that S → Z, except in the case of し, which goes to じ. Exceptions will breed exceptions, so make sure you keep this in mind. To remember the S → Z part, though, consider the following mnemonic: My saw (さ) just zapped (ざ) me when I tried to use it. (imagine yourself trying to use a saw/さ and getting zapped/ざ). Do you remember what the K-column converts to? Do you remember what the S-column converts to? What is the exception in the S-column? When you're able to answer all that, move on to the next dakuten set. た　→　だ The T-column kana change to ""D"" sounds, except for the exceptions (which are ち and づ). Remember: Exception breeds exception! た　→　だ (da) ち　→　ぢ (dzi) つ　→　づ (dzu) て　→　で (de) と　→　ど (do) The two exceptions (ぢ and づ) very rarely show up, which is lucky for you. They mostly sound like じ and ず, but not quite. You'll get by pronouncing it like that if you must, but the correct pronunciation is more like what's written above... a combination of the D + Z sounds. Everything else is pretty normal. To remember that the た column changes to become the だ column, think of it this way: Changing these kana to the dakuten versions is a bit like magic... ""TADA!"" (ta & da) Do you remember what the K-column changes to? Do you remember what the S-column changes to? What about the T-column? Do you remember the three exceptions we've run into so far? If you can answer all of those questions it's time to move on to the last dakuten set, which is really two sets in one. は　→　ば, ぱ The H-column is a bit strange. It has two different kinds of dakuten that can be applied to it. One is that ""quotes"" symbol you've seen so far, the other is a little circle. は　→　ば (ba), ぱ (pa) ひ　→　び (bi), ぴ (pi) ふ　→　ぶ (bu), ぷ (pu) へ　→　べ (be), ぺ (pe) ほ　→　 ぼ (bo), ぽ (po) You have to remember that the H-column goes to both a ""B"" and a ""P"" sound. What a pain. Think of it this way: You're saying ""hahaha"" at the bar, because you've been drinking too much. You say ""hahaha"" so much at the bar that somebody punches you. Imagine through that story with you being the one saying ""hahaha"" (i.e. you're laughing) a couple of times, trying to get the details as vivid as possible (especially the details that have to do with laughing, the bar, and getting punched). To help you a little more, you can remember that the P-column is the one that uses the little circle. Why? Because that little circle is like a little fist that's about to punch you. Before moving on, try to recall the mnemonics we used for the following (and remember what each converts to): When you're able to do and recall everything, it's time to practice and see how good you really are! Dakuten Practice This practice will mainly focus on dakuten but also include all the kana you've learned up until this point. Using RealKana , select only the dakuten kana and drill those for 5-10 minutes until you feel somewhat comfortable. Now, add in all the other kana, mixed in with the dakuten kana. Using this worksheet , fill in all the blanks. When you're all done with that you should know all the kana fairly well, some better than others. I imagine there will be a few nagging ""difficult"" kana for you (it will depend on each individual which kana these are), but over time as you use hiragana and read more everything will get easier and easier. The whole point of this guide is to help you to get you reading, making it so you can use various other resources to continue your Japanese study. Combination Hiragana There's only one more section to complete. You're not really learning much that's new here, but you are going to learn how to combine different types of kana together to make some new sounds. Mainly, we're going to focus on what small ゃ, ゅ, and ょ can do to kana from the い row (that includes き, し, じ, に, etc). First let's take a quick look at the size difference. It's hard to see when they're not next to each other! To use these, you'll need to combine them with something from the い column. When you do this, you're essentially combining the first (English) letter of the い-kana with the small ゃ, ゅ, ょ sound. For example: き + ゃ → KIYA → KYA じ + ょ → JIYO → JYO See how the ""i"" gets dropped and it just becomes one syllable of sound? Here's a list of them all: きゃ、きゅ、きょ　→　KYA, KYU, KYO ぎゃ、ぎゅ、ぎょ　→　GYA, GYU, GYO しゃ、しゅ、しょ　→　SHA, SHU, SHO じゃ、じゅ、じょ　→　JYA, JYU, JYO (or JA, JU, JO) ちゃ、ちゅ、ちょ　→　CHA, CHU, CHO ぢゃ、ぢゅ、ぢょ　→　DZYA, DZYU, DZYO (you'll never see these, pretty much ever) にゃ、にゅ、にょ　→　NYA, NYU, NYO ひゃ、ひゅ、ひょ　→　HYA, HYU, HYO びゃ、びゅ、びょ　→　BYA, BYU, BYO ぴゃ、ぴゅ、ぴょ　→　PYA, PYU, PYO みゃ、みゅ、みょ　→　MYA, MYU, MYO りゃ、りゅ、りょ　→　RYA, RYU, RYO As you may have noticed, there's no いゃ sound and there's no combination kana for the Y-column. The first kana has to be a sound with a strong consonant in it, and both ""i"" and ""yi"" don't fit the bill. Also, ""yi"" doesn't exist in modern Japanese. Combination Hiragana Practice With this knowledge it's time to practice. I've made a worksheet that covers these combination kana. Go through it and fill in all the blanks. When you're done, you should be able to read almost everything that hiragana throws at you. Everything except one little thing... Small Tsu (っ) The small tsu is a weird little thing but we'll make sense of it. The easiest way to think of it, I think, is to call it a ""double consonant."" Basically, by adding a small っ to something, you are making the (English) letter that follows it double into two consonants. Luckily, you won't see a small tsu before any of the あいうえお kana, so that never becomes an issue. Let's take a look at how the following hiragana converts to romaji. したい → shitai しったい → shit_tai かこ → kako かっこ → kak_ko いた　→　ita いった　→　it_ta See how that worked? したい is just plain ""shitai"" without the small っ. But when you add it in, it becomes shittai . The small っ that comes before the ""ta"" causes the consonant to double, making it ""shit_tai."" Make sure you understand how that works with kako/kakko too. In terms of pronunciation, this is different as well. It's almost as if you add a small stop where the small っ exists, with one of the double consonants on either side. いしょ → isho いっしょ → ish_sho You will hear both of the consonants as separate sounds. One that ends the first part of the word, and one that starts the second half (with the small っ) showing you where that half point is. For a while it will probably be difficult to distinguish a small っ and a large つ, especially in handwriting. After you get more experience and read a lot more you'll be able to make this distinction quite easily. Additional Practice Although you could probably go out into the real world and practice hiragana on your own, I thought I'd provide for you some ways to practice your newfound skills. I wouldn't recommend doing everything here all at once but instead spread it out over the course of a couple weeks. Spacing your practice is very important if you want to get better at something more quickly. Doing all this at one time won't be all that effective. Luckily you can always start working on other parts of Japanese in the meantime while you continue to practice hiragana. Worksheets We made a couple more worksheets for you to download/copy/print out. They're a little different from before though. This time they're real sentences and we're not keeping track of what kana we're using, so it's a bit more like real life. You'll still want to put the romaji above the kana and read each kana out loud. Don't worry too much about meaning, that's not what we're learning right now and it will definitely be way above your ability level. When you finish those, I bet you'll be feeling pretty special, like some kind of hiragana master. If you don't, there's always more ways to practice. Apps & Other Programs There are plenty of apps and resources out there to help you drill as well. Some of them you've seen already because of this guide, others you have not. I'm sure there are plenty of other resources out there as well, but this should be good enough to get you to that level where you can start using the hiragana with other resources. ""Real Life"" Practice Of course, if you'd like to practice more there are plenty of ""real life"" ways to practice hiragana. Just go to any Japanese website and read all the hiragana that you can find. If anything it will teach you to differentiate between kanji, katakana, and hiragana, which is a nice skill to have. Try the Yomiuri Online , or any of these beginner Japanese reading resources . Moving On Practice After learning hiragana to a moderately slow level, you don't have to keep drilling it until it's fast. In fact, you can just move on to something else. Hiragana will keep popping up just about everywhere, so by learning something new you're actually reviewing the hiragana at the same time! What Next? Hiragana is only the start of things to come. You have so much more to do and hiragana will help you to get there. Although the answer to the question ""what's next"" is going to be somewhat vague / dependent on the individual, here are some suggestions to move you along your way. Kanji I do highly recommend that you get started on kanji right away. A lot of people think they should wait until they have a higher level of Japanese but that is usually a terrible idea. Being good at kanji speeds up just about every other facet of learning the Japanese language, from grammar to reading to speaking to listening. If you're weak at kanji you'll be weaker at everything else. Many people think kanji is difficult, but we made WaniKani to show that it's not as hard as people think. Katakana At the same time as kanji it's worth learning katakana. Katakana won't come up nearly as much especially at a really early stage of Japanese, but it's not rare enough to ignore. If you liked this guide to hiragana, check out our guide for learning katakana . It's just as stellar. Grammar Along with kanji or after you have a foundation in kanji, it's time to learn some Japanese grammar. There are many resources to help you to do this. We made TextFugu (an online Japanese textbook) for this, but there are other sites like Tae Kim's Guide To Japanese as well as textbooks ( we like the Genki series ). - I hope this guide helped you to learn hiragana effectively and quickly! Keep working hard and you'll continue to get better and better. With hiragana you have the tools to start your Japanese studies no matter what resource or textbook you end up choosing, so try a lot of things and see what works for you. Fell free to check out some of our reviews on Japanese resources ! P.S. We're working on adding videos to this guide, so check back occasionally if you're having trouble with pronunciation (because videos will help a lot with that!).",en,45
428,2803,1480598673,CONTENT SHARED,-759389978085260161,-8241940599580729220,-4284824539388316942,"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36",MG,BR,HTML,https://www.grammarly.com/blog/commonly-confused-words/,top 30 commonly confused words in english,"Everyone knows the problem with spell-check: your word might be spelled right, but it may be the wrong word. English is full of confusing words that sound alike but are spelled differently. It's also full of words that share similar (but not identical) meanings that are easy to misuse. Below are some of the most commonly confused and misused words in English. Here's a tip: Looking for a specific pair of commonly confused words on this page? Use ctrl+F or command+F to search the page quickly. Affect/Effect Affect is usually a verb: Chester's humming affected Posey's ability to concentrate. Effect is usually a noun: Chester was sorry for the effect his humming had. If you find yourself stumped about which one to use in a sentence, try substituting the word ""alter"" or ""result."" If ""alter"" fits (Chester's humming altered Posey's ability to concentrate), use affect . If ""result"" fits (Chester was sorry for the result his humming had), use effect . Among/Amongst Among is the preferred and most common variant of this word in American English. Amongst is more common in British English. Neither version is wrong, but amongst may seem fussy to American readers. Among/Between Among expresses a collective or loose relationship of several items: Chester found a letter hidden among the papers on the desk. Between expresses the relationship of one thing to another thing or to many other things: Posey spent all day carrying messages between Chester and the other students. The idea that between can be used only when talking about two things is a myth-it's perfectly correct to use between if you are talking about multiple binary relationships. Assure/Ensure/Insure Assure means to tell someone that something will definitely happen or is definitely true: Posey assured Chester that no one would cheat at Bingo. Ensure means to guarantee or make sure of something: Posey took steps to ensure that no one cheated at Bingo. Insure means to take out an insurance policy: Posey was glad the Bingo hall was insured against damage caused by rowdy Bingo players. Empathy/Sympathy Empathy is the ability to understand another person's perspective or feelings. Sympathy is a feeling of sorrow for someone else's suffering. A sympathizer is someone who agrees with a particular ideal or cause. It's/Its It's is a contraction of ""it is"": Posey needs to pack for her trip because it's only two days away. Its is a possessive pronoun that means ""belonging to it"": Chester is obsessed with both the book and its author. To lay means to put or to place. One way to remember this is that there is an a in both to lay and to place : Posey will lay out her outfit before she goes to bed. To lie means to recline. One way to remember this is that there is an e in both to lie and to recline : Chester will lie down for a nap. Be careful, though. The past tense of to lay is laid : Posey laid out her outfit. The past tense of to lie is lay : Chester lay down for a nap over an hour ago. Lead/Led Lead , when it rhymes with ""bed,"" refers to a type of metal: Posey wore a lead apron while the dentist X-rayed her teeth. Led is the past tense of the verb to lead , which means to guide or to be first: Chester led the way. Inquiry/Enquiry Inquiry and enquiry both mean ""a request for information."" Inquiry is the standard American English spelling. Enquiry is the British spelling. Their/There/They're Their is the possessive form of ""they"": Chester and Posey took their time. There indicates a place: It took them an hour to get there. They're is a contraction of ""they are"": Are Chester and Posey coming? They're almost here. To is a preposition that can indicate direction: Posey walked to school. She said hello to Chester when she saw him. To is also used in the infinitive form of verbs: Chester waited until the last minute to do his homework. Too is used as an intensifier, and also means ""also"": Posey waited too long to do her homework, too. Who's/Whose Who's is a contraction of ""who is"": Who's calling Chester at this hour? Whose is a possessive pronoun that means ""belonging to [someone]"": Chester, whose phone hadn't stopped ringing all morning, barely ate anything for breakfast.",en,45
429,1750,1468408679,CONTENT SHARED,4061646345741687197,6735372008307093370,-2464388279861641102,,,,HTML,http://www.cartacapital.com.br/sociedade/o-homofobico-dentro-de-cada-um,o homofóbico dentro de cada um,"Grupo de jovens presta homenagem em memorial às vítimas do Pulse em Orlando O atirador que matou 49 pessoas e deixou mais de 50 feridas em uma boate gay em Orlando, nos Estados Unidos, tinha um homofóbico dentro de si. Assim como você, que lê esse texto, também tem sua dose de homofobia. E até eu mesmo, que sou gay e já enfrentei a minha. É compreensível, afinal nossa sociedade foi construída sob uma perspectiva patriarcal, heteronormativa. Incompreensível é não querer desconstruir esses valores e aceitar o mundo como ele é, diverso. O massacre em Orlando tem, sim, a ver com a cultura de armas nos Estados Unidos. Tem, sim, a ver com fundamentalismo religioso . Mas, sobretudo e fundamentalmente, tem a ver com a homofobia, com a transfobia e qualquer aversão à diversidade sexual . E a repercussão da tragédia só reforça essa ideia. Ninguém nasce homofóbico. A homofobia se aprende em casa, na escola, nas ruas, nas igrejas, nas mesquitas e nas sinagogas. É aquela obsessão dos pais de que rosa é de meninas e azul é de meninos. É a bronca dos professores se a criança brinca de carrinho ou de boneca. É o sermão do padre ou do pastor falando que viver a homossexualidade é viver no pecado. É o bullying que os LGBTs sofrem ou já sofreram algum dia, com piadas infelizes, agressão verbal e física. O primeiro desafio de qualquer um antes de sair do armário é enfrentar a própria condição homossexual, é perceber que não existe nada de errado consigo. Daí não é de se estranhar o alto nível de depressão e suicídio entre adolescentes LGBTs. Aí começa o processo de desconstrução da homofobia plantada em cada um de nós. Mas grande desafio é enfrentar a homofobia de cada dia, presente nas sutilezas de gestos e palavras, nas lampadadas na cabeça ou nos tiros que matam mais de 300 LGBTs por ano no Brasil , vitimizando sobretudo as transexuais. É urgente a desconstrução do ódio contra a diversidade. É preciso debater, sim, as questões de gênero nas escolas, em casa, nas igrejas. Não se trata de ensinar ninguém a ser LGBT, mas de respeitar a existência de cada um na sociedade. É preciso enfrentar o potencial homofóbico que todo mundo carrega dentro de si. A notícia de que o atirador de Orlando frequenta boates e aplicativos gays já foi o pretexto para que alguns apontassem a sexualidade do assassino como razão principal do ataque. Se é possível que a homofobia internalizada do rapaz o motivou a cometer o crime, é certo que a homofobia arraigada na sociedade chancelou sua atitude. Quantas vezes ele escutou seu líder religioso dizer que gays eram pecaminosos? Quantas vezes sofreu bullying no colégio? Quantas vezes apanhou dos pais? Quantas piadas homofóbicas escutou no trabalho? Por isso, é importante que a sociedade fale sobre homofobia . E a dificuldade de enfrentar o tema ficou visível na repercussão do atentado. Muitos jornalistas, ativistas e lideranças políticas falaram sobre a farta disponibilidade de armas nos Estados Unidos, ressaltaram a religião do atirador, mas resistiam em dizer de forma contundente que se tratava de um ataque sobretudo homofóbico. Na TV inglesa, o ativista Owen Jones deixou o estúdio da SkyNews depois que dois apresentadores insistiam em negar o caráter primordialmente homofóbico do atentado. O mesmo se pode dizer da nota do Itamaraty e do presidente interino Michel Temer, que sequer citaram que o ataque foi a um clube gay. O massacre tampouco gerou a comoção vista nos recentes atentados em Paris e, em muitos casos, rendeu na internet comentários incitando a morte de LGBTs. Em todos os casos, homofobia explica. À medida que os LGBTs vem conquistando direitos e vem aos poucos deixando a condição de cidadãos de segunda classe percebe-se uma reação virulenta de grupos conservadores e fundamentalistas religiosos. De repente, viramos o bode expiatório da sociedade, culpados de todas as mazelas. Somos destruidores de famílias, espalhamos doenças, somos pedófilos, desestruturamos os costumes e agora espalhamos a tal ""ideologia de gênero"", o último factoide da temporada. É fácil apontar responsáveis pelo discurso de ódio, como o deputado fascista Jair Bolsonaro. Difícil, mas necessário, é desconstruir a homofobia arraigada dentro de si. Aquela que não se traduz em comentários e ações discriminatórias no dia a dia, mas aquela que é suficiente para espalhar, ainda que sem perceber ou em doses homeopáticas, a cultura homofóbica na sociedade.",pt,44
430,2338,1473795553,CONTENT SHARED,-6707474286910397949,3891637997717104548,-1482578711359637617,,,,HTML,https://medium.com/third-grove/the-death-of-drupal-commerce-as-an-ecommerce-solution-daa6c30dff94,the death of drupal commerce as an ecommerce solution - third & grove,"In a report on the sweeping changes in ecommerce platforms from 2012-2015 there's a telling omission in a graph near the top: Drupal Commerce doesn't appear. Not once. Today, 75% of all online stores in the Alexa top one million run on just three platforms: Magento (the Community and Enterprise editions), WooCommerce (commerce on WordPress), and Shopify. These three platforms represent a compelling cross-section of use cases. Magento is open source, has a mature for-profit company supporting the platform, and can handle any enterprise client, no matter how custom the requirements, traffic, or sales volume. WooCommerce offers an easy-to-use solution familiar to the millions already comfortable with WordPress. Shopify provides a platform-as-a-service solution that allows people to launch a store in a few minutes without the need for software engineers. (Shopify is making an exciting play at the enterprise market - more below.) There have been tectonic shifts in the ecommerce platform space since 2012, with new entrants devastating once-dominant platforms and pushing them into permanent decline. But in this report's overview of that history, Drupal Commerce doesn't even appear as a contender. Drupal Commerce faces four major challenges that together are insurmountable for a platform in such a weak position to overcome : Vanishing market opportunities as SaaS tools devour the low-end market; An inability to serve the enterprise market due to a lack of corporate backer for the platform; Inability to decouple from a content management system that lacks backwards compatibility; and, Maturity of coupled content and commerce platforms. Challenge 1: On-demand managed solutions like Shopify and BigCommerce cannot be stopped from consuming the low-end market Shopify has enjoyed explosive growth in the last several years. It is a cost-effective, compelling commerce platform for clients with modest annual revenue or for teams that don't need custom integrations with other systems. Shopify saves our clients with simpler needs time and money and speeds up their time-to-market by orders of magnitude over traditional commerce systems. Interestingly, Shopify recently launched a new enterprise platform called Shopify Plus targeted at high-traffic enterprise stores. They clearly aspire to break out of the low-end market. Shopify isn't going away. It's in heavy growth and market adoption phase. SquareSpace, a leading website SaaS company recently added commerce support. Just a few short weeks after the company backing Drupal Commerce shut down (see next challenge), BigCommerce raised $30 million in venture capital . This is a startlingly clear message from the market. The SaaS space is eating into the low-end custom space, and that spells trouble for platforms like Drupal Commerce that haven't broken out of being niche players. Challenge 2: Lack of robust corporate support You know that old adage from the early days of technology about no one getting fired for buying IBM? When C-level leadership of large companies invest in platforms they are making a big bet on the long-term success of the platform. These folks don't normally bet big on open-source solutions unless the technology is backed by a major company that understands the unique needs and challenges that face senior leadership in large organizations. Profitable companies with revenue fade slower than orphaned open-source projects. Sadly, earlier this year the for-profit company behind Drupal Commerce dissolved , devolving into a hosting company ( platform.sh ) and a boutique Drupal agency that focuses on Commerce. Without a mature, stable company that understands the needs of enterprise clients, using Drupal Commerce will always be higher risk than other solutions. Challenge 3: Monolithic systems are dead The last major pressure facing Drupal Commerce is arguably the most deadly because it strikes at the heart of the platform's most compelling feature: the ability to provide rock-solid content management (Drupal) and commerce within the same system. As demonstrated by our projects with awesome brands like Quicken, Benefit Cosmetics , and Dwell, the era of monolithic single systems is over. Tightly-coupled integrations between best-of-market tools are proving to be far more robust digital platforms than single systems. Magento, Hybris, and ATG aren't great at content, but Drupal sure is. Combined with the Acquia platform (to Drupal what RedHat is to Linux) Drupal operates at enterprise scale for any digital experience need. When you combine best-of-breed content (Drupal) with best-of-breed-platform (Acquia) with best-of-breed commerce (like Magento 2, Hybris, or Shopify Plus) using well-designed connectors you get a winning solution every time. Challenge 4: Cinderblock shoes, or not forking Drupal Unlike Drupal's competitors, major releases of Drupal are not backwards-compatible. That means moving a site from Drupal 7 to Drupal 8 probably requires the same amount of effort as moving a WordPress site to Drupal. Drupal Commerce decided to stick with Drupal and is moving to Drupal 8. Why was sticking with Drupal a challenge? Because instead of focusing on building a compelling commerce platform, the core Commerce team has and will spend countless hours upgrading Commerce to Drupal 8. Imagine what the teams at Shopify or WooCommerce or Magento can build in the time that the core Commerce team spends moving Drupal Commerce to Drupal 8 . While many of our clients find us because of our reputation for Drupal excellence at any scale, we frequently spent time telling our clients it's not about the technology; it's about how your digital strategy furthers the goals of your business. Arguably, the Drupal Commerce team is too focused on the technology and not enough on the strategy side of their business. But is Drupal Commerce really dead? An easy counterargument that Drupal Commerce is alive and thriving would be the usage statistics chart on Drupal.org for Drupal Commerce . At first blush, the chart tells the story of a growing platform: going up and to the right: In 2013, it took Drupal Commerce just one year to double the number of sites using the platform. That's amazing growth. But by 2014, growth slowed to 33% and in 2015 growth was just 18%. In 2016 growth was essentially flat. Slowing growth is a not necessarily a concern when a platform reaches market maturity, but less than 70,000 installations is hardly market maturity when there are millions of online stores. Lessons from the market Two critical market moves provide additional, troubling news for Drupal Commerce: First, look at Acquia's commerce strategy over the last four years. It appears Acquia and Drupal Commerce attempted to collaborate early on, but little actually occurred. In the years since, Acquia has distanced itself from Drupal Commerce, announcing an exciting partnership with enterprise commerce leader Hybris last June. Second, look to platform.sh , the hosting platform created by the Drupal Commerce team and recently spun out as its own company. In April, Magento announced at their annual Imagine conference a forthcoming enterprise platform for hosting Magento called Magento Enterprise Cloud (a similar product to what Acquia offers for Drupal). What powers Magento's cloud platform? None other than platform.sh. The hosting platform originally created by Drupal Commerce has pivoted to power an entirely different commerce platform. A tremendous amount of awesome engineering work by really smart people has gone into Drupal Commerce over the years. But the combined market forces standing in Drupal Commerce's way are simply too strong for the platform to overcome. The writing is on the wall.",en,44
431,2210,1472392267,CONTENT SHARED,-1291869519847635026,3302556033962996625,1025575970804202705,,,,HTML,https://medium.freecodecamp.com/learning-how-to-learn-the-most-important-developer-skill-7bf62dfaf67d?gi=8b568dd5f8c0,learning how to learn: the most important developer skill - free code camp,"Learning How to Learn: The Most Important Developer Skill Being an efficient learner is at least as important as being an efficient coder. When you're a developer, your job requires you to learn every single day - in spite of the constant lure of distractions like Hacker News, Twitter, Reddit, and Facebook. You constantly encounter new code bases and new technical challenges at work. Home is no better, as you tackle open source repos and personal projects, each with their own processes and challenges to tackle. The tech world changes fast, and it can feel like a full-time job just keeping up with the latest tools, languages and frameworks. Long story short: learning is hard. Yet, we need to be able to learn quickly and effectively to thrive. In the past year, I went from not knowing how to use the Chrome debugger to working as a software engineer for a leading cryptocurrency company. In the process, I rapidly learned a new skill (coding). That said, learning didn't come easy for me. Honestly, every new concept was a struggle. There were too many unknowns, and too much uncertainty. ""How in the world is this sustainable?"" I thought to myself. ""If this is what learning to code is supposed to feel like every day, I'll be miserable. Is this really my passion?"" ""Wouldn't this be easy for me if this was my passion? Do artists struggle to produce art? Do writers struggle to write a great book? Do athletes struggle to do well in a race? Are we supposed to struggle when we're pursuing our passions?"" ""Shouldn't I be finding pleasure in this?"" Does it ever get easier? Yes, it does. A year later, tackling new programming concepts is still ""difficult"" in the sense that it requires discipline and hard work. But it's also become an enjoyable process, rather than an overwhelming one. What happened in the last year to make that shift possible? Simple: I changed my perspective on learning. What once struck me as ""difficult"" became ""engaging."" In the rest of the post, I'll explain how this transformation happened. Just getting started Learning to code is hardest at the beginning. For example, think about the first programming language you have to learn. You want to tackle the small things like syntax and style. But first, you have to comprehend difficult core concepts like values, types, operators, control flow, functions, higher order functions, scopes, closures, recursion, and so much more. It feels like learning to juggle - but starting with eighteen pins instead of two. When I first learned about closures, it took me many weeks to truly understand the concept. I thought I understood it when I read about it. But when I tried to identify and use closures in practice, I'd find myself stumped. That wasn't unusual. I've observed this process as a teacher as well: new concepts don't usually click the first time around. Or the second. Or even the tenth. But for those who stick it out long enough, there will be a ""breaking point"" where things suddenly begin to make sense. In my example, I read literally every blog post, Stack Overflow post, and spec on the internet about closures. Everything I read and experimented with gave me a new perspective, until eventually, I had a 360-degree mental picture of how closures worked. Closures ""clicked."" Getting to a point where I felt this sense of understanding of closures was super important, because it was rewarding and encouraged me to go for more - including writing my own blog post that explained the concept. Learning is a process, not a goal If we see learning as something we ""have"" to do, then we rush to get it done so that we can spend the rest of our time doing something more ""fun"" - something we ""want"" to do. The problem is that it's impossible to know everything about anything, so viewing learning as a race leads to burnout and disappointment. Instead, if you see learning as a process, you'll appreciate the small victories and insights along the way. This will drive you to constantly move forward. You can compare it to exercise. Workouts hurt, and then the pain ends as soon as your workout ends. But it's never gone. It's waiting for you the next time you workout. Except each time, the pain becomes less piercing. You learn to cope with it. You become familiar with the pain, and it just becomes part of the routine. You are rewarded by better health and a better physique and are incentivized to keep going. Exercise creates a positive feedback loop: The same is true for learning. Turning learning into an engaging process Imagine building your very first web application. At first, all you've got is a daunting, blank text editor. The task of building the app seems almost insurmountable. You know nothing, and have so much to learn before you can make this happen. Thankfully, you decide to go for it anyway. From then on, your main focus becomes to do one small step at a time. First, you create an idea. What will you build? Who's the end user? What are the constraints? Second, you prototype or sketch out some rough designs for what you think it might look like. You ask your friends or the internet for feedback, and iterate to make it better. Third, you research languages, tools, and frameworks that will work best with your requirements. Step by step you discipline your mind to channel all its energy towards this one goal. Sometimes you're writing code. More often than not you're stalled at some bug or error. Sometimes you're too tired to do any work, so you take a break. Other times, you don't feel like writing code. That's okay. You spend your time researching or reading up on topics related to your project. Eventually, after a few weeks of hard work, you've built a foundation that can handle your big ideas. Suddenly, working on your app doesn't feel as painful. You see the reward of the initial set of hard work, and now it's just another piece of code you need to write or another bit of refactoring you need to do - which you've done 100s of times already, no problem. You turned what was once a daunting or dreadful activity into one that is complex and engaging. This is how we grow. This is how we get better. Whether it's programming, dancing, running, or reading: it's not easy, and there won't ever be a time or place when you're ""done"" learning. Instead, enjoy the process of investing your energy into something, and enjoy the pain that comes along with it. You'll start to notice that you no longer describe it as ""pain"" - because what was once painful becomes a symbol for what's next: a sense of personal accomplishment and self-satisfaction. In other words, struggle and enjoyment will start to mean one and the same thing. Remember the cycle: One approach to learning technical topics Let me tell you a little about the learning process I follow. This isn't the be-all-end-all of learning styles, so if something different works for you, please share it in the comments! In case you can't tell, I'm a nerd about this stuff :) Let's use the process of learning the React.js library as an example. What is the motivation for learning this? First step: I'd start with a Google search for the React.js documentation and read a bit about the background and motivation for the library. Knowing the ""why"" behind any topic is incredibly helpful for framing the learning process. It answers questions like: How is this different from other solutions? How useful is this to me? What problems does this solution aim to solve? Is this just a new shiny tool that'll only be useful for a few months or will it fundamentally change the way I think and code? Reading and understanding core concepts Second, I'd read through any intro articles or examples provided in the docs. Notice I'm not touching any code yet. Reading and sinking in the core concepts comes before hands-on experimentation. It's incredibly important to do this because it lays the foundation for the rest of my learning. Even though I might be able to get away with blindly using React.js without learning the core concepts, eventually it'll catch up to me when I run into a bug. First time coding After spending some time on the above steps, I start to get the gist of what's going on, or maybe even feel like I totally get it. Then it's time to jump into some code. I typically try to build something really small with any new tool by following a video tutorial (e.g. on egghead.io) or a written tutorial before jumping into custom projects. When you get stuck ...And then, inevitably, I get stuck. Reading the docs seemed like a piece of cake, but actually using it in practice makes me realize I have no idea what's going on. This is when I start to feel that dreaded ""just give up"" feeling. But instead of giving in when the going gets tough, I remind myself that pain == gain. Turning back would be cowardly. Here's what I do instead: I first narrow down and figure out what I'm actually stuck on - i.e. define the problem. Then I come up with a hypothesis for what I think could be the root cause or causes of the problem. Even if I have no idea, I just make a guess. Then I step away from the problem and my computer and do something that relaxes me. This is incredibly hard to do when I'm so upset about the problem I'm stuck on, but letting go of the problem works wonders. (Ever notice how great ideas always strike in the shower?) Now I try to debug with my hypothesis in mind. I get as far as I can on my hypothesis without looking for answers online - there's something beautiful that happens when you try to solve problems by truly thinking deeply about them on your own first. Even if you're going down the wrong path, the fact that you made the effort teaches you a lot and you remember the problem space much better next time you run into it. If my hypothesis leads to an answer, hooray! I'm done. If not, I Google search for documentation, blog posts, or Stack Overflow posts that could help me get closer to the answer. While reading, I take notes on any and all pieces of information that could potentially be helpful. Still no solution? That's fine. I'm sure I learned something valuable by reading through all that, even if it didn't directly help me solve the problem at hand. Who knows when this knowledge might come in handy next time? At this point, if I'm truly stuck, I will either post a question on Stack Overflow or ask a co-worker or developer I know. Otherwise, I rinse and repeat until I get closer to the final solution. At some point, the answer always comes. At times this process takes a few seconds, and other times it takes hours (or days). Either way, the process itself is incredibly beneficial to your skill set as a developer. Getting stuck on a bug feels like stumbling in a dark tunnel looking for a ray of light. You eventually find it, but along the way you discover so much about the tunnel - and it's knowledge about the ""tunnel"" that makes you strong as a coder. Think of debugging as a chance to explore rather than a detour from your goal, and it becomes much more fun. Rinse and repeat By this point in the learning process, I've built something small and tackled some small hurdles along the way. As you can see, it was a struggle - clearly, I need some more practice with the new tool. So, once again I try to build something on my own. Rather than jumping straight to a big custom project, I'll look for a repo to base my application on. For example, if there's an online CRUD todos example (of course) using React.js, maybe I'll build a different type of CRUD application. Just different enough to keep me engaged, but not so different as to make me discouraged if something goes wrong. Mastery Mastery requires repetition, so I keep building more small projects until I feel like I've got the core concepts down. Eventually, I begin to be able to piece things together on my own without constantly referencing documentation or examples. Only then do I finally adventure out and build something from scratch on my own. Throughout this process, I aim to make the process fun and engaging. I'm constantly pushing myself work on things that are harder than what I am capable of in the moment, but not throwing myself into the deep end so that I get discouraged and never finish. Finally, I make sure to step away as soon as I find myself getting too frustrated to enjoy the project. Learning is fun With some effort and structure, learning programming turns out to be incredibly fun. At first it's incredibly complicated, and in my opinion that's why so many people get scared away - not because it's ""boring,"" but because it's ""hard."" After you go through this learning process a few times, processing new information becomes a muscle memory. You don't really think about it. You just learn to ride the pain wave and find joy in the reward. Like magic, it becomes ""easier"" to learn.",en,44
432,1699,1467982561,CONTENT SHARED,358192030955601318,-1130272294246983140,5253165022949525032,,,,HTML,http://www.b9.com.br/65902/arquitetura/nova-sede-da-lego-na-dinamarca-e-toda-sobre-colaboracao/,a nova sede da lego na dinamarca é toda sobre colaboração,"Quem pensa que trabalhar na LEGO é uma eterna brincadeira, está enganado. Claro que também pode significar muita diversão, mas trata-se de administrar um negócio de 5.2 bilhões de dólares. É por isso que na expansão de sua sede, que está sendo construída em Billund, na Dinamarca, a empresa quer valorizar muito mais a colaboração entre as pessoas do que forçar a barra num ambiente descolado e colorido. Do lado de fora, a construção - batizada de ""People House"" - parece uma junção de blocos gigantes, com muitas referências (ainda que não óbvias) ao design que transformou a LEGO na segunda maior fabricante de brinquedos do mundo (só ficando atrás da Mattel ). Do lado de dentro, porém, os espaços são abertos e encorajam o contato direto entre as pessoas, longe das mesas de trabalho. Os quatro andares do edifício contam com janelas enormes, com bastante entrada de luz natural, e os pisos circulares permitem uma visão geral de todo o lugar. Isso parte da crença da empresa de que atualmente menos trabalho acontece atrás de um computador e muito mais das conexões casuais entre as pessoas. É por isso que o espaço oferece mesas, cadeiras, sofás e estacões de trabalho temporárias por todo lado, incentivando assim os encontros para reuniões inesperadas. No terraço tem mini golfe, e praticamente todo o lado externo é um parque público. O novo quartel general da LEGO vai abrigar 4 mil funcionários - a população da cidade de Billund é de apenas 6 mil habitantes, pra se ter uma ideia - e a expectativa é de que fique pronto em 2020.",pt,44
433,1141,1464619991,CONTENT SHARED,2134259819148812594,-1616903969205976623,-1897176301316773531,,,,HTML,http://www.forbes.com/sites/edmundingham/2015/04/09/forget-the-internet-of-things-there-is-a-digital-revolution-taking-place-in-our-shopping-malls/,"forget the internet of things, there is a digital revolution taking place in our shopping malls","2015 has been heralded as the year the Internet of Things finally takes hold, but while we wait for driverless cars and save up for our Smartwatches and assorted Wearables, the omnichannel retail revolution is gathering pace. This is no small market. In the US alone research from JC Decaux reveals [...]",en,44
434,931,1463090345,CONTENT SHARED,8938071859536254465,3217014177234377440,6985013944863195494,,,,HTML,http://blog.caelum.com.br/design-sprint-onde-o-design-e-a-velocidade-importam/,design sprint: onde o design e a velocidade importam,"Da minha experiência ao trabalhar com startups nos últimos anos, como UX/UI designer, posso dizer que muitas vezes você é obrigado a fornecer soluções rápidas para problemas que as vezes não são tão claros assim. Outras vezes, você encontra uma solução para o problema, mas não tem como validá-lo antes de ter o produto propriamente dito. Geralmente, isso acontece por que falta tempo ou orçamento, ou até por que não há ainda uma cultura de UX tão estabelecida. Foi buscando melhorar essa equação, tempo versus processo de UX, que acabei descobrindo o Design Sprint. Com a atualização do curso de UX da Caelum , esse conteúdo também passou a fazer parte da ementa e aqui nesse post você vai aprender um pouquinho sobre o que é e quais etapas compõem o design sprint. Esse método foi desenvolvido e anunciado pela Google Ventures , um braço do Google criado em 2009, que atua de forma independente e procura acelerar empresas que possam ter futuro, em diversas áreas como em Internet, software, hardware, biotecnologia e cuidados de saúde. Para você ter ideia, a Google Ventures já investiu em cerca de 300 empresas de diversos tipos de atuações, entre elas, uma das startups que mais tem causado barulho no mercado mundial nos últimos tempos, a Uber. O que é o Design Sprint, afinal? O Design Sprint é uma metodologia centrado no usuário, iterativa, prática e colaborativa. Se baseia em design thinking e metodologias ágeis para que as equipes possam criar e prototipar soluções de forma bem rápida, previsto para 5 dias. Ao invés de gastar horas e mais horas de desenvolvimento para lançar um produto e só depois conseguir entender se a ideia é bom ou não. O Design Sprint oferece um atalho de aprendizado, onde é possível elaborar e testar praticamente qualquer ideia em apenas 40 horas, sem precisar construir e lançar o produto propriamente dito. Se você parar para pensar, inclusive, esse método, vai de encontro com uma das premissas do Lean Startup : testar hipóteses rapidamente e acelerar o aprendizado. Mesmo sendo um processo de curta duração, de 2-5 dias, o Design Sprint é formado por etapas que ora amplia a sua visão do problema, ora foca em algo específico. Em uma visão geral de cada etapa, o que acontece é o seguinte: ENTENDER/DEFINIR Na primeira etapa o foco é buscar um melhor entendimento do problema proposto pela equipe como um todo. Nessa etapa é super importante que as pessoas compartilhem seus conhecimento sobre aquele problema, isso porque a experiência e conhecimento do time é sempre assimétrica, por exemplo: vendas sabe coisas que o desenvolvimento não sabe, suporte ao cliente conhece as coisas que o design não sabe e assim por diante. DIVERGIR Busca-se explorar tantas ideias quanto for possível, esse é a fase onde tudo é possível. A princípio parece um processo de brainstorming, mas nessa etapa cada indivíduo do time vai começar trabalhando sozinho para desenhar soluções detalhadas no papel e não em grupo como é um típico brainstorming. Isso se faz dessa forma, pois como Jake Knapp (criador e evangelista do Design Sprint) percebeu, as melhores ideias tendem a vir de indivíduos e não de grupos. O problema é que grupos tem dinâmicas onde não necessariamente as melhores ideias sobrevivem, mas sim, as ideias que são mais bem vendidas e contadas com mais empolgação por alguém do grupo. Você, com certeza, já participou de trabalho em grupo e de alguma forma percebe que o indivíduo mais articulado sai em vantagem de um indivíduo mais introspectivo em situações de dinâmicas de grupo. O que o design sprint tenta fazer é eliminar essa vantagem para que todas as ideias tenham o mesmo peso. DECIDIR Na terceira etapa, você terá mais de uma dúzia de soluções para você e seu time escolher. É o momento das decisões difíceis e onde as dinâmicas de grupo também atrapalham um pouco na tomada de decisão. O que se passa em um grupo é que normalmente se toma decisão por consenso e, na maioria das vezes, consenso escolhe o caminho mais fácil de aceitar e gerar menos atrito entre o grupo, isso faz com que as ideias mais corajosas ou mais integras acabem ficando de lado. Novamente, o design sprint tenta eliminar isso, por exemplo, usando uma técnica chamada dot voting (você pode ouvir falar dela também como zen-voting). PROTOTIPAR Nessa fase o objetivo é prototipar a solução pensada pelo time. Aqui é importante ter ao fim do dia de trabalho um protótipo de média/alta fidelidade, pois como ele será testado com usuários reais, um protótipo de baixa fidelidade não lhe daria um feedback claro e efetivo. Como você e seu time só tem um dia para prototipar é super importante que o time seja muito produtivo e de preferência trabalhe com ferramentas de prototipagem com as quais o time já esteja habituado. VALIDAR Depois de todo o trabalho nas etapas anteriores, a questão mais importante que ainda nos resta é ""Como podemos saber se nossa ideia é boa?"". É nessa fase que a gente recebe a resposta para essa pergunta já que você vai mostrar os protótipos para os potenciais usuários do produto. Nesse momento o usuário vai interagir com o seu produto e lhe dar feedback real sobre que experiência ele está tendo. Óbvio que quando uma ideia arriscada tem êxito nessa etapa é uma recompensa fantástica para equipe de trabalho pois gera motivação instantânea. Mas é, na verdade, os fracassos nesse processo que, mesmo doloroso, fornecem o maior investimento, pois, afinal de contas, seu time soube que teria que buscar outro caminho ou desistir da ideia em apenas 5 dias de trabalho. Nesse post você pode ter uma visão geral do método e notar que o design sprint é todo estruturado para que você gere boas ideias, selecione-as e valide-as. No próximo post, vou retomar esse método mostrando em detalhes como planejar e organizar um design sprint e como montar o time para fazer um bom sprint. Ate lá &#x1f609;",pt,44
435,1821,1468937811,CONTENT SHARED,-7891293130731329672,4227773676394505435,-3639577459523734004,,,,HTML,http://hipsters.tech/sobre/,sobre o hipsters ponto tech,"No começo de 2016 a Alura montou, em parceria com o JovemNerd, o podcast NerdTech . São episódios sobre tecnologia apresentados de forma bem humorada e sem muita pretensão. O podcast teve bastante repercussão e, atendendo a pedidos, montamos o nosso próprio... só que semanal! Toda terça feira traremos um episó dio novo para falar de tecnologia, startups, programação, design e... outras modinhas! Paulo Silveira , o da fotona aí, é o hipster por trás do podcast e nosso host. O conteúdo é bolado com a ajuda de muita gente da Caelum e da Alura. Gabriel Ferreira faz o social e mantém o nosso recém lançado grupo no Facebook . Maurício Linhares é responsável por diversos convidados e figura presente em diversos episódios. Raphael Lacerda é quem deu o pontapé inicial lááá atrás. Yuri Padilha faz o árduo trabalho no WordPress e Fabio Gushiken é o gênio do logo e das imagens. Quem faz a edição do conteúdo é a turma do Radiofobia . Quer logos para linkar pra gente? Tem aqui ó .",pt,44
436,2613,1476984097,CONTENT SHARED,8289311442635691404,3396828831632510915,1233659230241778611,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36",GA,US,HTML,http://news.microsoft.com/features/have-a-coke-take-a-photo-and-celebrate-the-coke-bottles-100th-anniversary-using-how-old-net/,"have a coke, take a photo and celebrate the coke bottle's 100th anniversary using how-old.net","A few months ago, the Microsoft team behind How-Old.net unleashed an unexpected viral sensation, with more than 575 million images submitted to test the power of real-time analytics and machine learning in guessing the ages of people in those pictures. More than 85 million users visited the site from all over the world. Coca-Cola patented its distinctive bottle design in 1915 , and it's as likely to be the go-to for families out and about now as it was for parasol-carrying ladies sipping them out of straws a century ago. Coke bottles reached soldiers serving in World War II, appeared in Time in 1950 (the first commercial product to appear on the magazine's cover) and have endured as a staple through generations. To celebrate its 100th anniversary, Coca-Cola and Microsoft have partnered to create an original experience inspired by How-Old.net. Coke fans are invited to upload a photo that includes a glass contour Coke bottle, or simply the bottle itself, to How-Old.net to unlock a surprise. ""In this tool we saw an innovative way to capitalize on one of the most iconic brand moments there is as we celebrated this birthday, that moment you share with an ice cold bottle of Coke,"" says Simon Cowart, global social media strategist with The Coca-Cola Company. ""We're onto something that is relatively unique,"" says Joseph Sirosh, corporate vice president of the Microsoft Data Group, including the team of engineers who built the Machine Learning and Big Data capabilities that are the heart of Cortana Analytics Suite. Developers on his team were the ones who came up with How-Old.net. Sirosh's team also worked on the challenge issued by Coca-Cola to take the How-Old.net app one step further by recognizing specific objects, such as a contoured glass Coca-Cola bottle, in pictures users upload. For the Coca-Cola project, the Microsoft data scientists and engineers started with How-Old.net, and enhanced the experience with the perceptual intelligence capabilities of the Cortana Analytics Suite. ""We had to build a new machine learning model that could recognize Coca Cola bottles,"" says Sirosh. ""There were two parts to getting this done right: to recognize the Coca-Cola logo and the unique glass contour bottles the logo appears on."" For the first step, they used Python scripts and image processing libraries, to detect the Coca Cola logo. They then used deep neural networks from Azure Machine Learning to create a dedicated model that can identify whether there's a bottle in the area surrounding the logo. The scientists trained it with thousands of images collected online and from Coca-Cola, continuously iterating to improve the accuracy of the models through a tool called the Visual Model Evaluator. After all, faces can take up a big part of a photo, while bottles are relatively small. And the logo within it, smaller still. ""The most exciting technology part of this exercise and what we hope to be using down the road is that we created the ability for people in marketing and social communications to augment social campaigns by being able to train a neural network-based machine learning model to detect objects, something that is very, very complex,"" Sirosh says. ""We made this process incredibly simple, and then integrated it into the How-Old experience. We made a super complex technology available, accessible and usable - and that was a very exciting part of the journey."" Sirosh thinks this detection technology will be appealing to developers, and is committed to making it easier for anyone to get started with data science. ""Computer vision technology - face recognition - has been around for a while, but only for a select few. Now with the Microsoft Face API, even a developer who knows very little about machine learning or computer vision will be able to set up a website within a week or two that uses sophisticated face recognition technology, and in minutes, create a super fun experience,"" Sirosh says. ""It's based on a ton of machine learning, but no one had to fidget around with the machine learning parts."" How-old.net face recognition is just one example of what's possible with Microsoft Machine Learning API's. This machine learning technology is the engine that enables the Web-based gender and age guessing experience that caught on so quickly with How-Old.net. ""Any developer can easily discover and try the capabilities of perceptual intelligence through the Cortana Analytics Gallery ,"" says Barb Edson, general manager of the Microsoft data marketing group. ""It's a very easy way for a developer to add smart insights into their apps without the need to be an expert in data science."" And because it runs on Azure, apps using this technology can scale big, fast. Developers can collate data using Microsoft face APIs, also offered as part of the Cortana Analytics Suite, which incorporates state of the art algorithms from Microsoft Research for facial detection and recognition, gender and age prediction, but also other capabilities such as speech detection and text analysis. ""We're trying to make this truly a developer experience,"" Sirosh says. ""We get learnings from detecting all these things and put it into a service that anybody can use to build their own object detection concept."" Object recognition could find applications in all kinds of ways, such as automatically detecting colors for washing machine cycles, or programming a robot that can detect food in the kitchen, or building a toy that responds only to your child. Gyms could automate workouts for their members (sorry, personal trainers!) and companies could link photos of their products directly to catalog availability. The fitness conscious could even take pictures of their meals, which would reveal nutritional values. ""We didn't know How-Old would lead us down this path,"" Sirosh says. ""But now people can see how machine learning can be cool, reaching 85 million people through a single app. People saw us do a pretty cool thing with our technology, and maybe we can help them do something cool, too."" Read more about how Joseph's team created the #CokeBottleBirthday experience . Lead image: Joseph Sirosh, corporate vice president, Microsoft Data Group",en,44
437,3037,1485857879,CONTENT SHARED,5429045128339158843,7645894863578715801,-8671975836733260591,Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:51.0) Gecko/20100101 Firefox/51.0,SP,BR,HTML,https://spring.io/blog/2017/01/30/spring-boot-1-5-1-released,spring boot 1.5.1 released,"One behalf of the Spring Boot team, and everyone that has contributed, I am pleased to announce that Spring Boot 1.5.1 has been released and is available now from repo.spring.io , Maven Central and Bintray . This release adds a significant number of new features and improvements. For full upgrade instructions and ""new and noteworthy"" features please see the release notes . Apache Kafka Support Spring Boot 1.5 includes auto-configuration support for Apache Kafka via the spring-kafka project. To use Kafka simple include the spring-kafka dependency and configure the appropriate spring.kafka.* application properties. Cloud Foundry actuator extensions Spring Boot's actuator module now includes additional support that is activated when you deploy to a compatible Cloud Foundry instance. The /cloudfoundryapplication path provides an alternative secured route to all NamedMvcEndpoint beans. Cloud Foundry management UIs can make use of the endpoint to display additional actuator information. For example, Pivotal Cloud Foundry shows health information next to the application status. Please see this blog post about Pivotal Cloud Foundry 1.9 for further details. Spring Data Ingalls Spring Boot 1.5 ships with the recently announced Spring Data Ingalls. Please refer to the announcement blog post to learn about all its new features. LDAP support Spring Boot now offers auto-configuration for any compliant LDAP server as well as support for the embedded in-memory LDAP server from UnboundID . Please see the documentation for more details. Loggers endpoint A new actuator loggers endpoint allows you to view and change application logging levels on the fly. Both JMX and MVC endpoints are available. For example, to change the logging level with the MVC endpoint, you can issue a POST to /loggers/com.yourcorp.application with the following JSON: To update the logger using the JMX endpoint you would use the setLogLevel operation. Please see the documentation for further details. Other changes There's a whole host of other changes and improvements that are documented in the Release Notes . You can also find a list of deprecated classes and methods that we plan to remove in the next version. We want to take this opportunity to again thank all our users and contributors. We've now had over 320 people submit code, and there have been over 10000 commits to the project. If you're interested in helping out, check out the ""ideal for contribution"" tag in the issue repository. If you have general questions, please ask at stackoverflow.com using the spring-boot tag or chat with the community on Gitter . What happened to 1.5.0? We noticed a problem with 1.5.0.RELEASE after it had been synced to Maven Central. Rather than requiring an exclusion for the incorrectly scoped dependency, we opted to fix it and release 1.5.1.RELEASE instead.",en,44
438,3114,1487783149,CONTENT SHARED,-4132331404553626868,9109075639526981934,9068765379852494896,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36",MG,BR,HTML,https://www.gartner.com/doc/reprints?id=1-3TYE0CD&ct=170221&st=sb,gartner reprint,"Gartner redesigned the Magic Quadrant for BI and analytics platforms in 2016, to reflect this more than decade-long shift. A year later, in 2017, there is significant evidence to suggest that the BI and analytics platform market's multiyear transition to modern agile business-led analytics is now mainstream.",en,44
439,2012,1470678308,CONTENT SHARED,-7722236679806608274,-1578287561410088674,7456304865870051305,,,,HTML,http://blog.chromium.org/2016/08/chrome-53-beta-shadow-dom.html,"chrome 53 beta: shadow dom, paymentrequest, and android autoplay","Unless otherwise noted, changes described below apply to the newest Chrome channel release for Android, Chrome OS, Linux, Mac, and Windows. Shadow DOM allows an element to encapsulate its style and child DOM away from the main document. Developers can now build components and safely include them in webpages regardless of other technologies being used. Chrome 53 supports Shadow DOM V1 , the latest version of the API. V1 has some significant changes from the V0 version, and is broadly agreed-upon by major browser vendors. Chrome will support both versions of the API until enough developers have moved to V1. The behavior of a shadow root is determined by which API call it was created with. Completing payments on the web can be a cumbersome process for users, leading to lower conversions on sites. While has made it easier to enter information, efficient data entry on mobile is still a challenge. PaymentRequest allows for fast, seamless, and secure payments on the web using a credit card or Android Pay . It also lets users provide a billing address, shipping details, and payer information without typing. PaymentRequest is available on Chrome for Android, with support for more platforms coming soon. Chrome for Android autoplays muted video Video is a great way for sites to reach their users, but it can be jarring when it plays unexpectedly. This is especially true on mobile, where the user may be in an environment where audio is unwanted. Chrome on Android now allows muted videos to begin playing without user interaction. If the video was marked as muted and has the attribute, Chrome will start playing the video when it becomes visible to the user. Developers can also use script to play muted videos without user interaction. Muted videos that begin playing sound before a user action will automatically be paused. Other features in this release Deprecations and interoperability improvements Events generated by script will no longer trigger default actions, improving spec compliance and browser interop. HTTP/0.9 has been deprecated in favor of , which adds response header support. TLS Diffie-Hellman ciphers have been removed , following their deprecation in M51 due to security concerns. Connections to servers unable to negotiate a non-DHE cipher will result in a ERR_SSL_OBSOLETE_CIPHER error. Posted by Hayato Ito, Shadow DOM Chaffeur",en,44
440,1648,1467674534,CONTENT SHARED,-3662111110237135231,-1443636648652872475,6885512560807851601,,,,HTML,http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/,"deep learning for chatbots, part 2 - implementing a retrieval-based model in tensorflow","Retrieval-Based bots In this post we'll implement a retrieval-based bot. Retrieval-based models have a repository of pre-defined responses they can use, which is unlike generative models that can generate responses they've never seen before. A bit more formally, the input to a retrieval-based model is a context (the conversation up to this point) and a potential response . The model outputs is a score for the response. To find a good response you would calculate the score for multiple responses and choose the one with the highest score. But why would you want to build a retrieval-based model if you can build a generative model? Generative models seem more flexible because they don't need this repository of predefined responses, right? The problem is that generative models don't work well in practice. At least not yet. Because they have so much freedom in how they can respond, generative models tend to make grammatical mistakes and produce irrelevant, generic or inconsistent responses. They also need huge amounts of training data and are hard to optimize. The vast majority of production systems today are retrieval-based, or a combination of retrieval-based and generative. Google's Smart Reply is a good example. Generative models are an active area of research, but we're not quite there yet. If you want to build a conversational agent today your best bet is most likely a retrieval-based model. The Ubuntu Dialog Corpus In this post we'll work with the Ubuntu Dialog Corpus ( paper , github ). The Ubuntu Dialog Corpus (UDC) is one of the largest public dialog datasets available. It's based on chat logs from the Ubuntu channels on a public IRC network. The paper goes into detail on how exactly the corpus was created, so I won't repeat that here. However, it's important to understand what kind of data we're working with, so let's do some exploration first. The training data consists of 1,000,000 examples, 50% positive (label 1) and 50% negative (label 0). Each example consists of a context , the conversation up to this point, and an utterance , a response to the context. A positive label means that an utterance was an actual response to a context, and a negative label means that the utterance wasn't - it was picked randomly from somewhere in the corpus. Here is some sample data. Note that the dataset generation script has already done a bunch of preprocessing for us - it has tokenized , stemmed , and lemmatized the output using the NLTK tool . The script also replaced entities like names, locations, organizations, URLs, and system paths with special tokens. This preprocessing isn't strictly necessary, but it's likely to improve performance by a few percent. The average context is 86 words long and the average utterance is 17 words long. Check out the Jupyter notebook to see the data analysis . The data set comes with test and validations sets. The format of these is different from that of the training data. Each record in the test/validation set consists of a context, a ground truth utterance (the real response) and 9 incorrect utterances called distractors . The goal of the model is to assign the highest score to the true utterance, and lower scores to wrong utterances. The are various ways to evaluate how well our model does. A commonly used metric is recall@k . Recall@k means that we let the model pick the k best responses out of the 10 possible responses (1 true and 9 distractors). If the correct one is among the picked ones we mark that test example as correct. So, a larger k means that the task becomes easier. If we set k=10 we get a recall of 100% because we only have 10 responses to pick from. If we set k=1 the model has only one chance to pick the right response. At this point you may be wondering how the 9 distractors were chosen. In this data set the 9 distractors were picked at random. However, in the real world you may have millions of possible responses and you don't know which one is correct. You can't possibly evaluate a million potential responses to pick the one with the highest score - that'd be too expensive. Google's Smart Reply uses clustering techniques to come up with a set of possible responses to choose from first. Or, if you only have a few hundred potential responses in total you could just evaluate all of them. Baselines Before starting with fancy Neural Network models let's build some simple baseline models to help us understand what kind of performance we can expect. We'll use the following function to evaluate our recall@k metric: Here, y is a list of our predictions sorted by score in descending order, and y_test is the actual label. For example, a y of [0,3,1,2,5,6,4,7,8,9] Would mean that the utterance number 0 got the highest score, and utterance 9 got the lowest score. Remember that we have 10 utterances for each test example, and the first one (index 0) is always the correct one because the utterance column comes before the distractor columns in our data. Intuitively, a completely random predictor should get a score of 10% for recall@1 , a score of 20% for recall@2 , and so on. Let's see if that's the case. Great, seems to work. Of course we don't just want a random predictor. Another baseline that was discussed in the original paper is a tf-idf predictor. tf-idf stands for ""term frequency - inverse document"" frequency and it measures how important a word in a document is relative to the whole corpus. Without going into too much detail (you can find many tutorials about tf-idf on the web), documents that have similar content will have similar tf-idf vectors. Intuitively, if a context and a response have similar words they are more likely to be a correct pair. At least more likely than random. Many libraries out there (such as scikit-learn ) come with built-in tf-idf functions, so it's very easy to use. Let's build a tf-idf predictor and see how well it performs. We can see that the tf-idf model performs significantly better than the random model. It's far from perfect though. The assumptions we made aren't that great. First of all, a response doesn't necessarily need to be similar to the context to be correct. Secondly, tf-idf ignores word order, which can be an important signal. With a Neural Network model we can do a bit better. Dual Encoder LSTM The Deep Learning model we will build in this post is called a Dual Encoder LSTM network. This type of network is just one of many we could apply to this problem and it's not necessarily the best one. You can come up with all kinds of Deep Learning architectures that haven't been tried yet - it's an active research area. For example, the seq2seq model often used in Machine Translation would probably do well on this task. The reason we are going for the Dual Encoder is because it has been reported to give decent performance on this data set. This means we know what to expect and can be sure that our implementation is correct. Applying other models to this problem would be an interesting project. The Dual Encoder LSTM we'll build looks like this ( paper ): It roughly works as follows: Both the context and the response text are split by words, and each word is embedded into a vector. The word embeddings are initialized with Stanford's GloVe vectors and are fine-tuned during training (Side note: This is optional and not shown in the picture. I found that initializing the word embeddings with GloVe did not make a big difference in terms of model performance). Both the embedded context and response are fed into the same Recurrent Neural Network word-by-word. The RNN generates a vector representation that, loosely speaking, captures the ""meaning"" of the context and response ( c and r in the picture). We can choose how large these vectors should be, but let's say we pick 256 dimensions. We multiply c with a matrix M to ""predict"" a response r' . If c is a 256-dimensional vector, then M is a 256×256 dimensional matrix, and the result is another 256-dimensional vector, which we can interpret as a generated response. The matrix M is learned during training. We measure the similarity of the predicted response r' and the actual response r by taking the dot product of these two vectors. A large dot product means the vectors are similar and that the response should receive a high score. We then apply a sigmoid function to convert that score into a probability. Note that steps 3 and 4 are combined in the figure. To train the network, we also need a loss (cost) function. We'll use the binary cross-entropy loss common for classification problems. Let's call our true label for a context-response pair y . This can be either 1 (actual response) or 0 (incorrect response). Let's call our predicted probability from 4. above y' . Then, the cross entropy loss is calculated as L= −y * ln(y') − (1 − y) * ln(1−y) . The intuition behind this formula is simple. If y=1 we are left with L = -ln(y') , which penalizes a prediction far away from 1, and if y=0 we are left with L= −ln(1−y) , which penalizes a prediction far away from 0. For our implementation we'll use a combination of numpy , pandas , Tensorflow and TF Learn (a combination of high-level convenience functions for Tensorflow). Data Preprocessing The dataset originally comes in CSV format. We could work directly with CSVs, but it's better to convert our data into Tensorflow's proprietary Example format. (Quick side note: There's also tf.SequenceExample but it doesn't seem to be supported by tf.learn yet). The main benefit of this format is that it allows us to load tensors directly from the input files and let Tensorflow handle all the shuffling, batching and queuing of inputs. As part of the preprocessing we also create a vocabulary. This means we map each word to an integer number, e.g. ""cat"" may become 2631 . The TFRecord files we will genrrate store these integer numbers instead of the word strings. We will also save the vocabulary so that we can map back from integers to words later on. Each Example contains the following fields: context : A sequence of word ids representing the context text, e.g. [231, 2190, 737, 0, 912] context_len : The length of the context, e.g. 5 for the above example utterance A sequence of word ids representing the utterance (response) utterance_len : The length of the utterance label : Only in the training data. 0 or 1 . distractor_[N] : Only in the test/validation data. N ranges from 0 to 8. A sequence of word ids representing the distractor utterance. distractor_[N]_len : Only in the test/validation data. N ranges from 0 to 8. The length of the utterance. The preprocessing is done by the prepare_data.py Python script, which generates 3 files: train.tfrecords , validation.tfrecords and test.tfrecords . You can run the script yourself or download the data files here . Creating an input function In order to use Tensorflow's built-in support for training and evaluation we need to create an input function - a function that returns batches of our input data. In fact, because our training and test data have different formats, we need different input functions for them. The input function should return a batch of features and labels (if available). Something along the lines of: Because we need different input functions during training and evaluation and because we hate code duplication we create a wrapper called create_input_fn that creates an input function for the appropriate mode. It also takes a few other parameters. Here's the definition we're using: The complete code can be found in udc_inputs.py . On a high level, the function does the following: Create a feature definition that describes the fields in our Example file Read records from the input_files with tf.TFRecordReader Parse the records according to the feature definition Extract the training labels Batch multiple examples and training labels Return the batched examples and training labels Defining Evaluation Metrics We already mentioned that we want to use the recall@k metric to evaluate our model. Luckily, Tensorflow already comes with many standard evaluation metrics that we can use, including recall@k . To use these metrics we need to create a dictionary that maps from a metric name to a function that takes the predictions and label as arguments: Above, we use functools.partial to convert a function that takes 3 arguments to one that only takes 2 arguments. Don't let the name streaming_sparse_recall_at_k confuse you. Streaming just means that the metric is accumulated over multiple batches, and sparse refers to the format of our labels. This brings is to an important point: What exactly is the format of our predictions during evaluation? During training, we predict the probability of the example being correct. But during evaluation our goal is to score the utterance and 9 distractors and pick the best one - we don't simply predict correct/incorrect. This means that during evaluation each example should result in a vector of 10 scores, e.g. [0.34, 0.11, 0.22, 0.45, 0.01, 0.02, 0.03, 0.08, 0.33, 0.11] , where the scores correspond to the true response and the 9 distractors respectively. Each utterance is scored independently, so the probabilities don't need to add up to 1. Because the true response is always element 0 in array, the label for each example is 0. The example above would be counted as classified incorrectly by recall@1 because the third distractor got a probability of 0.45 while the true response only got 0.34 . It would be scored as correct by recall@2 however. Boilerplate Training Code Before writing the actual neural network code I like to write the boilerplate code for training and evaluating the model. That's because, as long as you adhere to the right interfaces, it's easy to swap out what kind of network you are using. Let's assume we have a model function model_fn that takes as inputs our batched features, labels and mode (train or evaluation) and returns the predictions. Then we can write general-purpose code to train our model as follows: Here we create an estimator for our model_fn , two input functions for training and evaluation data, and our evaluation metrics dictionary. We also define a monitor that evaluates our model every FLAGS.eval_every steps during training. Finally, we train the model. The training runs indefinitely, but Tensorflow automatically saves checkpoint files in MODEL_DIR , so you can stop the training at any time. A more fancy technique would be to use early stopping, which means you automatically stop training when a validation set metric stops improving (i.e. you are starting to overfit). You can see the full code in udc_train.py . Two things I want to mention briefly is the usage of FLAGS . This is a way to give command line parameters to the program (similar to Python's argparse). hparams is a custom object we create in that holds hyperparameters, nobs we can tweak, of our model. This hparams object is given to the model when we instantiate it. Creating the model Now that we have set up the boilerplate code around inputs, parsing, evaluation and training it's time to write code for our Dual LSTM neural network. Because we have different formats of training and evaluation data I've written a create_model_fn wrapper that takes care of bringing the data into the right format for us. It takes a model_impl argument, which is a function that actually makes predictions. In our case it's the Dual Encoder LSTM we described above, but we could easily swap it out for some other neural network. Let's see what that looks like: The full code is in dual_encoder.py . Given this, we can now instantiate our model function in the main routine in udc_train.py that we defined earlier. That's it! We can now run python udc_train.py and it should start training our networks, occasionally evaluating recall on our validation data (you can choose how often you want to evaluate using the --eval_every switch). To get a complete list of all available command line flags that we defined using tf.flags and hparams you can run python udc_train.py --help . Evaluating the Model After you've trained the model you can evaluate it on the test set using python udc_test.py --model_dir=$MODEL_DIR_FROM_TRAINING , e.g. python udc_test.py --model_dir=~/github/chatbot-retrieval/runs/1467389151 . This will run the recall@k evaluation metrics on the test set instead of the validation set. Note that you must call udc_test.py with the same parameters you used during training. So, if you trained with --embedding_size=128 you need to call the test script with the same. After training for about 20,000 steps (around an hour on a fast GPU) our model gets the following results on the test set: While recall@1 is close to our TFIDF model, recall@2 and recall@5 are significantly better, suggesting that our neural network assigns higher scores to the correct answers. The original paper reported 0.55 , 0.72 and 0.92 for recall@1, recall@2, and recall@5 respectively, but I haven't been able to reproduce scores quite as high. Perhaps additional data preprocessing or hyperparameter optimization may bump scores up a bit more. Making Predictions You can modify and run udc_predict.py to get probability scores for unseen data. For example python udc_predict.py --model_dir=./runs/1467576365/ outputs: You could imagine feeding in 100 potential responses to a context and then picking the one with the highest score. Conclusion In this post we've implemented a retrieval-based neural network model that can assign scores to potential responses given a conversation context. There is still a lot of room for improvement, however. One can imagine that other neural networks do better on this task than a dual LSTM encoder. There is also a lot of room for hyperparameter optimization, or improvements to the preprocessing step. The Code and data for this tutorial is on Github, so check it out.",en,44
441,1057,1463776160,CONTENT SHARED,-5924996404988010,-1387464358334758758,4255314756189922878,,,,HTML,http://googlediscovery.com/2016/05/20/google-lanca-aplicativo-de-ciencias-para-android/,"google lança science journal, aplicativo de ciências para android | google discovery","O Google anunciou a disponibilidade do Science Journal - um novo aplicativo para Android que promete transformar qualquer smartphone em ""notebook de laboratório"" que pode ser levado para qualquer lugar. ""O Science Journal é uma ferramenta de ciências para seu smartphone. Use os sensores do seu telefone ou conecte-o a sensores externos para fazer experimentos no mundo ao seu redor. Organize suas ideias em projetos, faça previsões, anotações e colete dados de diferentes testes, depois registre e analise os resultados"", diz o buscador. Science Journal usa os sensores do telefone para medir e registrar dados em tempo real, incluindo o movimento, luz e som; e registra todos os dados em gráficos e tabelas, além de organizar todas as suas observações. Aplicativo do Science Journal está disponível, sem custo, no Google Play. é bacharel em administração de empresas e fundador da FragaNet Networks - empresa especializada em comunicação digital e mídias sociais. Em seu portfólio estão projetos como: Google Discovery, TechCult, AutoBlog e Arquivo UFO. Também foi colunista de tecnologia no TechTudo, da Globo.com.",pt,44
442,2953,1484321076,CONTENT SHARED,-2034054811536485879,3609194402293569455,-4642144323861325474,"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36",SP,BR,HTML,https://www.embarcados.com.br/aplicacao-de-visao-computacional-com-opencv/,aplicação de visão computacional com opencv,"Visão computacional é a visão de máquinas, é possível obter informações de imagens, sejam elas astronômicas, microscópicas ou em tamanho natural, podemos utilizar algoritmos computacionais para descrever e analisar o conteúdo de qualquer imagem digitalizada. Essa prática é cada vez mais comum na indústria no controle de qualidade de processos e orientação de robôs, a visão computacional é capaz de realizar análises com precisão e velocidades que o olho humano não poderia alcançar, traz um novo leque de possibilidades como navegação de veículos autônomos, descoberta de novos planetas e análises biológicas em células. Sobre o projeto Um sistema de visão computacional para identificar e analisar biscoitos em tempo real, desenvolvido com software livre e neste caso voltado para uso educacional. Funcionamento do sistema O sistema de visão computacional utiliza uma câmera digital, uma iluminação uniforme tipo domo e um computador com software para processar e analisar as imagens. O software utiliza técnicas de processamento de imagens que são como os filtros do photoshop, os filtros têm o objetivo de tratar as imagens, retirar o fundo (background) e deixar o objeto com o melhor contraste possível e sem ruídos que possam atrapalhar a análise geométrica. Após filtrada a imagem, temos uma imagem binária, uma imagem preto e branco somente com a forma do objeto, chamamos essa imagem binária de máscara e é nela que o software irá realizar as análises de padrões geométricos. Primeiro o software verifica a presença do biscoito na imagem pela cor/tom com uma função simples e rápida chamada trigger, depois se houver a presença detectada do objeto a imagem passa por filtros é retirada a máscara e em seguida passa para análise do padrão geométrico que vai identificar as peças boas e ruins e ao final o resultado é mostrado na tela em tempo real. OpenCV - Biblioteca aberta de visão computacional da Intel OpenCV é a principal biblioteca de código aberto para a visão computacional, processamento de imagem e aprendizagem de máquina, e agora apresenta a aceleração de GPU para operação em tempo real. OpenCV é liberado sob uma licença de BSD e daqui é livre para o uso acadêmico e comercial. Possui interfaces C++, C, Python e Java e suporta Windows, Linux, Mac OS, iOS e Android. OpenCV foi projetado para eficiência computacional e com um forte foco em aplicações em tempo real. Escrito em C/C++ otimizado, a biblioteca pode aproveitar o processamento multi-core. Adotado em todo o mundo, OpenCV tem mais de 47 mil pessoas da comunidade de usuários e número estimado de downloads superior a 6 milhões. O uso varia de arte interativa, a inspeção de minas, costura mapas na web ou através de robótica avançada. Documentação oficial do OpenCV . Recomendo a leitura da documentação oficial do OpenCV para aprendizado e consulta! Considero que você possui conhecimentos em programação, tem o OpenCV instalado e testou alguns, ou todos os exemplos dele em C++ e Python e fez um hello world com CMAKE. Esse tutorial é de nível intermediário, apesar da simplicidade do código, é necessário algum conhecimento mas qualquer um pode aprender seguindo a documentação. Vamos apresentar a utilização do OpenCV em um projeto realtime simples, completo e de baixo custo. Esse projeto poderá ser modificado para muitos outros casos de inspeção visual de objetos e sua arquitetura é fruto de anos de pesquisa e desenvolvimento para obter o melhor desempenho e manter a simplicidade do código com menor custo possível. Funcionamento algoritmo básico: Parametrização, antes de capturar a imagem é necessário configurar a câmera, resolução, fps, tempo de exposição e ganho do sensor manual. 1 - Captura - abrir conexão, primeiro vamos abrir uma conexão com a câmera para imagem ser capturada. 1.0 - Conexão da câmera, este passo ocorre fora do loop principal, na inicialização do sistema: 1.1 Captura de frame , esta função é um loop e roda dentro de uma thread independente, aqui utilizei um mutex que tem a função de impedir que a imagem seja acessada simultaneamente por duas threads, pois a frame é uma variável tipo global e é utilizada por duas threads. 2 - Detecção , uma função simples e rápida que verifica se o objeto existe na frame capturada! A detecção ou trigger é uma área retangular no centro da imagem que testa a média da cor/tom. Se a comparação for verdadeira a frame capturada contém o objeto e podemos inspecioná-la. 3 - Pré-processamento , Esta etapa é muito importante, é onde filtramos a imagem cinza e transformamos em uma máscara, ou seja, uma imagem binária preto e branco, onde o objeto é branco e o fundo preto. 3.1 Canal - Escolho entre os canais de cor RGB o que melhor representa o objeto 3.2 Canal R - Os canais RGB, são imagens em tons de cinza com profundidade de 8 bits cada, neste caso escolhemos o RED que contém a melhor informação dos tons do biscoito, note que ele é cinza mas representa o vermelho de 0 á 100% na imagem colorida. 3.3 Threshold - Função que binariza a imagem, ou seja, transforma uma imagem em tons de cinza em uma imagem preto e branco, note que ainda existem alguns ruídos na imagem. 3.4 Cleanup - Remoção de ruídos (Reduz a imagem na metade do tamanho, passa filtros de transformação morfológica de abrir e fechar e depois a mplia a imagem para o tamanho original). Após esses passos eliminam se os ruídos e temos uma imagem mais uniforme. Essa operação está dentro da função processing. 4- Inspeção / análise , Após o objeto ser detectado, a frame filtrada e transformada em máscara é chamada a função de inspeção, que vai analisar a geometria do objeto. Essa função analisa a máscara e verifica se o objeto é convexo, (possui todos os cantos arredondados) e valida as dimensões do objeto. Podemos medir a área, perímetro, altura e largura. 4.1 Contornos , é uma função que transforma a máscara binária em linhas ou contornos. 4.2 - Aproximação poligonal , transforma as linhas em um polígono com ajustes de precisão do comprimento das linhas. Essa precisão é chamada epsilon. 5 - Resultado , Após a análise temos o resultado e podemos mostrar os contornos na tela com cores verde e vermelho indicando passa ou falha e em um sistema completo acionar uma saída digital de rejeito de defeitos. Existem diversas maneiras de analisar imagens, com muitos outros algoritmos como por exemplo ORB, AKAZE e SURF. Neste caso nós utilizamos análise geométrica básica, que é um código simples, leve e eficiente. Na maioria das aplicações é necessário otimizar o código ao máximo para atingir alta velocidade de inspeção com precisão e segurança. Materiais utilizados no teste Fonte 12V, Fita Led, Bola de Isopor, pseye, um suporte de abajur de mesa e uma mesa giratória com fundo de EVA preto para simular o movimento da esteira. Montagem da iluminação domo, feita com meia bola de isopor e fita led, a lente da câmera fica no centro do domo, os leds apontam para dentro do domo, esta iluminação é difusa, utilizada para evitar reflexo e obter uma iluminação uniforme. Funciona bem próximo ao objeto a ser inspecionado. Observações Em projetos industriais utilizamos equipamentos industriais de alto desempenho e precisão, como as câmeras inteligentes da COGNEX, que possuem processadores dedicados com software próprio, iluminações como as da ADVANCED ILLUMINATION e lentes de alta qualidade, para realizar inspeções em linhas de alta velocidade com precisão e repetibilidade, além de conexão com robôs e outros equipamentos industriais. É possível utilizar este software com câmeras industriais, mas é necessário fazer a integração! Nosso objetivo aqui é fornecer um ambiente de estudos de visão computacional de baixo custo com materiais simples e promover o uso de software livre para fins educacionais! Isso é o mais simples que posso fazer, não é utilizado para fins industriais e sim para estudos! Repositório do projeto para download Vocês podem baixar o exemplo completo neste repositório . Vídeo Existe uma versão em Python e uma em C++ para este projeto. Espero que gostem! Téc eletrônica/ automação da manufatura P&D em visão computacional",pt,44
443,2733,1478784272,CONTENT SHARED,5263606973029607223,7645894863578715801,-7392748146172665703,Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:49.0) Gecko/20100101 Firefox/49.0,SP,BR,HTML,https://medium.com/@derrickburns/building-a-modern-scalable-backend-modernizing-monolithic-applications-15fc3b8101fa,"building a modern, scalable backend: modernizing monolithic applications","One year ago my consultancy was hired to construct a modern backend for a travel site that serves millions of unique visitors each month after the previous team had failed. This is the story of our success. The Problem If your business is based on content, then you must solve two separate problems: 1) how to create new content and 2) how to serve content via your web site, via your mobile applications, and perhaps via third parties. If your business has tenure, then your content creation suite is likely varied, and your data is probably spread across many different data sources. Worse, each of your applications access each data source directly with inconsistent APIs. If you want your business to grow faster than your applications teams can drive it, then you need to provide third parties access to your data in a manner that you control. This is the state that we found our client in. Their rich content was stuck behind Ruby on Rails applications, Wordpress instances, and various other data stores. They needed 1) to create new and richer content faster and 2) to serve their content to new web and mobile applications at scale. Their previous attempts to solve this problem had failed. Those teams had tried to solve both problems simultaneously. With so many stakeholders, they failed to get consensus and churned. We took a different approach. We decoupled the two problems. We left the content creation workflow alone and instead introduced a new serving layer atop the storage layer using a data model that could be easily understood and served via a truly RESTful API. With this architecture, we enabled the client to build new web and mobile applications quickly, moving independently of the content creation tooling development. This is the story of what we delivered to client and how it satisfies their current and foreseeable future needs. The Requirements The Data Sources As a travel company with a global brand, our client had accumulated a wealth of content, some generated in house, and some integrated from third parties. In-house objective content and subjective narratives about places and points of interests are created by a custom Ruby/Rails content management system backed by a Postgres database. A rticles and news are created in various Wordpress instances backed by MySQL databases. Books and e-books are sold in a custom e-commerce application backed by a Microsoft SQL Server database. Bookings services for lodgings are provided by booking.com and Hostel World. Booking services for tours are provided by G Adventures and Viator. Additional objective content such as hours of operation or location (address or latitude/longitude) data from Google Places and Factual.com would also need to be integrated. The Data Model With so many disparate data sources, we needed an organizing principle. As a travel site, internal clients access the data either by navigating or searching via the web or mobile applications. We needed to support navigation following a simple geographic (place) hierarchy : world, continents, countries, states/regions, cities, and neighborhoods. Points of interest would be indexed by the places that contain them. Articles, news, books, and e-books would be indexed by the places mentioned within them. Individual lodgings would be indexed by the places in which they reside. Tours would be indexed by the places that they visit. The Traffic As one of the most visited travel sites, we needed to support 30-60 GET requests per second on launch. New mobile applications were also being built, so demand would be unpredictable. We needed to build a system that would scale elastically. The Solution Microservices and Cloud Infrastructure Given the failure of the previous team, we knew that we needed to show results quickly. Moving quickly meant that we would leverage popular open source software, well-thought standards, and cloud infrastructure. Our client was already using Amazon Web Services (AWS) to run dozens of Ruby on Rails applications, each on a separate EC2 instance. So the decision to host our backend on AWS was easy. We needed a high degree of scalability, support for multiple programming languages, and clear isolation of services, so we opted for a microservice architecture using Docker for containerization and using Json for data interchange using the principles of the 12-factor application . API We needed to design our APIs first so that clients to the services that we were building could be developed in parallel to those services. We opted to write our APIs using the RAML API modeling language and Json Schema , a vocabulary that allows you to annotate and validate JSON documents. We wrote tools using the Ramlfications RAML parser and the jsonschema Json schema validation tool. Prior to implementing services, we wrote example requests, responses and their schemas. This proved invaluable in expediting client development and in identifying inconsistencies between our implementation and our specifications. To structure our APIs, we opted to standardize on a well-structured approach to building Json based web APIS, JsonAPI . While many alternatives exists (including HAL, JSON-LD, Collection+JSON, and SIREN), JsonAPI provided structure and direction with acceptable overhead. Orchestration My team consisted of two Ruby/Elixir developers, one senior Python engineer, and two Java/Scala engineers. Standardizing on a single language would have to wait. We allowed each developer to use the language of his or her choice, as long as it was contained in a Docker container and built according to the JsonAPI standard. Later, after demonstrating success, we would standardize on one or two languages. To manage our containers, we opted for Kubernetes , an open-source system for automating deployment, scaling, and management of containerized applications. We chose Kubernetes over its competitors because of its maturity and growing user base. Microservices Over the course of several months, we implemented a microservice for each data set. Each micro-service consists of two sub-services: 1) a single synchronization sub-service that indexes data by location and writes it into a AWS RDS Postgres/PostGIS datastore and 2) a stateless, replicable API sub-service that serves Json data from that datastore. Exactly one instance of each synchronization sub-service runs on each cluster, but using Kubernetes auto-scaling , we scale the API sub-services based on CPU load. This allows us to adapt to increasing/decreasing demand without human intervention. Our first microservice, written in Python, serves places (e.g. New York City) and points of interests (e.g. the Statue of Liberty) . The synchronization sub-service replicates data from a remote non-hosted Postgres database into an AWS RDS Postgres database. The API sub-service uses the Flask framework, a gevent.pywsgi web server, and the p sycopg PostgreSQL adapter. Gevent provides co-routines that allow scaling the web server without the overhead of heavyweight threads. Our second microservice, written in Elixir, serves lodgings from booking.com and HostelWorld . The synchronization sub-service polls the web sites for changes several times each day, and indexes the data into an AWS RDS Postgres database. The API sub-service uses the Phoenix web framework. Elixir lacks the maturity of the other languages, which required us to extend open source monitoring and serialization libraries in order to use them in our project. However, Elixir proved to be a solid performer with low memory requirements. Our third microservice, written in Ruby, serves activities and tours from Viator and G Adventures . The synchronization sub-service polls the web sites for changes several times each day, and indexes the data into an AWS RDS Postgres database. The API sub-service uses the Rails web framework. This service proved to be a memory hog. Our fourth microservice, written in Scala, serves articles and news from Mysql databases managed by Wordpress . Unlike the other micro-services, it does not have a separate synchronization layer, since the source data is already indexed by location via a separate custom Wordpress plugin. Instead, the API sub-service reads directly from a read replica of the source Wordpress database, and serves the data using the spray framework. Our fifth microservice, written in Elixir, serves the inventory of books and e-books from a custom e-commerce system backed by Microsoft SQL Server. The synchronization sub-service polls a custom XML feed several times each day, and indexes the data into an AWS RDS Postgres database. Logging Logging in Kubernetes is supported with several cluster add-ons, including: 1) elasticsearch , a distributed, open source search and analytics engine; 2) Fluentd , an open source log data collector; and, 3) Kibana , an open source log data visualization platform. We frequently ran out of space when storing elasticsearch data locally within the Kubernetes cluster, so we are moving that storage outside of the cluster To get the most our of logging, we standardized on Json-formatted log messages with a required minimum set of fields. We validated the log messages according to a Json schema that we created. Monitoring To monitor our Kubernetes cluster and provide alerts, we used the Prometheus toolkit. Unlike many monitoring tools, Prometheus operates using a pull model, so each service must implement a standard API endpoint that provides metrics. We implemented a small set of standard metrics for each sub-service. Each synchronization service provides a simple binary flag indicating whether synchronization is in progress. Each API sub-service provides a request time histogram. We use labels to identify request types for fine grain insight into response time. We analyze and visualize the metrics that Prometheus collects using Grafana . We augmented Prometheus metrics with exceptional events from our elasticsearch logs using Grafana annotations. During early development, this allowed us to count exceptions and to correlate exceptions with operational metrics, which helped us to find bugs quickly. For longer term analysis and visualization of exceptions, we used Airbrake , which has better support for seeing long term trends. Testing Prior to the availability of actual application traffic, we tested our services under artificial load using the open source Locust load testing tool. When used with Prometheus response time metrics that we visualized in Grafana, we were able to gain insights into the performance and behavior of our system. Once we had actual traffic hitting our production site, we were able to replay that live traffic into our staging cluster using Gor in order to continuously test our system with real data. Gateway To direct traffic to the individual microservices, we used the open source proxy Nginx . In front of our Nginx instance, we used the open-source API gateway and microservices management tool Kong to authorize and rate-limit API requests from third parties. Success! Overall the project was a major success. Within three months with five developers and one DevOps person we had a proof of concept up and running. Two months later, we had support for orchestration with Kubernetes. Our developers were able to create a new simple service and deploy it to any of three Kubernetes clusters (QA, staging, and production) in under two hours without any assistance or support from DevOps. More importantly, our client was able to realize tangible and quantifiable benefits. First, the point of interest web pages were reimplemented to use the new microservices. Now, 100% of traffic is being directed to the new Kubernetes production cluster, using Kubernetes auto-scaling for elasticity and efficiency. Development was fast due to the design-first approach and the employ of mock servers. Second, for the first time in company history, the company is able to offer third parties access to an API to the custom content. This will offer new revenue streams. Third, applications are being ported from the old AWS EC2/CloudFormation/Chef based infrastructure. This will lower the operating cost since Kubernetes will provide greater EC2 instance utilization. On the down side, as mentioned in What I Wish I Had Known Before Scaling Uber to 1000 Services , there are real costs to supporting multiple language environments. First, expertise in each environment must be acquired. This splits teams into camps, and slowed code review and bug fixing. Second, deficiencies in dependencies needed to be fixed in each language environment. This occurred quite frequently, and slowed adoption of new monitoring and tracing tools. We will migrate away from Ruby and one of the other languages. Conclusions Unlocking the value of content takes more than simply bolting on a Json API to an existing application or web service. It requires a well considered data model, a clean API, and a scalable infrastructure. But with containerization, cloud compute and storage services, open source tools, and a solid architecture and engineering team, you can indeed unlock that value.",en,44
444,1428,1466165752,CONTENT SHARED,8620166073102656574,-534549863526737439,-8981930056560085758,,,,HTML,http://jrsinclair.com/articles/2016/one-weird-trick-that-will-change-the-way-you-code-forever-javascript-tdd/,one weird trick that will change the way you code forever: javascript tdd,"This is a presentation delivered to the Squiz Melbourne Engineering team. It repeats some of the material I've covered in other posts. So apologies if you're a regular reader and you've heard all this before. Introduction One weird trick is a cheesy title, I know. Originally I was using it as a draft placeholder title for a joke. But the more I thought about it, the more it seemed appropriate because it's true. Test Driven Development is one weird trick that will change the way you code forever (in a good way). I will explain why as we go on. I've broken this talk up into three parts: In the how section I'll work through a step-by-step example of how to write a single function with TDD. It will be very brief, because I don't have a lot of time, and I just want to give you a flavour of how TDD works. For now though, let's start with the why . Why? I want to think back to some of those 'oh cr*p' moments in your coding career. Have you ever fixed a bug, only to find that it broke something horribly in another part of the system? And you had no idea until the client called support in a panic? Have you ever been afraid to touch a complicated piece of code for fear that you might break it and never be able to fix it again? ... Even though you wrote it? Have you ever found a piece of code that you're pretty sure wasn't being used any more and should be deleted? But you left it there just in case? Have you ever felt like your code was a tower made of soft-spaghetti, held together with Clag glue and wishes? If you haven't, then you probably don't need TDD. Or you haven't been coding for very long. What if all of these could be a thing of the past? Imagine going back to some old code and thinking ""Actually, this code isn't too bad. It feels clean. I know what's going on. Whoever wrote this was a genius!"" Sounds like unicorns and rainbows, right? But bear with me. I really do want you take a moment and imagine what that would feel like. What would it be like to come back to some of your own code, months (or years) later and not have that ""Ewww"" reaction? How would it feel to be able to fix a bug and know for sure that it had been fixed, and that you didn't break everything doing it? Imagine surprising yourself with some of the elegant coding solutions you write. I know that sounds a bit dramatic and cheesy, but it is possible. It's a side-effect that I wasn't expecting when I started using TDD, but it's something I've actually experienced. There are some projects I look forward to working on again because I know the code is clean and organised. Excuses Now, you may have heard of TDD before. And maybe you thought ""Ah yes, testing. That's definitely something I should do."" And then you didn't do it. Anyone? I hear that all the time. I think there are two reasons why: The first reason is that testing seems like an optional extra-gold plating; a nice-to-have. You don't need the tests to have working code. And what's the first thing to get dropped when a project starts getting behind? Those 'superfluous' tests, right? Why waste time on something that isn't absolutely essential to getting the project completed? The second reason we don't practice TDD more often (I think) is because of the word 'test'. Testing sounds tedious; boring; time-consuming. You're under the pump and you've got to get this project out the door. You don't have time to write tests on top of everything else that has to get done. It seems like a nice-to-have. It's like doing your taxes-you might understand that it's important, but it's definitely not sexy or fun. I felt the same way about TDD. But so many smart people seemed to be saying it was a good idea that I reluctantly gave it a go. And eventually I discovered a secret: Test Driven Development is not about testing. Did I just blow your mind? Let me elaborate a little: Test Driven Development is not about testing. It is a way of thinking and coding that just-so-happens to involve tests. What do I mean by this? What is it about then, if it's not about the tests? TDD is a technique that gives you confidence in your code. It's a life-hack. It's not really about the tests. Those are just a useful side-effect. The real benefit of TDD is the way it teaches you to think about code, and the confidence it gives you to know that your code definitely works. More excuses Doesn't TDD slow you down and make you less creative? The short answer is no. Yeah, TDD seems slower at first. And when you start out it does take more time as you get used to id-just like any new skill. But as you go on it starts saving you more and more time. This is because you spend less time figuring out why things are broken and more time getting things done. In turn, spending less time bug-hunting gives you more time for creativity and refactoring. If you're practicing TDD properly, it encourages you to try the stupid-simple dead-obvious thing first, and see if it works. It allows you to try things with less risk of blowing everything up. And one more thing before I go any further: Test Driven Development is not the same thing as unit tests. Unit tests are a type of test. TDD is a coding technique. In our organisation, we have a bad habit of referring to TDD as 'unit testing' (and I'm just as guilty as anybody). But they are not the same thing. Unit testing is a particular type of test that we use frequently for TDD (hence the confusion), but it's not the only type of test. I'm trying really hard to stop using the two interchangeably, so if I do, please let me know. But if TDD is not about tests, and it's not the same as unit testing, what is it, exactly? What? TDD is a technique for writing code where you write a test before you write any 'proper' code. But that's just the single-sentence summary. In the book Test-Driven Development By Example , Kent Beck explains that TDD has two simple rules that imply three simple steps. The rules are: Write new code only if you first have a failing automated test. Eliminate duplication. And the three steps follow on from the two rules: Red -write a little test that doesn't work, perhaps doesn't even compile at first Green -make the test work quickly, committing whatever sins necessary in the process Refactor -eliminate all the duplication created in just getting the test to work These steps are fairly simple, but when followed they produce some powerful results, so long as you are using your brain . As I said, the real value is not in the tests themselves, but in the way it teaches you to think about coding, and the confidence it gives you in your code. To show how that works, we'll run through a very short example: How? Imagine we're going to create the following application: The Pugs of Flickr Web Application All it does is connect to the Flickr API and find the latest pictures of Pugs. I'm not going to run through building the whole application, but just a single step. We will pick one function from one module and build just that. (If you're interested I've written out a step-by-step tutorial for building the whole application with TDD ). So, before we do anything, let's set up the project. First we'll need a folder to work in, so let's create that: Next we'll install Mocha , the testing framework we'll be using (if you don't have it already). And we'll install Chai locally-a module that helps write assertions in a more readable fashion. (Assertion is just a fancy name for the bit that does the actual test, as opposed to all the setup stuff ): Then, we create a file for our tests: The file name is just the name of the module with -spec added on the end. In my file I set up my very first test as follows: This test is super-simple. It does nothing other than check that my module exists. That's it. The describe() function says ""I'm starting a new group of tests here"", and the it() function says ""Here's one test"". So, I run my test suite like so: ...and we get a sad cat. We have completed Step 1-Red. This is good news, because it means I can move forward. So, step two is to make the test pass. What's the simplest possible thing I can do to make that test pass? The simplest thing is to create the module: I run my test again... and I have a happy cat. Step 2-Green is complete. So we're up to the refactoring step. Is there any duplication going on here? Not yet. Is there anything I could do to improve the code? Maybe. I'll tweak things just a little: This makes it a bit clearer what's going on without adding any new (untested) functionality. And I run my test again... and the cat is still happy. So we've completed Step 3-Refactoring . Let's do something a little bit more useful (and more instructive). The Flickr API gives us photo data in JSON form. It doesn't give us URLs for the images (because we have to tell it what size we want). So, we need a function that will take a photo object and transform it into a URL. Photo objects look like this: We want a URL that looks like this: The Flickr API documentation describes the way we make the transform using the following template: So, that gives us enough information to write a test: This just passes the example photo object into the new function, then checks that the actual output matches what we expect. Most of your tests should look roughly like that. You define an input , the actual value, and the expected value. Then you check to see if the actual result matched what you expected. Let's run the test... sad cat (red). So, we can write some code. Now, what's the quickest, simplest, easiest way to make this test pass? You guessed it: Return the URL we expect. Run the tests again, and... happy cat. That's it. Test passes, we're done. But let's pause a moment and talk about what we just did there: Creating an almost useless function that still passes the test. This was the part I didn't understand when I first started practicing TDD. You write only enough code to make the test pass. No more. And it's really hard. This is the main reason why it feels like TDD slows you down. It takes a lot of discipline to only write the bare minimum code. If you're like me, you just know how to write the code, and have all sorts of ideas for making it super-efficient and elegant. But there's no point in writing more code than you have to. Doing TDD right means restraining yourself and only writing enough code to make the test pass. Let's keep going... This function is not complete. What happens if we pass a different photo object? Let's find out... by writing a new test. Run the test again... and it fails, as expected. So... what's the simplest, shortest way to make this test pass? Yep. An if-statement. We run the test again, and... happy cat (green). Are you getting frustrated yet? Don't you just want to get in there and write the whole function? Bear with me, and think about the next step-refactoring. Could this code be any more efficient to past these tests? Well, no, not really. But the next question is very important. Is there any duplication here? ... Actually, yes, there is. But just to drive the point home, let's add one more test. Run the tests again... and sad cat (red). We can write some code. What's the quickest, simplest way to get this code to pass then? Yep, another if-statement. Remember, we're ""committing whatever sins necessary in the process"" to make the test pass: If we run the test again, the cat is happy (green). So, we're up to the refactoring stage. Now, do we have duplication going on? Heck yes! Let's refactor: Now, isn't that much nicer? Does it work? Let's re-run the tests... ...and happy cat (green). Let's savour that for a moment. We have some nice efficient code, that we know works, because we've got three separate tests verifying it. But, we're not done refactoring yet... do we still have duplication going on? Yep. There's a whole bunch of it in our tests. So let's refactor those: Now our tests are nice and clean too. We run them again and we still have a happy cat (green). Everything is nice and tidy and efficient. Final thoughts I'm hoping after this that you'll give TDD a go. But I have one final word of advice: Start small . Don't try and do everything at once. Pick one small, easy bit of a project and do TDD with that. If it's easier to set something up in a Code Pen , then do that. Once you're comfortable with the three steps, then start thinking about how you can bring more stuff into tests. Think about how to restructure your code to make it easier to test. Slowly, all your code will start to improve. And, as you practice, you will become a better developer because you will learn to see the code differently. More resources I've written about TDD before on my website. There's a step-by-step tutorial and some advice on where people get stuck: If you'd prefer advice from someone who isn't me, check out Eric Elliot's helpful articles: Or Rebecca Murphey:",en,44
445,2320,1473694470,CONTENT SHARED,-2720012228333470715,3217014177234377440,4115185045256903778,,,,HTML,http://corporate.canaltech.com.br/noticia/empresas-tech/nubank-comeca-a-testar-seu-programa-de-fidelidade-79675/,nubank começa a testar seu programa de fidelidade - empresas tech,"Por Redação | em O Nubank iniciou testes com seu novo programa de fidelidade. A empresa selecionou um grupo de usuários Android para acessar a novidade já nesta segunda-feira (12). Com o programa de fidelidade, é possível acumular pontos de acordo com os valores das compras realizadas com o cartão, que podem ser utilizados para pagar diárias em hotéis, passagens aéreas, corridas de Uber e mensalidades de serviços de streaming como Spotify e Netflix. Segundo Cristina Junqueira, vice-presidente de desenvolvimento de negócios e cofundadora do Nubank, o programa de fidelidade foi totalmente desenvolvido pela companhia, que destacou uma equipe de 20 pessoas para trabalhar exclusivamente no projeto durante um ano. ""Não queríamos ter um programa de fidelidade igual ao dos outros cartões. Esperar até o fechamento da fatura para saber quantos pontos foram creditados não faz parte da experiência que queremos oferecer"", disse a executiva. Os que têm interesse em utilizar o programa de fidelidade terão de pagar uma anuidade. O valor ainda não foi revelado pela empresa, mas os 30 primeiros dias serão gratuitos. Os testes realizados serão expandidos, a partir de outubro, também para usuários de iPhone. Até o final do ano, a empresa pretende disponibilizar o programa de recompensas para todos os clientes. Os que foram selecionados receberão um convite para aderir ao programa a partir de hoje. Para cada R$ 1 em compras, o usuário receberá 1 ponto. Na conversão para dinheiro, cada ponto valerá menos do que R$ 1, mas a empresa não revelou o valor de tal conversão. O saldo do programa de fidelidade será exibido na tela do aplicativo e atualizado em tempo real, sempre que o usuário realizar uma compra através do cartão. Os pontos acumulados também não terão data para expirar. Para utilizar os pontos, o usuário deve realizar a compra de uma passagem aérea, por exemplo, e depois acessar o programa de fidelidade e ""apagar"" a despesa utilizando os pontos acumulados. De acordo com Cristina, por enquanto o Nubank não procura novos parceiros para o programa de fidelidade, mas eles poderão ser ampliados no futuro. Com o programa de fidelidade, o Nubank espera atrair consumidores com renda maior que o dobro da média de seus clientes atuais. Mais de 5,5 milhões de brasileiros já solicitaram o cartão de crédito do Nubank desde 2013, quando a empresa foi fundada. A startup não revela quantos clientes receberam aprovação para receber, de fato, o cartão.",pt,43
446,2855,1481553137,CONTENT SHARED,-4358357193736101128,3609194402293569455,4506332254516554808,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36",SP,BR,HTML,https://www.infoq.com/br/articles/internet-of-things-reference-architecture,uma arquitetura de referência para a internet das coisas - parte 1,"Esse é o primeiro de dois artigos onde tentamos trabalhar a partir de um nível abstrato de arquiteturas de referência de Internet das Coisas (ou IdC) em direção a uma arquitetura e implementação concreta para casos de uso selecionados. Esse primeiro artigo cobre a definição de uma arquitetura mais concreta e abrangente, e o segundo artigo irá aplicar essa arquitetura a casos de uso reais. Estamos próximos de um novo mundo interconectado. Sob o nome ""Internet das coisas"" - IoT, em inglês Internet of Things - ou ""Indústria 4.0"", o qual empresas estão desenvolvendo uma nova rede de objetos interligados para nossa vida cotidiana. O objetivo da Internet das Coisas é de interligar objetos, a fim de trocar informações para cumprir tarefas para seus usuários. Para exemplificar, soluções de geladeiras que se comunicam não só com o seu smartphone, mas também com os servidores do fabricante ou com uma usina de energia em breve se tornarão realidade. As empresas por trás do crescimento dessa nova tecnologia e comunicação vêm de todas as indústrias - não só os grandes nomes de big data, como Google, Microsoft ou Apple, estão indo nessa direção, mas também grandes companhias de seguros, os produtores de periféricos e fabricantes de automóveis estão promovendo a Internet das Coisas.. A chave para permitir a comunicação entre todas estas diversas ""coisas"" é a padronização. Estabelecer um padrão, no entanto, é fácil de ser reivindicado num ambiente de pesquisa, mas difícil de ser alcançado no mundo real. Arquiteturas de referência são de grande ajuda para a padronização, já que definem diretrizes que podem ser usadas quando se planeja a implementação de um sistema utilizando a Internet das Coisas. A fim de alcançar uma padronização é necessário criar arquiteturas de referência de alto nível como o definido pela IOT-A . No entanto, arquiteturas de referência de alto nível são difíceis de entender por serem muito abstratas. Se você está em um trabalho de consultoria, verá que é impossível mostrar essas arquiteturas de referência de alto nível para clientes reais na indústria. Gostaríamos de ir além e fornecer diretrizes sobre como fundamentar uma arquitetura mais concreta a partir da arquitetura de referência IOT-A. A ideia é pegar a arquitetura de referência IOT-A e criar a partir dela uma arquitetura menos abstrata que poderia até mesmo ser colocada em um ""Resumo Executivo"" - que é o que está acontecendo neste artigo. Além disso, selecionamos alguns casos de uso e instanciamos nossa arquitetura de referência a fim de mostrar o ciclo de vida completo até a implementação de um sistema real dentro da IoT - que será apresentado em um artigo subsequente. Primeiramente, vamos definir alguns termos: Coisa: é um objeto do nosso dia a dia colocado em nosso ambiente diário. Uma coisa pode ser um carro, uma geladeira, mas também pode ser abstraído a uma casa ou uma cidade, dependendo do caso de uso; Dispositivo: um sensor, atuador ou tag. Geralmente o dispositivo é parte de uma coisa. A coisa processa informações de um contexto e informa sobre os dados coletados para outras coisas. Além disso, a coisa pode passar ações para atuadores. Há uma certa quantidade de ""componentes inevitáveis da Internet das Coisas"" que é possível encontrar (de uma forma ou de outra) em cada uma das arquiteturas de referência da Internet das Coisas (como por exemplo, em Brillo, no Google , ou na IOT-A ou Z-Wave ). Entre elas, podemos destacar: Componentes de interoperabilidade e integração em relação às coisas e dispositivos; Técnicas de computação orientada a contextos, como a definição de um contexto e o modelo de ação, bem como as definições de objetivos por motores de regras; Diretrizes de segurança que variam ao longo da arquitetura completa. De certa forma as arquiteturas atuais para a Internet das Coisas podem ser vistas como uma versão em maior escala do context toolkit de Anind K. Dey. O context toolkit foi projetado em um nível de aplicação, já que foi projetado para Sistemas de Informação Geográfica (GIS, na sigla em inglês). Na Internet das Coisas temos que estender o context toolkit para a intercomunicação entre coisas. No entanto, a idéia básica de objetivo, informações de contexto e ações resultantes permanece no mundo da Internet das Coisas. No mundo da Internet das coisas nós não só definimos a meta no nível do usuário (por aplicação), mas as próprias coisas podem trabalhar em certas metas sem incluir ativamente do usuário. No final, os dispositivos ainda servem o usuário, mas eles agem de forma autónoma no fundo - que é exatamente a idéia de computação ubíqua . De forma a obter uma melhor imagem do termo ""contexto"" vamos primeiro introduzir nosso modelo de contexto e, em seguida, saltar para a introdução de nossa arquitetura de referência. Contexto define o estado de um ambiente (geralmente o ambiente do usuário) num determinado lugar e em um determinado momento. O modelo de contexto geralmente faz a distinção entre os elementos do contexto e da situação de contexto. Os elementos do contexto definem contextos específicos, geralmente no nível do dispositivo. Um elemento de contexto pode ser por exemplo um valor de temperatura num determinado momento e local. Localização e tempo são elementos de contexto em si, mas eles desempenham um papel especial já que são necessários para localizar valores de sensores no espaço e no tempo. Afinal de contas, sem saber onde e quando a temperatura foi medida a temperatura não ajuda muito para a tomada de conclusões. Certos elementos de contexto podem ser padronizados de imediato (por exemplo, um valor de temperatura já é definido por um valor double e uma unidade de medição, tais como graus Celsius ou Fahrenheit). Outros elementos de contexto são específicos de aplicação (""específico da coisa""), e não podem simplesmente ser padronizados imediatamente. Estes elementos são definidos como contexto ""de alto nível"" e requerem um mecanismo para defini-los para cada coisa. A situação de contexto é uma agregação de elementos de contexto. A situação contexto é, portanto, uma visão sobre o ambiente em um determinado local em um determinado momento. Como mencionado anteriormente, determinados elementos de contexto podem ser padronizados imediatamente (porque eles já são padronizados) mas outros não (porque tratam de caso de usos específicos). De forma a permitir que uma coisa saiba se ela pode realmente se comunicar com outra coisa, um certo padrão de comunicação deve ser acordado. Para este fim, apresentamos o esquema situação de contexto. O esquema situação de contexto define o que a coisa é capaz de fazer dentro de um contexto. Você pode avançar o modelo de contexto ainda mais e definir certas ""funcionalidades padrão"" que devem ser introduzidas por todos e incluir funcionalidades adicionais que serão definidas para cada coisa, por exemplo, seguindo o padrão Z-Wave . De forma similar ao modelo de contexto, também é possível definir um modelo de ação que defina o que as coisas podem ativar (por exemplo, abrir uma janela, tirar uma foto). Ações só podem ser ativadas com a combinação de informação de contexto e com objetivos definidos. Objetivos são geralmente descritos como regras de um motor de regras (por exemplo, se a temperatura for > 25º ENTÃO abra a janela). Sempre que uma situação de contexto é dado a uma coisa, a coisa avalia se uma ação deve ser acionada de acordo com seus objetivos definidos (ou seja, regras). Dependendo do caso de uso do modelo de contexto, ação e o objetivo, pode ser mais ou menos complexo para uma determinada coisa. Algumas coisas só podem consumir ações e não produzirão informações de contexto, enquanto outros vão publicar informações de contexto (ou mesmo objetivos) para serem consumidos por outras coisas. Agora que entendemos o papel da computação contextualizada na âmbito da Internet das Coisas, podemos ir direto para a definição da nossa Arquitetura de Referência em Camadas da Internet das Coisas, ou ""RILA"" (da sigla em inglês, "" Reference IoT Layered Architecture ""). No contexto de IoC, a RILA atua entre coisas, dispositivos e o usuário, como mostrado na figura abaixo. A RILA consiste em 6 camadas. Além dessas camadas existem duas ""camadas transversais"" que afetam todas as outras camadas, ou seja, ""Segurança"" e ""Gestão"". Vamos analisar cada camada da RILA e dentro de cada componente. Iniciaremos a partir da camada inferior da pilha (camada ""Integração de dispositivos"") e subimos em direção ao topo. A camada de integração de dispositivo conecta todos os diferentes tipos de dispositivos e consome as medições do dispositivo, bem como comunica ações (no nível do dispositivo). Esta camada pode ser vista como um tradutor que fala muitas línguas. A saída dos sensores e tags geradas dependem do protocolo o qual implementam. A entrada dos atuadores também são definidas pelo protocolo implementado. A camada de integração de dispositivo é formada por três componentes principais. O componente de nível mais baixo é o componente driver que se comunica com os diferentes sensores, tags e atuadores de baixo nível, informações específicas do fornecedor e protocolos de comunicação. Ele contém instâncias do driver para cada tipo de dispositivo de nível baixo conhecido para o sistema. O próximo componente é nomeado de componente de descoberta de dispositivos . Este componente pode ser acionado por dois eventos, uma vindo da camada de gestão do dispositivo, que diz a este componente para adicionar um novo dispositivo e por outro vindo do componente driver, que notifica este componente no caso de um novo dispositivo ser adicionado. Da mesma forma o componente de descoberta de dispositivos também lida com cancelamento do registro de dispositivos. O último componente é o de comunicação do dispositivo. É a ponte entre a camada de gerenciamento de dispositivos e o driver . Este componente decide qual o driver é chamado quando a camada de gerenciamento de dispositivos trata um dispositivo. O gerenciamento de dispositivos é responsável por tomar registros de dispositivos e medições dos sensores da camada de integração de dispositivos. Além disso, comunica mudanças de status para os atuadores da camada de integração de dispositivos. A camada de integração de dispositivos, em seguida, apenas confirma que a mudança de status (ou seja, a ação) está em conformidade com o atuador e, em seguida, traduz a alteração de status para o atuador. A camada de gerenciamento de dispositivo controla os dispositivos de uma forma que ela saiba quais dispositivos estão conectados ao sistema. Cada modificação no registo de um dispositivo, bem como os dados de medição recebidos precisam ser comunicados da camada de integração de dispositivo para a camada de gestão do dispositivo, de modo que a informação possa ser atualizada e armazenada. Dessa forma, a camada de integração de dispositivos gere o registo do dispositivo (que inclui o enriquecimento dos metadados, como por exemplo informações sobre qual unidade ou frequência um sensor está enviando dados) e a comunicação do dispositivo (capture as medições atuais e mova para a gestão de dados, bem como passe as ações adiante para o atuador de dispositivos). A gestão de dados pode ser vista como uma base de dados central que armazena todas as informações de uma ""coisa"", mas esta é apenas uma das possíveis implementações. Para coisas maiores dentro do sistema (por exemplo, um sistema de monitoramento de ciclo de vida de dispositivo que colhe de dados de outras coisas) o gerenciamento de dados pode ser um data warehouse ou até mesmo um farm de dados completo. A aplicação da camada de gerenciamento de dados depende fortemente do caso de uso específico para a coisa o qual se está trabalhando. O gerenciamento de contexto define a lógica de negócios central dentro da RILA e é responsável por seis tarefas: Definir os objetivos da coisa; Consome as situações de contexto de outras coisas; Produz a (própria) situação de contexto da coisa; Avalia a (própria) situação de contexto em direção ao objetivo; Dispara ações que ajudem a cumprir o objetivo de acordo com as regras avaliadas; Publica situações de contexto para outras coisas. De acordo com estas tarefas podemos dividir a gestão de contexto em oito componentes, como mostrado na figura a seguir. Motor de Regras / Inteligência Artificial (IA) : Define e gerencia todas as regras necessárias para a avaliação de contexto. Isso inclui o objetivo (que é basicamente como um conjunto de regras), bem como as regras para criar a situação de contexto e as ações. Módulo de Integração de Situação de Contexto : escuta situações de contexto de outras coisas e integra as situações de contexto recebidas. Módulo de Integração de Ação : ações recebidas de outras coisas são avaliadas e repassadas à camada de gerenciamento de dispositivos por esse componente. As regras precisam ser consideradas pois definem em quais situações uma ação recebida de outra coisa pode ser transmitida para disparar um atuador. Módulo Criador de Situação de Contexto : coleta dados do sistema e constrói as situações de contexto. Isto também pode ser conduzido por regras. Módulo Criador de Ações : similar ao Módulo Criador de Situação de Contexto, objetos de ação tem que ser criados quando acionados durante a avaliação de regras. Módulo Publicador de Situação de Contexto : fornece situações de contexto para a camada de integração de coisas. De acordo com o nível de sofisticação da implementação, o publicador de situação de contexto pode fornecer um conjunto de situações de contexto para coisas diferentes que são assinantes dos eventos ou uma situação de contexto para todos. O Módulo Publicador de Situação de Contexto tem que cuidar de níveis de permissão de dados para outras coisas. Apenas outras coisas confiáveis devem receber informações de contexto selecionadas. Além disso, este módulo deve se encarregar de definir os esquemas de situação de contexto que são comunicados a outras coisas que desejam se inscrever para receber seus eventos. O esquema é usado para avaliar se uma coisa é capaz de se comunicar com outra coisa. Módulo de Publicação de Ações : similar ao módulo publicador de situação de contexto este módulo é responsável por comunicar ações para a camada de integração de coisas para ser comunicado a outras coisas. Além disso, os esquemas de ação são geridos por este componente. Módulo de Avaliação de Contexto : avalia as regras usando a situação de contexto (atual) e dispara ações que são comunicadas para os dispositivos ou para o Módulo Criador de Ação. O Módulo Criador de Ação, por sua vez, passa as ações criadas para a Publicador de Ações que comunica as ações para outras coisas. Uma forma de avaliar regras de forma simples é criar árvores de decisão a partir das regras definidas pelo mecanismo de regras. A arquitetura concreta e a complexidade da funcionalidade oferecida dependem fortemente do caso de uso para a coisa em desenvolvimento. Especialmente o motor de regras e o componente de inteligência artificial podem não precisar ser muito sofisticados para as coisas menos inteligentes (por exemplo, um geladeira). No entanto, para coisas que coletam informações de contexto de outros sistemas estes componentes serão muito sofisticados. Um exemplo de maior sofisticação pode ser no uso em ciência de dados e técnicas de mineração dados. A Camada de Integração de Coisas é responsável por encontrar outras coisas e se comunicar com elas. Uma vez que duas coisas se encontram elas têm que passar por um mecanismo de registro. A Camada de Integração de Coisas precisa avaliar se é possível realizar a comunicação com a coisa que está chegando. Com esse intuito, a situação de contexto e/ou esquemas de ação tem que ser comparados. Estes são fornecidos pela Camada de Gestão de Contexto. Se o esquema de correspondência é avaliado de forma positiva, a coisa pode notificar a outra coisa sobre a nova situação de contexto ou criação de ação. As situações e ações de contexto a serem comunicadas a outras coisas são fornecidos pela camada de gerenciamento de contexto. O registro da coisa pode ser feita em um componente central ou pela própria coisa (por exemplo, a varredura de rede com auto-discovery ). A camada de integração de aplicativo conecta o usuário à coisa. Aplicativos que estão (diretamente) em cima da arquitetura RILA estão localizados aqui. A integração de aplicações pode ser vista como uma camada de serviço, ou até mesmo como um simples interface de usuário no topo da pilha. A implementação concreta da camada depende do caso de uso. Neste momento concluímos a explicação sobre as camadas. Agora vamos lembrar os desafios das camadas transversais e dar uma olhada em segurança. Quando construímos um sistema de Internet das Coisas temos de considerar a segurança em todas as camadas. As fontes de ataque de ataque precisam ser identificados a fim de mapear padrões de segurança adequados. As seguintes fontes de ataque podem ser identificados. Usuário: O usuário final é uma possível fonte de ataque, pois pode afetar o sistema seja de propósito ou sem o seu conhecimento. Um ataque comum deste tipo é um ataque de phishing, que tenta adquirir informações confidenciais da vítima. Interface Web: Se o aplicativo oferece uma interface web, então esta pode ser sujeita a ataques ""convencionais"", como injeção de SQL ou XSS. O OWASP (Open Web Application Security Project) criou uma lista dos 10 cenários de ataque mais provável contra sites. A Coisa : Dispositivos inteligentes se comunicam muitas vezes com um sistema externo por meio de um aplicativo, que se baseia em alguma forma de sistema operacional. As duas dependências principais são o próprio app que pode não oferecer mecanismos de segurança suficientes ou o sistema operacional que pode ser invadido ou infectado. Componentes de hardware de baixo nível: Quando consideramos componentes de hardware e a segurança que eles oferecem, devemos levar em consideração seu poder computacional. A dependência principal é representada por dispositivos de baixa potência que simplesmente não têm o poder de processamento necessário para fazer uma comunicação criptografada segura. Quando se trabalha com vários sensores, pode-se eliminar os valores discrepantes para obter uma imagem precisa, mas a segurança não pode ser alcançada. Se a exatidão dos dados fornecidos pelos sensores é essencial, então é necessário um hardware mais potente, aumentando o custo de aquisição por uma ordem de grandeza. Canais de comunicação : proteger o canal de comunicação depende do protocolo usado. Iremos discutir os protocolos relevantes para a Internet das Coisas e o que eles oferecem para proteger a comunicação: RFID e NFC : A comunicação entre a etiqueta e o leitor é sem fio e pode ser facilmente escutada. Utilizar criptografia de dados é, portanto, essencial. Os algoritmos de criptografia simétrica que hoje são considerados suficientemente seguros são 3DES e AES-128. Ao gravar dados em uma nova etiqueta, a chave de autenticação padrão deve ser alterada. A gestão de chaves para as etiquetas é feito pelo sistema que controla o leitor. As etiquetas RFID em si são muito variadas e com isto, a segurança também deve ser levada em conta quando adquiri-las. A etiqueta Mifare Plus, por exemplo, é uma atualização da etiqueta Mifare Classic, porque oferece criptografia AES-128. A etiqueta Mifare Classic usa um algoritmo proprietário com base em uma chave de 48 bits que já foi violado; Zigbee: A comunicação entre um dispositivo Zigbee e a aplicação é protegida porque o algoritmo usado para criptografia é o AES-128. Por outro lado, a troca de chave inicial deve ser considerada insegura. Quando um novo dispositivo é adicionado à rede, a chave é enviada para o dispositivo na forma de texto aberto (não criptografado) e pode ser encontrada por sniffers , se o timing for correto; Thread: A comunicação entre dois dispositivos Thread é protegida por uma criptografia AES . O estabelecimento de chaves entre um novo dispositivo e a aplicação é feito de forma segura usando um algoritmo de troca de chaves. Fontes de ataque também podem ser agrupados em tipos de ataques mais técnicos que miram componentes específicos do sistemas. São eles: Autenticação; Autorização; Validação de autenticidade: assinatura para mensagens; Mecanismos de troca de chaves; Encriptação; Configurações - podem ser uma ameaça de segurança para configurações ruins e que sejam padrões; Bibliotecas de terceiros: quando não atualizados podem conter vulnerabilidades e exploits conhecidos; Segurança de rede. O Triângulo da Segurança nos mostra o dilema de escolher a quantidade certa de segurança de acordo com nosso caso de uso. Este triângulo de alguma forma representa um compromisso que ocorre em todos os casos de uso. Você só pode escolher um ponto no interior do triângulo que representa o que você quer ou precisa em termos de requisitos de segurança, custo e de negócios. Vejamos alguns exemplos: Exemplo 1 : Banco Acme constrói um cofre: É crucial usar hardware seguro para este caso de uso, que não pode ser adulterado. Para alcançar o máximo de cobertura em requisitos de negócios e de segurança, os custos vão aumentar muito. Exemplo 2: O fazendeiro Billy Bob quer comprar alguns sensores novos para saber em seu smartphone como sua lavoura está indo, mas ele não precisa de um monte de segurança. Por agora, o fazendeiro Billy Bob tem seus requisitos cumpridos, teve poucos custos e pode seguir vivendo feliz. Bem, pelo menos até que o filho do fazendeiro Jimmy Junior, que foi estudar engenharia da computação se gradue... Portanto, encontrar as medidas de segurança adequadas ao longo da arquitetura completa é sempre uma caminhada na corda bamba, uma vez que os requisitos de negócios e os custos muitas vezes se contradizem com relação às medidas de alta segurança. Além disso, pode acontecer que certos requisitos técnicos nos restrinjam de utilizar as mais altas medidas de segurança, como por exemplo dispositivos de baixa potência que não são capazes de aceitar uma certa sobrecarga ao enviar pacotes, pois isso resultaria em consumir mais energia. Neste artigo, buscamos mostrar que é possível quebrar o mundo da Internet das Coisas em um nível mais compreensível. Técnicas de computação contextual ajudam a fazer certas partes deste mundo mais compreensíveis. Em um próximo artigo mostraremos como casos de uso podem ser derivados da arquitetura de referência RILA para fornecer um quadro mais completo sobre como a RILA pode realmente nos ajudar com a implementação de sistemas da Internet das Coisas. Sobre os Autores Hannelore Marginean é desenvolvedora na Senacor Technologies, Alemanha. Entre outras atividades, gosta de descobrir tecnologias inovadoras e passou um tempo pesquisando sobre as possibilidades, os riscos de segurança e os benefícios da IoT. No seu tempo livre, ela gosta de pintura e de tocar violão. Daniel Karzel é consultor técnico na Senacor Technologies, Alemanha. Durante seus estudos de mestrado em computação móvel trabalhava com Computação Contextualizada e a Internet das Coisas. Além de ficar pensando sobre arquitetura de software para o futuro, em suas horas vagas, gosta de tocar acordeon e viajar pela China. Tuan-Si Tran é desenvolvedor na Senacor Technologies, Alemanha. É entusiasta de interfaces visuais e interessado em tecnologia de ponta. Em seu tempo livre gosta de jogar tênis.",pt,43
447,2829,1481025637,CONTENT SHARED,6104598399077154797,-3643155458357242906,-3145802272298965700,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.98 Safari/537.36",MG,BR,HTML,https://medium.com/cocoaheads-br/https-medium-com-calebeemerick-como-consumir-conteudo-de-qualidade-em-ios-c3dec62b8993,como consumir conteúdo de qualidade em ios - cocoaheads br,"Quando iniciei minha jornada em Swift, saindo da versão 1.1 para a 1.2, tive muuuuuitos problemas. Era a primeira vez que estava aprendendo uma linguagem em desenvolvimento (mas não sabia disso). Achei então um livro de Swift na Casa do Código . Comecei a ler, fazer os exemplos e os códigos davam erros constantemente, o que me deixava maluco. ������ Procurava no Google e no Youtube por Swift e aparecia ela, Taylor Swift...�� Hoje a linguagem está mais madura, até a data dessa publicação está na versão 3.0.1. A comunidade cresceu absurdamente, é muito mais fácil encontrar conteúdo, talvez nem sempre de qualidade, mas já é alguma coisa. Chega de historinha, hora de falar onde encontrar conteúdo de qualidade relacionado à Swift e iOS. Pegue a pipoca��, porque o artigo promete! ( Ser bem grande ��). �� Livros Aqui estão alguns livros que vão desde o básico até o avançado. ���� Cereja do Bolo �� segundo Ezequiel França �� �� Github �� Medium Algumas pessoas que sigo e que costumam publicar com certa frequência. Uma dica legal é seguir as tags #Swift e #iOS �� Blogs Alguns dos melhores blogs para consumir conteúdo em iOS. �� Vídeos Para quem gosta de aprender com vídeo tutoriais, aqui têm alguns sites (a maioria pago) que abrangem isso. Eu mesmo sou assinante do Treehouse ��. �� Twitter Alguns dos Twitters mais influentes de iOS. Nem sempre tweetam sobre, mas vale a pena seguir, porque sempre tem novidade ;) ���� Versão americana do Douglas Fischer �� �� Eventos �� Links Interessantes Ufa! Acho que é isso. �� Se acha que está faltando alguma fonte que não foi mencionada, não hesite em comentar! Sugestões e críticas são bem vindas. Agradecimentos ���� Gostaria de agradecer o Douglas Fischer , Josenilton Cabral, Thiago Holanda , Diego Ventura , Tales pinheiro , Marcelo Fabri , valentebruno , Fernando Bunn , Ezequiel França , Daniel Bonates , Bruno Bilescky e toda a galera do Cocoa Heads BR . ������ Espero que tenham gostado. Obrigado aos envolvidos e leitores! ❤",pt,43
448,2954,1484321367,CONTENT SHARED,-6444494220871855741,-4028919343899978105,5195779277161351081,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36",SP,BR,HTML,http://www.techtudo.com.br/noticias/noticia/2016/10/novo-padrao-de-rede-cabeada-oferece-ate-5-gbps-de-velocidade-para-internet.html,novo padrão de rede cabeada pode oferecer até 5 gbps transferência de dados,"Um novo padrão para redes cabeadas, que promete melhorar a transmissão de dados, recebeu aprovação internacional do Instituto de Engenheiro Eletricistas e Eletrônicos (IEEE) nesta semana. Conhecido como '802.3bz-2016, 2.5G / 5GBASE-T', o novo padrão pode variar entre 2,5 Gbps a 5 Gbps de velocidade, superior aos 1 Gbps das redes mais comuns. Lembrando que o IEEE também tem representação no Brasil. MTU: entenda como recurso do roteador afeta velocidade da Internet Um dos diferenciais da tecnologia é que ela não necessita de um cabo especial para chegar a essas taxas de transferência. Com o novo padrão é possível atingir 2,5 Gbps com cabos Cat 5e e a velocidade máxima com os cabos Cat 6. Novo padrão funciona em cabos Cat 5e e Cat 6 (Foto: Divulgação) Download grátis do app do TechTudo : receba dicas e notícias de tecnologia no Android ou iPhone O novo padrão foi desenvolvido para ser um meio termo entre as redes de 1 Gbps e as de 10 Gigabit Ethernet, que necessitam de cabeamento especial - consequentemente mais caro - e são mais comuns em ambientes corporativos, como em data centers e servidores. Os pesquisadores conseguiram desenvolver uma tecnologia semelhante às redes 10GBASE-T, porém com uma largura de banda menor - entre 100 MHz e 200 MHz, contra 400 MHz. Com isso, os cabos não necessitam de uma blindagem de alta qualidade, permitindo o uso de cabos Cat 5e, comuns em redes domésticas, e Cat 6. Atualmente, as redes domésticas contam com equipamentos - roteadores e placas de rede - que podem atingir em torno de 1 Gbps de taxa de transferência. Por outro lado, as redes Wi-Fi que utilizam o protocolo AC podem chegar a mais de 6 Gbps. Com o novo padrão, a taxa de transferência de dados dentro da rede cabeada será mais do que o dobro da atual. Assim, a troca de grandes arquivos, como filmes em 4K, ou o acesso a NAS ( Network Attached Storage ), para armazenar arquivos em uma rede de computadores, será muito mais rápido. Lembrando que isso não influenciará na velocidade da Internet, que depende do provedor contratado. No entanto, o padrão 2.5G/5GBASE-T ainda deve demorar algum tempo até se tornar popular, visto que ainda são vendidos roteadores sem portas Gigabit. Qual roteador comprar? Comente no Fórum do TechTudo.",pt,43
449,1993,1470395785,CONTENT SHARED,-2740396993975951758,7645894863578715801,3144696150872601726,,,,HTML,http://sensedia.com/blog/soa/soa-e-apis-o-melhor-seria-soa-com-microservicos/,soa com microserviços - sensedia,"SOA (arquitetura orientada a serviços) é uma arquitetura que provê uma cadeia de valores através de serviços corporativos. Estes serviços são compostos pela camada de processos e agregam as camadas de informação (dados /semântica) e aplicação. Já a abordagem de microserviços tem a ver com granularidade e evolução arquitetural tecnológica da camada de serviços. Renove suas aplicações monolíticas usando SOA com microserviços Analisando o cenário da maioria das empresas e voltando no tempo, antes de serviços serem criadas, aplicações monolíticas proviam todos os processos corporativos, como uma ERP. Esse cenário era e ainda é desenvolvido por muitas empresas. Essas aplicações contém uma massa grande das 3 camadas citadas anteriormente ( processos, informação e aplicação ). A realidade é que nenhuma ou poucas empresas tem apenas um monolítico no seu conjunto de aplicações. Existem aplicações legadas, pacotes de terceiros e uma porção de aplicações que são criadas no meio do caminho. Muitas vezes essas aplicações proveem de maneira diferente essas 2 camadas iniciais, gerando uma sobreposição, replicação e redundância nas mesmas. Para resolver este problema, em um primeiro momento, a arquitetura SOA entra como uma camada de serviços superior às 2 camadas (Informação/aplicação), definindo regras e padrões para a conectividade dessas aplicações monolíticas e uma padronização de informação ou modelo canônico para que os processos de negócios possam compor esses serviços de maneira singular, sem replicação ou regras divergentes. Ou seja, para acabar com a redundância e prover de maneira organizada toda essa cadeia de valores embutida em aplicações monolíticas , serviços corporativos são criados em uma camada superior chamada Camada de Serviços. Com a evolução tecnológica e arquitetural, ALM e provisionamento automatizado (relacionado a Devops), muito se pensou sobre como evitar essa redundância e como evitar o retrabalho. Os microserviços surgiram com um pensamento simples: se a empresa já vê valor nos serviços corporativos , e se eles já são os owners da informação, por que eu precisaria da camada de aplicação em baixo da camada de serviços? Ou seja, por que continuar a criar replicação de dados e redundância em processos , ou até mesmo visões diferentes dos mesmos processos em aplicações distintas? Na prática, esse problema arquitetural dificulta a criação da camada superior, já que para prover processos e informações consolidadas em uma visão única, são precisos esforços enormes frente às diferentes abordagens utilizadas no Portfólio de aplicações. O resultado Com uma simples modelagem, a camada de aplicação foi retirada desse modelo, e a camada de serviço virou realmente o owner da informação , e não um simples consumidor das informações contidas nas aplicações. Nesse cenário as aplicações são agora canais que consomem os microserviços expostos via APIs da camada de serviços, e ela agora é uma camada agregadora de informação e composta por processos de negócio. Se um dos processos de negócio ou semântica da informação é alterada, basta ir ao serviço e alterar essa regra. Não é mais necessário alterar aplicações, integrações, camada de serviços e processos. Também não há sobreposição de processos de negócio ou implementações diferentes das mesmas informações, pois, cada pedaço, cada contexto de informação é realizado por um microserviço distinto, e apenas ele é o owner dessa informação. Lógico que ainda vão existir aplicações legadas, modelos de convivência e modelo canônico , porém, uma arquitetura event-driven pode ser uma abordagem para sanar estes gaps! Mas este é assunto para outro post �� Artigo publicado no meu Linkedin A imagem de capa é parte da capa do livro SOA with REST, do Thomas Erl. Leitura recomendada ��",pt,43
450,1981,1470329568,CONTENT SHARED,-6273159470243757969,-1443636648652872475,6669758861106392225,,,,HTML,http://www.theverge.com/2016/8/4/12369494/descartes-artificial-intelligence-crop-predictions-usda,this startup uses machine learning and satellite imagery to predict crop yields,"Mark Johnson wants to beat the United States Department of Agriculture at its own game: predicting yields of America's crops. The USDA puts boots on the ground, deploying hundreds of workers to survey thousands of farms a month ahead of the October corn harvest, America's biggest crop. Johnson's startup, Descartes Labs , has just 20 employees, and they never leave the office in Los Alamos, New Mexico. Instead, Descartes relies on 4 petabytes of satellite imaging data and a machine learning algorithm to figure out how healthy the corn crop is from space. Corn yield prediction is big business in the US. Billions of dollars are at stake along the ag supply chain each year as corn starts to come out of the ground in August. Grain elevator operators, ethanol producers, commodities traders, hedge funds, insurance companies, and even the farmers growing the corn will all look to the USDA's August crop report being released August 12th to try and understand how the supply side of the corn market will behave. Descartes says it can consistently out-predict the USDA's corn estimates Descartes, which launched in 2013, began releasing corn yield estimates ahead of the USDA's August crop report last year. Johnson says its model has consistently out-predicted the USDA's estimates at the national level at every point in the growing season. It beat the accuracy of the USDA's 2015 August predictions by a percentage point, according to numbers provided by Descartes. Now, Johnson says their algorithms have gotten even more precise, with a 2.5 percent average margin of error when run through historical backtests. Johnson argues that the depth and frequency of data his company is able to analyze is a game-changer in crop prediction. ""What's great about our techniques is that traditionally you have to talk to tons of farmers in the US to get a USDA-style number,"" Johnson tells The Verge . ""With machine learning techniques, with us, we look at tons of pixels from satellites, and that tells us what's growing."" shrinking sensors and cheap cloud computing It's a familiar narrative at the moment - companies across industries are tapping into large data sets accumulated by the proliferation of shrinking sensors and processing them using cheap cloud computing services from companies like Google and Amazon. The Weather Company, for example, just announced its hyper-local weather forecaster called Deep Thunder, which uses machine learning to crunch through historical weather reports to predict future conditions. But big data and machine learning are just one side of the equation with Descartes. The rise in popularity of nanosatellites - small satellites roughly the size of a shoebox - over the past five years has opened up a broad realm of possibilities for Descartes and startups like it. In the past, if a company wanted satellite imagery data, it would turn to US government-run satellite programs like Landsat or MODIS, which image the entire globe at 20- to 30-meter resolutions roughly once a week. Now new nanosatellite constellations, like the one run by satellite imaging startup Planet , are taking snapshots of the entire globe at 3- to 5-meter resolutions every day. The amount of imaging data being collected right now is enormous. ""One way to think about it,"" says Johnson, ""is that it took Landsat over 40 years to collect under a petabyte of data with 7 satellites. Planet will easily produce over a petabyte a year."" Companies have been increasingly using this data to analyze global trends. Satellite imagery provider DigitalGlobe helped Facebook create a map of 2 billion disconnected people across the world earlier this year. Orbital Insights tracks industrial development in China and monitors the parking lots of over 50 US retailers from space to gain insight into store traffic. Johnson and his partners chose to track agriculture for a few reasons. First, food scarcity and global climate change are pressing issues. Second, year's-worth of data sets already existed from images taken by Landsat and MODIS that could be used to train their machine learning models. Third, corn grows slowly, and farmers can benefit from observing it with extra-spectral bands like infrared, which both older and newer generations of imaging satellites record. Measuring chlorophyll from space Finally, and most importantly, Johnson says, it was a hard problem. ""It's not like a satellite looking at all the Walmart parking lots and picking out the cars,"" says Johnson. ""That's a problem of automation; it's something a human could do. What we do is an automation of what humans can't do."" Descartes uses spectral information, not visible to the human eye to measure chlorophyll levels. ""There are well-established methods for getting a proxy for crop health from chlorophyll levels,"" says Josh Alban, Planet's vice president of business development. Planet, which is formal partners with Descartes, provides it with data to calculate yield estimates and helps it build custom products for corporate clients. Johnson wouldn't provide the names of any of his company's clients, saying that people on the supply side of the ag business are very tight-lipped about where they get they get data. Sources familiar with the industry weren't surprised. Descartes says it analyzes satellite data of every single farm in the US on a daily basis (provided there is no cloud cover) and updates its corn yield prediction every two days. The USDA only updates its forecasts once a month. While the USDA provides country and statewide predictions, Descartes delivers both in addition to county-level predictions, which the USDA only provides at the end of the season, when those numbers matter less. Damien Lepoutre, founder of Geosys, a global crop and analytics company that has operated for nearly three decades, says the ability to deliver local estimates is paramount. ""One thing I've seen over the past 35 years is the complexities of agriculture,"" Lepoutre said. ""Agriculture is always local. You don't have the two same soils in different places. Every time is a bit different."" The hard part is building your neural network Geosys also analyzes spectral satellite data, as do other large players in the industry. But Lepoutre thinks recent advances in technology, including the specific focus on machine learning, give startups a better chance at surviving than they had in the past. It all comes down to a company's model, says Chris Curran, chief technologist at PriceWaterhouseCooper. ""The hard part is the training of your algorithm, the building of your neural network to actually create value out of your data."" ""With better satellites, better access to data, and more powerful algorithms, our models will continue to get better,"" Johnson says. And as Descartes accomplishes that, Johnson says they'll begin moving on to other crops (Descartes has already begun tracking soybeans) and other regions such as Brazil and Argentina, the Black Sea region, China, and the EU. After that, Johnson says his company has ambitions to better understand the planet as a living organism. Descartes, he says, aims to ""understand our natural resources, understanding how those resources move around, and then how we as humans change the planet.""",en,43
451,3031,1485777754,CONTENT SHARED,2508620055362300376,3118792477913513242,-3154840927888771594,Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:50.0) Gecko/20100101 Firefox/50.0,SP,BR,HTML,https://medium.com/@iammert/using-diffutil-in-android-recyclerview-bdca8e4fbb00,using diffutil in android recyclerview,"DiffUtil is a utility class that can calculate the difference between two lists and output a list of update operations that converts the first list into the second one.It can be used to calculate updates for a RecyclerView Adapter. Most of the time our list changes completely and we set new list to RecyclerView Adapter. And we call notifyDataSetChanged to update adapter. NotifyDataSetChanged is costly. class solves that problem now. It does its job perfectly! Lets create a usecase for that. Lets say we have a person list which we add persons randomly. Then we sort the list by age and update recyclerview. I am going to use static data to keep implementation simple. DataProvider.class Here is our player comes to the ring. DiffUtil.Callback is an abstract class and it has 4 abstract methods and 1 method non-abstract method. Let explain them shortly. DiffUtil.Callback Methods getOldListSize() : It returns the size of the old list. getNewListSize() : Returns the size of the new list; areItemsTheSame(int oldItemPosition, int newItemPosition) : Called by the DiffUtil to decide whether two object represent the same Item.If your items have unique ids, this method should check their id equality. areContentsTheSame(int oldItemPosition, int newItemPosition) : Checks whether two items have the same data.You can change its behavior depending on your UI. This method is called by DiffUtil only if areItemsTheSame returns true. getChangePayload(int oldItemPosition, int newItemPosition) : If areItemTheSame return true and areContentsTheSame returns false DiffUtil calls this method to get a payload about the change. For my example I didn't use payload object but if you want to use it you can go through this example . MyDiffUtilCallback.java Update method in my RecyclerViewAdapter That's all. We call dispactUpdatesTo(RecyclerView.Adapter) method and adapter will be notified about the change. References",en,43
452,1552,1467116818,CONTENT SHARED,-6592496241044177853,3302556033962996625,-8391507522593787613,,,,HTML,http://android-developers.blogspot.com.br/2016/06/create-intelligent-context-aware-apps.html,"create intelligent, context-aware apps with the google awareness apis | android developers blog","Posted by Bhavik Singh, Product Manager Last month at Google I/O 2016 we announced the new Google Awareness APIs, enabling your apps to intelligently react to user context using snapshots and fences with minimal impact on system resources. Today we're proud to announce that the Google Awareness API is available to all developers through Google Play services. Using 7 different types of context-including location, weather, user activity, and nearby beacons-your app can better understand your users' current situations, and use this information to provide optimized and customized experiences. The Awareness API offers two ways to take advantage of context signals within your app: The Snapshot API lets your app easily request information about the user's current context. For example, ""give me the user's current location and the current weather conditions"". The Fence API lets your app react to changes in user's context - and when it matches a certain set of conditions. For example, ""tell me whenever the user is walking and their headphones are plugged in"". Similar to the Geofencing API, once an awareness fence is registered, it can send callbacks to your app even when it's not running. As a single, simplified surface, the Awareness APIs combine optimally processed context signals in new ways that were not previously possible, providing more accurate and insightful context cues, while also managing system resources to save battery and minimize bandwidth. We've worked closely with some of our partners, who have already found amazing ways to integrate context awareness into their apps: Trulia ,an online residential real estate site, uses our Fence API to suggest open houses. When the weather is perfect and the user is walking around near a house they are interested in, Trulia sends a notification reminding them to stop by. This sort of tailored notification can help users engage with open houses at the perfect time for them. SuperPlayer Music , on the other hand, uses our Snapshot API and Fence API to suggest the perfect music to match your mood. Whether you're just finishing up a run and beginning to stretch, setting off on a long car ride, or just getting to the gym, their assistant can understand your context and suggest the right playlist for you. With our initial set of signals and our awesome partners, we're just getting started with the Awareness APIs. Join us on a journey to build tailored experiences within your apps, by getting started with the Google Awareness API developer documentation , and learn more by watching our Google I/O session",en,43
453,1104,1464200745,CONTENT SHARED,-3170783292462058942,2146546851324474301,-1095512429704997588,,,,HTML,https://www.ecommercebrasil.com.br/artigos/nao-existe-frete-gratis/,não existe frete grátis!,"Mais uma edição do Dia do Frete Grátis no Brasil está se aproximando. Esta é mais uma das diversas datas-chave para o e-commerce por aumentar consideravelmente o número de acessos e de compras nos sites de varejo online. É uma ótima oportunidade dos clientes adquirirem seus produtos economizando com transporte, porém deve-se ter consciência que o serviço inteiramente grátis não existe. Se ele não está sendo cobrado do cliente final, alguém está arcando com esse custo. A política do frete grátis surgiu na década de 1990, nos primórdios do desenvolvimento deste segmento nos Estados Unidos. Naquele tempo, a maioria dos varejistas oferecia essa opção como forma de estimular clientes a comprar nessa modalidade até então desconhecida. Funcionou no início. Era muito cômodo para os consumidores comprarem seus produtos, recebê-los em casa sem o menor esforço e não arcar com o custo deste serviço. Além disso, custear o serviço de entrega não afetava substancialmente os lucros das empresas daquele país. Ao aterrisar no Brasil, o modelo de negócios se baseou na experiência americana e passou a oferecer o frete grátis. No entanto, vale ressaltar que a logística brasileira é muito deficitária e, por isso, as despesas com transporte são maiores aqui. Sendo assim, muitas empresas do e-commerce brasileiro passaram a operar no vermelho devido à falta de planejamento logístico, o que fez com que os índices de frete grátis venham caindo ano após ano. Neste cenário, as empresas têm preferido repassar os custos do frete para os clientes. Mas de que forma fazer com que o cliente pague esse valor sem que haja o abandono de carrinho? A saída encontrada pelas empresas foi a oferta de diversas opções de entrega aos clientes, cada uma com um custo diferenciado baseado nos benefícios oferecidos. Por exemplo, entrega no mesmo dia cobra um valor mais elevado, já a compra sem necessidade de emergência na entrega possui um valor mais baixo ou gratuito. Apesar de estar diminuindo gradativamente, a oferta de frete grátis ainda é muito comum no Brasil, pois ainda é uma tática eficiente para atrair e fidelizar clientes, aumentar o tíquete médio e investir em categorias específicas. Quando bem programada, uma campanha de fretes pode trazer diversos benefícios: Conquista de clientes Você pode oferecer frete gratuito na primeira compra para atrair novos clientes, limitar o frete grátis para determinados produtos (geralmente os de maior valor), em datas comemorativas, oferecer cupons de desconto ou gratuidade do frete na próxima compra, etc. Essas ações podem garantir uma boa experiência de compra aos consumidores, que voltarão a comprar e se tornarão promotores da sua marca. Aumento do tíquete médio Diversas pesquisas demonstram que os clientes se dispõe a comprar mais produtos desde que recebam o frete grátis. Então oferecer o serviço a partir de um valor mínimo estimula os consumidores a consumir mais visando a gratuidade do transporte de seus produtos. Estímulo do consumo de um público-alvo O frete pode ser um aliado para promover as compras em uma parcela específica da população. Pode ser feita uma ação voltada à uma região específica do país (geralmente a que possui maior recorrência de vendas ou a mais barata para levar os produtos). Vale lembrar que devido à baixa qualidade da logística no Brasil, muitas pessoas preferem pagar mais caro por um serviço de qualidade. O essencial é garantir a satisfação do cliente e a oferta de condições de frete favoráveis a um custo acessível ajuda, e muito, neste aspecto. Aproveite as soluções inovadoras disponíveis no mercado e elabore campanhas de frete segmentadas e mais eficientes, de forma que não afetem a margem de lucro de sua empresa.",pt,43
454,1864,1469456882,CONTENT SHARED,1567984561327445971,-4465926797008424436,-3498187659449117655,,,,HTML,https://medium.com/android-dev-br/testes-unit%C3%A1rios-vs-aceita%C3%A7%C3%A3o-30691fc8578d,testes unitários vs aceitação - android dev br,"Testes unitários vs aceitação Constantemente recebo perguntas como: Sicarelli, o que devo testar unitariamente ou não, no Android? ou até Devo gastar meu tempo fazendo testes de aceitação ou devo apenas focar nos unitários? Antes de responder essas perguntas e outras que você possa ter, vamos entender melhor o contexto de teste no Android. Linha do tempo Todos os testes no Android são baseados no JUnit . De modo geral, um teste em JUnit é um método, anotado, que contém sentenças que testam (ou provam) partes da sua aplicação. Você organiza seus métodos de testes em classes chamadas test cases , ou até mesmo organizar seus testes usando alguns suítes. No JUnit , você constrói uma ou mais classes e usa algum test runner para executá-los. Esses test runners , são processadores de teste que nos ajudam (lê-se fazem tudo por nós) a instanciar nossos testes em tempo de execução, na execução do próprio teste e a geração de relatórios de resultados. Esses processadores são anotados com o famoso no início do seu test case . Mas, há muitos anos, todos nós constantemente monitorávamos em qual estado andava os testes no Android, tentando descobrir uma maneira fácil e integrada de se testar. Isso por que, até alguns anos atrás, a única forma suportada de rodar testes era executando-os em alguma Dalvik VM , seja em um device ou emulador . As ramificações que isso causava fazia seus testes unitários (que devem ser leves e executados rapidamente) demorarem bastante por depender da demora de ""subir"" um emulador, o que é a única opção quando se fala de um ambiente com CI. Mesmo se você tivesse a audácia de configurar e deixar pronto um AVD, ainda era necessário fazer mais mágica para exportar seus relatórios em XML do JUnit de dentro do emulador. Um pé no saco, né? Para nossa sorte, isso era solucionado pelo Roboletric , que basicamente move a execução dos seus testes da Dalvik para uma JVM qualquer, e isso resolve lindamente a sua dependência com algum device/emulador e dos XML's de resultados, tornando realidade um CI em seu projeto. Porém, isso era uma pequena dor de cabeça, obrigando te dar a volta ao mundo para retirar isso da teoria e bota-la em prática. Mas, graças ao time de Android Tools e para a nossa alegria, o Android Studio 1.2 removeu qualquer workaround ou hack para utilizar o Roboletric . Essa solução foi introduzida na versão 1.1 , que tinha uma flag experimental para habilitar testes unitários, e na versão 1.2 essa flag não era mais experimental. Como funciona? Basicamente , o plugin do Gradle irá compilar todo código fonte encontrado no caminho src/test/java e irá executá-lo usando os mecanismos de teste do próprio Gradle . Em tempo de execução, os testes serão executados encima de uma versão modificada (reduzida) do android.jar , onde todos os modificadores finais foram retirados, o que significa a possibilidade de usar bibliotecas de mocking , como nosso querido Mockito . Tipos de teste Tudo bem, entendemos qual foi a trajetória dos testes no Android, mas a pergunta ainda continua: o que devo testar unitariamente? Teste unitário Um teste unitário testa uma pequena unidade de uma funcionalidade, geralmente um método ou uma função (dado uma classe com um estado particular, chamando o método X dessa classe, Y deve acontecer). Eles devem ser focados em uma funcionalidade em particular (quando chamar o método de popular informações na tela, e os dados estiverem vazios, deve ser lançado alguma exceção). Idealmente, todos os testes devem ser executados na memória, o que significa que seu teste e o código que você está testando não devem realizar: Chamadas externas para colaboradores (bibliotecas) Acesso a rede Bater em algum banco de dados Usar arquitos de sistemas Rodar uma thread etc. Qualquer tipo de dependência que deixe devagar/difícil de entender/ inicializa/manipula seus testes deve estar devidamente tratado através de stubs e mocks utilizando as técnicas apropriadas para que você foque no que a unidade do seu código está fazendo, não no que sua unidade está dependendo. Em suma, os testes unitários são os mais simples possíveis, faceis de debuggar , de confiança (devido a fatores externos reduzidos), rápido para executar e ajuda a provar que mesmo a menor parte do seu código/funcionalidade está funcionando como planejado antes que sejam integradas . A grande questão é que, embora você possa provar que eles funcionam perfeitamente isoladas, as unidades de código podem ""explodir"" quando combinados, o que nos leva a... Teste de integração Os testes de integração tem como foco juntar/combinar as unidades do seu código e testar se o resultado dessa combinação funciona corretamente, seja em um só sistema ou a combinação de vários outros. Além disso, outra coisa que diferencia os testes de integração de testes unitários é o ambiente. Testes de integração podem e irão usar threads , acessar o banco de dados ou fazer o que for necessário para garantir que todo o código e as diferentes alterações de ambiente irão funcionar corretamente. A principal vantagem é que eles irão encontrar bugs que os testes unitários não foram capazes de encontrar. Porém, a principal desvantagem é que os testes de integração ""tocam"" em mais código, são menos confiáveis, os testes falhados são mais difíceis de se diagnosticar e os testes são mais difíceis de manter. Também, testes de integração não necessariamente prova que sua funcionalidade funciona por completo. Talvez o usuário não se importe com os detalhes internos da sua aplicação, o que nos leva a... Testes funcionais Testes funcionais verificam uma característica particular comparando com os resultados para uma determinada entrada contra a especificação (esperado). Testes funcionais não se preocupam com os resultados intermediários ou efeitos colaterais, apenas o resultado final (eles não se importam que depois de fazer x , y objeto tem estado z ). Eles são escritos para testar parte da especificação, tais como, ""chamando a função calcula(x) com o argumento 2 , retorne 4"". Mas, em nosso mundo, testar se a funcionalidade funciona dessa maneira não garante que a experiência do usuário dentro do sistema está garantida, o que nos leva a... Testes de aceitação Um teste de aceitação padrão envolve a execução de testes em todo o sistema para garantir se a funcionalidade da aplicação satisfaz a especificação. Por exemplo. ""Clicando no botão de login, deverá mostrar um progresso na tela sumir quando o login for completado"". Não há continuidade real dos resultados, basta uma aprovação ou reprovação resultado. A vantagem é que os testes são escritos em uma ""língua"" ou contexto (dado situação X deve acontecer Y ) e garante que o software, como um todo, tem suas funcionalidades feitas e completas. A desvantagem é (como talvez vocês já tenham percebido) que, conforme seu teste move de uma camada para outra, ""toca"" em mais código e rastrear um erro no meio disso tudo pode ser muito difícil. Por exemplo, uma funcionalidade falha em 1% das vezes. Achar o motivo que causa esse 1% de erro pode ser muito difícil (ou até impossível) o que nos força a gastar horas e horas investigando o problema. Um conjunto de testes de aceitação é basicamente uma especificação executável escrito em uma linguagem específica que descreve os testes na linguagem utilizada pelos usuários do sistema. No caso do Android, um teste de aceitação requer o próprio Android, ou seja: você precisa de um device para provar que seu aplicativo está funcionando como esperado. Isso implica algo que comentei no início do post: seus testes irão demorar mais por toda essa dependência que você vai ter. Mas, qual devo utilizar? Aqui está a parte delicada. Por um lado, temos o teste unitário: rápido, dedicado a pequenas partes do seu código que garantem a funcionalidade individual dos seus componentes, possibilitando de maneira fácil e rápida identificar um bug específico, te dizendo o comportamento esperado e mostrando qual foi a falha. Porém, requer toneladas de testes e outra tonelada de mocks (sabendo que é difícil você não ter dependência do Android em algum método. Um mock do context já vira o DNA do seu teste) e, no caso de aplicativos para celular, temos casos de uso muito específicos que precisam garantir uma experiência agradável para todos os usuários de todos os telefones. Por outro, temos os testes de aceitação, que junto com o Espresso , garantem que sua funcionalidade será aprovada pelo usuário. Você irá cobrir muito mais partes do seu sistema evitando crashs específicos de plataforma e não precisando se preocupar em mockar muita coisa (tendo em vista que roda encima de algum device/emulador). Mas, quando acontece um erro, vai ficar muito difícil responder a pergunta: por que esse erro aconteceu? Conclusão Idealmente, quanto menos regras de negócio seu aplicativo ter ( unitários ), mais tempo você irá ter para testar casos de uso e garantir uma experiência limpa e fluida no seu aplicativo ( aceitação ). Pense em um cenário onde você tenha diversos clientes (Android, iOS ou até Windows Phone) e sua regra de negocio não esteja centralizada. Por exemplo: a exibição da data para o usuário final deve ser a mesma na plataforma, e cada plataforma vai ter um jeito diferente de tratar essa data, criando código de formatação e consequentemente criando testes unitários para garantir que independente do formato que a data vir do servidor, o sistema irá conseguir formatar de acordo com a requisição. Percebe que você pode evitar isso já retornando a data formatada do servidor que, por sua vez ,já tem um (e em só um lugar) teste unitário criado para garantir isso? Se falarmos das vantagens que temos possuindo as regras de negócios centralizadas em um só lugar, daria um belíssimo e extenso post. Mas, só com esse exemplo você provavelmente conseguiu entender que, em nossa realidade, é muito mais vantajoso criarmos mais testes de aceitação do que unitários, o que significa que nosso código está com poucas regras de negócios, assim podemos focar mais em entregar uma aplicação solida em qualquer device. Então, mão na massa e bora testar!",pt,43
455,1212,1464876125,CONTENT SHARED,-7812176982819372240,5660542693104786364,-6767030235092364675,,,,HTML,http://iradex.net/podcasts/setereinospodcast/,sete reinos,"Povo Westerosi! Continuamos nossa cobertura da 6ª temporada da série e falamos hoje sobre esse segundo episódio que trouxe tantas emoções. Ah, e dessa vez temos uma surpresinha no final do programa pra quem curte um papo com spoilers! Spoilers até o 2º episódio da 6ª Temporada da série. Links Sobre O Sete Reinos é um podcast quinzenal sobre o universo criado por George RR Martin para o livro As Crônicas de Gelo e Fogo. Aqui, abordaremos Game of Thrones (Guerra dos Tronos) como uma franquia, passando por literatura, TV, games, jogos de tabuleiro e demais escritos do velho Martin. Quando a série da HBO estiver em cartaz, faremos uma edição do podcast a cada dois episódios inéditos da série. Assine o Sete Reinos Contatos",pt,43
456,1746,1468367545,CONTENT SHARED,-5039153034159656707,5206835909720479405,5420738927372117137,,,,HTML,http://conquistecomhinode.com.br/mercado-de-cosmeticos-no-brasil/,mercado de cosméticos no brasil - setor que não conhece crise!,"Conquiste sua Liberdade Financeira Você também pode! Previous Next MERCADO DE COSMÉTICOS NO BRASIL - CRISE? ONDE? Não é segredo para ninguém que o mercado de cosméticos no Brasil , e de beleza em geral, desconhece esse negócio chamado crise! Cuidar do corpo tem sido prioridade não só para as mulheres, mas para muitos homens atualmente. Em outros tempos, bastava que a mulher levasse seu batom, blush e rímel que estava tudo certo, mas hoje a lista de itens básicos está muito maior. Mas o mercado de cosméticos no Brasil não é somente responsável pela beleza das mulheres, ele movimenta uma economia de milhões de reais, gerando renda, empregos e até mesmo incentivando a sustentabilidade. 113% DE CRESCIMENTO AO ANO Esse é o crescimento médio da Indústria de cosméticos no Brasil nos últimos 5 anos. Isso é fabuloso, é literalmente nadar contra a correnteza em um mercado tão conturbado. De 2010 a 2015 o mercado de cosméticos no Brasil cresceu 567% , passando de 72 mil para 482 mil profissionais em Janeiro de 2015. Seguindo essa média podemos ser otimistas e esperar um bom crescimento para o mercado de cosméticos no Brasil em 2016, mesmo diante da crise que o país enfrenta atualmente. Nós somos o 3º país em consumo de higiene pessoal, perfumaria e cosméticos no mundo, ficando atrás apenas dos EUA e da China. Veja abaixo duas reportagens da Globo mostrando como anda o mercado de cosméticos no Brasil! Agora nem tudo é um mar de rosas. Com o crescente aumento deste nicho de mercado, aumenta também a concorrência, tornando a disputa muito acirrada. Os clientes estão dispostos a gastar, mas em contrapartida estão cada vez mais exigentes. Já diziam os Nonatos: ""É fundamental saber além do fundamental"" . O crescimento da indústria de cosméticos se deve principalmente a inovação. Se você tem interesse em entrar nesse mercado, seja criativo, surpreenda, inove. Se você quer trabalhar apenas com o básico, com certeza perderá terreno para a concorrência. Há hoje em dia uma tendência cada vez maior na personalização dos produtos, que atendam as especificidades de cada tipo de cabelo, pele, idade e sexo e existem até lojas específicas para melhor atender as necessidades dos diferentes públicos. Veja aqui a linha completa de produtos que a Hinode oferece a você E como havíamos falado no início do artigo, o mercado de cosméticos no Brasil , assim como o de higiene e beleza, não atrai só mulheres. Estima-se que uma parcela entre 25% a 30% desse nicho, seja formado por homens e que esta parcela pode ser até maior, pois muitos deles por puro preconceito não assumem que usam cosméticos. ÍNDICE BATOM Você já ouviu falar do ""Índice Batom""? O termo foi cunhado pelo presidente da Estée Lauder , tradicional marca de cosméticos norte-americana, Leonard Lauder . É um indicador que se baseia em estatísticas de que as vendas de cosméticos sobem proporcionalmente com a queda do poder de compra dos consumidores . Com a queda das torres gêmeas o Estados Unidos enfrentou grandes problemas de ordem econômica, mas em contrapartida, a indústria de cosméticos cresceu de forma surpreendente. E porque aqui no Brasil seria diferente? Pois é, segundo a ABIHPEC (Associação Brasileira da Indústria de Higiene Pessoal e Perfumaria), no ano de 2010 o setor faturou R$ 29,9 bilhões , quando a economia do Brasil cresceu 7,5% . Já em 2014 o setor atingiu a marca de R$ 42,6 bilhões enquanto a economia cresceu apenas 0,15% ou seja: o crescimento da indústria de cosméticos é inversamente proporcional ao crescimento da economia no país. Em 2016 espera-se um crescimento ainda menor ou até mesmo déficit na economia , o oposto do que se espera da indústria de cosméticos , que conforme foi dito acima, deve alcançar um bom crescimento! Agora você deve estar se perguntando, por que isso acontece? Bem a explicação é a seguinte: são em momentos de crise financeira que os produtos mais baratos chamam a atenção e atraem a preferência tanto dos homens quanto das mulheres, que dão preferência a aquisição de produtos como batons e outros cosméticos no lugar de bolsas, vestidos e sapatos. Não só por serem mais baratos, mas também por levantarem a auto estima, que é essencial para buscar novos postos de trabalho. Sem esquecer também que muitas delas começam até mesmo a comercializar estes produtos como uma alternativa ao desemprego. Veja aqui as 4 opções de Franquias de baixo investimento que a Hinode oferece para você iniciar seu negócio próprio O mercado de cosméticos deixou a muito tempo de ser considerado um nicho de luxo dentro da economia e passou a ser para muitas pessoas algo extremamente necessário. Muitas empresas dão preferência a contratar pessoas que se preocupem com a boa aparência e todo mundo sabe que a auto estima é fundamental para enfrentar os desafios da vida. Além disso por movimentar milhões e ser um mercado que cresce a passos largos em comparação a outros ramos de atividade, muitas portas são literalmente ""escancaradas"" para as oportunidades. Muitas pessoas estão, através do mercado de cosméticos no Brasil, descobrindo-se verdadeiras empreendedoras , mudando radicalmente seu estilo de vida e por sua vez ajudando a aquecer a economia do país como um todo. Conheça aqui o Plano de Revenda da Hinode. Baixo investimento e Lucro de 100%! RESUMINDO: Não existe crise para o mercado de Cosméticos no Brasil O setor cresceu em média 113% nos últimos 5 anos Os Homens estão cada vez mais adeptos aos cosméticos e cuidadosos com a beleza Ótima oportunidade de iniciar seu negócio próprio com baixo investimento ( saiba mais aqui ) Viu só como para o Mercado de Cosméticos no Brasil não tem crise? Mostre isso a mais pessoas! Curta! Você também vai gostar de ler estes posts:",pt,43
457,2339,1473857779,CONTENT SHARED,-3302834510061927448,-6030696784871381528,-5052144656549396793,,,,HTML,http://g1.globo.com/economia/negocios/noticia/2016/09/bayer-anuncia-compra-da-monsanto-por-us-66-bilhoes2016.html,bayer anuncia compra da monsanto por us$ 66 bilhões,"O grupo farmacêutico alemão Bayer anunciou nesta quarta-feira (14) ter fechado acordo para a compra da empresa norte-americana Monsanto, líder mundial dos herbicidas e engenharia genética de sementes, por US$ 66 bilhões. ""Bayer e Monsanto assinaram nesta quarta-feira um acordo de fusão"", anunciou a Bayer em um comunicado. ""Os membros do Conselho de Administração da Monsanto, assim como os Conselhos Diretivo e de Supervisão da Bayer, aprovaram o acordo por unanimidade"", diz ainda o texto. As negociações começaram no primeiro semestre. Em maio, a Bayer fez oferta de US$ 122 por ação , aumentando em seguida para US$ 125 . A proposta seguinte, também recusada, foi de US$ 127,50 por ação. O acordo foi fechado por US$ 128 por ação. ""Com esta transação alcançamos um notável valor para nossos acionistas, clientes, empregados e para a sociedade"", disse Werner Baumann, presidente da Bayer, segundo a EFE. Já o presidente e diretor-executivo de Monsanto, Hugh Grant, destacou que ""o anúncio de hoje é a confirmação de tudo o que alcançamos e do valor que conseguimos para os acionistas"" da fabricante americana. Maior aquisição Autoridades antitruste ainda precisam aprovar a fusão. Trata-se do maior acordo de aquisição fechado neste ano e, se concretizada, a maior aquisição já feita por uma empresa alemã, destaca a Deutsche Welle. Ainda segundo a agência, o acordo supera a fusão da alemã Daimler com a americana Chrysler, em 1998, que rendeu à montadora dos EUA mais de US$ 40 bilhões. A compra da Monsanto também deve ser a maior transação em dinheiro já realizada, superando a oferta de US$ 60,4 bilhões feita pela cervejaria InBev à Anheuser-Busch em 2008. Se concretizado, o negócio criará a maior fabricante de herbicidas e sementes do mundo. Segundo o jornal The Wall Street Journal, juntas, a Bayer e a Monsanto controlariam 28% das vendas de herbicidas. Elas também seriam fortes no mercado de sementes de cereais e de soja nos Estados Unidos .",pt,43
458,1030,1463683246,CONTENT SHARED,-4974757204495953627,-709287718034731589,-7690842286223007778,,,,HTML,https://github.com/Alecrim/AlecrimCoreData,alecrim/alecrimcoredata,"A powerful and simple Core Data wrapper framework written in Swift. The basics AlecrimCoreData provides a default NSManagedObjectContext subclass that can be used as is, extended, or not used at all. If used it will help you with several Core Data related tasks. It is possible to use the framework with a ""vanilla"" NSManagedObjectContext too. In this case you will have the liberty to configure the Core Data stack as you want. Mixing DataContext and ""vanilla"" NSManagedObjextContext instances is possible but is strongly discouraged. Note: The DataContext() function provides a unique instance of a dataContext object on the main thread, so make sure to avoid core data concurrency violations. The generic Table<T> struct is the base for AlecrimCoreData functionality and is where the fun begins. T , in this case, is a NSManagedObject subclass type. Say you have an NSManagedObject subclass type called Person , related to a Department . To get all of the Person entities as an array, use the following methods: You can also skip some results: Or take some results only: Or, to return the results sorted by a property: Or, to return the results sorted by multiple properties: Or, to return the results sorted by multiple properties, ascending or descending: If you have a unique way of retrieving a single entity from your data store (such as via an identifier), you can use the first method: You can filter the results using the filter method: You can combine multiple filters and other methods as well: Or: You can count entities in your persistent store using the count method. Or: Or: When you need to create a new instance of an Entity, use: You can also create or get the first existing entity matching the criteria. If the entity does not exist, a new one is created and the specified attribute is assigned from the searched value automatically. To delete a single entity: To delete many entities: You can save the data context in the end, after all changes were made. Another important part of AlecrimCoreData is the use of strongly-typed query attributes. A lot of boilerplate code is required to support strongly typed queries. With this in mind, the ACDGen tool was created. All you have to do is point ACDGen to your managed object model and the source code for the entities is automatically generated, including the AlecrimCoreData query attributes if you want. Using the generated strongly-typed query attributes is completely optional, but with them the experience with AlecrimCoreData is greatly improved. The use of strongly-typed query attributes requires a project that has generated extensions of it's model classes using ACDGen . ACDGen binary and source code are avaible on ""ACDGen/Bin"" and ""ACDGen/Source"" folders respectively. Advanced use OK. You can write code like this: But you can do even more with AlecrimCoreData . There is a implementation of NSFetchedResultsController (for OS X) and FetchRequestController that is a strongly-typed wrapper for NSFetchedResultsController . You are invited to read the code and discover more possibilities (and to help us to improve them and create new ones). There are methods for aggregating, asynchronous fetching in background and many others. You can read the AlecrimCoreData documentation at for more information. You can order and filter entities not using the AlecrimCoreData query attributes at all. In this case you lose the strongly-typed attributes, but gain in flexibility. You can even mix the two approaches without any problem. You can order the entities using NSSortDescriptor instances: Or: You can also use the sortByAttributeName::: method: You can filter entities using NSPredicate instances: If you want to use the DataContext class and initialize it's instance without parameters AlecrimCoreData will try to infer the managed object model and the persistent store locations, based on most common cases. You can however create and configure a DataContextOptions struct and pass it as parameter to DataContext initializer. There are helper initializers on DataContextOptions , but if you know the URLs or know how to construct them it may be better to pass these locations yourself to the DataContextOptions initializer directly. Other options can be configured using DataContextOptions (the defaultBatchSize and defaultComparisonPredicateOptions properties). Unlike the managed object model and persistent store locations, these options are global and static for the entire framework (and your project). Since AlecrimCoreData version 4 the configuration of your Core Data managed object contexts to include iCloud integration are not made by the framework anymore. This type of integration is very trick and we think it is better a manually approach, case by case, including your own observers and handlers. You can, however, use the configureUbiquityWithContainerIdentifier::: method and the ubiquityEnabled property from the DataContextOptions struct to help you to configure iCloud integration if you want. Since AlecrimCoreData version 4 the DataContext is an NSManagedObjectContext subclass and the framework can work with ""vanilla"" NSManagedObjectContext instances as well. So you can integrate and use other frameworks as you are using only NSManagedObjectContext instances and there should be no side effects. Minimum Requirements Swift 2 Xcode 7.0 OS X 10.9 / iOS 8.0 / watchOS 2.0 Installation CocoaPods is a dependency manager for Cocoa projects. CocoaPods 0.36 adds supports for Swift and embedded frameworks. You can install it with the following command: To integrate AlecrimCoreData into your Xcode project using CocoaPods, specify it in your Podfile : Then, run the following command: You can add AlecrimCoreData as a git submodule, drag the AlecrimCoreData.xcodeproj file into your Xcode project and add the framework product as an embedded binary in your application target. master - The production branch. Clone or fork this repository for the latest copy. develop - The active development branch. Pull requests should be directed to this branch. If you want to contribute, please feel free to fork the repository and send pull requests with your fixes, suggestions and additions. :-) The main areas the framework needs improvement: Correct the README, code and examples for English mistakes; Write more and better code documentation; Write unit tests; Replace some pieces of code with more ""elegant"" ones. AlecrimCoreData is released under an MIT license. See LICENSE for more information.",en,43
459,2401,1474541751,CONTENT SHARED,2229625391864201918,-8205402408645015051,-1750329158213644012,,,,HTML,http://blog.onedaytesting.com.br/como-o-layout-de-aplicativos-influencia-o-usuario-e-por-que-voce-deveria-testa-lo/,como o layout de aplicativos influencia o usuário e por que você deveria testá-lo - one day testing blog,"O processo de criação e usabilidade de aplicativos para dispositivos móveis envolve muitos desafios. No processo, os detalhes são cruciais: botões de ação com contraste maior, cores, ícones e imagens . Todos os elementos de design são fundamentais se o seu objetivo for (e acreditamos que seja) satisfazer o usuário. Existem vários elementos que você precisa prestar atenção para seu aplicativo cumprir seu papel e não confundir o consumidor. Um layout de aplicativos mobile ideal é fluido e garante boa usabilidade e experiência ao usuário . Isso significa que a aplicação deve funcionar a qualquer momento, em qualquer dispositivo móvel e para qualquer pessoa. Os elementos que mais impactam o usuário Muitos elementos de uma aplicação influenciam a boa ou má experiência do usuário. Abaixo, separamos alguns elementos relacionados ao layout, que irão ajudar seu usuário a se engajar e não dar uma nota negativa para o seu app: Cores Cores são fundamentais em qualquer trabalho que envolva design. Você pode usá-las para direcionar e incentivar ações (cores quentes, por exemplo, como vermelho e laranja) ou mesmo se quiser passar uma imagem de classe e qualidade (azul e verde). Fora isso, as cores são parte importante da experiência do usuário. Se você não sabe muito sobre combinações de cores, pesquise no Adobe Kuler , ferramenta que combina cores automaticamente a partir de uma cor base. No exemplo abaixo, para o aplicativo de monitoramento de ciclo menstrual, com público-alvo majoritariamente feminino, a empresa fugiu das cores convencionais e acertou em apostar na mistura de cores quentes e frias. Ícones Os ícones são muito importantes porque indicam ações e por isso visam tornar as seções e funções dentro do aplicativo mais claras para os usuários . A medida padrão geralmente é de 11×11 pixels, mas isso varia de acordo com a plataforma para a qual o app será desenvolvido. Por isso, recomendamos sempre testar a medida para saber se é a ideal. No aplicativo ao lado, os ícones são parte fundamental da página inicial. A propósito, eles formam a ""home"" da aplicação. Com um clique, o usuário tem acesso à categoria que deseja. Fontes Como os smartphones têm as telas bem menores que computadores, verificar se o tamanho das fontes e grafias são de fácil leitura é ainda mais importante. Você precisa encontrar um equilíbrio que possibilite o usuário navegar pelo aplicativo confortavelmente , sem precisar se aproximar muito da tela ou utilizar o zoom. Ousada, a tipografia do aplicativo se deve a um fator: a persona . O aplicativo é feito para designers, por isso a escolha de fontes fora do convencional foi certeira. Padrões de interação Arrastar, clicar, rolar e selecionar. Essas ações já são esperadas por usuários, por isso, escolha sempre padrões de interação conhecidos , ou seja, ações às quais ele já está familiarizado. Tente com cuidado algo muito novo ou diferente do usual porque pode confundir o usuário e fazer com que ele abandone seu aplicativo. Ao utilizar menus laterais, por exemplo, que ocupam apenas uma parte da tela, você pode prever a ação de arrastar, bem conhecida dos usuários. No exemplo abaixo, a rolagem na vertical para uma revista, com matérias destacadas e a disposição das imagens ajudam na navegação. Outra dica que já aprofundamos aqui no blog é tomar cuidado com o menu de hambúrguer ! Número de cliques As pessoas querem fazer o que precisam rapidamente. Por isso, até o número de cliques em um aplicativo para chegar a alguma tela é importante. Lembre-se da regra dos 3 cliques: toda ação deve ser feita em, no máximo, três cliques . O que isso tem a ver com o layout? Subtelas aumentam o número de cliques, por isso você deve pensar em criar novas seções quando o processo se tornar muito trabalhoso. Veja o exemplo abaixo: O que você espera de um aplicativo para buscar passagens aéreas? Facilidade . O aplicativo cumpre bem a função, exigindo poucos cliques para obter o resultado da busca. Destaque e posicionamento dos botões de ação A maioria da população é destra , e por essa razão os botões são dispostos no lado direito da tela. Imagine um botão de ação posicionado na esquerda. Nesse caso, o usuário destro teria que cruzar toda a tela com o polegar. Nada prático, correto? A dica aqui é fazer de tudo para facilitar o uso do smartphone com apenas uma das mãos . Além disso, o contraste do botão também pode ser utilizado para orientar o usuário, criando uma hierarquia de informações. Neste exemplo, a tela de início apresenta um botão vermelho enorme com a call to action também destacada. Para completar, a palavra ""start"" convida o usuário a começar a utilizar o aplicativo no momento em que é aberto. 4 bons exemplos de layout de aplicativos O Toggl é um aplicativo para medir o tempo da realização de tarefas diversas. Dito isto, repare no posicionamento do tempo na tela e como é simples criar novas tarefas dentro do ambiente. O aplicativo cumpre o que promete e aposta em um design simples mas eficiente. Esse aplicativo foi um dos vencedores do Apple Design Awards 2015 . A desenvolvedora Hipster Whale conseguiu fazer um jogo que faz jus ao nome hipster mas que é muito viciante. O ambiente é divertido, bastante rápido e de fácil navegação. O trunfo está no uso de um design nada convencional para um jogo e na facilidade de entendimento de como a plataforma funciona. Em meio a tantos aplicativos de organização de tarefas, o Taasky está se superando. A interface é super clean e fácil de usar. Pode fazer com que o aplicativo seja usado com mais frequência, já que tem uma interface intuitiva. O B&H Photo Audio Pro é um dos cases de Material Design para Android. O Material Design é um conjunto de padrões de ícones, cores, animações, tipografia e hierarquias indicados pelo Google. O aplicativo móvel é de um e-commerce, mas se destaca por fotos de alta resolução e carregamento rápido, comentários de profissionais sobre os produtos e claro, muitas informações relevantes sobre o que é vendido. O layout torna fácil a navegação pelo site, a pesquisa de produtos, o estado do estoque, etc. Certamente, é um ótimo exemplo de layout para aplicativos de e-commerce. A importância de testar o layout de aplicativos Se o sucesso do seu produto depende de ações ou envolvimento dos usuários, é sua obrigação prevenir erros ou falhas que dificultem a utilização do mesmo. Por causa disso, testar o layout de aplicativos é imprescindível, já que reduz custos de desenvolvimento e têm um impacto direto na melhoria de usabilidade do aplicativo e da experiência do usuário . Como consequência disso, a taxa de adoção, conversão e retenção também irão aumentar. Formas de testar o layout Mesmo que os testes de layout apresentem um custo maior em curto prazo - envolvem recrutamento, tempo, deslocamento, contratação de uma empresa, etc - eles apresentam um retrato mais realista da usabilidade do aplicativo. Se você quer ter um panorama mais realista da usabilidade e layout de aplicativos , para torná-lo mais rápido e dinâmico, há duas formas principais de testá-lo: Inspeção de interface Neste teste, um especialista em usabilidade faz uma inspeção da interface completa da aplicação, avaliando-a de acordo com a conformidade e diagnosticando se o fluxo permite atingir os objetivos do projeto sem obstáculos. Interação com usuário As interações são testadas em todas as interfaces do sistema. São testadas tarefas distintas que a aplicação se propõe a realizar em contextos específicos. Assim, é possível avaliar se estão funcionando corretamente e atendendo às regras de negócio. Conheça outros módulos de teste de aplicação ou fale com um de nossos especialistas para tirar suas dúvidas. Por fim, testar o layout de aplicativos vai ajudar sua equipe a chegar em um produto final leve, rápido e dinâmico.",pt,43
460,2322,1473698732,CONTENT SHARED,-3944300307678659798,-9047547311469006438,1703941476780379613,,,,HTML,http://www.fabioprado.net/2016/09/dba-brasil-no-interopmix-2016.html,dba brasil no interopmix 2016,"Olá pessoal, No papel de um dos Organizadores do grupo DBA BRASIL, quero lhe comunicar através desse post, que teremos 2 novos eventos (em São Paulo e Curitiba) nos próximos meses (10 e 11/2016). Segue abaixo a mensagem CONVITE formatada pela ORGANIZAÇÃO, para que você venha desfrutar do conhecimento que será adquirido em nossas palestras: Olá Convidamos todos a participarem do INTEROPMIX 2016. O evento tratará de vários assuntos sobre interoperabilidade, IOT e tecnologias abertas. O DBA Brasil não poderia ficar de fora e também estará participando com um dia inteiro de palestras sobre Bancos de Dados. Vitor Tadeu Fava , Dennes Torres , Thiago Cruz , Fábio Telles Rodriguez z, Douglas Paiva , Juliano Atanazio e Thiago Ferreira serão alguns dos palestrantes presentes em São Paulo. Em Curitiba teremos a participação do Milton Bastos . Além disso teremos a gravação em São Paulo de um episódio do DatabaseCast . Para maiores informações: Não fique de fora! Se tiver qualquer dúvida sobre os eventos, deixe um comentário que eu o responderei o mais breve possível.",pt,43
461,1232,1464906723,CONTENT SHARED,5962268176939710955,-8020832670974472349,2348433893919184737,,,,HTML,https://cloud.google.com/blog/big-data/2016/06/bigquery-111-now-with-standard-sql-iam-and-partitioned-tables,"bigquery 1.11, now with standard sql, iam, and partitioned tables! | google cloud big data and machine learning blog","BigQuery is Google Cloud Platform 's serverless analytics data warehouse. It's used by thousands of companies - both big and small - to store, understand, and analyze large amounts of data. Today, we're announcing a host of new features that make BigQuery more compatible with traditional big data workflows: Standard SQL Beta If you're familiar with the SQL standard or have used another standard-compliant SQL engine, you'll feel right at home with the Beta of BigQuery's revamped SQL dialect . Notable features that we've added with this new dialect include: More advanced query planning and optimization: BigQuery now provides very robust decorrelation, which allows you to write complex subqueries in any clause of your SQL statement (SELECT, FROM, WHERE, etc.) A richer type system with fully composable types: In addition to the existing data types BigQuery users are used to, we've added dates, times, arrays and structs, as well as additional support for timestamps Extended JOIN support: BigQuery now supports Theta JOIN , which offers the ability to use inequalities in your join key comparisons, as well as arbitrary expressions as JOIN conditions. While we think the updated dialect is a wonderful addition, there's no requirement that users switch, and for production use cases, we recommend users remain on the legacy SQL dialect. After we have a few more miles on the new dialect, we plan to launch it to general availability and recommend it as the default language for all projects. Identity & Access Management (IAM) Beta Now that BigQuery supports Standard SQL, you'll have more and more teams in your company requesting access to BigQuery projects. Earlier this year we announced Cloud IAM for Cloud Platform. We're now making IAM available for BigQuery as well, in Beta. This feature is currently being rolled out, so if you don't see it today you can expect it enabled on your projects this month - BigQuery roles will be made available in Google Cloud's ""IAM & Admin"" control panel. Cloud IAM fully automates managing permissions for BigQuery projects. Administrators can define fine-grained security policies they want for their users and resources. For example, let's say your marketing team needs to query data, but doesn't need to modify or upload data to get their job done. With Cloud IAM and Google accounts, this process is a breeze. Administrators can create several roles, such as viewer, editor, user or owner. Time based table partitioning Time based partitioning makes it easy and cost-effective for you to manage your data and write queries that span multiple days, months or years. You can now create tables with time-based partitions - you load the data, and BigQuery will automatically put it in the right partition. Querying partitioned tables is easy - you simply provide a date or range of dates you want, and BigQuery will limit data processed to the time interval specified. Imagine that you had a marketing campaign that spanned the length of 3 months. Here's an example of a query to analyze logs from the past 3 months. You no longer have to manually partition your tables beforehand - it just works! BigQuery only scans the data in the specified partitions, rather than the whole table. Not only is this query more performant, it also costs you a fraction of the cost of querying the full table! BigQuery and Kabam Businesses come to Cloud Platform for our unique data analytics platform, and today we'll show you one such example. Kabam i s an international entertainment company that produces free-to-play video games for mobile devices. Kabam's games include Marvel: Contest of Champions, Fast & Furious: Legacy, and Star Wars: Uprising. Their average gamer enjoys more than two hours of gameplay a day! Data analytics is critical to Kabam's ability to keep gamers engaged in a highly competitive marketplace. Marvel: Contest of Champions became wildly successful after its release in December of 2014. With big popularity comes big data - in-game events and logs were generating well over 1 TB of data per day, and their previous data warehouse was not able to handle this sudden scale. As a result, Kabam had no way to gather insights about the game for several weeks. Costas Piliotis, lead analyst on the game, had used BigQuery at a previous company and suggested the team to give it a try. In six days they were able to ingest hundreds of terabytes of past game telemetry data. More importantly, they could now query data over the lifetime of the entire game. Thanks to BigQuery, Kabam is able to store and understand every single player action within their games, from game monetization to fraud, from production bugs to level-completion statistics. Now that the data lives in BigQuery, Costas and his team are most excited about applying Cloud Platform's Machine Learning tools to the data to make their games even more entertaining. Here in the BigQuery team, we often hear stories like this one and immediately the question that follows from people who've never tried BigQuery is, ""How does this compare to my current data warehouse?"" Two weeks ago we shared a summary of a great series of experiments on cloud data warehouse solutions performed by Mark Litwintschik where BigQuery really stood out. We've made this short video to explain why BigQuery is unique, which we hope will inspire you to give it a try.",en,42
462,2631,1477332201,CONTENT SHARED,-1525688508571111688,-89388927330481219,-5349207496967624261,"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36",TX,US,HTML,http://blogs.atlassian.com/2016/05/introducing-jira-software-app-ios/,introducing jira software for iphone,"A recent Atlassian user survey found that 75% of JIRA Software Cloud users were seeking a better way to track work in JIRA from their smartphone. And as the pressure to release better software faster steadily increases for every software team, we wanted to give JIRA Software users a competitive edge and a new way to stay connected. Enter JIRA Software for iPhone . With our new mobile offering for JIRA Software, great software development doesn't have to stop when you're not at your desk. Let's see how. Track what's important JIRA Software for iPhone keeps one curated feed of all your most important projects and work items at your fingertips. So whether you're grabbing breakfast or waiting in the doctor's office, ensuring that everything is still on track is as easy as scrolling through a feed on your iPhone. And, whenever you return to your desk you can immediately jump on the next item in your backlog, instead of catching up on conversations and work that occurred while you were away. Join the conversation from anywhere Our new mobile offering prevents team members on the go from being the bottleneck on a project. In the event there's an action item for you, JIRA Software for iPhone will notify you, then allow you to easily respond to the question or leave a comment. You can even mention other team members, pulling anyone relevant into the conversation. Jumping in to ensure that work on a particular issue continues is as painless as a few key strokes. No blocked issues means no missed deadlines. Capture and assign issues Oftentimes the best ideas arise when you least expect them. A stroke of brilliance may hit you when you're on a morning run or while enjoying a glass of wine after work. And, if you don't capture it in the moment, it may be gone forever. For this reason we've added the ability to create, assign, and prioritize issues directly from your phone. Whether it's a bug you forgot to log, or a new idea you want to capture when you're out and about, creating new to-dos is now as easy as reaching in your pocket. A better way to track important projects and stay connected to your team has arrived. JIRA Software for iPhone is available now as a free download on the iPhone App Store . Grab it today! FAQs : In case you might be wondering... 1. Is JIRA Software available for Android? At this time JIRA Software is only available for iPhone. However, keep a close eye out for updates on JIRA Software for Android. 2. Does JIRA Software for iPhone include boards? This version of JIRA Software for iPhone does not include boards, but we know boards are an important component to the way teams work. Please stay tuned! 3. Can I use JIRA Software for iPhone with JIRA Software Cloud and Server? At this time JIRA Software for iPhone is only available for JIRA Software Cloud users. Fired up about JIRA Software for iPhone? Share this post on your social network of choice and spread the good word about using JIRA while on the move.",en,42
463,1537,1466986072,CONTENT SHARED,-555187669877280641,-1479311724257856983,-2198572043169729427,,,,HTML,https://www.coursera.org/learn/nlpintro,introduction to natural language processing - university of michigan | coursera,"About this course: This course provides an introduction to the field of Natural Language Processing. It includes relevant background material in Linguistics, Mathematics, Probabilities, and Computer Science. Some of the topics covered in the class are Text Similarity, Part of Speech Tagging, Parsing, Semantics, Question Answering, Sentiment Analysis, and Text Summarization. The course includes quizzes, programming assignments in Python, and a final exam. Course Syllabus Week One (Introduction 1/2) (1:35:31) Week Two (Introduction 2/2) (1:36:26) Week Three (NLP Tasks and Text Similarity) (1:42:52) Week Four (Syntax and Parsing, Part 1) (1:48:14) Week Five (Syntax and Parsing, Part 2) (1:50:29) Week Six (Language Modeling and Word Sense Disambiguation) (1:40:33) Week Seven (Part of Speech Tagging and Information Extraction) (1:33:21) Week Eight (Question Answering) (1:16:59) Week Nine (Text Summarization) (1:33:55) Week Ten (Collocations and Information Retrieval) (1:29:40) Week Eleven (Sentiment Analysis and Semantics) (1:09:38) Week Twelve (Discourse, Machine Translation, and Generation) (1:30:57) The course assignments will all be in Python. Course Format The class will consist of lecture videos, which are typically between 10 and 25 minutes in length. The lectures contain 1-2 integrated quiz questions per video. Grading is based on three programming assignments, weekly quizzes, and a final exam.",en,42
464,1469,1466508431,CONTENT SHARED,-4336877432539963613,-1578287561410088674,2634667938638203431,,,,HTML,http://www.survivingwithandroid.com/2015/12/comparing-iot-platforms.html,comparing iot platforms: compare 4 iot platform for iot projects,"Easy to use comparing IoT Platforms table . Building IoT project is a process that could involve the step of finding free platforms. As you may know, Internet of things is a set of physical objects that use network support to exchange data. These objects can be sensors, software, boards and so on. This is an interesting ecosystem where the software can be connected directly to real hardware or devices. The most known board for building internet of things project are Arduino (with its several versions) and Raspberry. Integrating these devices with cloud platforms is possible to collect and analyze data, create ""smart"" object that can be controlled remotely . One way to control such devices is using smartphones like Android and iOS devices. Dev Boards like Arduino or Raspberry are cheap and everyone can experiment IoT projects. Cloud IoT platforms help developers and maker to build IoT project and test them fast and easily . It is useful to compare IoT Platforms so that it is possible to select the right one according to our needs. Comparing IoT Platforms: analysis for building projects Cloud IoT platforms provides several kind of services that can be very useful in building IoT project : cloud data store data Event logic Platforms integration Cloud data store enables developers to store data sent from several boards (like Arduino or Raspberry); for example, it is possible to store values read from a sensor. This information can be visualized using a graph or analyzed with other tools. Event logic is web based programming logic that can be used to trigger some action when an event occurs. Using this kind of platforms is possible to implement some ""business logic"" using just a web interface without knowing much about the board we are using for the project. Usually ,the logic is like IF-THEN, for example if an event occurs then do this action. An event can be a signal read from a sensor and the action can be an email or an SMS. Platforms integration is a set of ""adapters"" that implements specific protocol so that it is possible without writing a line of code mix different internet services to make a chain of actions. For example, using Arduino with Ethernet shield is possible to send an alert via SMS when a value read from a sensor is higher than a threshold level. Build IoT project with free platform: Description Below, you find a list comparing IoT platforms that can be useful to create IoT projects with a brief description. Comparing IoT platforms isn't easy because they have different features and they are focused on different IoT aspects. This list comparing IoT platforms is built using six different IoT aspects: Data store (the capability to save and store data) Service Integration (the capability to integrate with other external systems) Data visualization (the capability to create charts using the data stored) SDK API (the availability of an SDK to simplify the development process) Event/rule Management (a built-in set of rules to manage the flow of the data) Free account Temboo : This is a very interesting platform that provides services to integrate Arduino, Raspberry and other platforms with different internet services (like SMS, Email and so on). This platform uses choreos that are connectors toward external services, so that events in Arduino, like sensor signals, can be transformed into different kind of events. Moreover, it provides some logic like IF-THEN. Carriots : This is another interesting platform that enables smart devices to store data. It uses the data stream concept to enable devices to send data. This platform moreover has a rule management system so that you can implement custom logic directly on the web. It can moreover send Email, SMS and Twitter messages NearBus : This proposes a different approach respect to other platforms. Usually, the basic concept that stands behind IoT platforms is connecting the device (Arduino, Raspberry and so on) to the cloud so that these boards can send data. NearBus provides a different way: it maps the device into the cloud so that it gets a part of the cloud itself. It uses an Agent to accomplish this task and it is possible to control this agent directly from the web using a set of API. Ubidots : This platforms support several kind of board and can be used to store data in the cloud. It offers data capture, data visualization with a built-in dashboard, rules management (or event management). With the built-in dashboard, it is possible to see in real time the graph built on the data sent by the device. It supports several kind of visualization. *Nearbus offers a different approach so it is quite difficult to categorize it **It offers a set of API easy to use The table above summarizes some aspects of these platforms, that i think they are important. The aim of this comparison is to provides some high-level information about existing IoT platforms and i invite the readers to read carefully each platform features directly on the respective website, before using it. The are other platforms that can be mentioned here like Xively or Sensorcloud that provide interesting services even if I still haven't time to use them. As you can notice, there are several type of platforms with different services, every platform has its unique aspects and it is up to developers and makers to choose the right one according to the project needs. This list comparing IoT platforms wants to help other developers when starting with an IoT project. Help me to increase the list to provide a better information.",en,42
465,443,1460738006,CONTENT SHARED,-6009377241272750648,801895594717772308,8460091227257577959,,,,HTML,https://www.nespresso.com/br/pt/pages/cafeteira-conectada-prodigio,nespresso,"Compre a Prodigio Prodigio Agora o seu telefone pode fazer o café perfeito com a Prodigio - a primeira cafeteira conectada da Nespresso - e com o nosso aplicativo mobile, capaz de gerenciar o seu estoque de cápsulas, agendar a preparação de um café e oferecer conteúdo de assistência e cuidados com a máquina. PRODIGIO&Milk A PRODIGIO&Milk vem com um Aeroccino integrado. Com ele, você poderá criar uma espuma de leite cremosa e outras bebidas como cappuccinos, lattes machiattos e as suas receitas de café com leite preferidas. Características Conectividade: gerenciamento do estique de cápsulas, agendamento da preparação do café e alertas de manutenção. Tamanhos de xícaras: Ristretto, Espresso & Lungo Ejeção automática de cápsulas Reservatório de água rotativo Aquecimento rápido em até 25 segundos O que a máquina conectada da Nespresso faz: Gerenciamento do estoque de cápsulas Programação de extração de café Assistência técnica Desfrute da melhor experiência em café Conveniência Com as opções de três tamanhos de xícara, um sistema de ejeção de cápsulas automático e aquecimento rápido, desfrutar seu café torna-se tão simples como deveria ser. A Prodigio também cabe facilmente em qualquer espaço graças ao seu tanque de água rotatório. O suporte para xícaras pode ser levantado para acomodar um copo de receitas. Simplicidade Para tornar a sua experiência ainda mais simples, as máquinas Prodigio possuem duas luzes que indicam quando a sua atenção for necessária. A luz com o logo da Nespresso se acende quando o seu estoque de cápsulas estiver baixo, assim você nunca ficará sem seu café preferido. A luz de manutenção se acende quando o tanque de água estiver vazio, o reservatório de cápsulas usadas estiver fora ou cheio ou uma descalcificação for necessária. PRODIGIO&milk A PRODIGIO&Milk vem com um Aeroccino integrado. Com ele, você poderá criar uma espuma de leite cremosa e outras bebidas como cappuccinos, lattes machiattos e as suas receitas de café com leite preferidas. Meu aparelho é compatível? Baixe o aplicativo Nespresso para o seu aparelho: FAQs O que é uma bomba de alta pressão? Todas as máquinas da Nespresso são equipadas com uma bomba de alta pressão. Ela provê a energia necessária para perfurar o filme da cápsula e liberar os aromas do café. A forma da cápsula da Nespresso foi desenhada para que a água sob pressão possa fluir igualmente através de todo o café durante a extração. Eu preciso de uma conta de membro para usar a conectividade? Sim, uma conta de membro válida é necessária para usar as funções de conectividade da sua Prodigio e Prodigio&Milk. Uma conta de membro única deve ser ligada a uma única máquina. Meu aparelho não consegue encontrar a minha máquina/Estou com problemas para me conectar com a minha cafeteira. O que devo fazer? Tenha certeza de que o Bluetooth está ativado no seu aparelho, que a sua máquina está ligada e que você está próximo dela quando estiver tentando se conectar. Os passos a seguir podem ajudar a resolver um problema de conexão: 1) Tenha certeza de que o aplicativo não está aberto em nenhum outro aparelho 2) Feche o aplicativo da Nespresso 3) Remova a Prodigio da lista de aparelhos Bluetooth do seu celular 4) Reinicie o seu aparelho 5) Aplique as configurações de fábrica à sua máquina. Veja abaixo como fazer. 6) Verifique que o seu aparelho é compatível com a máquina. visite: www.nespresso.com/prodigio. Estas operações devem ajudar a resolver o problema. Caso você ainda não consiga conectar, o problema pode estar ligado ao ambiente. Aparelhos Bluetooth ou microondas muito próximos podem causar interferências na sua conexão. Se você ainda tiver dúvidas, entre em contato com o Nespresso Club. O fluxo de café da minha Prodigio está baixo. Se o fluxo de café estiver muito baixo verifique que:1) O fio de alimentação não está preso entre o tanque de água e a máquina 2) A reservatório de água está suficientemente cheio e corretamente posicionado 3) A sua máquina está ligada 4) A sua cafeteira pode estar precisando de uma descalcificação. Por que a luz de manutenção ou o logo da Nespresso estão acesos? A luz de manutenção tem vários significados: a sua máquina pode precisar de uma descalcificação; o reservatório de água está vazio; o reservatório de cápsulas está cheio etc. Verifique o aplicativo da Nespresso para entender o problema. Já o LED da Nespresso avisa quando as suas cápsulas estiverem acabando, assim você não esquecerá de comprar os seus cafés favoritos. Compo eu aplico as configurações de fábrica à minha máquina? Para reprogramar sua máquina de acordo com as configurações de fábrica, desligue a sua cafeteira e, em seguida, pressione simultaneamente os botões de lungo e espresso por 5 segundos. Quando o processo estiver completo, o LED de manutenção piscará. Note que se os três botões de tamanho de xícara estiverem acesos antes desta operação, a sua máquina apenas entrará em modo de espera. Neste caso, você precisará repetir o mesmo procedimento. Também é possível fazer o mesmo através da aplicação mobile. Pressione o ícone da máquina e selecione a opções de detalhes da sua cafeteira. Pressione a opção de reinicialização da máquina com as configurações de fábrica. Os ajustes de fábrica para lungo, espresso e ristretto são respectivamente 110 ml, 40ml e 25ml. A máquina é desligada após 9 minutos de inatividade e o nível de dureza da água é automaticamente selecionado como o mais duro (corresponde à preparação de 1000 xícaras de espresso) Como eu posso cuidar da minha máquina?? 1) Descalcifique a sua máquina pelo menos uma vez por ano se a água que você utiliza não for dura. Para águas mais duras, aconselhamos a descalcificação duas vezes por ano. Para descobrir a dureza da sua água, observe o reservatório: água mais dura tende a deixar marcas brancas) 2) Troque a água do reservatório regularmente, usando água fresca e potável a cada troca 3) Esvazie e limpe o reservatório de cápsulas usadas regularmente 4) Se você possui um Aeroccino, ative o ciclo de limpeza após cada uso 5) Antes de usar a máquina pela primeira vez, pressione qualquer botão sem colocar uma cápsula para extrair apenas água. Você tem outras perguntas?",pt,42
466,1393,1465953444,CONTENT SHARED,1878784080584939832,-709287718034731589,8812735263251701985,,,,HTML,http://techcrunch.com/2016/06/14/differential-privacy/,what apple's differential privacy means for your data and the future of machine learning,"Apple is stepping up its artificial intelligence efforts in a bid to keep pace with rivals who have been driving full-throttle down a machine learning-powered AI superhighway, thanks to their liberal attitude to mining user data. Not so Apple, which pitches itself as the lone defender of user privacy in a sea of data-hungry companies. While other data vampires slurp up location information, keyboard behavior and search queries, Apple has turned up its nose at users' information. The company consistently rolls out hardware solutions that make it more difficult for Apple (and hackers, governments and identity thieves) to access your data and has traditionally limited data analysis so it all occurs on the device instead of on Apple's servers. But there are a few sticking points in iOS where Apple needs to know what its users are doing in order to finesse its features, and that presents a problem for a company that puts privacy first. Enter the concept of differential privacy, which Apple's senior vice president of software engineering Craig Federighi discussed briefly during yesterday's keynote at the Worldwide Developers' Conference . ""Differential privacy is a research topic in the area of statistics and data analytics that uses hashing, sub-sampling and noise injection to enable this kind of crowdsourced learning while keeping the information of each individual user completely private,"" Federighi explained. Differential privacy isn't an Apple invention; academics have studied the concept for years. But with the rollout of iOS 10, Apple will begin using differential privacy to collect and analyze user data from its keyboard, Spotlight, and Notes. Differential privacy works by algorithmically scrambling individual user data so that it cannot be traced back to the individual and then analyzing the data in bulk for large-scale trend patterns. The goal is to protect the user's identity and the specifics of their data while still extracting some general information to propel machine learning. Crucially, iOS 10 will randomize your data on your device before sending it to Apple en masse, so the data is never transported in an insecure form. Apple also won't be collecting every word you type or keyword you search - the company says it will limit the amount of data it can take from any one user. In an unusual move, Apple offered its differential privacy implementation documents to Professor Aaron Roth at the University of Pennsylvania for peer review. Roth is a computer science professor who has quite literally written the book on differential privacy (it's titled Algorithmic Foundations of Differential Privacy) and Federighi said Roth described Apple's work on differential privacy as ""groundbreaking."" Apple says it will likely release more details about its differential privacy implementation and data retention policies before the rollout of iOS 10. So what does this mean for you? Keyboard Apple announced significant improvements to iMessage yesterday during the WWDC keynote. Differential privacy is a key component of these improvements, since Apple wants to collect data and use it to improve keyboard suggestions for QuickType and emoji. In iOS 9, QuickType learns phrases and updates the dictionary on your individual device - so if you type ""thot"" or ""on fleek"" enough times, autocorrect will eventually stop changing the phrases to ""Thor"" and ""on fleet."" But in iOS 10, Apple will use differential privacy to identify language trends across its billions of users - so you'll get the magical experience of your keyboard suggesting new slang before you've ever used it. ""Of course one of the important tools in making software more intelligent is to spot patterns in how multiple users are using their devices,"" Federighi explained. ""For instance you might want to know what new words are trending so you can offer them up more readily in the QuickType keyboard."" Differential privacy will also resolve the debate over which emojis are most popular once and for all, allowing for your emoji keyboard to be reordered so hearts aren't inconveniently stashed at the very back near the random zodiac signs and fleur-de-lis. Spotlight Differential privacy builds on the introduction of deep linking in iOS 9 to improve Spotlight search. Federighi unveiled deep linking at last year's WWDC using the example of recipes. He demonstrated that searching for ""potatoes"" in Spotlight could turn up recipes from within other apps installed on his device rather than merely surfacing web results. As more and more information becomes siloed in apps, beyond the reach of traditional search engines, deep linking is necessary to make that content searchable. However, questions remained about how iOS 9 would rank deep-linked search results to prevent app developers from flooding Spotlight with irrelevant suggestions. Apple plans to use differential privacy to address that concern. With obfuscated user data, Apple can identify highly popular deep links and assign them a higher ranking - so when you're using Spotlight to look for potato recipes, you'll get suggestions for the most delicious potato preparations apps like Yummly have to offer. Notes Notes is the final area where iOS 10 will apply information gleaned through differential privacy to improve features. Federighi also discussed the upgrades to Notes during yesterday's keynote. In iOS 10, Notes will become more interactive, underlining bits of information that's actionable - so if you jot down a friend's birthday in Notes, it might underline the date and suggest that you create a calendar event to remember it. In order to make these kinds of smart suggestions, Apple again needs to know what kinds of notes are most popular across a broad swath of its users, which calls for differential privacy. How it works So what exactly is differential privacy? It's not a single technology, says Adam Smith, an associate professor in the Computer Science and Engineering Department at Pennsylvania State University, who has been involved in research in this area for more than a decade, along with Roth. Rather, it's an approach to data processing that builds in restrictions to prevent data from being linked to specific individuals. It allows data to be analyzed in aggregate but injects noise into the data being pulled off individual devices, so individual privacy does not suffer as data is processed in bulk. ""Technically it's a mathematical definition. It just restricts the kinds of ways you can process the data. And it restricts them in such a way that they don't link too much information about any single interval pick up points in the data set,"" says Smith. He likens differential privacy to being able to pick out an underlying melody behind a layer of static noise on a badly tuned radio. ""Once you understand what you're listening to, it becomes really easy to ignore the static. So there's something a little like that going on where any one individual - you don't learn much about any one individual, but in the aggregate you can see patterns that are fairly clear. ""But they're not as sharp and as accurate as you would get if you were not constraining yourself by adding this noise. And that's the tradeoff you live with in exchange for providing stronger guarantees on people's privacy,"" Smith tells TechCrunch. Smith believes Apple is the first major company that's attempting to utilize differential privacy at scale, although he notes other large commercial entities such as AT&T have previously done research on it (as has, perhaps surprisingly, Google via its Project Rappor ). He notes that startups have also been taking an interest. The future of AI? Apple's adoption of differential privacy is very exciting for the field, Smith says, suggesting it could lead to a sea change in how machine learning technologies function. The debate over privacy in Silicon Valley is often viewed through a law enforcement lens that pits user privacy against national security. But for tech companies, the debate is user privacy versus features. Apple's introduction of differential privacy could radically change that debate. Google and Facebook, among others, have grappled with the question of how to deliver feature-rich products that are also private. Neither Google's new messaging app, Allo , nor Facebook's Messenger offer end-to-end encryption by default because both companies need to vacuum up users' conversations to improve machine learning and allow chat bots to function. Apple wants to glean insights from user data, too, but it's not willing to backpedal on iMessage's end-to-end encryption in order to do so. Smith says Apple's choice to implement differential privacy will make companies think differently about the tradeoffs between protecting privacy and improving machine learning. ""We don't need to collect nearly as much as we do,"" Smith says. ""These types of technologies are a really different way to think about privacy."" Although iOS 10 will only use differential privacy to improve the keyboard, deep linking, and Notes, Smith points out that Apple may use the strategy in maps, voice recognition, and other features if it proves successful. Apple could also look for correlations between the times of day people use certain applications, Smith suggests. Apple's choice not to collect raw user data could encourage more trust from users. Conveniently, it also helps Apple harden itself against government intrusion - a cause that Apple notoriously fought for during its court battle with the FBI. Since differential privacy has been studied for a decade, it's a relatively low-risk security strategy for Apple. Smith said Apple's adoption of the concept hits a ""sweet spot"" between innovation and user safety. ""Whether or not they're entirely successful, I think it will change the conversation completely,"" Smith says. ""I think the way people think about collecting private information will change drastically as a result of this. And that may ultimately be the biggest legacy of this project at Apple, possibly far beyond the financial implications for Apple itself.""",en,42
467,657,1461867760,CONTENT SHARED,-3338916066794638254,3609194402293569455,4312885763327590308,,,,HTML,http://tableless.com.br/iniciando-com-o-docker-criando-suas-proprias-imagens/,iniciando com o docker: criando suas próprias imagens - tableless,"por André Kiffer No artigo anterior , eu descrevi alguns comandos básicos e como iniciar com o pé direito no mundo do Docker , trazendo de forma direta alguns conceitos que com o passar do tempo se tornaram fundamentais no meu fluxo de desenvolvimento. Hoje eu quero partir um pouco mais para o lado prático da coisa, vamos construir uma imagem para encapsular uma pequena aplicação em GO . Primeiros passos O arquivo de manifesto do Docker é o Dockerfile, nele você coloca as instruções de como você quer que sua imagem seja construída. Você pode na construção da imagem setar outro arquivo com o parâmetro -f. Abaixo temos um exemplo de Dockerfile, esse é um exemplo de um app em go já compilado para ubuntu então eu só preciso copiar o arquivo executável elasticpush para dentro do docker: FROM debian:jessie RUN mkdir /app ENV ACCESS_TOKEN abc ENV SECRET_TOKEN xyz COPY ./bin/elasticpush /app/elasticpush ENTRYPOINT [""/app/elasticpush""] Detalhando os comandos utilizados: FROM : Este é o comando mais importante, pois ele especifica a imagem base para a construção de uma nova. Na maioria das vezes a imagem especificada vai ser uma distribuição linux, se essa imagem não for encontrada na máquina local, o docker tentará buscar em algum repository. Caso queira, por exemplo, fazer a build do seu app em GO dentro do container, você vai precisar de uma imagem que tenha o GO instalado e configurado. Outra forma também seria criar diversas instruções com o comando RUN para fazer essa instalação. RUN : Esse comando serve para executar outros comandos que a versão do sistema operacional permite. Por exemplo, se for debian vc pode instalar pacotes com apt-get, se for CentOS você pode utilizar o yum para pegar as dependências que seu serviço precisa para rodar. Com o RUN você também pode criar arquivos, pastas, enfim acho que deu pra entender que ele executa os mesmo comando do que você executaria na sua máquina, logo você consegue fazer praticamente tudo, e deixar a sequência de comandos versionada aqui dentro. ENV : Serve para você setar variáveis de ambiente, você pode tanto deixar essas variáveis setadas de forma fixa dentro do Dockerfile quanto passá-las dinamicamente na hora que você instanciar o container. Para passar essas variáveis de ambiente na instanciação do container basta usar o parâmetro -e. Exemplo: docker run -e ACCESS_TOKEN=abcd [nome da imagem] . COPY : O COPY serve para você poder copiar arquivos e pastas para dentro da imagem do Docker, nesse exemplo eu copiei o arquivo elasticpush que estava dentro da pasta bin na minha máquina local para dentro da pasta /app na imagem do docker. ENTRYPOINT : Com esse parâmetro você pode setar se quer que algo seja executado na hora da instanciação do container. Então, quando você der um docker run nessa imagem, ela já vai instanciar e executar o programa que está no caminho que você colocar entre colchetes. No nosso caso queremos que essa imagem execute nossa aplicação do Elasticpush, o mesmo vale para quaisquer outros serviços como Redis, Elasticsearch, Nodejs, etc... Build - Construíndo a imagem A essa altura provavelmente você já tem o Docker instalado na sua máquina, caso contrário ensinamos a fazer isso nesse artigo . Para construir a imagem você precisa executar o seguinte comando, na mesma pasta que está o Dockerfile : sudo docker build -t app/elasticpush . Eu escolhi que o nome da minha imagem fosse app/elasticpush, mas isso fica a seu critério, escolha o nome que melhor se adéque ao seu serviço. Executado o comando, se tudo correr bem terá uma saída semelhante a essa: Sending build context to Docker daemon 45.03 MB Sending build context to Docker daemon Step 0 : FROM debian:jessie - > a582cd499e0f Step 1 : RUN mkdir /app - > Using cache - > 3763257cc26e Step 2 : COPY ./bin/elasticpush /app/elasticpush - > cc4b56f3fd8e Removing intermediate container 0bb2091ca437 Step 3 : ENTRYPOINT /app/elasticpush -> Running in ad99734cd065 - > 3ffec68d5499 Removing intermediate container ad99734cd065 Successfully built 3ffec68d5499 sudo docker images Agora você já tem uma imagem construída. Execute o seguinte comando: sudo docker run -d [nome da imagem] A imagem com a tag que você escolheu vai estar listada. A partir dessa imagem você pode iniciar o container com o seguinte comando: 19895b08f19d7a4436afa1cb8af8f815939000d5468c7db10c4498317fd81cc3 Observe que utilizei o parâmetro -d que serve para jogar em segundo plano a inicialização do container, o que é opcional. Após isso será entregue um token que identifica o container, tipo esse: sudo docker exec -it 19895b08f19d7a4436afa1cb8af8f815939000d5468c7db10c4498317fd81cc3 bash Para navegar dentro do container utilize o seguinte comando: Com isso você estará dentro do container para caso precise fazer alguma coisa específica. Para sair é só digitar exit . Bom pessoal, por enquanto é isso, uma dica que dou é tentar criar imagens mais complexas do que a que eu exemplifiquei, caso tenham alguma dúvida é só deixar um comentário. Até a próxima! Publicado no dia",pt,42
468,2195,1472153743,CONTENT SHARED,-2271000159302779222,-3390049372067052505,-8234741624611736934,,,,HTML,https://medium.freecodecamp.com/hash-table-attack-8e4371fc5261?gi=7ca4362e014c,the moment when you realize every server in the world is vulnerable - free code camp,"The moment when you realize every server in the world is vulnerable Hash tables. Dictionaries. Associative arrays. Whatever you like to call them, they are everywhere in software. They are core. And when someone finds a vulnerability in such a low-level data structure, almost all software is implicated. This is a story of one of those core vulnerabilities, and how it took a decade to uncover and resolve. The story is pretty amazing. But for context, let's review what hash tables are. Hash Tables 101 (Skip this if you already know how hash tables work.) Hash tables are incredibly convenient and fast. They let you put labels on things and throw them into memory buckets, and later on you can pull them back out and use them for whatever you want. They were invented in the 1950s and their underlying mechanics haven't changed much over the years. Let's create a hash table and put some stuff in it: Each key and value will be stored in a bucket in memory. Lets say we start out with 5 empty buckets. When we add the key 'a', which bucket should it go into? We want to be able to find it easily later. This is where the hashing function comes in. Every hash table is backed by a deterministic hashing function that turns any key into to a large, fixed-length number, which we call the hash. So the hash for 'a' might be 12416037344. Because it's deterministic, if we run the hash function on 'a' again sometime later, we'll still get 12416037344. Now that we have the hash for our key, we need to reduce that hash into a bucket number (0-4 in our case). The simplest way to do this is to modulo by the number of buckets: Great. So 'a' goes into bucket #4. Now, if we keep going and hash 'b', we get 12544037731. And (12544037731 % 5) is 1. So 'b' goes into bucket #1. Now let's add 'f' to the table. Hashing 'f' yields 13056039271. And (13056039271 % 5) is 1. But we already have something in bucket #1! What now? We have a collision. Hash table collisions happen pretty often. One of the simplest ways to resolve collisions is to set each bucket up as a list, and just keep adding to that list whenever a collision occurs. Here's what that might look like: As we add more keys, it's fine to grow these lists. When we are looking up a key, we simply look through the target bucket's list for the key we're interested in. Collisions open the door to the biggest weakness in hash tables. As soon as we have collisions, the time required for accessing an element starts creeping up because we have to loop through the list within the bucket. When hash tables were invented, the selection of a great hash function came down to two things: Performance. It must be fast as hell. Of course. Uniform density output. A great hash function should consistently, uniformly distribute arbitrary keys nicely across the hash table, because we want to avoid collisions as much as possible. And that was it. Security was not in the picture. Some very simple, very fast general purpose hash functions were developed over the years, and they worked well for several decades.",en,42
469,2910,1482756143,CONTENT SHARED,672199059798181601,-8020832670974472349,4283749150543186146,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36",SP,BR,HTML,https://medium.com/microscaling-systems/labelling-automated-builds-on-docker-hub-f3d073fb8e1,labelling automated builds on docker hub - microscaling systems,"This is a follow up to my previous post Labelling problems with Docker automated builds . There is now a solution using a build hook . Automated builds are a popular feature on Docker Hub that build a new Docker image whenever a new commit is pushed to a git repository on GitHub or BitBucket. Docker labels are a powerful mechanism for adding metadata to your containers. As the New Stack says there are now some good practices for using them at label-schema.org . Build hooks allow you to generate up-to-date labels at build-time. So that your metadata is updated automatically as part of the automated build process. Why use an automated build? An automated build avoids the manual work of building, tagging and pushing Docker images. It also makes it easier to keep the code in your images in sync with the code in your version control system. Lastly there are some important security benefits if you rebuild your images regularly to incorporate security updates. If you're interested in automating your Docker build processes MicroBadger provides image change notifications . We check public Docker images you're interested in and call your webhook if they change. You can use this to trigger a build in your CI system or notify your team via Slack. Your first notification is free and you can join our private beta if you'd like more. How widely used are automated builds? Our research shows 26% of the the 410k public images on Docker Hub are using automated builds. They account for 32% of the 5.5 billion image pulls, 28% are official image pulls and the remaining 40% are pulls for non automated builds. What's the issue with labels? In general labelling your images is straightforward, using the LABEL command in your Dockerfile. However there are 2 label-schema.org labels that are very useful but need to be dynamic. These are org.label-schema.build-date and org.label-schema.vcs-ref. The vcs-ref lets you link your image to the code used to build it. The build-date is useful for several reasons. If your Dockerfile installs operating system packages your image might contain different versions depending on when it was built. Another example is when managing images at scale you might want to know all images in use that were built over, say, 3 months ago. For sensible reasons Docker don't allow dynamic code to run in the Dockerfile. Instead the ARG command is provided to pass data into the Dockerfile. This is easy when building images locally but for automated builds you need to use a build hook script. How does the hook work? Create a file called /hooks/build relative to your Dockerfile. This overrides the Docker build process so your script needs to build and tag the image. Here are the Docker Cloud docs on build hooks . Thanks to Pablo Chico de Guzmán and Justin Cormack @ Docker for helping us find this. The full example is on GitHub and the image is on Docker Hub . Here you can see the output from a build. Finally here you can see all the metadata on MicroBadger .",en,42
470,2678,1477669626,CONTENT SHARED,3262705634545542432,7645894863578715801,3480723906734623856,Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:49.0) Gecko/20100101 Firefox/49.0,SP,BR,HTML,https://medium.com/built-to-adapt/on-orchestrated-microservices-5c8bd787ead4,on orchestrated microservices - built to adapt,"I have this terrible habit of subtweeting during meetings. What can I say? It's a reasonably effective way for me to blow off the stress associated with a discussion that's driving me a little bit crazy without inflicting said crazy on the room. Except this particular subtweet went viral. I was amazed honestly. I didn't think this statement was particularly profound, but for some reason it resonated with a lot of people. One thousand retweets and one thousand likes are both within striking distance. Anyway, as the conversation has continued throughout the past week, I thought it might be fun to tell this tweet's story in another medium (see what I did there?) and deal with some of the feedback in more than 140 character snippets. Without naming names, I was watching a presentation centered around a proposed set of enhancements to a certain cloud application platform that would make the agile feedback loop a first class citizen. An ancillary point was raised about the necessity to facilitate the deployment of a set of related microservices as a set and in a specific order. I immediately started to twitch just a little, and fired off the tweet. You see, I've heard this particular requirement several times before. And I've never understood it. I believe the greatest power to be found in what we're calling ""microservice"" or ""cloud native"" architectures is the notion of absolute decoupling of deployment lifecycle. When I discuss these architectures with my customers, I often use analogies like ""change out the engines on an aircraft while it's in flight"" and ""replace the cells on a living organism."" In an always on services world, I simply don't see how you can create a 24/7 responsive user experience, and deploy new features, without the ability to replace any component of your system at will. And to further the point, I don't see how you can make that system secure (see my colleague Justin Smith's post below for context) without this ability either. So, if you're telling me that you're willing to punt on the supreme value add of microservices, then why would you be willing to take on all of the additional pain that comes with a distributed system? Read the blog above. Read the linked content. Keep going. If you don't figure out immediately that distributed systems are HARD then I'm not sure you're paying attention. As a system architect I would never agree to pay the distributed systems tax unless I was getting a clear and valuable benefit. Going on to some feedback, my good friend Dan Woods brings up the following: Well played Dan. You're absolutely right. There are in fact plenty of examples of distributed systems where order matters. But there are many more examples where order absolutely does not matter. Seldom do you get lucky. The overwhelming majority of the time you must design this order independence into the system. It's your responsibility to make each component not care if one of its dependencies is present. In fact, this sounds suspiciously like fault tolerance . Why do we care about fault tolerance? Because we know that the probability of failure in any distributed system increases exponentially with the number of nodes present in the system! One of your lovely little microservices is going to get sick on a fairly regular basis, and what are the rest of your microservices going to do? If you can't even orchestrate your initial deployment if something's missing, how in the world are you going to survive production? So if you're going to embrace distributed systems, please start from the first principle that every node in your system can at least start and run in a gracefully degraded state regardless of what else is transpiring in the system. If (and only if) you come to a point where this is seemingly impossible (prove it if you can), introduce an ordering dependency. It's just like the notion of database transactions. They're absolutely useful. And over the course of the last couple of software engineering decades, we've made it stupid easy to add transactional behavior to applications. In doing this, somewhere along the way we became convinced that everything should exist within a transaction. There is no business behavior that can exist without ACID! The interesting thing that I see is that almost nothing seems to be ACID in the real world. Processes truly requiring ACID really do exist, but there really aren't that many of them. Distributed systems requiring complex orchestration really do exist, but there really shouldn't be that many of them. So what now? Time for the standard consultant answer: it depends. Should you recompose all of your microservices into a monolith? Or should you apply foundational distributed systems patterns to eliminate your orchestration issue? Yes. I don't know your context. I don't know which of these paths is going to be successful for you. I've absolutely chosen both of them in different situations based on careful consideration of the data in front of me: In that particular situation, I was smokejumping in to save an architectural dumpster fire. These were OSGi-based microservices, so they weren't paying the distributed systems tax, but there was still a complete failure to define effective component boundaries and service contracts. A monolith paying the OSGi tax is just as painful as a monolith paying the distributed systems tax. I wasn't convinced we had enough time or money to put out the fire any way other than to rip OSGi out and reify the monolith. Wouldn't you know that within 6 months this team was doing continuous delivery of working software to pre-production environments? But please don't tell me you're special... You're doing it wrong. I've spent the better part of the last three years helping Fortune 500 customers across every vertical imaginable build or migrate to microservices and distributed systems architectures. At the same time, I had the privilege of managing product for another distributed system you may have heard of: If there's one thing I've learned during this amazing journey, it's this: you're not special. There's no special business problem that you're trying to solve that puts you in a special class where multiple decades of distributed systems theory doesn't apply to you. You either need a distributed system that follows distributed systems principles, or you need a well-factored monolith that follows the design principles of OO or Functional or whatever programming paradigm you choose. There really isn't another option. ✹ This article is part of Built to Adapt , a publication by that shares stories and insights on how software is changing the way businesses are built.",en,42
471,2278,1473276480,CONTENT SHARED,-5446204508747882127,5127372011815639401,-468470094905372100,,,,HTML,http://www.martinfowler.com/bliki/StranglerApplication.html,bliki: stranglerapplication,"application architecture · legacy rehab tags: When Cindy and I went to Australia, we spent some time in the rain forests on the Queensland coast. One of the natural wonders of this area are the huge strangler vines. They seed in the upper branches of a fig tree and gradually work their way down the tree until they root in the soil. Over many years they grow into fantastic and beautiful shapes, meanwhile strangling and killing the tree that was their host. This metaphor struck me as a way of describing a way of doing a rewrite of an important system. Much of my career has involved rewrites of critical systems. You would think such a thing as easy - just make the new one do what the old one did. Yet they are always much more complex than they seem, and overflowing with risk. The big cut-over date looms, the pressure is on. While new features (there are always new features) are liked, old stuff has to remain. Even old bugs often need to be added to the rewritten system. An alternative route is to gradually create a new system around the edges of the old, letting it grow slowly over several years until the old system is strangled. Doing this sounds hard, but increasingly I think it's one of those things that isn't tried enough. In particular I've noticed a couple of basic strategies that work well. The fundamental strategy is EventInterception , which can be used to gradually move functionality to the strangler and to enable AssetCapture . My colleague Chris Stevenson was involved in a project that did this recently with a great deal of success. They published a first paper on this at XP 2004 , and I'm hoping for more that describe more aspects of this project. They aren't yet at the point where the old application is strangled - but they've delivered valuable functionality to the business that gives the team the credibility to go further. And even if they stop now, they have a huge return on investment - which is more than many cut-over rewrites achieve. The most important reason to consider a strangler application over a cut-over rewrite is reduced risk. A strangler can give value steadily and the frequent releases allow you to monitor its progress more carefully. Many people still don't consider a strangler since they think it will cost more - I'm not convinced about that. Since you can use shorter release cycles with a strangler you can avoid a lot of the unnecessary features that cut over rewrites often generate. There's another important idea here - when designing a new application you should design it in such a way as to make it easier for it to be strangled in the future. Let's face it, all we are doing is writing tomorrow's legacy software today. By making it easy to be strangled in the future, you are enabling the graceful fading away of today's work. reposted on 30 Jun 2014",en,42
472,2379,1474383177,CONTENT SHARED,-5414889822782394955,-1616903969205976623,-2746725884070612810,,,,HTML,http://exame.abril.com.br/publicidade/ibm/conteudo-patrocinado/6-exemplos-de-como-usar-computacao-cognitiva,6 exemplos de como usar computação cognitiva | exame.com,"Por permitir uma inovadora forma de aprendizado, a computação cognitiva, usada em sistemas como o Watson, da IBM, pode soar como uma tecnologia do futuro. Isso porque esse sistema de inteligência artificial é capaz de compreender e aprender com a linguagem natural e com textos, imagens e outros dados não estruturados, aqueles que ainda não foram convertidos para a linguagem tradicional das máquinas. Algoritmos sofisticados permitem ainda a análise de uma quantidade massiva de dados, para gerar insights e soluções. E isso é coisa do presente. Já existem muitas empresas, em diversas áreas, que utilizam a computação cognitiva. Conheça, a seguir, seis desses exemplos. Atendimento no Bradesco Um dos grandes potenciais dos sistemas cognitivos é sua capacidade de trabalhar com linguagem natural, em seus diversos idiomas, como o português. O sistema aprende a sintaxe da língua e pode diferenciar até pequenas variações idiomáticas e sotaques. Assim, torna-se fluente, graças a seus avançados algoritmos de aprendizado. O sistema pode chegar a um nível de fluência tão sofisticado que se torna capaz de interagir com clientes no atendimento por telefone, por exemplo. É isso o que o Bradesco está testando. O banco fez uma parceria com a IBM em que o Watson, em um primeiro momento, foi treinado e está respondendo a dúvidas para gerentes de agências por meio de um chat interno e, posteriormente, ajudará os atendentes a responder às perguntas dos correntistas. ""O sistema vai auxiliar o atendente a tratar melhor nossos clientes"", diz Marcelo Camara, gerente do departamento de pesquisa e inovação tecnológica do Bradesco, em entrevista a EXAME.com. Hoje, o Watson está em operação em 700 agências e tem respostas para mais de 50 000 perguntas sobre mais de 15 produtos do Bradesco. Fitness e saúde na Under Armour Fundada em 1996, a Under Armour é uma empresa americana de roupas e materiais esportivos que investe pesado em tecnologia. Possui uma série de aplicativos para smartphones e aparelhos que fazem a leitura de informações biométricas para ajudar na rotina de treinos de seus usuários, com dados como gasto de caloria, alimentação e tempo de sono. É aí que entra a computação cognitiva. A empresa anunciou, na feira de eletrônicos CES deste ano, que o Watson fará a análise dos dados registrados no seu principal aplicativo, o UA Record. Assim, os usuários vão ganhar novas possibilidades de insights sobre sua condição física e poderão compará-la à de outros integrantes da comunidade do app. O Watson utilizará sua capacidade de interpretar dados para orientar o usuário sobre quais caminhos usar para otimizar seu desempenho fitness e melhorar sua saúde. Combate ao câncer no Memorial Sloan Kettering Pioneiro no tratamento de câncer nos Estados Unidos, o Memorial Sloan Kettering Cancer Center está na fronteira do uso da tecnologia para o combate à doença, que matou 8,2 milhões de pessoas em 2012. Por conta de sua natureza mutante e diversificada, cada tumor é único e o tratamento também precisa ser planejado de forma única. Isso pode representar um desafio enorme para os médicos, mas não para o Watson, que consegue utilizar sua capacidade imensa de processar grandes volumes de informação para gerar novos insights. ""Nosso papel é o de professores do Watson. Assim como ensinaríamos um oncologista trainee, treinamos o sistema. Inserimos informações sobre como nossos médicos tratam os pacientes e muita literatura médica"", diz Mark Kris, médico chefe do departamento de computação cognitiva do Memorial Sloan Kettering. Com base em milhões de páginas de literatura, o sistema é capaz de sugerir possíveis tratamentos. ""Isso transformará todo médico no mais experiente do mundo naquele problema específico"", diz Larry Norton, chefe do departamento de câncer de mama do hospital. O Watson atua como um assistente superinteligente, dotado de um vasto conhecimento que pode ser utilizado para ajudar os médicos até nos casos mais complicados. Ajuda a engenheiros na Engeo Assim como ocorre na medicina, a computação cognitiva pode ajudar outros profissionais a tomarem melhores decisões no trabalho. A empresa Engeo, que tem escritórios nos Estados Unidos e na Nova Zelândia, tem feito isso por seus engenheiros. A empresa tem alimentado o Watson com informações fundamentais sobre engenharia e infraestrutura, que podem ser acessadas por profissionais ao redor do mundo, inclusive em países em desenvolvimento e em áreas atingidas por desastres naturais. Os engenheiros podem, assim, obter respostas com base na análise de uma variedade imensa de dados, o que seria difícil sem um sistema informatizado e inteligente. Turismo eficiente na startup WayBlazer Onde quer que estejam as perguntas difíceis, a computação cognitiva pode ajudar a respondê-las. Mesmo se a questão for cotidiana, como: ""Onde encontrar um bom lugar para jantar com três crianças em São Paulo, que tenha boas opções vegetarianas e esteja nas proximidades de uma estação de metrô?"", sistemas como o Watson conseguem cruzar uma vasta base de informações e oferecer respostas com alto grau de precisão. A startup americana WayBlazer está utilizando a computação cognitiva para isso. Especializada em roteiros de viagem, a startup oferece a outras empresas de turismo, como redes de hotéis e companhias aéreas, um serviço de recomendação altamente segmentado. Assim, quando um cliente faz uma pesquisa em seus websites, as palavras-chave são processadas pelo Watson, que usa sua capacidade de entender linguagem natural para oferecer uma série de opções, muito mais efetivas. Novos serviços financeiros no Citigroup Grandes empresas prestadoras de serviços financeiros, como o Citigroup, armazenam uma vasta quantidade de dados sobre transações e características dos clientes. Como usar esses dados para oferecer insights que facilitam a vida dos correntistas, além de novos produtos e maneiras de interagir com os clientes? O Citigroup está experimentando os potenciais da computação cognitiva para isso. ""No Citi, desenvolvemos constantemente novas formas de atender melhor nossos clientes"", afirma Don Callahan, chefe de operações e tecnologia. ""Agora, estamos trabalhando para repensar e redesenhar as várias maneiras como nossos consumidores interagem com dinheiro. Vamos explorar a tecnologia do Watson para prover serviços novos e seguros em torno de suas vidas, cada vez mais digitais.""",pt,42
473,559,1461522787,CONTENT SHARED,3035329611795434601,-1032019229384696495,-7645414203434198015,,,,HTML,http://dangrover.com/blog/2016/04/20/bots-wont-replace-apps.html,bots won't replace apps. better apps will replace apps.,"Lately, everyone's talking about ""conversational UI."" It's the next big thing. But the more articles I read on the topic, the more annoyed I get. It's taken me so long to figure out why! Conversations, writes WIRED , can do things traditional GUIs can't. Matt Hartman equates the surge in text-driven apps as a kind of ""hidden homescreen"" . TechCrunch says ""forget apps, now bots take over"" . The creator of Fin thinks it's a new paradigm all apps will move to . Dharmesh Shah wonders whether the rise of conversational UI will be the downfall of designers . Design, says Emmet Connolly at Intercom is a conversation. Benedict Evans prophecized that the new lay of the land is ""all messaging expands until it includes software."" ""People don't want apps for every single business that you interact with,"" says David Marcus, head of Facebook Messenger , ""...just have a message within a nicely designed bubble ... [that's a] much nicer experience than an app."" Under his charge, Facebook Messenger has tested this approach, building integrations with high profile partners as well as opening up a bot API. We've even seen avant-garde attempts at taking this idea to its extreme, like Quartz's latest app , which presents the news as a conversation, or the game Lifeline . Apps like Mailtime even promise to save us from our emails by turning them into chats. I guess I might be partially to blame for this, with a few pieces citing a section in a 2014 piece of mine that I literally titled ""Chats as Universal UI."" This recent ""bot-mania"" is at the confluence of two separate trends. One is agent AIs steadily getting better, as evidenced by Siri and Alexa being things people actually use rather than gimmicks. The other is that the the US somehow still hasn't got a dominant messaging app and Silicon Valley is trying to learn from the success of Asian messenger apps. This involves a peculiar fixation on how these apps, particularly WeChat, incorporate all sorts of functionality seemingly unrelated to messaging. They come away surprised by just how many differently-shaped pegs fit into this seemingly oddly-shaped hole. The thesis, then, is that users will engage more frequently, deeply, and efficiently with third-party services if they're presented in a conversational UI instead of a separate native app. It's that part which, having spent the past two years in my current job eating and breathing messaging, seems a major misattribution of what makes chat apps work and what problems they're best at solving. Note: The opinions expressed here are purely my own and do not reflect that of my employer. As I'll explain, messenger apps' apparent success in fulfilling such a surprising array of tasks does not owe to the triumph of ""conversational UI."" What they've achieved can be much more instructively framed as an adept exploitation of Silicon Valley phone OS makers' growing failure to fully serve users' needs, particularly in other parts of the world. Chat apps have responded by evolving into ""meta-platforms."" Many of the platform-like aspects they've taken on to plaster over gaps in the OS actually have little to do with the core chat functionality. Not only is ""conversational UI"" a red herring, but as we look more closely, we'll even see places where conversational UI has breached its limits and broken down. But first, let's retrace how this state of affairs really came about in the first place. A BRIEF HISTORY OF THE CHAT BUBBLE We'll begin by taking a closer look at the apparent atomic unit of the ""conversational UI"", our friend the message bubble. To do that, we're going to go back in time a bit. Let's take a stroll to, oh, about 2003. In those days, sending a quick text meant dealing with a UI that looked like this: In many phone's UIs, SMSes were treated like mini-emails, often complete with an inbox, outbox, and drafts. So fussy! Later, some time in the last decade, perhaps owing to a prototype by Jens Alfke , our IMs began taking on their familiar appearance as cartoon dialog bubbles. When smartphones took off later, it was a natural fit for the system SMS apps on the first versions of iOS and Android. Soon after smartphones launched, those default SMS apps were eclipsed instantly by third-party messaging apps emerging in Europe and Asia (in the US, we have somehow still clung to SMS). They had started as direct clones of the system SMS apps - the only difference being that messages were counted against one's data quota instead of the stingy and arbitrary SMS allotment given by carriers. These apps that came along initially to replace SMS have styled the message bubble every way imaginable: round and square, flat and puffy, green and blue. Free from the constraints of a 20-year-old protocol, these apps evolved, taking on more features. The bubbles displayed in these apps developed a number of affordances for new features like read receipts, names in group chats, and more. New kinds of bubbles emerged to accommodate new types of content these apps supported: The app I've been working on really takes the cake for this. WeChat's got bubbles for text, voice messages, big videos, l'il ""Sight"" videos, full-width cards with hero shots for news headlines, bubbles for payments, files, links, locations, and contact cards. Mucking through some code once, I saw definitions for nearly 100 types of supported messages, most I'd never seen in actual use. Aside from supporting so many different types of messages, another advance WeChat made was realizing a messaging app needed different types of accounts as well. They'd seen brands and celebrities registering personal accounts and making series of giant group chats to invite their fans into. There had to be a better way! Thus was born Official Accounts. Here's what one of the first accounts, China Southern Airlines, looked like when the feature launched in 2012: Yeah...this bot ain't exactly HAL 9000. Here's what the account for my city's subway system looked like: Why was the user asked to enter numbers, as if on an IVR system? Were the creators of these accounts so unimaginative to the possibilities of a new medium as to replicate their old-school hotline? Actually, no! In fact, keywords could be defined, and messages could be even routed through the third party's server to formulate a response using whatever method it pleases. Yet in this case, entering keywords or more complex queries in Chinese (or god forbid, formulating a complete sentence) would be even worse . At the time, typing in numbers really was the best UI choice given the constraints. Critically, these experiences were still often preferable to downloading a separate app on a data plan or spotty WiFi connection, or having to call someone's customer service hotline and wait on hold. The Official Account platform was a rousing success; there are over 8 million of these accounts today. As it took off, the APIs offered to third parties to build their accounts expanded to accommodate a growing array of use cases and demands. Some of these new APIs deepened and enabled new possibilities within the ""conversational"" nature of these interactions. Voice messages were transcribed via speech recognition before being sent to the OA's server. Objects could be recognized in pictures. Advanced natural language processing could even extract named entities and certain types of queries from text sent by users. Users could be patched in to agents at service centers to carry on a conversation exactly as they would with a friend in the app. There was even a special integration whereby I can select a message in a chat and forward it to Evernote's Official Account (as I would to a friend) to save it to a note. Cute, right? On the other hand, far greater and more successful were the enhancements made running counter or orthogonal to the idea of conversational UI. One affordance added right off the bat was the three-tabbed fixed menu. Now accounts could offer fast access to all their features without having to send a prompt or depend on state information. Here's what the menu looks like today on the Guangzhou Metro's main official account: Not only can those tabs send keywords, but they can open up webpages as well. Web apps invoked in this way can identify the user (using OAuth). They even have an extensive JavaScript API at their disposal to integrate with all sorts of features elsewhere in the app, even reacting to Bluetooth beacons. OAs gained the ability to send and recieve money. The accounts could have QR codes - both for the account itself, as well as parametric ones that can send along extra data (like what product I've picked up in a store or what table I'm sitting at). They gained the ability to authenticate me on their owners' WiFi hotspots (a development that emerged, no doubt, from merchants who had written the welcome message in the OA they made for their shop to tell customers their router's WiFi password). Official accounts could not only send out headline news to users, but, if they wish, host the linked articles on WeChat itself, letting users add comments and even send cash tips via the app. None of these things have anything at all to do with chat, but they're darn nifty! While this craziness was flying around out here, what sort of vision did those disruptors back on the west coast begin conjuring for our future bot overlords? Let's ponder this example from the homepage of Microsoft's recently-launched Bot Framework . Here's how they think we'll be ordering pizzas in the future: Good gravy, that's over 73 taps to tell Pizza Bot what I want. And this is when he already knows me on a first-name basis! I'd hate to see him when he's just warming up to someone. Man, counting those taps sure has made me hungry! We haven't quite got pizza here, but there's Pizza Hut , which is almost the same. Let me open their official account... I have, in 16 taps, ordered a pizza. That includes 1 for choosing 'medium', 1 for dismissing their coach marks, and 6 for entering my PIN. For some reason, it's not set up to use my TouchID. Afterwards, Pizza Hut's account even sent me a special transaction message with a link to let me track it: Well, it isn't exactly Ray's, that's for sure, but it's pizza. And I didn't have to leave my chat app to get it. The key wins for WeChat in the above interaction (compared to a native app) largely came from steamlining away app installation, login, payment, and notifications, optimizations having nothing to do with the conversational metaphor in its UI. It shouldn't require any detailed analysis, then, to point out the patent inanity of these other recent examples of bots and conversational UI proffered by companies on the vanguard of the trend: This notion of a bot handling the above sorts of tasks is a curious kind of skeumorphism . In the same way that a contact book app (before the flat UI fashion began) may have presented contacts as little cards with drop shadows and ring holes to suggest a Rolodex, conversational UI, too, has applied an analog metaphor to a digital task and brought along details that, in this form, no longer serve any purpose. Things like the small pleasantries in the above exchange like ""please"" and ""thank you"", to asking for various pizza-related choices sequentially and separately (rather than all at once). These vestiges of human conversation no longer provide utility (if anything, they impede the task). I am no more really holding a conversation than my contact book app really is a l'il Rolodex. At the end, a single call to some ordering interface will be made. Designing the UI for a given task around a purely conversational metaphor makes us surrender the full gamut of choices we'd otherwise have in representing each facet of the task in the UI and how they are arranged spatially and temporaly. Consider those made in Pizza Hut's acccount: I can see exactly how many slices a medium is, how much corn is inexplicably sprinkled on top of a ""Tianfu Beef"" pizza, what address it thinks it's delivering to, and exactly how much it will cost. So let's take these past few years in China as ""The Great Conversational UI Experiment."" Here, you have a messaging platform that achieved such total saturation among both users and businesses (to an extent that Facebook, Kik, and Telegram would die for). It boldly and earnestly carried the ""make every interaction a conversation"" torch as far as it could. It added countless features to its APIs - and yet those that actually succeeded in bringing value to users were the ones that peeled back conventions of ""conversational"" UI. Most instructively, these successes were borne out of watching how users and brands actually used the app and seeking to optimize those cases. You can see from Facebook and others' early forays into bots that they're already beginning to have the same hunch. Telegram's take is true to its inspiration in IRC-style slash commands. To be fair, it's still surprising the range of apps and services that can be shoehorned into a chat-style UI. No doubt it can be expanded with great AI and little UI affordances here and there. I should concede, too, that performing certain tasks in a chat brings along some useful side-benefits. It can be, compared to apps, a low-bandwidth, snappy, and consistent way to get a task done. I'm even left with a handy, timestamped, offline-viewable record of everything that's transpired. I can search it and quickly jump to media and links. I can clip parts of it and forward it to friends within the app, or save it to an archive. By interacting with certain services via a messaging app instead of via independent apps, when things happen that might deserve my attention, the thread gets bumped up in my inbox instead the message getting lost in a sea of push notifications and emails. And though it's clear pure ""conversational UI"" is ultimately a failed conceit, that last piece may be more important than it first seems... THE INBOX IS THE NEW HOME SCREEN The inbox is where it's really at. I am, of course, heavily biased, but I feel WeChat's is the best in class. I'd even go as far as to say it's an overlooked piece of genius in the app. Some key improvements (compared to the inboxes we're used to in email and SMS apps) include: Stickyability : If I want to stay on top of a particular thread in the inbox (whether it represents a person, a group, an official account, or another feature exposed here), I can ""sticky"" the thread to the top of the inbox. Mutability : I can mute notifications from any thread, but it will still pop up in the inbox as any thread does, only with an indeterminate red badge instead of a numbered one. Killablity : If I don't want to receive messages from something anymore, it's two taps to kill it. Hierarchy : News and promotions can be pushed to me through official accounts, but when they arrive, they just make the ""Subscriptions"" category pop up and show me the latest headline without interfering with other messages. When a service has a real reason to send me, personally, a message, it can pop out and appear in the main inbox. I find this approach superior to Gmail's ""sidelining"" messages into separate inboxes. Status Items : Persistent processes/statuses can be displayed in a special cell at the top. This includes things like being logged into a web/desktop client, using WeChat to authenticate on wifi, playing a song, or migrating data between phones. Searchability : The search bar on the main screen not only searches my contacts but my groups, chat history, favorited content, articles on the web, my newsfeed, and names of features in the app. It is telling, then, that in all localizations, the name of the first tab in the app is not ""Chats"" or ""Inbox"" (as in other messengers), but rather just the name of the app. Indeed, the cornerstone of whole experience is effectively a common, semi-hierarchical stream of messages, notifications, and news with a consistent set of controls for handling them. It's no stretch to see WeChat and its ilk not as SMS replacements but as nascent visions of a mobile OS whose UI paradigm is, rather than rigidly app-centric, thread-centric (and not , strictly speaking, conversation-centric). When you think about it this way, the things listed there in my inbox don't need to be conversations per se . But everything there, most abstractly, is something that can send me updates and notifications, will change in position when it does so, retains a read/unread status, and most essentially, allows me, the user, the aforementioned modes of control. And if we really run with this idea to its extreme, what actually might appear when I tap on a cell in the inbox doesn't matter - I could see a conversation, a song or video, news headlines, a map showing me my route, a timer, or a sub-group of other such threads. Anything, really. Though I guess it'd be best when it's at least something dynamic or based on a service (I certainly wouldn't want to access my calculator or camera this way). RISE OF THE TORTILLA CHIP APP This term - ""app"" - is rather old, yet only entered common parlance with the proliferation of smartphones. This is no coincidence. The app paradigm introduced on smartphone OSes circa 2007 was a radical improvement over what we'd had on the desktop. For the first time, software was easy to install, even easier to delete, and was guaranteed to not totally screw with your system (due to sandboxing/permissions models). At the time, smartphone apps were envisioned as baby brothers to desktop apps. On iOS, apps like Mail and Calendar were designed to evoke their Mac versions. Apple came out with pocket-sized editions of apps like Pages, GarageBand, and iMovie. For the first few years, setting up an iPhone even required plugging it into a desktop and syncing with that monstrosity known as iTunes. Though some apps indeed are mini-desktop apps that make full use of the supercomputer I carry in my pocket, well over half fall into another category. These apps are just a vessel for a steady stream of news, notifications, messages, and other timely info ultimately residing in a backend service somewhere. They don't really do much on their own. It's much like how a tortilla chip's main value is not so much in its appeal as a chip but as a cheese and chili delivery mechanism. The smartphone OS we use are still largely based on the assumption of my phone being a mini-desktop, rather than, well, an information nacho, if you will. Consequently, if you're making one of these apps, your app must give me something new daily (or more), or else it has no reason to live. Its information would be better shown to me via another app I do check often, like a social news feed or a messaging app. The only recourse the OS affords these apps in avoiding such a fate is the rather blunt instrument of push notifications (and things like Today widgets or Android home screen gadgets). THE OTHER WAYS SMARTPHONE OSES ARE FAILING US After coming to rely on WeChat in China, it can seem a bit like its own separate environment. After all, within it are not only my chats, but my social news feed, my news and blog subscriptions (many only available via the app), my digital wallet, my reading list. It even directly reads my step count from the various Bluetooth devices my friends and I use. It can scan QR codes, something my OS should do, but doesn't (more on this later). It can recognize songs being played, even books and other objects from photos. And you can pretty easily sling all types of data between these different areas of the app in ways you'd expect. Sometimes it reminds me of those awkward transitional days in the early 90's when one might launch Windows or other shell environments from DOS, then occasionally drop back out to do other stuff. That's what switching out of WeChat, to my homescreen, and into other apps is slowly heading towards. It should be no surprise, then, that I say it feels like my OS just isn't doing much for me lately. How is that? These days, a smartphone OS's job, aside from the low-level drudgery we take for granted (managing memory and thread pools and the like), is to provide some common infrastructure and higher-level services that apps can rely on. So that apps can focus on doing what they do best. And, well, it seems like there are so many areas where the OS is just not making much of a difference. Each item below seems like a petty, inconsequential annoyance - to the point where I feel like some kind of strange, cranky, millenial version of Andy Rooney for even writing it - but they quickly add up! Notifications - When I glance at my homescreen, there's red dots splattered everywhere. My eyes dart first towards a few I can interpret. WeChat, naturally, then Mail. My inboxes have 8,108 unread messages, but I surely would notice if it changed to 8,109. My ""Social"" folder has 4, one from Facebook which I will check, and three from other stray apps displaying ""1""s. I'm not sure what those apps are telling me, or what I'll need to do after opening the app to clear the dot. I think one might be from when my friend checked me in on Foursquare at a bar a few weeks ago on a trip back to SF, a fact I was aware of because I was standing next to him when he did it, and because the notification already appeared on my phone then. Another might be Instagram, which just throws up a red badge from time to time when it feels lonely. But I mainly know that if my ""Social"" folder is displaying a 3, there's probably nothing to see, and a 4 or a 5 may deserve checking. The system Messages app, which I still keep on my home screen, is showing 39 unreads, mostly one-time-passwords, transaction notifications from my bank, and spam. Messages, for most here, serves no other purpose. My ""News"" folder displays the sum of a few apps that are trying to tell me something. Airpocalypse is displaying the current AQI of 93 for Guangzhou in its badge. Starbucks has a '1'. What's that? Have I got a free coffee credit to redeem? Possibly a scone? Let's see. No, it's an unread message within the app's own inbox saying ""Welcome to the Starbucks App!"" from 43 days ago. Christ on a crutch. Even worse than those notifications gazing at me longingly from my homescreen are those that interrupt me. When I install a new app, I'm usually prompted right-off-the-bat to enable notifications for it. I'm taking a risk in doing so, not knowing how often or for what they'll be sent. When I'm interrupted by a superflous notification on my iPhone (or worse, on my Apple Watch), there's no quick way to tell it ""Shut up, and never bother me with this sort of thing again."" I must fish through Settings, find the app, and tweak it there. It is often easier to delete the app entirely. MIUI and some other flavors of Android at least allow me to mute a given app's notifications right after seeing one. Many apps offer settings to specifiy what sort of things merit notifications, but they're often located in different places and not worth the trouble. On iOS, if I miss a critical notification on the lock screen because I actually wanted to unlock my phone to make a call or look something up, until recently, there was no way to quickly go back and find what it was. iOS 9's notifications drawer, like Android's, now defaults to sorting notifications reverse-chronologically, instead of grouped by app - an advance five years in the making. Lastly, things become even more clunky across multiple devices. When I get home from work and crack open my personal laptop, I am notified a second time of all the Facebook messages I recieved during the past couple days, all of the LinkedIn invitations I already saw (because they sent me an email and another push on my phone), and all of my friends' birthdays. - When I left the US, QR codes were a joke . Putting them on things was a way to tell people you're a douche, like using lots of hashtags or wearing a Bluetooth headset. They were once this way in China, too, until WeChat doubled-down on them. Now, they're used for people, group chats, brands, payments, login, and more. They're in plenty of other apps as well. In a place where everyone has adopted them and knows how to scan them, they've become a wonderful, fast way to link the offline and online worlds that saves untold amounts of time. But they have a few downsides. One is that they look like robot barf. The other is that, at least here, if you scan a code in the wrong app, you'll get a webpage telling you to go install the right app, if not something totally inscrutable. Something that was once defined as an open standard is now non-inoperable. I predict great things for Facebook and Snapchat's de-uglified take on QR codes. Still, I wish my phone's OS could scan any such code (or detect them in photos) and do the right thing, but it seems the window of opportunity has passed for this. App Distribution - Aside from the obvious gripes - the app store's poor discovery mechanisms and inconsistent approval process - I'd written an aside in my last piece about the ways iOS's App Store misses the mark in China . In short, it's dog slow and doesn't support QR codes (which appear in every app advertisement here). Apps Are Too Big - Not to mention, apps are just too darn big these days. Twitter, an app that displays 140 character messages, weighs in at 72 MB. Bigger apps are less likely to be downloaded on data plans, or even on bad wifi connections. And much more likely to be deleted, forcing users to go through the setup process again every time they re-install them. Apple's tried to solve this problem via app thinning and on-demand resources , but it hasn't seemed to make a difference yet. David Smith astutely summed up the issue in his post ""16GB is a bad experience"" , and, I would add, this experience is one disproportionately had by mobile users in the developing world. Contacts & Social Graph - The idea behind the Contacts app (beyond giving me a way to tag phone numbers with names) is to act as a central repository where a single entry for a person can be linked to every kind of phone number, address, or ID I know for them. iOS's version has roots in the Address Book in OS X and NeXTStep . In theory, I should be able invoke it in an app to store or retrieve a person's info for the task at hand, rather than maintaining the same contacts in a bunch of separate app-specific databases. In practice, well, it doesn't really work that way. The concept of a person as they exist in Facebook or WeChat is rather disjointed from their profile elsewhere. Not only this, but adding people could be far better. Something clicked in my mind the first time I met a cute girl and she asked to scan my QR code (rather than type in my phone number or search for me on Facebook). Once I got in the habit of adding just-met friends and colleagues via QR code (or Bluetooth) I never wanted to add someone any other way. Why can't I pull out my phone and, with a swipe from the lock screen, add someone I've just met to my phone's contacts, with whatever phone numbers, websites, or messaging app usernames they've chosen to expose to me? Connectivity - I wrote before about how apps here get around people's reluctance to use their data plans . I'd mentioned WeChat, Alipay, and Xiaomi's attempts to make their WiFi-dependant users' lives easier. This is as big a problem in China as it is in many other developing countries. It's an issue the OS could address more directly, whether it's improving the process for authenticating on public hotspots or giving me better ways to monitor my usage. Authentication - When I open most apps for the first time, they either make me sign up for a new account with my email, use Facebook or other third-party services to log in, or, as is increasingly common, use my phone number to send me a one-time password. These are super clunky. Apps should be already logged in the first time I open them. There should be some flexible concept of identity that the OS can provide to apps immediately without asking, and then, with permission, supplement with further details. If users must switch identities, maybe a Mozilla Persona -like system could be adopted. Anything would be better than the mess that is app login now. Data Interop - My apps are terrible at sharing data. Lots of friends send me screenshots of articles, chats, tweets, even other apps as a way to share the underlying information. It's particularly annoying when compression artifacts make the text illegible or I want to go read the rest of the article or engage with the thing in the screenshot somehow. If I open a page in Facebook and want to share it in Twitter, I have to choose ""Open in Safari"", re-load the page, and do it from there (though Facebook clearly knows exactly what they're doing in that instance.) I wish the data in my apps was more atomic and could be freely shared, persisted offline, and searched in a consistent way. But this sort of thing has been a pipe dream since OpenDoc and OLE , so maybe it's just one of those things you should never do . Offline Storage & Storage Management - As a consequence of people being so reluctant to use their data plans, apps here are big on offline storage. All the music and movie apps do it, as do news apps and the third-party browsers popular here. Some give users detailed interfaces to manage their storage, even showing little pie graphs. I like this level of control, and I wish all my apps had it. I'd prefer not to think about storage, but if I have to clear data, I'd rather do it from a central UI rather than going into each individual app to manage the things it has saved (or deleting the app out of frustration). - I wrote before about how nifty online payments are in China . Any website or app that takes my money pretty much uses Alipay or WeChat Wallet. In the US, I have to type in and update credit card and address info for every new app I install. We have OS-provided solutions in Apple Pay and Android Pay, but these seem to be accepted in few places and strictly NFC-based, limiting potential network effects. The nice thing about the solutions here is just how many combinations of scenarios and hardware they've covered, whether it's expensive POS equipment that just requires me to hold my phone up, to scanning a pre-generated QR code the merchant has printed on a vinyl mat, to web payments, to 3rd-party app payments, to peer to peer payments between normal users who aren't connected. Whether you're an app startup or a mom and pop convenience store, you have no excuse to not accept one of these solutions. And as a user, there's no place where it's more frictionless to part with your money. When will blowing my hard-earned dough in US apps be this easy? So the meta-platforms - WeChat, Facebook, LINE, and the like - have come and addressed many of the pain points above. They've delivered solutions neither the open web nor those behind the closed app store model were coordinated enough, thoughtful enough, or perhaps incentivized enough to produce. Originally, the whole tradeoff we were promised with locked-down devices and app stores was that things were much nicer inside the "" walled garden ."" But over the years, as so many weeds sprung there, others came and built another wall with another garden inside of it, with yet another gatekeeper to deal with. In the 1990's, OS makers shook in their boots over the prospect of web browsers disintermediating them, but somehow it's taken more than another decade for the next challenger to emerge in the peculiar form of messaging apps. And though they're still quite far from wholly replacing the high-level features OS offer to users and app developers, we can clearly see the beginning of this encroachment. So here we are. What do we do? A LITTLE LESS CONVERSATION, A LITTLE MORE ACTION I don't know about you, but here's what I want to see happen. I want the first tab of my OS's home screen to be a central inbox half as good as my chat app's inbox. It want it to incorporate all my messengers, emails, news subscriptions, and notifications and give me as great a degree of control in managing it. No more red dots spattered everywhere, no swiping up to see missed notifications. Make them a bit richer and better-integrated with their originating apps. Make them expire and sync between my devices as appropriate. Just fan it all out in front of me and give me a few simple ways to tame them. I'll spend most of my day on that page, and when I need to go launch Calculator or Infinity Blade, I'll swipe over. Serve me a tasty info burrito as my main course instead of a series of nachos. The next time I'm back stateside, I want my phone to support something like Chrome Apps, but retaining a few useful properties of apps instead of being big, weird icons that just link to websites. I want to sit down at T.G.I Friday's and scan a QR code at my restaurant table and be able to connect to their WiFi, order, and pay. Without having to download a big app over my data plan , set up an account, and link a card when it is installed. Imagine if I could also register at the hospital or DMV in this fashion. Or buy a movie ticket. Or check in for a flight. As a user, I want my apps - whether they're native or web-based pseudo-apps - to have some consistent concept of identity, payments, offline storage, and data sharing. I want to be able to quickly add someone in person or from their website to my contacts. The next time I do a startup, I want to spend my time specializing in solving a specific problem for my users, not getting them over the above general hurdles. I don't actually care how it happens. Maybe the OS makers will up their game. Maybe Facebook, Telegram, or Snapchat can solve these problems for me by bolting solutions onto their messaging products. Hell, maybe Chrome or UC Browser will do it. Or maybe it'll be delivered in some magic, blockchain-distributed, GNU-licensed, neckbeard-encrusted solution that the masses, in a sudden epiphany, repent to. As they say at Pizza Hut, there's more than one way to skin a cat. But more than anything, rather than screwing around with bots, I want the tech industry to focus on solving these major annoyances and handling some of the common use cases I described that my phone ought to do better with by now. SEE ALSO If this is the first essay you've read that exposed you to the wacky and wonderful world of Chinese software, you may enjoy my 2014 summary on the topic and its 2016 followup . ACKNOWLEDGEMENTS Thanks to Kevin Shimota, Jeff Dlouhy, Andrew Badr, Jon Russel, Muzzammil Zaveri, Sonya Mann, Stephen Wang, Hank Horkoff, Mark Evans, Michael Belfrage, and Jake Rozin for reviewing drafts of this essay.",en,42
474,1748,1468370602,CONTENT SHARED,8559816910657872807,-1032019229384696495,-7293767101067256668,,,,HTML,http://www.businessinsider.com/google-cloud-boss-on-aws-and-microsoft-2016-7,google cloud boss diane greene: we're winning against aws and microsoft,"Fortune/screenshot Google's head of cloud computing says that when companies do invite Google into the bidding process for cloud computing contacts, Google almost always wins. ""We can actually win an RFP pretty much every time against AWS or Azure. Our growth is great,"" Greene told attendees of the Fortune Brainstorm Tech show in Aspen, Colorado on Monday. ""We have the best infrastructure, our own network backbone, our own fiber, a very cost-effective data centers, very automated."" And that means Google can offer low prices on those request for proposals (RFP). Greene also said that internally, the culture at Google didn't have to change that much to serve enterprises, which was even a surprise to her. While she's spent her first seven months creating the kind of classic internal organizations that enterprises need (sales, support, a CTO office to help Google do more custom projects), the underlying culture was already a fit, she says. ""I wouldn't have given Google a good rating when I started,"" she says of her misconception. ""But every Google engineer wants to make users happy, they really do. They were focused on making internal users happy. Now they can make the whole world happy. We don't have to shift the culture."" For instance, she says Google engineers work the weekends to give the customers more capacity, and people go to the bat if a customer has a problem. ""You can't believe the dedication."" All of this, she thinks, is the winning combination to take on her two bigger, more established rivals, Amazon and Microsoft. Disclosure: Jeff Bezos is an investor in Business Insider through his personal investment company Bezos Expeditions.",en,42
475,808,1462456359,CONTENT SHARED,-655909700262679295,-48161796606086482,3791045604731925300,,,,HTML,http://exame.abril.com.br/negocios/dino/noticias/banco-desafia-empreendedores-e-estudantes-de-tecnologia-a-buscarem-inovacao-para-processos.shtml,banco desafia empreendedores e estudantes de tecnologia a buscarem inovação para processos | exame.com,"""No Hackatri, os inscritos terão como foco um desafio específico do banco: desenvolver um sistema eficaz, com um dashboard amigável, para os varejistas fazerem a gestão das transações com cartão de débito e crédito."" Gabriel Ferreira - Diretor da i9 Cada vez mais empresas têm lançado mão dos hackathons - maratonas de programação e desenvolvimento de soluções - para encontrarem soluções inovadoras para seus processos. Em Uberlândia, no Triângulo Mineiro, o Tribanco - instituição financeira do Grupo Martins - propôs um desafio aos empreendedores e estudantes de tecnologia, por meio do Hackatri - Hackathon do Tribanco - programado para os dias 14 e 15 de maio, na sede o banco. Ao todo serão 34 horas de imersão de equipes formadas por programadores, designers e profissionais de negócios para a criação e desenvolvimento de uma solução para o desafio proposto pela empresa. A equipe que apresentar a solução mais adequada e inovadora receberá uma premiação e ainda terá a possibilidade de desenvolver seu produto, projeto e/ou solução junto ao banco. O evento é uma iniciativa da i9 Uberlândia - centro de desenvolvimento de inovação e tecnologia do Triângulo Mineiro - em parceria com o Tribanco. ""Foi uma oportunidade que criamos para os participantes programarem, pensarem, planejarem, executarem, desenharem e prototiparem, por várias horas seguidas, desenvolvendo algo incrível e que ainda não existe. O objetivo é buscar uma inovação, pensando nos aspectos da abrangência do desafio lançado. No Hackatri, os inscritos terão como foco um desafio específico do banco: desenvolver um sistema eficaz, com um dashboard amigável, para os varejistas fazerem a gestão das transações com cartão de débito e crédito"", informa o diretor de projetos da i9, Gabriel Ferreira. O desafio dos participantes do Hackatri será solucionar esse problema real do Tribanco e de seus clientes, detectado junto aos comerciantes e varejistas no controle e administração das operações de cartão. Ao final de dois dias de evento, os participantes deverão apresentar um projeto que solucione o processo. Esta é a última semana para os interessados se inscreverem para o Hackatri. Serão 100 vagas no evento. As inscrições são gratuitas e realizadas no site. Segundo presidente da i9, Robson Xavier, promover hackathons é mais uma expertise da associação que tem como missão desenvolver o ecossistema de inovação do Triângulo Mineiro. ""O Tribanco confiou em nós, abriu as portas para promovermos o evento e assim temos mais uma oportunidade de fomentar o ecossistema de inovação da região. Uberlândia reúne empreendedores e estudantes com grande potencial criativo e inovador. Essa é uma maneira de aproveitarmos isso. Agradecemos ao Tribanco pela confiança"", comenta. Premiação O Tribanco premiará os 03 (três) melhores projetos apresentados durante o Hackatri, e julgará conforme os critérios estabelecidos, quais sejam, dentre outros, (1) aplicabilidade, (2) criatividade, (3) grau de inovação, (4) usabilidade, (5) potencial de impacto no mercado, (6) design da solução, (7) status e evolução da solução, e (8) apresentação do projeto desenvolvido. O julgamento dos projetos será realizado em uma única etapa, sendo concedida uma premiação principal para o projeto vencedor, que será conhecido no dia 15/05/2016 no Hackatri, conforme programação divulgada. Serão premiados três grupos. Premiação 1º Lugar: * Possibilidade de contrato com o Tribanco; * 2 Smartphones Samsung Galaxy S6; * 1 Óculos Samsung Gear VR; * R$1.000,00 em compras no site efácil; * 1 anuidade de startup na ABStartup; * 12 meses de mensalidade paga na i9 Uberlândia para um membro ou para uma Startup; * 2 Ingressos para o Startup Farm Day. 2º Lugar: * 1 Smartphone Samsung Galaxy S6; * 1 Óculos Samsung Gear VR; * 1 anuidade de startup na ABStartup; * 12 meses de mensalidade paga na i9 Uberlândia para um membro ou para uma Startup; * 2 Ingressos para o Startup Farm Day. 3º Lugar: * 1 Óculos Samsung Gear VR; * 1 anuidade de startup na ABStartup; * 12 meses de mensalidade paga na i9 Uberlândia para um membro ou para uma Startup; * 2 Ingressos para o Startup Farm Day. Serviço O quê: O primeiro HACKATRI ocorrerá nos dias 14 e 15 de maio de 2016. Quando: Início dos Trabalhos: dia 14/05/2016 às 08:00. Finalização dos Trabalhos: dia 15/05/2016 às 18:00. Onde: O evento ocorrerá no Tribanco, no Auditório Jerônimo Martins do Nascimento, entrada pela a Rua Jataí Nº 1.150 - Bairro Aparecida - Uberlândia/MG - CEP: 38400-632. Mais informações: Site: Regulamento: Facebook: Website:",pt,42
476,1331,1465556994,CONTENT SHARED,-861913102049789637,-2979881261169775358,6661249173876371595,,,,HTML,http://blog.deepart.io/2016/06/04/color-independent-style-transfer/,transfer style but not color,"DeepArts are beautiful! Using deepart.io , we can turn photos into beautiful pieces of art. But what if we want to keep the beautiful colors in the photograph and still draw it in the style of the great Picasso? So far this wasn't really possible, since our style transfer algorithm would also transfer the colors from the style image. Well, we fixed that. We found a way to transfer only drawing style, but maintain the color composition of the original photo: Pretty amazing, isn't it? Below you can flip through more examples. The first image is always the original photograph, followed by renderings in the style of five different artists (style images are shown at the end). And here are the style images we used to generate these examples: So what do you think? Do you like this new style transfer? Should we include this option on deepart.io ? Share your thoughts in the comments.",en,41
477,2848,1481198861,CONTENT SHARED,564282370384747453,-2979537012405607453,1533128269484353266,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.98 Safari/537.36",MG,BR,HTML,https://www.scrumalliance.org/community/articles/2016/march/modifying-behavior-with-sprint-data,scrum community - scrum alliance,"Can I tell you a secret? One of my biggest pet peeves is not completing stories in the sprint during which we've agreed to do them. Carrying them over is like having that itch you cannot scratch. The closure offered by delivering them at the end in a potentially shippable product increment is satisfying. Having to carry them over has consequences: We have to continue working on them. We can't pick something else up. We are potentially blocked from shipping an increment. That last is the worst. Our stakeholders love when we deliver new increments. If we have planned correctly, it solves their most current problem in a very short amount of time. If we fail, stakeholders don't get what they need, when they need it. End-of-sprint data plays an important role in helping us understand where we have failed and what we need to do to improve. For this article, we are interested in the percentage of story points coming from new content vs. points from carryover content, and the percentage of points added after sprint planning. I want to focus on the first part (new content vs. carryover), its root causes, and its relationship to the second part (stories added after sprint planning). Two general patterns emerge from looking at our end of sprint data over the last nine months. First, since we committed ourselves to getting work done within a sprint, our percentage of carryover points has generally decreased. To reach this point, we made a few simple (although not necessarily easy) changes: Most simply, we decided that lots of carryover was not acceptable and was holding us back. We could not deploy incrementally. It made us unpredictable. We also started writing smaller, better stories. We can better estimate what we can complete within a sprint by limiting the scope of stories. We learned, changed our behavior, and adapted. Second, we see fluctuation from sprint to sprint in the amount of carryover we experience. I think there are several root causes for this. These provide areas for our continued improvement. Story scope: We have done a better job of reigning in the scope of individual user stories. Playing regular sprint poker has helped us with this. However, sometimes we still miss the mark. Our most recent instance of this was a missed requirement that we needed to quickly implement. Its scope turned out to be larger than anticipated. Our definition of what needed to be done was inadequate. This led to misunderstanding in implementation and failed tests. We would have been better served to have taken more time understanding what needed to be done and breaking a big story down into smaller ones that could be taken up by more team members. Lesson learned (again). Cross-training: Great Scrum teams have all of the skills they need to deliver a potentially shippable product sprint after sprint. In general, we have this. However, we are fragile in this area. We could be stronger with respect to testing skills. If our primary testers are pulled into field support, we need to support each other by stepping in to fill the testing gap during that sprint. We also need to continue to become fluent in all of the technologies our product uses. Often, we see server-side experts struggle with client-side code. Similarly, client experts may not be as fluent with server code. Pairing helps bridge this gap. Communication helps. Also, see #3 below. Asking for help: We operate in a ""Scrumban"" environment. We plan our sprints carefully, but we also find that team members are able to pull in small items along the way. This is motivating because, ideally, the team completes what it signed up for plus some additional content that provides extra value or reduces pain for a stakeholder. However, there is a balancing dynamic at work here. Look at the figures below. The top panel shows the measured data. The bottom panel represents a more ""ideal"" state. The top panel represents aggregated data across the first three sprints in our current release. The x-axis shows whether content was added after sprint planning or not. The blue bars represent items that were not carried over from a previous sprint. The red bar represents carryover. The issue occurs when comparing the red bar to the blue bar to its right (""late adder - no/carryover - yes"" vs. ""late adder - yes/carryover - no""). We should be decreasing the percentage of points coming from these two in favor of increasing the height of the ""late adder - no/carryover - no"" bar. This is shown in the bottom panel. Note that the absolute percentages shown in the bottom panel are for illustration only. The point is the relative contribution of each bar. Operationally, we need to make sure that our priority is on finishing what we commit to first. The hurdle is in reaching out for help when we are stuck. We are a team of extremely smart people. Development problems are personal challenges. However, we need to continue to recognize that we are here to support each other. Asking for help is a good thing. It's the behavior we want to encourage. It is not symptomatic of being unable to meet a personal challenge. It's not an intellectual insult. We need to start asking the question, ""Before I pull something else in off of the backlog, can I help someone else complete what we committed to?"" Come to the ""alter"": We are experimenting with a mid-sprint ceremony we call ""come to the alter."" (Yes, ""alter"" is spelled correctly.) This is a mid-sprint assessment of how likely it is that we will complete everything we signed up for in sprint planning. Based on the assessment, we might alter the sprint plan. For example, if we are pulled into more technical support than we anticipated, we may be short on resources for testing or for specialized protocol work that not everyone is capable of doing. In that case, we might pull stories out of the sprint in order to not leave them half-implemented in the code. Alternatively, we might split a large story and do a piece of it so that we can deliver some value to stakeholders. Ideally, there are no alterations. However, the ""real world"" does not always make that possible. Most importantly, it gives the team another explicit opportunity to step in and help each other. If we internalize that stories are in jeopardy, someone may step up with a clever way of reducing the risk. Enabling flexibility allows us to focus and be sustainable. The bottom line is that we need to continue measuring how we do sprint over sprint. Further, we need to be careful about the behaviors we reinforce. We need to balance adding value for one stakeholder by pulling an issue into a sprint vs. harming another stakeholder by not finishing a story we committed to. In general, we should bias our behavior toward the stakeholder we made the commitment to during sprint planning. Opinions represent those of the author and not of Scrum Alliance. The sharing of member-contributed content on this site does not imply endorsement of specific Scrum methods or practices beyond those taught by Scrum Alliance Certified Trainers and Coaches. Current rating: 4 (3 ratings) The community welcomes feedback that is constructive and supportive, in the spirit of better understanding and implementation of Scrum.",en,41
478,1948,1470055760,CONTENT SHARED,-1111518890369033396,5127372011815639401,-3485954857846807242,,,,HTML,https://cloudfour.com/thinks/the-business-case-for-progressive-web-apps/,the business case for progressive web apps,"Technical articles about Progressive Web Apps abound, but few tackle the question of why businesses should build Progressive Web Apps and why they should do so now. Progressive Web Apps describe a collection of features that were previously only available in native applications. These features include: Extremely fast load times Ability to use apps offline (without an internet connection) Push notifications Installation to the home screen Now that these features are available using web technology, browser vendors such as Google, Opera, Firefox and Microsoft are providing incentives for businesses to adopt Progressive Web Apps. Browser vendors want to promote Progressive Web Apps not simply because they are the latest and greatest technology. They want to promote them because Progressive Web Apps represent the best practices for websites and applications. The biggest current incentive is that browsers will prompt visitors to install your Progressive Web App on their second visit. This criteria will change in the future as browsers learn more about when people install Progressive Web Apps. This is just the beginning. Microsoft is exploring how to add Progressive Web Apps to Windows Store . Google has a long list of ideas for app discovery that they are investigating. And it seems inevitable that search engines will give special treatment to Progressive Web Apps in the future. So you can consider Progressive Web Apps your early-bird ticket to future SEO . Progressive Web Apps aren't simply about bringing native features to the web. They solve real problems that businesses are facing. Problems that caused Recode to recently declare that the app boom is over . Standing out among the over 2 million available apps on the iOS App Store or Google Play Store is a tall task. The ability to convert paid or organic search visitors into Progressive Web App installs provides a better path than competing in app stores for attention. Comscore recently reported that most smartphone users download zero apps per month. Dan Frommer, summarizing the report for Quartz , said: Only about one-third of smartphone owners download any apps in an average month, with the bulk of those downloading one to three apps. While it is still early, Progressive Web Apps are designed to bypass app fatigue because: People discover apps naturally via social media links or while browsing the web. Install app prompts are only shown when the web app meets a defined criteria and the user has demonstrated interest through repeat visits. App installation is instantaneous. The heavy lifting is done in the background the first time a person visits a site. Apps are much smaller in size because they leverage the capabilities of the browser. Push notifications, offline mode and all of the other features still work even if the visitor never installs the app. App abandonment has been a problem since the earliest days of the app store, and the percentage of people who abandon apps continues to increase . Andrew Chen worked with mobile intelligence company Quettra to analyze app loyalty data : Based on Quettra's data, we can see that the average app loses 77% of its DAUs [Daily Active Users] within the first 3 days after the install. Within 30 days, it's lost 90% of DAUs. Within 90 days, it's over 95%. Push notifications help promote user engagement and encourage people to return to the app. Both United eXtra Electronic and Jumia saw significant increases in conversion and engagement after implementing web push notifications. Push notifications aren't a panacea for app loyalty. Andrew Chen cautions against notification spam: To me, this is further validation that the best way to bend the retention curve is to target the first few days of usage, and in particular the first visit. That way, users set up themselves up for success. This is why the combination of push notifications with the re-discoverability of the web via search and social is a boon. Product managers need to do everything they can to promote user engagement, but if someone drops off, Progressive Web Apps give us more tools to bring them back in via traditional web methods. Native apps can only be used on the platform for which they are created. You can no more use an iOS app on your Mac than you can on an Android device. Plus, native apps only benefit those who install them. Progressive Web Apps will work anywhere the web does whether installed or not. They will even work on platforms that don't yet support all of the features of Progressive Web Apps in the same way old computers can access the newest websites-minus some of the bells and whistles. Maintaining applications for multiple platforms is costly. In the future, Progressive Web Apps could reduce this cost by providing a single app that works on any platform. When you're not on the app store, you're not limited by the app store rules and you don't have to pay the app store 30% of sales. There are two main downsides to Progressive Web Apps. Progressive Web Apps are new technology so there are fewer examples to follow. Progressive Web Apps are not available everywhere yet. Yet, neither of these are impediments to moving forward with a Progressive Web App today. The phrase progressive enhancement refers to the idea that you can build an experience that works anywhere and then enhance the experience for devices that support more advanced features. Progressive Web Apps share this ethos. Devices that don't support all of the features of Progressive Web Apps will still benefit from the improvements made to support them. This is why AliExpress saw an 82% increase in iOS conversion after building a Progressive Web App despite the fact that iOS doesn't yet support Progressive Web Apps. The steps you take to build a Progressive Web app benefit anyone who visits your website regardless of what device they choose to use. If Progressive Webs Apps are brand new and not supported everywhere yet, why should businesses start building them now? Why not wait? Google has been publishing case studies from companies that adopt Progressive Web Apps and their results are encouraging. AliExpress increased conversion rates for new users by 104%. United eXtra Electronics saw 4x increase in re-engagement and 100% more sales from users arriving via web push. 5miles decreased bounce rate by 50% and increased conversions by 30%. Konga used 92% less data for the initial load versus their native app. The Washington Post has built a Progressive Web App that is installed in the background when people view its AMP pages in Google search results. The Washington Post has seen a 12% increase in return visits from Google search results due to AMP. It remains to be seen how the Washington Post's Progressive Web App will do-they only launched the beta in May-but the performance improvements are impressive. They've gone from articles requiring 8 seconds to load in 2013 to 80 milliseconds in the Progressive Web App. Nearly every feature of a progressive web app is something that you should consider adding to your website anyways. The performance benefits alone are reason enough to work towards a Progressive Web App. Better performance means more revenue. Depending on the site, there is a chance the performance improvements alone could justify the investment in a Progressive Web App. Each incremental improvement you make on the path to a Progressive Web App is an improvement that will benefit your visitors immediately. Next year or the year after at the latest, everyone will be talking about how they need to make a Progessive Web App. The benefits are obvious and the incentives will grow. But as more companies start building Progressive Web Apps, it will become harder to get noticed. Right now, it only takes two visits to your site for a user to get prompted to install a Progressive Web App, but as the space gets more crowded, that heuristic will likely change making it harder to gain app installs. It is a green field for Progressive Web Apps. The time to start is now.",en,41
479,831,1462545164,CONTENT SHARED,3608243634452078989,4670267857749552625,-8979363093776014877,,,,HTML,http://www.administradores.com.br/noticias/negocios/carne-artificial-acabara-com-o-agronegocio-brasileiro-afirma-especialista/110412/,"carne artificial acabará com o agronegócio brasileiro, afirma especialista","As próximas décadas apresentarão mudanças radicais para a indústria alimentícia com o desenvolvimento dos alimentos artificiais, produzidos de forma muito mais rápida e barata. A previsão é do professor do Vale do Silício José Luis Cordeiro, fundador e docente da Universidade da Singularidade da Nasa, que atualizou o público sobre os estudos científicos mais inovadores do mundo durante a palestra ""O Futuro do futuro: As promissoras perspectivas para a raça humana"", realizada na 32ª edição da APAS 2016 - Feira e Congresso de Gestão Internacional, evento promovido pela Associação Paulista de Supermercados (APAS). Pautado pelos avanços tecnológicos dos últimos anos e pelos investimentos globais no desenvolvimento de novas tecnologias para as áreas de medicina, alimentação e comunicação, Cordeiro afirma que nos próximos 20 anos a humanidade irá vivenciar mais mudanças do que as vistas ao longo dos últimos dois mil anos. ""A tecnologia avança exponencialmente e, em três décadas, teremos desenvolvido aparelhos mais complexos do que o cérebro humano"", afirma o professor. Para ele, a tecnologia do futuro parece magia aos olhos do presente. ""A ficção hoje é a realidade de amanhã."" Para o professor, mudanças substanciais na indústria em geral, como na alimentícia com a criação e popularização dos alimentos artificiais, vão contribuir para uma produção mais ecológica e humana, além de gerar economia de tempo e custo. Para o agronegócio como o conhecemos hoje, essa perspectiva não é positiva. ""A carne artificial deve, em 10 anos, acabar com o agronegócio brasileiro"", afirma Cordeiro. Além disso, ele prevê grandes transformações no mundo do trabalho. Segundo ele, ""o trabalho também irá se transformar radicalmente, uma vez que não há futuro para o trabalho tradicional. Estes serão mais criativos e divertidos"". Hoje já vemos exemplos disso no mercado atual, ao qual houve uma transferência de valor do processo de manufatura para o que ele denomina como mentefatura, em que os produtos possuem um valor agregado pelas ideias.",pt,41
480,2533,1475851156,CONTENT SHARED,1990052153136096105,-534549863526737439,-1405487009929629533,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36",MG,BR,HTML,http://themacro.com/articles/2016/09/introducing-ask-a-female-engineer/,introducing ask a female engineer,"We've recruited a group of female engineers with years of industry experience to try an experiment with us called ""Ask A Female Engineer."" There has been an active discourse around women in tech over the last few years. It can sometimes be frustrating, though, because not everyone who has something to say feels comfortable participating in the (often excessively polarized) conversation. We'd like to try creating a place where readers feel comfortable engaging in the women-in-tech discourse, and female programmers can openly discuss their experiences. To kick off the project, we asked YC employees to send in questions. We received many great ones, and chose three for this post. The identities of all participants were kept anonymous (we've given them different names) to help promote frank conversations. We're excited about the opportunity to introduce new perspectives on this topic, but we also recognize that the opinions of a few people by no means represent the opinions or experiences of all women who code. We'd love to hear feedback and more perspectives on these questions, so we're continuing the discussion on Hacker News . I'm a software engineer at YC and will be moderating these posts. If you have questions you'd like to anonymously ask, or if you're a female software developer who would like to participate, please email ask@ycombinator.com . On a typical day, how conscious are you of being a female engineer and can you tell us about the ways in which you feel (or are made to feel) this way? Ada : It depends. When I work on a team or at a company where there are other women, it's easier to feel like just an engineer. But, the fewer women there are, the more conscious I am of being the odd one out. My appearance, body language, and voice brand every piece of in-person communication. It becomes difficult to separate whether coworkers are reacting to the content of my statements or the packaging, so to speak. At my current job, where I'm the only woman, it's something that's almost constantly on my mind. I listen to video bloggers and podcasts while working just so I can hear non-male voices. Otherwise I get home at night and realize I haven't heard another woman's voice all day. Grace : I am conscious that I am a woman every single day. The main thing that reminds me is probably my voice. For example, I once had one of my male managers tell me that I would be treated with more respect in meetings if I lowered and deepened my voice. Once one of my managers told me to take a HUP pill (harden up princess) because I had to go to the hospital with an arm injury. Over the years I've learned to avoid managers that don't respect me or women in general. That can take a little time to figure out, but as soon as I realize it's the case, I'll promptly organize a transfer to a different department. It's a waste of time trying to work as an engineer while someone is pushing you down. It just doesn't work. In America I'm recognized as a foreigner first and I'm more frequently asked about that. But in both places I have had managers and other engineers ask if I would quit working soon to have a kid. I started getting asked by colleagues I hardly knew if I was planning to have kids soon. That was 8 years ago and I still don't have kids by choice. It's just wasn't something I wanted to think about. Klara : I feel conscious of my gender when I walk into meetings and I'm the only female. I immediately figure out who is likely to hear me out based on our previous interactions. Then I'll purposefully sit somewhere near those allies. If I'm feeling less confident when I'm speaking, I'll look at them first. I'm less likely to even speak up in meetings where I don't feel like I have supportive people. I don't often feel like I'm not heard at meetings, but I think that's because I'm careful about when I choose to speak. Frances : I'm constantly aware of being female. When a coworker looks to a junior male engineer on the team for help instead of me, a senior engineer. Or when someone corrects my code in a way that shows their default assumption is that I don't know something rather than that I made a mistake. I feel like guys are more inclined to give each other the benefit of the doubt and assume a mistake is just a mistake rather than a demonstration of a knowledge gap. Once I was representing my company at a career fair with a male engineer. He and I were standing next to each other ready to take resumes and answer questions, but all the candidates lined up in front of him. I tried writing ""SOFTWARE ENGINEER"" in big letters on my name tag, but even then had to continuously tell people in line ""I'm an engineer and can answer your questions too!"" I'd like to know if you wanted to take coding classes in middle school / high school but decided not to because the classes felt unfriendly / unwelcoming (or were dissuaded for other reasons). If this happened, were there alternatives? Jean : I'll talk from both my own and my kids' experiences. I did take a couple programming electives in high school. This was back in the early 1980s. We wrote in APL (yikes!) and BASIC. I chose to take them because my brother had taken some several years earlier and brought home stuff that my 10-year-old self thought was cool. I realized I was good at it. Both my husband and I are software engineers, so for our 3 kids, we had a family high school graduation requirement of at least one programming class (a half-year class). We told them they didn't have to like it or major in it, but genetically, they were likely to be good at it! So far 2 out of 3 are in CS-related fields. The 3rd is a college freshman and is ""undeclared engineering."" Our daughter took a half-year of C++, then AP CS and was on the programming team in HS (she was the only female). None of her equally high-achieving female friends took any programming class of any kind. They all took AP Physics, AP Calculus, AP Chemistry, etc. They all planned to major in a STEM field, but just not in the T. One friend finally took an Intro to Python class this past spring (in her junior year of college) at my daughter's urging. This woman discovered that not only did she really enjoy it, but it came easily and she was good at it. She's now scrambling to try to get a CS minor at her liberal arts college. If only she'd tried it sooner... Grace : I'd actually recommend women interested in computer science go and study at university in Australia. The education is affordable and high quality. You can study in Australia and get a great job as an engineer both during your study and when you graduate. An important thing to remember is every country is different and has different cultural norms. Working in different countries really opens your eyes to this. Frances : I wish someone in my family had suggested programming to me growing up. It never occurred to me to try it or that I might like it. It certainly wasn't something my friends did or ever talked about as a possible area of interest so it just wasn't on my radar until I went to college. What are the non-obvious things men can do to make things better for female engineers? Frances : Don't be afraid to talk to a woman about the fact that she is female. She knows she is, it's not a secret. I've found my managers and coworkers often avoid talking about it because it's one of those uncomfortable topics. It's a giant elephant in the room. But whenever someone talks openly about it to me I'm so overjoyed! If I heard more things like ""Hey, I noticed X thing, does that make you uncomfortable?"" or ""I know it's weird being the only woman on this team, how can I make the experience more comfortable for you?"", that would make a huge difference. Just knowing the people around you are sympathetic and on your side is huge. Grace : Offer to mentor us. Encourage us to take on new opportunities, projects, and initiatives. Invite women you work with for coffee and ask what they like or dislike about their day-to-day. If they mention concerns or problems they're having, ask if there's any way you could help make it better. Klara : Invite them to team activities, even if you think they won't be interested. Also try to find activities they might have in common with other team members. As the only woman on the team, my male teammates tend to have more interests in common with each other. Since they make up the majority of the team, we tend to have team activities that fit more comfortably into their ways of socializing, like playing Call of Duty or StarCraft (though some women definitely enjoy those games!). The pressure is on me to fit in with what the all-male group likes. I end up feeling conflicted because on the one hand, the older I get, the more I realize building relationships at work is valuable, but on the other hand, I'm now less likely to conform. These days I tend to take a DGAF attitude and instead of conforming to try and force a bond, I just don't participate. Thanks to the participants for answering questions, YC folks for submitting questions, Dan Gackle, Scott Bell, Aaron Stein, Colleen Taylor, Kevin Hale, Kat Manalac, and Craig Cannon for helping out in various ways. If you have questions you'd like to anonymously ask, or if you're a female software developer who would like to participate, please email ask@ycombinator.com .",en,41
481,846,1462647856,CONTENT SHARED,-1492574794514500033,-1443636648652872475,-5937465818071606757,,,,HTML,http://www.posteriorscience.net/?p=206,programming by poking: why mit stopped teaching sicp,"In this talk at the NYC Lisp meetup, Gerry Sussman was asked why MIT stopped teaching the legendary 6.001 course, which was based on Sussman and Abelson's classic text The Structure and Interpretation of Computer Programs (SICP). Sussman's answer was that: (1) he and Hal Abelson got tired of teaching it (having done it since the 1980s). So in 1997, they walked into the department head's office and said: ""We quit. Figure out what to do."" And more importantly, (2) that they felt that the SICP curriculum no longer prepared engineers for what engineering is like today. Sussman said that in the 80s and 90s, engineers built complex systems by combining simple and well-understood parts. The goal of SICP was to provide the abstraction language for reasoning about such systems. Today, this is no longer the case. Sussman pointed out that engineers now routinely write code for complicated hardware that they don't fully understand (and often can't understand because of trade secrecy.) The same is true at the software level, since programming environments consist of gigantic libraries with enormous functionality. According to Sussman, his students spend most of their time reading manuals for these libraries to figure out how to stitch them together to get a job done. He said that programming today is ""More like science. You grab this piece of library and you poke at it. You write programs that poke it and see what it does. And you say, 'Can I tweak it to do the thing I want?'"" . The ""analysis-by-synthesis"" view of SICP - where you build a larger system out of smaller, simple parts - became irrelevant. Nowadays, we do programming by poking. As to why they chose Python as an alternative, Sussman joked that it was ""late binding"" decision. Python has a ton of libraries that make it applicable to many types of projects that instructors might want to assign (like writing software to control a robot.) Sussman admitted that the SICP curriculum was ""more coherent"" than what they have now and that they still don't know what the right curriculum should be. The full text of SICP is available online. The SICP video lectures by Sussman and Abelson from 1986 (given to HP) are also available. - Yarden Katz is a fellow in the Dept. of Systems Biology at Harvard Medical School.",en,41
482,2428,1474914360,CONTENT SHARED,4577451034168098359,3061273947166110301,-4294318496550281618,,,,HTML,http://www.cbarker.net/projects/cubr,cubr,"Cubr is a project I completed in three weeks at the end of my introductory computer science class at CMU. The idea is simple: you mix up a Rubik's cube. You show the cube to your computer's webcam. Some magic happens, and your cube appears onscreen. Then, the cube begins to solve itself, and all you have to do is follow along and you will have solved your cube! Here's the web version . The web version is missing webcam integration for now. Here's a video presentation: Color Recognition One of the biggest challenges in this project is color recognition. Figuring out what color a pixel is showing on your monitor is easy, because your computer has a digital model of it. But it's a little more tricky to read the input from a webcam to figure out what color is on a shiny sticker on a plastic cube. Every room has different lighting, stickers show glare if your monitor's brightness is too high, and so on. The best we can do is guess what color a sticker is. Of course, we can try to make our guesses really good. I gathered a fairly large set of data on the HSV and RGB values of each colored sticker in different lighting situations. Looking back now, I could have done a lot better with that data. I could have trained a machine learning model, and I could have taken into account whole-setting attributes rather than looking at each sticker individually (i.e., notice that this blue sticker is lighter than most blue stickers -> ask the model how that will most likely affect the orange stickers). Alas, I didn't do that. Instead, I did this: I wrote code to find the mean HSV and modal RGB from each sticker. I assumed that the dataset's HSVRGB curve for each sticker color had a normal distribution. The ""model"" finds the deviation from the mean along each dimension for each sticker color, and determines a probability for each sticker color. It returns the most likely color. Sounds good, right? Well... it was correct less often than hard-coded thresholds. If I had known what an HMM was at that time, maybe it wouldn't have been so inaccurate. But I didn't, and it was. And I needed accurate results for my presentation... so as much as it killed my sense of robust design, I eyeballed the HSV and RGB values and found some thresholds that worked 95% of the time in 95% of lightings. Ahh... it burns. It burns my soul. But it works. Of course, I included a GUI to manually modify the colors on each cell of the cube. A 3D ""mirror"" of the cube is shown on the same screen as the webcam's view. The colors on this mirror represent what will be stored internally, so the user can notice if it doesn't match up with the physical cube. The highest level functions I could use for drawing onto the numpy.ndarray image were polygon-filling functions. This made drawing a 3D Rubik's cube a bit of a challenge. Fortunately, I already had written some 3D-Vector-polygon-to-2D-output code for interfacing with Tkinter. A good GUI in Tkinter is possible, but... You have to reinvent the wheel. And the spoke, and the axle. Not wood, though. Wood is already invented for you. To create a GUI in Tkinter, I used one widget: a Canvas. In early versions of the game, I used Tkinter's Button and Label widgets, but they were unappealing and offered little flexibility. I wanted as much flexibility as possible for this project. That meant no VPython, because I would be unable to draw 2D shapes directly onto the Canvas, I would be unable to create polygons (they would need to be really thin polyhedrons), and the events to which I could bind listeners would be limited (VPython takes over clicking and dragging). If anyone knows how to get around any of that, I'd love to hear from you about it. But as it was, I was stuck using Tkinter. So, I wrote a geometry module to implement a Vector class and a Camera class. The Camera class has a flatten method that takes a 3D vector and ""flattens"" it to 2D coordinates based on the Camera's view. After I had this foundation, I could write all my code to reason in 3-space, and drawing onto the 2D canvas was taken care of. As for the buttons, Tkinter does have decent event handling. You can trivially change the color or border-width that a shape displays when it is moused over: And binding an event listener to the shape is equally trivial: I was able to design the GUI in an intuitive and responsive way, for the purposes of this project anyway, using Tkinter. The main limitation is that it could get slow. Rendering 3D on a 2D canvas using CPython is not the quickest thing in the world. Since I did not do any shading or texturing for this project, the performance was fine, but I would not recommend it for graphics-intensive projects. Tkinter does not interface with the OS as well as frameworks like .NET. For this project, though, most of the interface was atomic. However, I did run into some trouble when integrating with OpenCV. To display video from the webcam, I had to hand over control to OpenCV's ""NamedWindow"" class and put my Tkinter window in the background. It would have been ideal if I were able to integrate the video stream into my GUI. So, Tkinter has its limitations, but for projects that do not need to surpass those limitations, it can be used effectively. ""I'm sure you all know how to solve a Rubik's cube, but I don't..."" This was my opening line for my presentation on this project. People laughed. He created a Rubik's cube solver; of course he knows how to do it! Well, I understand the steps. I understand what each algorithm does. But I really haven't memorized those algorithms. I didn't get into software design because I wanted to memorize and regurgitate algorithms. I did it because I want to design and write them. If you gave me a Rubik's cube right now, I could solve it part of the way. I could solve the first two layers and a bit of the third. But after that, I'd have to get my computer out and fire up Cubr. I had a couple of choices when I was deciding which Rubik's cube solution algorithm I wanted to implement. One is the beginner's three-layer method. This is the one that most people know and memorize. I used to know it, and I could solve a cube in around 50 seconds using it ( that's not very good ). There are other algorithms that require a lot more memorization and quick pattern recognition; however, they are in the same class of solutions as the three-layer method. The other option was to write an algorithm to find an optimal solution. This would have been a lot easier to implement, because all I would have to do is walk a tree of all possible move combinations until I find a solution. Simple, right? Yes, it would be no more than 10 lines of code. BUT- it would take a while to find a solution. The maximum number of move sequences that would need to be tried is on the order of 18^20, which is about 1.275E25. That's an upper bound, of course, because many of those sequences could be excluded by not exploring the same node twice. But at that upper bound, assuming we have a super-fancy problem-specific logic gate that can execute 20 Rubik's cube turns on an array in one clock cycle, at 4GHz (why not, right?) that's 101,060 millennia to find the optimal solution. So my ten lines of code may not work so well.. Upon further research, I found that there are profile tables that can be used to speed up the search for an optimal solution. However, the algorithm I found was for a grad school project, and I only had three weeks, so learning how to create tables to solve NP-complete problems with a defined search space had to wait for another day. Instead, I implemented the three-layer algorithm. It is the most popular algorithm among beginners because it is intuitive and requires relatively little memorization. There were a couple hurdles to pass. Most beginners are taught to ""solve the cross"" on the top layer by moving the edges pieces on one face into their correct locations. This is intuitive for a human to understand, but special care was needed to cover all the possible cases in my solutions module. I have to admit, the solutions module is a lot more supervised than ""try to put that piece over there"". It examines each piece of the cube, in a specified order (except the third layer, which is done all at once), and chooses the correct algorithm from a set based on its position relative to its desired position. All in all, it came to over a thousand lines of code to explicate the algorithm. The most difficult part of writing the algorithm was translating ""if the piece is directly under where it needs to be, then turn the bottom face clockwise, turn the right face counterclockwise, ..."" into ""if the piece's location minus the edge face's location dotted with the desired location minus the edge face's location is equal to negative one..."". It was very much a spacial reasoning problem, and in order to abstract it as much as possible, that is, to generalize as many cases as possible, I found myself expressing conditions as various vector expressions that described a general situation, like crossing to vectors and dotting that with the face they share to determine whether a piece is clockwise or counterclockwise from another piece. And so on. The good news is, with all this guidance, the algorithm works very quickly, even with the overhead of python lists (as compared with ndarrays). On average, the algorithm takes less than 100,000 millennia.",en,41
483,1743,1468352919,CONTENT SHARED,2262656830196923781,7645894863578715801,-5674219745453993645,,,,HTML,http://blog.takipi.com/tabs-vs-spaces-how-they-write-java-in-google-twitter-mozilla-and-pied-piper/,"tabs vs spaces: how they write java at google, twitter, mozilla and pied piper | takipi blog","In spite of the suggestive image above, we don't want to commence any unnecessary holy wars. When it comes down to coding styles, most choices are pretty arbitrary and depend on personal preference. Yes, even if tab width changes between editors, and spaces tend to be more precise. If there were such a thing as developer team anthropology, style guidelines would probably be a major part of it. In this post we will highlight formatting guidelines and different Java coding styles in companies like Google, Twitter, Mozilla, the Java standard and our own teams at Takipi. Once and for all, do you use tabs or spaces? - Takipi (@takipid) July 6, 2016 Why use guidelines in the first place? Readability is the main consideration here. It's almost certain that you will not be the only person to read the code that you write. And the best thing you can do for the next person who reads your code is to stick to conventions. A consistent style of writing not only helps create good looking code, but also makes it easier to understand. The twitter guidelines specify one exception and we tend to agree, ""if the more 'readable' variant comes with perils or pitfalls, readability may be sacrificed"". The full style guides are available right here: Google Java style guide (there's another one for Android) Twitter style guide Official Java code conventions (New OpenJDK guidelines are available right here ) Mozilla guidelines Our own guidelines at Takipi Let's see what they have in store. 1. Indentation: tabs vs spaces First, we need to get this off our chests before proceeding. There's a clear preference for spaces over tabs in the style guides. We'll not go into pros and cons here and just share the findings: Perhaps developers who use tabs don't like writing style guides �� Data from Github suggests that around 10-33% of the Java repositories prefer tabs, and the majority use spaces in different formations, preferring 4 spaces over 2. There's actually a pretty nice module for running this analysis (comparing different languages). BTW, peeking over to other JVM languages like Scala and Clojure, we see almost 100% 2 spaces. Considering a larger dataset that covered individual commits gave us different results (The convention analysis project , one of the Github data challenge winners ), but we can estimate it to be somewhere in-between probably closer to 10%. (In case you were curious, at Takipi we prefer tabs. We're not barbarians. Go Richard Hendricks! ) Java Tabs vs Spaces - Analyzing popular code conventions (Source: outsideris/popularconvention ) 2. Line length, wrapping and breaks Sometimes lines of Java code tend to get long, and the style guides set conventions on when is it appropriate to break or wrap. The general convention is around 80-100 max length Of course apart from natural breaks after semicolons, line breaks are used not only when lines get too long but also for logic separation. The general convention is to break after commas, before operators, and use a hint of common sense. Here's an example from twitter's style guide that makes good use of this concept: This helps separate statements and create a logic where each line of code represents a contained / ""atomic"" operation. The style guides tend to agree here. Blank lines also have an important role in the mix, separating logical blocks. The Java standard style guide also has a reference to double line breaks, separating interface and implementation. 3. Variable Naming Wide agreement across all style guides. FirstLetterUpperCase for class names camelCase for method and variable names, all lower case package names and ALL_CAPS for final static constants. A common exception for this would be the logger, which we usually define as: Twitter's guideline adds another interesting style of including units in variable names: 4. Exception Catch Clauses Exceptions are a thorny issue. Recently we've covered a research that looked into over 600,000 projects on Github and Sourceforge and uncovered some grim truths about non standard use of exceptions. Google's and Twitter's style guides reference the infamous empty catch blocks: Moreover, another guideline we recommend to at least try to keep is making sure your exceptions are actionable , and avoid control flow exceptions. The amount of noise so called ""normal"" exceptions cause in a production environment is terrifying. The current state of exceptions and error handling, relying mostly on log files to get to their root cause in production, is our main motivation behind building Takipi . If you haven't already, check it out! We'd love to hear what you think. 5. Parentheses for clarity, and curly braces Even when they're not necessary, parentheses can help improve readability. With compound predicates, it's common to use parentheses for clarity, even when the order of execution is obvious. For example: Block statement styles - Analyzing popular code conventions (Source: outsideris/popularconvention ) So what do the style guides say about grouping parentheses? When dealing with curly braces, all style guides examined support breaking after the opening brace. At Takipi, we actually do the opposite, but we're not alone, while the ""inline"" curly brace is used by the majority of Java developers, 37% of code commits examined here use a new line: 6. No Brains Inside Constructors One guideline that we keep and didn't find in any of the style guides is not to keep any ""brains"" inside constructors (so they're safe from zombies - but not from weak jokes apparently). If we need to create an object and do some heavy duty operations to build it, we use a creator method instead. For example: Final Thoughts There are many more style guidelines that we didn't cover in this post to avoid making this an exhaustive list, and they're all available in the original docs linked at the beginning of the post. Readability is a major factor in keeping your code error free, and... it just feels right to keep these OCD senses from tingling. What are some of the unique guidelines / quirks that you follow? Does your company / team use a style guide of your own? Please feel free to share those in the comments section below!",en,41
484,2431,1474915111,CONTENT SHARED,7993526700719577624,6013226412048763966,7761905460736875088,,,,HTML,https://hbr.org/2016/09/what-science-tells-us-about-leadership-potential?,what science tells us about leadership potential,"Although the scientific study of leadership is well established, its key discoveries are unfamiliar to most people, including an alarmingly large proportion of those in charge of evaluating and selecting leaders. This science-practitioner gap explains our disappointing state of affairs. Leaders should drive employee engagement, yet only 30% of employees are engaged, costing the U.S. economy $550 billion a year in productivity loss. Moreover, a large global survey of employee attitudes toward management suggests that a whopping 82% of people don't trust their boss . You only need to google ""my boss is..."" or ""my manager is..."" and see what the autocomplete text is to get a sense of what most people think of their leaders. Unsurprisingly, over 50% of employees quit their job because of their managers . As the old saying goes, ""people join companies, but quit their bosses."" And the rate of derailment, unethical incidents, and counterproductive work behaviors among leaders is so high that it is hard to be shocked by a leader's dark side. Research indicates that 30%-60% of leaders act destructively, with an estimated cost of $1-$2.7 million for each failed senior manager. Part of the problem is that many widely held beliefs about leadership are incongruent with the scientific evidence. As Mark Twain allegedly noted, ""It ain't what you don't know that gets you into trouble. It's what you know for sure that just ain't so."" For example, it is quite common for people to believe that leadership is largely dependent on the situation, that it's hard to predict whether someone will be a good (or bad) leader, and that any person can be a leader. In reality, some people have a much higher probability of becoming leaders, regardless of the context, and this probability can be precisely quantified with robust psychological tools. What do we really know about the measurement of leadership potential? Here are some critical findings: Who becomes a leader? Although leaders come in many shapes, a few personality characteristics consistently predict whether someone is likely to emerge as a leader. As the most widely cited meta-analysis in this area shows, people who are more adjusted, sociable, ambitious, and curious are much more likely to become leaders. (53% of the variability in leadership emergence is explained by these personality factors.) Unsurprisingly, higher levels of cognitive ability (IQ) also increase an individual's likelihood to emerge as a leader, though by less than 5%. Of course, emergence doesn't imply effectiveness , but one has to emerge in order to be effective. What are the key qualities of effective leaders? The ultimate measure of leader effectiveness is the performance of the leader's team or organization , particularly vis-à-vis competitors. Leadership is a resource for the group, and effective leaders enable a group to outperform other groups. While the same personality and ability traits described above help leaders become more effective - they are not just advantageous for emergence - the best leaders also show higher levels of integrity , which enables them to create a fair and just culture in their teams and organizations. In addition, effective leaders are generally more emotionally intelligent , which enables them to stay calm under pressure and have better people skills. Conversely, narcissistic leaders are more prone to behaving in unethical ways, which is likely to harm their teams. How will the person lead? Not everyone leads in the same way. Leadership style is largely dependent on personality. Ambitious, thick-skinned leaders tend to be more entrepreneurial , so they are focused on growth and innovation. Curious, sociable, and sensitive leaders tend to be more charismatic , though charisma often reflects dark side traits , such as narcissism and psychopathy. Studies also highlight gender differences in leadership styles, with men being more transactional and women more transformational. However, gender roles are best understood as a psychological and normally distributed variable, as people differ in masculinity and femininity regardless of their biological sex. Are leaders born or made? Any observable pattern of human behaviors is the byproduct of genetic and environmental influences, so the answer to this question is ""both."" Estimates suggest that leadership is 30%-60% heritable , largely because the character traits that shape leadership - personality and intelligence - are heritable . While this suggests strong biological influences on leadership, it does not imply that nurture is trivial. Even more-heritable traits, such as weight (80%) and height (90%), are affected by environmental factors. Although there is no clear recipe for manipulating the environment in order to boost leadership potential, well-crafted coaching interventions boost critical leadership competencies by about 20%-30%. What is the role of culture? Culture is key because it drives employee engagement and performance . However, culture isn't the cause of leadership so much as the result of it . Thus leaders create the explicit and implicit rules of interaction for organizational members, and these rules affect morale and productivity levels . When people's values are closely aligned with the values of the organization (and leadership), they will experience higher levels of fit and purpose . How early can we predict potential? Any prediction is a measure of potential or the probability of something happening. Because leadership is partly dependent on genetic and early childhood experiences , predicting it from an early age is certainly possible. Whether doing so is ethical or legal is a different question. However, most of the commonly used indicators to gauge leadership potential - educational achievement, emotional intelligence, ambition, and IQ - can be predicted from a very early age, so it would be naïve to treat them as more malleable. Perhaps in the future, leadership potential will be assessed at a very early age by inspecting people's saliva. Does gender matter? Less than we think. The fact that so many leaders are male has much more to do with social factors (people's expectations, cultural norms, and opportunities) than actual gender differences in leadership potential, which are virtually nonexistent . In fact, some studies have shown that women are slightly more effective as leaders on the job, but this may be because the standards for appointing women to leadership positions are higher than those for appointing men, which creates a surplus of incompetent men in leadership positions. The solution is not to get women to act more like men, but to select leaders based on their actual competence. Why do leaders derail? We cannot ignore the wide range of undesirable and toxic outcomes associated with leadership. It is not the absence of bright side qualities, but rather their coexistence with dark side tendencies , that makes leaders derail. Indeed, as Sepp Blatter, Dominique Strauss-Kahn, and Bernie Madoff demonstrate, technical brilliance often coexists with self-destructive and other destructive traits. This is just one reason why it is so important for leadership development and executive coaching interventions to highlight leaders' weaknesses , and help them keep their toxic tendencies in check. Although these findings have been replicated in multiple studies, a skeptic could ask, ""Now that we're (allegedly) living in an era of unprecedented technological change, could some of these findings be outdated?"" Not really. Leadership evolved over millions of years , enabling us to function as group-living animals. It is therefore unlikely that the core foundations of leadership will change. That said, the specific skills and qualities that enable leaders and their groups to adapt to the world are certainly somewhat context dependent. For example, just as physical strength mattered more, and intellectual ability less, in the past, it is conceivable that human differentiators such as curiosity, empathy, and creativity will become more important in a world of ever-growing technological dependence and ubiquitous artificial intelligence. In short, the science of leadership is well established. There is no real need to advance it in order to improve real-world practices. We should focus instead on applying what we already know, and ignoring what we think we know that isn't true.",en,41
485,1377,1465905043,CONTENT SHARED,-1617279065634731389,-709287718034731589,5373256410045376795,,,,HTML,http://9to5mac.com/2016/06/13/apple-file-system-apfs/,"apple file system (apfs) announced for 2017, scales 'from apple watch to mac pro' and focuses on encryption","Startup Disk : APFS volumes cannot currently be used as a startup disk. Case Sensitivity : Filenames are currently case-sensitive only. Time Machine : Time Machine backups are not currently supported. FileVault : APFS volumes cannot currently be encrypted using FileVault. Fusion Drive : Fusion Drives cannot currently use APFS. The File system isn't Open Source at this time and Apple isn't quite saying if it will eventually Open Source the File System. Apple plans to document and publish the APFS volume format when Apple File System is released next year. Apple File System is a Next-Generation File System for Apple Products HFS+ and it's predecessor HFS are more than 30 years old. These file systems were developed in an era of floppy disks and spinning hard drives, where file sizes were calculated in kilobytes or megabytes. Today, solid-state drives store millions of files, accounting for gigabytes or terabytes of data. There is now also a greater importance placed on keeping sensitive information secure and safe from prying eyes. A new file system is needed to meet the current needs of Apple products, and support new technologies for decades to come. The following sections describe the general characteristics of Apple File System, as they relate to functionality in the HFS+ file system: Containers and Volumes A container is the base storage unit for APFS. It generally maps 1:1 to GUID Partition Table (GPT) entries, and manages its own space allocations and crash protection scheme. Each container exports one or more volumes , or file systems, each of which have their own namespaces , or sets of files and directories. Note: Apple File System does not directly implement software RAID, however APFS can be combined with an Apple RAID volume to support Striping ( RAID 0 ), Mirroring ( RAID 1 ) and Concatenation ( JBOD ). APFS can also be used with direct-attached hardware RAID solutions. Encryption Security and privacy are fundamental in the design of Apple File System. On OS X, Full Disk Encryption has been available since OS X 10.7 Lion. On iOS, a version of data protection that encrypts each file individually with its own key has been available since iOS 4, as described in iOS Security Guide . APFS combines both of these features into a unified model that encrypts file system metadata. APFS supports encryption natively. You can choose one of the following encryption models for each volume in a container: no encryption, single-key encryption, or multi-key encryption with per-file keys for file data and a separate key for sensitive metadata. APFS encryption uses AES-XTS or AES-CBC, depending on hardware. Multi-key encryption ensures the integrity of user data even when its physical security is compromised. Compatibility Existing third party utilities will need to be updated to support Apple File System. Consult the utility's documentation, or contact the vendor for compatibility information. APFS formatted volumes are not recognized on OS X 10.11 Yosemite and earlier. You can share APFS formatted volumes using the SMB network file sharing protocol. The AFP protocol is deprecated and cannot be used to share APFS formatted volumes.",en,41
486,835,1462560263,CONTENT SHARED,6910763794618680440,2062670502532932588,5100455041335502868,,,,HTML,https://dzone.com/articles/javaserver-faces-23-1,javaserver faces 2.3 quick reference - dzone java,"Introduction JSR 372 is currently in the Early Draft Review stage. The JSF 2.3 spec is not complete and will likely change significantly before finalization. In the meantime, here's a pragmatic deep-dive into JSF 2.3 in its current state. For more resources, see JavaServer Faces 2.3 Tutorial and JSF 2.3 Repository Examples . CDI Alignment Injection and EL Resolving of JSF artifacts As you probably know, JSF uses static entry methods and chaining to let the user obtain the various artifacts that it provides, such as the FacesContext, session map, external context, etc. However, this is pretty verbose and sometimes hard to intuit and understand. JSF 2.3 will therefore provide default producers for the most important artifacts, which at the moment are: FacesContext #{facesContext} ExternalContext #{externalContext} UIViewRoot #{view} ServletContext #{application} Flash #{flash} Application Map #{applicationScope} Session Map #{sessionScope} View Map #{viewScope} Request Map #{requestScope} Flow Map #{flowScope} Header Map #{header} Cookie Map #{cookie} Init Param Map #{initParam} Request Param Map #{param} Request Values Map #{paramValues} Header Values Map #{headerValues} Resource Handler #{""resource""} The following artifacts are not injectable: Composite Component (#{cc}), Component (#{component}), Request (#{request}), Session (#{session}) Must Read Injection and EL resolving of JSF artifacts (by Arjan Tijms) Related JSF 2.3 Injection and EL resolving of JSF artifacts Injection in More JSF Artifacts JSF 2.0 provides very modest support for injection in JSF artifacts. In JSF 2.1, very few JSF artifacts were injection targets. Starting with JSF 2.2, injection is possible in many more artifacts (check Mastering JavaServer Faces 2.2 ), but as the specification says, converters, validators, and behaviors are still not injection targets. It seems that this will be available from JSF 2.3. Until JSF 2.3, there were a few tricks to obtain validators/converters eligible for injection: Until JSF 2.3 - Custom Validator Eligible For @Inject You probably remember the days when you did this to obtain validators/converters eligible for injection: Until JSF 2.3 - Custom Converter Eligible For @Inject A more complicated task is using @EJB for injecting Enterprise JavaBeans (EJB) session beans. In this case, we need to manually lookup the EJB session bean from Java Naming and Directory Interface (JNDI). When the EJBs are deployed in a Web application ARchive (WAR), the lookup is generally of the following type: When the EJBs are in an Enterprise ARchive (EAR), the common lookup type is as follows: Inject EJB Bean From WAR Inject EJB Bean From EAR Starting with JSF 2.3 this gap has been filled and we can inject (using @Inject) in @FacesConverter (javax.faces.convert.Converter), @FacesValidator (javax.faces.validator.Validator), and @FacesBehavior (javax.faces.component.behavior.Behavior): Specify a new attribute called ""managed"" on the corresponding annotations; Add into WEB-INF the faces-config.xml with JSF 2.3 XSD (needed for JSF 2.3 milestones); In milestones, you need to manually add the JSF 2.3 XSD, as below: Must Read Injection in more JSF artifacts (by Arjan Tijms) Related JSF 2.3 Converters, validators, and behaviors as injection targets Lifecycle System Event Published After View Rendered As you probably know, JSF 2.2 comes with a significant number of events, but none of them are published right after the view is rendered. In other words, the PreRenderViewEvent event doesn't have a ""friend"" like PostRenderViewEvent. Well, starting with JSF 2.3 this is not true anymore, because PostRenderViewEvent is available and comes to add consistency to JSF events suite. Subscribe via <f:event> From Page to Listen for PostRenderViewEvent ( example ) Subscribe Programatically to Listen for PostRenderViewEvent From a Custom Component ( example ) And, the output will be: Obviously, PostRenderViewEvent hits processEvent() after encodeEnd(), which means that the rendering process is over. Must Read System event published after view rendered (by Arjan Tijms) Related Just tested JSF 2.3 PostRenderViewEvent under Payara 4.1 Java API Support for the Iterable/Map Interface in UIData and UIRepeat Iterating in UIData (e.g. <h:dataTable>) JSF 2.2 supports the List, native array, and JSF specific DataModel as input for its value binding; you can directly use the Iterable (e.g. Set, Queue); for Map you can use the OmniFaces function, of:mapToList() ; since Iterable is directly supported, you can use it as #{fooBean.fooMap.entrySet()} or #{fooBean.fooMap.keySet()} or#{fooBean.fooMap.values()}; supports the List, native array, and JSF specific DataModel as input for its value binding; you can directly use the Iterable (e.g. Set, Queue); you can use Map directly: Iterating in UIRepeat (e.g. <ui:repeat>) JSF 2.2 supports the List, native array, and JSF specific DataModel as input for its value binding; for Iterable (e.g. Queue, Set) you can use the OmniFaces function, of:iterableToList() ; for Set only (especially useful to Hibernate/JPA users, who are usually using the Set collections for entity relationships) you can use the OmniFaces function, of:setToList() ; if EL 2.2 is present, for Iterable, you can use toArray() (e.g. #{fooBean.fooIterable.toArray()}); for Map you can use the OmniFaces function, of:mapToList() ; if EL 2.2 is present, for Map, you can use toArray() (e.g. #{fooBean.fooMap.entrySet().toArray()} or #{fooBean.fooMap.keySet().toArray()} or #{fooBean.fooMap.values().toArray()}); JSF 2.3 supports the List, native array and JSF specific DataModel as input for its value binding you can use directly the Iterable (e.g. Set, Queue) Must Read Support for the Iterable interface in UIData and UIRepeat (by Arjan Tijms) Support for the Map interface in UIData and UIRepeat (by Arjan Tijms) Related JSF 2.0-2.3 Progress of Iterable/Map support in UIData/UIRepeat (OmniFaces support via utility functions) Support for Custom Types in UIData and UIRepeat JSF 2.3 will provide support for custom types in UIData and UIRepeat. With @FacesDataModel custom DataModel wrappers can be registered. Check the figure below: Usage involve two steps: register your wrapper by annotating it with @FacesDataModel designates the type this wrapper is able to handle via forClass attribute Wrapper Model Use Collection in Your Beans In Data Table - It Works out of the Box With @FacesDataModel custom DataModel wrappers can be registered, but those wrappers cannot (yet) override any of the build-in types. Must Read Support for custom types in UIData and UIRepeat (by Arjan Tijms) JSF 2.3 new feature: registrable DataModels (by Arjan Tijms) Related Registrable DataModels Example Type-Safety Generics for ExternalContext#getInitParameterMap In JSF 2.2 the ExternalContext#getInitParameterMap() returns a raw Map. Starting with JSF 2.3, this method was extended to support generics types. JSF 2.2 - Raw Map JSF 2.3 - Map<String, String> Generics for Converter and Validator Interfaces As you probably know, in JSF 2.2 we can write a custom converter by extending the Converter interface, and a custom validator by extending the Validator interface. The methods defined in these interfaces works with Object class. Starting with JSF 2.3, Converter and Validator have now been parameterized and implementations can concisely define the exact input type. Mojarra 2.3.0-m04 Converter Snippet of Source Code Mojarra 2.3.0-m04 Validator Snippet of Source Code For example, let's suppose that we have the following simple class: We can convert/validate a string against this class by indicating the User type in our converter/validator: JSF 2.3 Custom Converter With Defined Type ( example ) JSF 2.3 Custom Validator With Defined Type ( example ) In JSF 2.3, you can still write custom converters/validators as in JSF 2.2. If you don't want to take advantage of the new generic parameters, you can use the classical approach. Must Read Generics for Converter and Validator interfaces (by Arjan Tijms) Related JSF 2.3 Take advantage of the new generic parameters in Converter<T> and Validator<T> Configuration Facelets default non-hot reload in production JSF has the ability to cache Facelets. In order to update the cache, JSF performs periodic checks of Facelets views changes. In the development stage, you may need to perform this check much more often than in production. For this, you can set the javax.faces.FACELETS_REFRESH_PERIOD context parameter as shown in the following example (the value represents the number of seconds between two consecutive checks). 5 Seconds Timeout There are two special values: -1 means no refresh (cache indefinitely) 0 means no caching at all (always hot reload) Starting with JSF 2.3, when the project stage is Production (default) the Facelets refresh period is -1 (no refresh). Must Read Facelets default to non-hot reload in production (by Arjan Tijms) Conversion/Validation Class-level Bean Validation on CDI-Based Backing Beans JSF 2.2 has several limitations in using Bean Validation. One of them involves the fact the JSF cannot validate the class or method level constraints (so called, cross‐field validation), only field constrains. JSF 2.3 will come with a new tag named, <f:validateWholeBean/>. As its name suggest, this tag is enables class level validation. This tag contains two important attributes: value - A ValueExpression referencing the bean to be validated. validationGroups - A comma-separated list of validation groups. A validation group is a fully-qualified class name. This feature causes a temporary copy of the bean referenced by the value attribute, for the sole purpose of populating the bean with field values already validated by <f:validateBean/> and then performing class-level validation on the copy. Regardless of the result of the class-level validation, the copy is discarded. JSF 2.3 Class-Level Validation ( example ) Here is a brief example to ensure that the provided name and e-mail fields (contacts) are individually valid and also the e-mail start with that name (e.g. valid: nick, nick_ulm@yahoo.com). ContactValidator Class Implementation Note that a ContactBean instance is passed to the isValid() method. This method will only be called if the individual properties of the ContactBean are valid. This fact allows the isValid() method to inspect the properties and perform effective class-level validation. ValidContact Class Implementation ContactBean Class Implementation This is the backing Bean: ContactGroup Interface Implementation Facelets View The following feature must explicitly be enabled by setting the following application parameter (context parameter) in web.xml: javax.faces.validator.ENABLE_VALIDATE_WHOLE_BEAN. If this parameter is not set, or is set to false, this tag must be a no-op: Must Read Class level bean validation (by Arjan Tijms) Related JSF 2.3 Class-level bean validation on CDI based backing beans JSR 310 Alignment JSF 2.3 will have Java 8 as a minimum dependency. One of the affected artifacts is the <f:convertDateTime/> built-in converter which has been updated in order to support new types for the type attribute, and, depending on the value of the type attribute, the converter will use the java.text.SimpleDateFormat or java.time.format.DateTimeFormatter class for formatting. The upcoming JSF 2.3 now adds new possible values for the type attribute, as follows: localDate, localTime, localDateTime, offsetTime, offsetDateTime, zonedDateTime Simple Bean Using offsetTime Using offsetDateTime Using zonedDateTime When the converter type attribute value is date, time or both, JSF (2.2 and 2.3) uses the java.text.SimpleDateFormat class. Starting with JSF 2.3, when the converter type attribute value is localDate, localTime, localDateTime, offsetTime, offsetDateTime or zonedDateTime, the java.time.format.DateTimeFormatter class will be used. Must Read JDK 8 time support in f:convertDateTime (by Arjan Tijms) Related JSF 2.3 align the <f:convertDateTime/> to the new data and time classes in JDK 8 (JSR 310) Networking Starting with JSF 2.3-m05 we can take advantage of a brand new feature - register a web socket push connection in client side . Thanks to the JSF team (especially to Bauke Scholtz (aka BalusC) ) this feature is available in today milestone via <f:websocket/> tag. R egister a web socket push connection in client side Let's see an example of using the <f:websocket/>. In JSF page, we need to add the <f:websocket/> tag with its two required attributes: channel - This is javax.el.ValueExpression that must be evaluated to String and it represents the name of the web socket channel. A channel name is restricted to alphanumeric characters, hyphens, underscores and periods. A channel can have multiple open web sockets, and each of these sockets will receive the same push notification from the server. onmessage - This is javax.el.ValueExpression that must be evaluated to String and it represents the a JavaScript listener function that is automatically invoked when a push notification is received from the server. The signature of the listener function for onmessage is of type: So, a simple <f:websocket/> tag usage will look like this: By default, when we start the application, the web socket is automatically connected and open. As long as the document is open the web socket is open. When the document is unloaded the web socket is automatically closed. In the web socket is initially successfully connected but the connection is closed as a result of e.g. a network error or server restart, JSF will try to auto-reconnect it at increasing intervals. Now, let's focus on the server side. Here we have to take into account the push messages mechanism. This mechanism is based on javax.faces.push.PushContextinterface and javax.faces.push.Push API. First, you need to know that by default the web socket is application scoped. This means that the managed bean that can push messages to this web socket must be in application scope (annotated with @ApplicationScope). In this case, the push message can be sent by all users and the application itself. Furthermore, you have to inject PushContext via @Push annotation on the given channel name in any CDI/container managed artifact. For example: Finally, we need to write an action method capable to push messages to web socket via PushContext. For example: Let's glue everything together. First, the JSF page: Next, our simple CDI bean: For those implementations that do not support adding an EndPoint dynamically (at the moment only GlassFish/Tyrus), a fake one has to be defined by the application. As BalusC pointed out, this fake endpoint should look like below: Finally, the m05 requires the following settings in web.xml: Done! The complete application was tested under Payara server and it is available here . Must Read WebSocket integration (by Arjan Tijms)",en,41
487,3107,1487608245,CONTENT SHARED,-7264217791213422584,6013226412048763966,7817933231113759598,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36",SP,BR,HTML,http://vocerh.uol.com.br/noticias/legislacao/qual-e-o-valor-da-area-de-t.phtml,qual é o valor da área de t&d na sua empresa?,"É fundamental que o departamento de Recursos Humanos das companhias se mantenha atento às melhores práticas, os indicadores de gestão e as tendências da área de T&D Fernando Cardoso* Treinar uma equipe sem um planejamento prévio é um dos caminhos para o desperdício de recursos | Crédito: Pixabay É impossível cobrar performance de um funcionário sem investir na qualificação dele. Em geral, a ação de treinar e desenvolver as habilidades técnicas e comportamentais de um profissional tende a trazer ganhos para todos os envolvidos: enquanto o funcionário se sente valorizado pela companhia, a empresa ganha em qualidade, produtividade e capacidade competitiva. A ação de capacitar funcionários deve ser enxergada pelas companhias como estratégica em todos os momentos da economia de um país. Em períodos de mercado aquecido, ter uma equipe engajada tende a se reverter em aumento da produtividade e, consequentemente, do faturamento da companhia. Já em períodos desafiadores, a necessidade está em garantir o perfil inovador para atrair a atenção dos clientes diante da concorrência acirrada. A boa notícia é que grandes companhias do País têm demonstrado amadurecimento quanto a essa nova realidade do mercado. Prova disso são os dados do estudo ""Panorama do Treinamento no Brasil"", produzido pela ABTD, em parceria com a Integração Escola de Negócios, que aponta o aumento da carga horária destinada à capacitação de profissionais no Brasil: subiu de 16,6 horas por funcionário, em 2015; para 22 horas, em 2016. Vejo esse resultado como animador, pois somente o desenvolvimento das lideranças e dos funcionários fará com que as empresas estejam preparadas para enfrentar as possíveis oscilações do mercado. Então, é fundamental que o departamento de Recursos Humanos das companhias se mantenha atento às melhores práticas, os indicadores de gestão e as tendências da área de T&D. Ressalto, porém, que treinar uma equipe sem um planejamento prévio é um dos caminhos para o desperdício de recursos. É preciso realizar um mapeamento do público-alvo, incluindo necessidades, preferências e localização geográfica, além dos objetivos da ação e das soluções possíveis e disponíveis para a transmissão do aprendizado. No Brasil, por exemplo, a modalidade presencial é a mais utilizada, seguida de atividades práticas no local de trabalho, do e-learning e do ""blended"", que mescla EAD e atividades em sala de aula. Por mais moderna e tecnológica que uma empresa possa ser ou parecer, ela precisa de seres humanos preparados, motivados e engajados, se quiser ter um futuro de sucesso. Então, não permita que a área de T&D da sua companhia se retraia de acordo com o humor da economia. Estruture um bom planejamento que lhe permita investir na eficiência dos seus funcionários à frente de suas funções em todas as épocas do ano. * Este artigo é de autoria de Fernando Cardoso, sócio-diretor da Integração Escola de Negócios, e não representa necessariamente a opinião da revista",pt,41
488,2662,1477564582,CONTENT SHARED,5258604889412591249,-1443636648652872475,6803419033714154168,Android - Native Mobile App,SP,BR,HTML,https://hbr.org/2016/10/machine-learning-is-no-longer-just-for-experts,machine learning is no longer just for experts,"If you're not using deep learning already, you should be. That was the message from legendary Google engineer Jeff Dean at the end of his keynote earlier this year at a conference on web search and data mining. Dean was referring to the rapid increase in machine learning algorithms' accuracy, driven by recent progress in deep learning, and the still untapped potential of these improved algorithms to change the world we live in and the products we build. But breakthroughs in deep learning aren't the only reason this is a big moment for machine learning. Just as important is that over the last five years, machine learning has become far more accessible to nonexperts, opening up access to a vast group of people. For most software developers, there have historically been many barriers to entry in machine learning, most notably software libraries designed more for academic researchers than for software engineers as well as a lack of sufficient data. With massive increases in the data being generated and stored by many applications, though, the set of companies with data sets on which machine learning algorithms could be applied has significantly expanded. In tandem, the last few years have seen a proliferation of cutting-edge, commercially usable machine learning frameworks, including the highly successful scikit-learn Python library and well-publicized releases of libraries like Tensorflow by Google and CNTK by Microsoft Research. The last two years have also seen the major cloud providers Amazon Web Services and Google Cloud Services release machine learning-specific services - both Machine Learning as a Service platforms and graphics processor unit machines optimized for machine learning work. The net effect of these new technologies is that a person interested in using machine learning need not understand the science of deep learning algorithms in order to experiment with cutting-edge techniques. Tutorials and public code exist for applications as diverse as AI-driven art generation , language translation , and automated image captioning . The accessibility of this code creates a virtuous cycle. Use by nonexperts creates even more demand for easier-to-use systems and uncovers new applications of machine learning, which inspires further research and development by experts. And these new technologies affect who works in machine learning as well. When hiring into applied machine learning positions, exceptional quantitative skills are critical, but direct education in machine learning itself has become less important. In many ways, this change in accessibility mimics the progression we've seen in software development as a whole. Over the last 50 years, software development has gradually migrated from ""low-level"" languages - highly technical languages that closely relate to a computer's underlying architecture - to high-level languages with significantly lower barriers to entry. Similarly, software deployment has migrated from hosted machines and data centers to cloud-based services, with massive decreases in the time and capital required to deploy a new system. These changes have not simply made software developers more efficient; they have allowed a much broader set of people to develop software and start software companies. Software bootcamps now train working engineers in a matter of months, and startups can turn ideas into products in a few development cycles. All of that is not to say, of course, that there's no place for experts - as in software engineering, untold amounts of scientific progress have yet to be made in machine learning. But for the first time in history it's possible, for example, for a person with knowledge of programming but no machine learning experience to create in one afternoon a neural network that can read handwritten digits. Try it for yourself.",en,40
489,126,1459446418,CONTENT SHARED,-8287402887944984163,-8020832670974472349,5407949268718900440,,,,HTML,https://blog.docker.com/2016/03/containers-are-not-vms/,containers are not vms,"I spend a good portion of my time at Docker talking to community members with varying degrees of familiarity with Docker and I sense a common theme: people's natural response when first working with Docker is to try and frame it in terms of virtual machines. I can't count the number of times I have heard Docker containers described as ""lightweight VMs"". I get it because I did the exact same thing when I first started working with Docker. It's easy to connect those dots as both technologies share some characteristics. Both are designed to provide an isolated environment in which to run an application. Additionally, in both cases that environment is represented as a binary artifact that can be moved between hosts. There may be other similarities, but to me these are the two biggies. The key is that the underlying architecture is fundamentally different between the two. The analogy I use (because if you know me, you know I love analogies) is comparing houses (VMs) to apartment buildings (containers). Houses (the VMs) are fully self-contained and offer protection from unwanted guests. They also each possess their own infrastructure - plumbing, heating, electrical, etc. Furthermore, in the vast majority of cases houses are all going to have at a minimum a bedroom, living area, bathroom, and kitchen. I've yet to ever find a ""studio house"" - even if I buy the smallest house I may end up buying more than I need because that's just how houses are built. (for the pedantic out there, yes I'm ignoring the new trend in micro houses because they break my analogy) Apartments (the containers) also offer protection from unwanted guests, but they are built around shared infrastructure. The apartment building (Docker Host) shares plumbing, heating, electrical, etc. Additionally apartments are offered in all kinds of different sizes - studio to multi-bedroom penthouse. You're only renting exactly what you need. Finally, just like houses, apartments have front doors that lock to keep out unwanted guests. With containers, you share the underlying resources of the Docker host and you build an image that is exactly what you need to run your application. You start with the basics and you add what you need. VMs are built in the opposite direction. You are going to start with a full operating system and, depending on your application, might be strip out the things you don't want. I'm sure many of you are saying ""yeah, we get that. They're different"". But even as we say this, we still try and adapt our current thoughts and processes around VMs and apply them to containers. ""How do I backup a container?"" ""What's my patch management strategy for my running containers?"" ""Where does the application server run?"" To me the light bulb moment came when I realized that Docker is not a virtualization technology, it's an application delivery technology. In a VM-centered world, the unit of abstraction is a monolithic VM that stores not only application code, but often its stateful data. A VM takes everything that used to sit on a physical server and just packs it into a single binary so it can be moved around. But it is still the same thing. With containers the abstraction is the application; or more accurately a service that helps to make up the application. With containers, typically many services (each represented as a single container) comprise an application. Applications are now able to be deconstructed into much smaller components which fundamentally changes the way they are managed in production. So, how do you backup your container, you don't. Your data doesn't live in the container, it lives in a named volume that is shared between 1-N containers that you define. You backup the data volume, and forget about the container. Optimally your containers are completely stateless and immutable. Certainly patches will still be part of your world , but they aren't applied to running containers. In reality if you patched a running container, and then spun up new ones based on an unpatched image, you're gonna have a bad time. Ideally you would update your Docker image, stop your running containers, and fire up new ones. Because a container can be spun up in a fraction off a second, it's just much cheaper to go this route. Your application server translates into a service run inside of a container. Certainly there may be cases where your microservices-based application will need to connect to a non-containerized service, but for the most part standalone servers where you execute your code give way to one or more containers that provide the same functionality with much less overhead (and offer up much better horizontal scaling). ""But, VMs have traditionally been about lift and shift. What do I do with my existing apps?"" I often have people ask me how to run huge monolithic apps in a container. There are many valid strategies for migrating to a microservices architecture that start with moving an existing monolithic application from a VM into a container but that should be thought of as the first step on a journey, not an end goal. As you consider how your organization can leverage Docker, try and move away from a VM-focused mindset and realize that Docker is way more than just ""a lightweight VM."" It's an application-centric way to deliver high-performing, scalable applications on the infrastructure of your choosing. Check out these resources to start learning more about Docker and containers: Learn More about Docker",en,40
490,1183,1464798119,CONTENT SHARED,7395435905985567130,3829784524040647339,-7426579400958584608,,,,HTML,https://www.oreilly.com/ideas/the-ai-business-landscape?imm_mid=0e446a&cmp=em-data-na-na-newsltr_20160601,the ai business landscape,"A data-driven analysis of companies that are adopting artificial intelligence. (source: Pexels ). This post is the beginning of an investigation into the business market for artificial intelligence, which will culminate in a free report about the larger AI market. Sign up to be notified when the new free report, ""The New AI Market,"" becomes available . Artificial intelligence plays an increasingly central role in everything from consumer electronics to retail, and from health care to cars. With such widespread applications, which companies have adopted AI as a new direction for their businesses? How are they using it and for what purpose? By studying the AI market using AI, the forthcoming report The New Artificial Intelligence Market aims to answer this question. A graph-based machine learning model developed at Spiderbook learns industry vocabularies around AI, reads the entire business Internet, and then classifies businesses into different levels of maturity and investments in AI. We canvassed almost 500,000 companies around the globe to develop a data-driven, in-depth understanding of the AI landscape and various related technologies, like cognitive computing, deep learning, machine vision, natural language understanding, and autonomous cars. This report culminates in providing insights into industries and geographies investing the most in AI, and what they are building with it. As a teaser to the report, we're providing a chart on how many businesses are using deep learning and a few examples of such businesses outside of the software/IT industry. We find that more than 600 companies have jumped into applying deep learning with real budgets. As you can see in the figure below, about 90 companies (level 3), have made strategic investments in deep learning for their businesses. Another 177 companies (level 2) are developing projects using deep learning with dedicated resources in staff. And more than 350 companies (level 1) are experimenting with deep learning in their labs. Figure 1. Number of companies investing in deep learning. Source: Spiderbook, used with permission. Given how early deep learning is as a technology, the majority of companies investing in deep learning are IT and software businesses. However, we discovered interesting champions in other industries that are adopting deep learning as well. Below are some examples of companies that are not in traditional software or IT businesses, but that are adopting deep learning. Given that deep learning has early roots in image processing, it is exciting to see health care companies like Siemens Healthcare and GE Healthcare leading the pack, along with research institutions like the NIH and Lawrence Livermore National Labs. We aim to provide more insights into what these companies are doing with deep learning and other AI technologies in the detailed report. Figure 2. Examples of companies investing in deep learning. Source: Spiderbook, used with permission. Article image: (source: Pexels ).",en,40
491,2782,1480337583,CONTENT SHARED,-6584408680441477889,-5527145562136413747,5610729222845918931,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.98 Safari/537.36",SP,BR,HTML,https://blog.runrun.it/o-que-e-lideranca-diversidade-funcionar,o que é liderança? liderança é fazer a diversidade funcionar.,"Compreender o que é liderança envolve principalmente compreender o que liderança não é. Quer ser um péssimo ou uma péssima líder? Desrespeite e desvalorize a diversidade. Desconsidere o quanto ela é uma necessidade. Subestime as contribuições que pessoas de diferentes gêneros, etnias e raças e vivências - o que inclui diferentes orientações sexuais e identidades de gênero. Afinal, quem lidera lidera pessoas, assim, no plural. Por isso vamos analisar por que a diversidade de um time é basilar, inclusive, para as finanças da sua empresa. Para quem, dia após dia, se pergunta como motivar uma equipe, esta é uma boa chance de ouvir o que Heidi Halvorson e David Rock, especialistas do querem que líderes saibam sobre a importância de montar equipes diversas. Tudo com base em estudos . Neuroleadership Institute , Equipes diversas são melhores em quê? Quando a dúvida surge, é hora de buscar respostas junto daqueles que vivem para isso: pesquisadores. E foi o que fizeram os especialistas Halvorson e Rock. No artigo ""Why Diverse Teams Are Smarter"" , publicado na Harvard Business Review em novembro de 2016, eles citam dois estudos sobre o impacto da diversidade - é isso mesmo - sobre as finanças das empresas. Em 2015, pesquisadores da avaliaram 366 companhias públicas de vários países, e o que constataram foi o seguinte. Aquelas que apresentavam a maior diversidade étnica e racial em posições de gestão eram 35% mais propensas a obter retorno financeiro acima da média de seu ramo. E aquelas que contavam com a maior diversidade de gênero eram 15% mais propensas a conquistar essa mesma vantagem. Outra com 2400 empresas de todo o globo, conduzida pelo Credit Suisse, não trouxe resultados contrários aos de cima. Organizações com pelo menos uma mulher no conselho diretor apresentam maior retorno sobre o patrimônio líquido, e seu lucro líquido cresce mais do que aquelas em que não há mulher alguma na diretoria. ""Equipes não homogêneas são mais espertas"", afirmam os pesquisadores, citando outro estudo recente, que afirma: trabalhar com pessoas diferentes de você desafia suas formas tradicionais de pensar e refina o desempenho do seu cérebro. Mais especificamente, dizem, equipes diversas: Foco nos fatos Pessoas com origens, vivências e repertórios culturais diversos trabalhando em equipe podem levar a um pensamento de grupo mais refinado e preciso. É o que aponta um publicado no Journal of Personality and Social Psychology. Cientistas distribuíram cerca de 200 pessoas em júris de seis pessoas. Alguns eram completamente compostos por pessoas brancas, enquanto em outros havia quatro brancos e dois negros. A tarefa era a seguinte: assistir ao vídeo do julgamento de um réu negro com vítimas brancas e decidir se o réu era culpado ou inocente. O que se observou foi que os júris com negros e brancos levantaram mais fatos relacionados ao caso do que os júris homogêneos, e realizaram menos erros factuais ao discutir as evidências. E quando ocorriam erros, eram corrigidos com mais frequência durante a deliberação. O motivo? Para os pesquisadores, os jurados brancos em grupos mistos buscariam evidências com mais precisão, em vez de se guiarem por pensamentos fáceis, baseados em estereótipos e preconceitos. >> Leitura recomendada: Guia para a igualdade de gênero no trabalho Outros estudos trouxeram resultados semelhantes. Numa série de experimentos conduzidos no Texas e em Cingapura, publicados no jornal científico , pesquisadores organizaram os participantes ora em equipes etnicamente diversificadas ora em grupos homogêneos. Todos apresentavam boa instrução em finanças e sua missão era cotar algumas ações de mercado. Resultado: aqueles que participavam de equipes mistas eram 58% mais propensos a fazer a cotação correta. Para Halvorson e Rock, ""quando estão em equipes diversificadas, as pessoas são mais suscetíveis a reexaminar os fatos, a permanecer objetivas. São mais propensas a avaliar criticamente suas ações"". É o que cientistas descrevem como ""manter seus recursos cognitivos afiados e vigilantes"" - o que parece também ser uma boa forma de descrever o que é liderança. Processamento cuidadoso de dados ""A diversidade pode mudar a maneira como equipes inteiras digerem informações e tomam melhores decisões"", insistem Halvorson e Rock. Em um estudo publicado no "" Personality and Social Psychology Bulletin"" , Katherine Phillips da Northwestern University e sua equipe distribuíram membros de fraternidades e irmandades universitárias em quartetos, propondo que lessem as entrevistas de um detetive que investigava um assassinato. Três pessoas de cada grupo sempre vinham da mesma irmandade ou fraternidade, enquanto a quarta poderia ser do mesmo grupo ou de outro aleatório. Então, os três veteranos de cada grupo se reuniam para determinar quem seria o suspeito mais provável do assassinato. Cinco minutos depois, o quarto integrante se juntava à deliberação e expressava sua suspeita. >> Leitura recomendada: Instigue a criatividade do seu time e critique ideias com maturidade Verificou-se que, embora os grupos com o quarto integrante de outras irmandades e fraternidades se sentissem menos confiantes sobre suas decisões, acertavam mais vezes o nome do assassino do que os grupos com todos já conhecidos entre si. Para os cientistas do experimento, o que acontece é: grupos diversos podem superar os homogêneos na tomada de decisão porque processam as informações com mais cuidado. ""Considerar a perspectiva de um estranho pode parecer contraintuitivo, mas a recompensa pode ser enorme"", Halvorson e Rock concluem. Ao romper com a homogeneidade no local de trabalho, você mostra que está entendendo o que é liderança. Pelo simples fato de que está permitindo que seus colaboradores se tornem mais conscientes de seus próprios vieses, isto é, de suas formas enraizadas de pensamento que, de alguma forma, podem cegá-las e impedi-las de encontrar informações-chave. O efeito disso, sabemos, é a maior probabilidade de falharem ao tomarem decisões. Cenário propício para inovações Você já está careca de saber que, para se manter competitiva, uma empresa deve inovar. O que você talvez não imaginava é que uma das melhores estratégias de aumentar a capacidade de transformação da sua marca e dos seus produtos envolve a contratação de mais mulheres e de pessoas de culturas diversas. O embasamento dessa informação é o publicado em ""Innovation: Management, Policy & Practice"". Nele os autores analisaram níveis de diversidade de gênero em equipes de Pesquisa e Desenvolvimento (R&D) de 4.277 empresas da Espanha. Adotando modelos estatísticos, eles descobriram que as empresas com mais mulheres eram mais propensas a introduzir inovações radicais no mercado no espaço de apenas dois anos. >> Leitura recomendada: Guia para empresas que desejam ser disruptivas Em outro , publicado na revista Economic Geography , os pesquisadores concluíram que o aumento da diversidade cultural é um propulsor da inovação. Eles reuniram dados sobre 7.615 empresas que participaram do London Annual Business Survey, um questionário realizado com os executivos da capital do Reino Unido, sobre o desempenho de suas empresas. Os resultados revelaram que as empresas dirigidas por equipes culturalmente diversas eram mais propensas a desenvolver novos produtos do que aquelas com liderança homogênea. ""Embora você possa se sentir mais à vontade trabalhando com pessoas que compartilham sua experiência, não se deixe enganar pelo seu conforto"", recomendam e Halvorson e Rock, que também publicaram o artigo ""Diverse Teams Feel Less Comfortable - and That's Why They Perform Better"" . Nele, enumeram outros estudos que só endossam a diversidade como uma necessidade. Confira: Estudos não faltam: Diversidade é uma necessidade Uma análise de 2009 com 506 empresas revelou que aquelas com maior diversidade racial e de gênero que a média alcançam maior receita de vendas, mais clientes e lucros superiores. Outra pesquisa , esta de 2016 e mais ampla, com mais de 20 mil organizações em 91 países, descobriu que aquela com mais mulheres em cargos executivos eram mais lucrativas. Em 2011, um estudo com equipes de gestores demonstrou que aquelas com um leque mais variado de bagagens de educação e repertório profissional criavam produtos mais inovadores. Em suma, como eles descrevem, contratar pessoas que não enxergam nem falam nem pensam como você pode evitar as ""armadilhas dispendiosas da conformidade que desencorajam o pensamento inovador"". Mais uma vez nos voltamos à primeira questão: O que é liderança? Pistas não faltam. Rompendo a homogeneidade Antes de finalizar, duas palavras merecem nossa atenção, por terem íntima relação com o problema da falta de diversidade. Ambas foram finalistas a ""Palavra do Ano de 2016"" do Oxford Dictionary. São elas: Glass Cliff (penhasco de vidro) e Woke (do verbo ""to wake"", despertar). Glass Cliff é usada para se referir à situação em que uma mulher ou um membro de um grupo minoritário sobe a uma posição de liderança, desafiando circunstâncias nas quais o risco de fracassar é elevado. Por sua vez, Woke , adotada já em 1962 e popularizada recentemente com a hashtag #StayWoke e a campanha #BlackLivesMatter, existe para designar a pessoa que está alerta ao racismo e ao sistema de perseguição e opressão aos negros. Palavras dessa natureza ainda são necessárias e não deixarão de ser enquanto atitudes concretas forem tomadas pelas lideranças de companhias e grupos. Criar um local de trabalho mais diverso é um importantíssimo passo para manter os preconceitos sob controle, até reduzi-los a lembranças, e fundamental para provocar as pessoas a questionarem suas suposições e prejulgamentos. >> Leitura recomendada: Combatendo o machismo no ambiente de trabalho ""Enriquecer seu quadro de colaboradores com pessoas de diferentes gêneros, raças e nacionalidades é fundamental para impulsionar o potencial intelectual coletivo da sua empresa"", resumem Heidi Halvorson e David Rock. Ao mesmo tempo, é preciso tratar como uma prioridade estratégica da empresa a implementação de práticas inclusivas, para que todos sintam que podem ser ouvidos - e que o sejam de fato. Para líderes, entender o que é liderança e fazer a diversidade funcionar são sinônimos. Mesmo porque respeitar a diferença simplesmente não é mais do que exercer sua cidadania e não violar preceitos éticos básicos. Uma ferramenta para entender o que é liderança O Runrun.it nasceu para ser o braço direito de gestoras e gestores que desejam entender os esforços de sua equipe e contribuir para que se tornem mais conscientes de seu próprio trabalho. Gosta da ideia? Experimente grátis:",pt,40
492,1355,1465661819,CONTENT SHARED,-799315203186826480,-709287718034731589,-1424527253526269635,,,,HTML,http://www.macrumors.com/2016/06/10/what-to-expect-at-wwdc-2016/,what to expect at wwdc 2016,"Apple's Worldwide Developers Conference kicks off on Monday, June 13 with a keynote event at 10:00 a.m. Pacific Time, where Apple is expected to show off the latest versions of its iOS, OS X, tvOS, and watchOS operating systems and perhaps debut new features for services like Apple Pay and iCloud . Ahead of the conference, we've compiled all of the rumors that we've heard about features that could potentially debut at the event to give MacRumors readers an idea of what to expect. iOS 10 Apple's operating system for iPhones and iPads has gone largely without design changes since iOS 7, so it's reasonable to assume iOS 10 may feature some design tweaks to update the look of the OS. A dark mode is one possibility that's been circulating based on the look of Apple's WWDC app and site, but there's no evidence suggesting such a feature will be implemented. Information on iOS 10 is limited, but we've heard some tantalizing details about the update. Apple is expected to introduce a Siri SDK, allowing Siri to interact with third-party apps in new ways. Developers will be able to build Siri integration into their apps, allowing Siri to access data and perform tasks within apps. Third-party app access has the potential to greatly expand what Siri is capable of and could put the personal assistant on par with more robust solutions like Google Now. In 2015, rumors indicated Apple was working on an ""iCloud Voicemail"" service able to allow Siri to answer missed calls and record and transcribe messages for users to read as text at a later time. The service, which could come in iOS 10, is also said to be capable of relaying location information and delivering details on why a user can't answer a phone call. Siri may also be improved with new capabilities Apple has made through acquisitions like VocalIQ . VocalIQ is able to retain the semantic context between conversations and recall user preferences. Apple has made other AI-related purchases over the past year that could have been purchased to improve Siri. Perceptio , for example, is a startup designed to allow for the creation of advanced artificial intelligence systems while limiting the amount of data stored in the cloud, functionality that could allow Apple to bridge the gap between its desire to introduce deeper Siri functionality with its unwavering focus on privacy. iOS 10 is rumored to include a redesigned Music app featuring a more intuitive user interface for Apple Music . It's said to use a ""bolder, yet simpler"" design that emphasizes black and white backgrounds and text to put more focus on album art. Apple is planning to replace the ""New"" tab in Apple Music with a ""Browse"" option that includes better organizational tools for discovering new content, and it will also feature a simplified ""For You"" section. Connect and Beats 1 will remain unchanged, but Apple may remove the Connect tab from the Apple Music interface. New 3D Touch shortcut previews will be added, sharing features will be emphasized, and there will be more of a focus on song lyrics, making them easier to access within the app. With the upcoming iPhone 7 Plus expected to gain a dual-lens camera, Apple is building Photos improvements into iOS 10. Apple is expected to reintroduce some abilities that were initially found in iPhoto before it was discontinued and while we don't know specifics, iPhoto previously had features like EXIF editing and touch-based brushes for adjusting brightness and other parameters on only specific parts of a photo. A Skitch-like photo editing feature allowing users to draw on and mark up images could also be in the works, mirroring features that are available in the Mail app for marking up PDFs and photos. Metadata keys added to App Store apps in April suggests Apple is potentially working on a much-desired feature to allow users to hide or delete unwanted stock apps . If such a feature is in the works, it could be included in iOS 10. Check out our full iOS 10 roundup for more info. OS X 10.12 OS X didn't get a major update in 2015, but 2016 may more than make up for it. OS X 10.12 is expected to bring Siri to the Mac for the first time, allowing the personal assistant to be used on Mac devices like it's used on iOS devices. Siri will be enabled through an icon in the OS X menu bar, the dock, a user-specified keyboard shortcut, or through a hands-free ""Hey Siri"" command. In May, we shared an image of the full Siri dock icon , featuring a colorful Siri waveform. Siri functionality on the Mac is likely to mirror much of the functionality on iOS, with Siri able to conduct searches, open apps, tweak system settings, answer simple queries, play music from iTunes, and more. Siri may also be able to interface with third-party Mac apps through the rumored Siri SDK. Another key feature of OS X 10.12 may be expanded Continuity features. Apple is working on an auto unlock function that would allow an iPhone to unlock a Mac when in close proximity, alleviating the need to enter a password on a password-protected machine in OS X 10.12. The feature, which uses Bluetooth LE frameworks, will presumably work similarly to the automatic unlocking feature on the Apple Watch , which allows an unlocked iPhone to bypass the passcode restriction on a connected Apple Watch. In this scenario, an iPhones Touch ID button would be used as a verification method for simpler logins. The unlocking function could also potentially be tied to the rumor suggesting Apple Pay integration is coming to web browsers. Users could make purchases online using Safari, confirming the Apple Pay payment through an iPhone's Touch ID when linked to a Mac. Features rumored for iOS 10, such as Photos improvements and Apple Music tweaks, are also expected to be included in OS X 10.12. An updated version of iTunes that includes a redesigned Apple Music experience with a simpler, more intuitive interface is a possibility, and as for Photos, it could gain some features that were removed during the transition from iPhoto to Photos. Beyond rumored features, there has been some speculation that Apple could rename OS X to MacOS to better match with iOS, watchOS, and tvOS, but whether or not Apple will actually introduce a change to its long-running Mac naming scheme remains to be seen. Check out our full OS X 10.12 roundup for more info. tvOS 2 and watchOS 3 Apple has said new versions of tvOS and watchOS 3 will be introduced at WWDC, but we have not heard any details on what new versions of these operating systems might include. Drawing from what we expect for iOS 10 and OS X 10.12, the two operating systems could feature Siri improvements and design tweaks to match any design changes coming to iOS 10, but beyond that, we don't have any insight into what to expect. We do know that as of June 1, Apple is requiring new Apple Watch apps submitted to the App Store to be native apps , suggesting Apple has plans for deeper native app capabilities. At the very least, a push towards native apps is encouraging because it will result in Apple Watch apps that are able to open more quickly and work more smoothly in watchOS 3. Services There are no specific Apple Pay rumors linked to WWDC, but Apple has several new Apple Pay features in the works that would be ideal to debut at the event. Apple is planning to bring Apple Pay to web browsers , allowing Apple Pay users to make Apple Pay purchases in online stores directly through Safari. As with standard transactions, payments would be approved on the iPhone through Touch ID. Using the expanded Continuity features allowing an iPhone to connect to a Mac, Apple Pay payments on Mac notebooks and desktop machines may also be verified through Touch ID. Apple is also working on a person-to-person update for Apple Pay, allowing users to send money to one another much like Square Cash or Venmo. It is not exactly clear how such a feature would work, but there have been "" whispers "" suggesting Apple may let customers send money to one another through iMessages. iCloud: Following its conflict with the FBI, Apple is likely to introduce new security features in iOS 10 and OS X 10.12. One possibility is encrypted iCloud backups, something Apple has been hesitant to implement in the past because it makes the restoring process more difficult. Rumors have suggested Apple is working on implementing stronger security measures ""even it can't hack"" to protect iOS devices. These improvements are likely to be introduced through a mix of hardware and software improvements, and it's quite possible we'll see the first of Apple's security enhancements in iOS 10. iMessage: One rumor says Apple is planning to announce iMessage for Android at the event, giving Android users access to one of the iPhone's best features. It's not clear how accurate this information is as it comes from a source without a reliable track record. Ahead of WWDC, Apple announced some major changes to the App Store , which we may hear more about at the event itself. Apple opened up app subscriptions to encompass all product categories and introduced a new subscription revenue split. After a customer maintains an in-app subscription for one year, developers will receive 85 percent of profits instead of 70 percent. Apple is also adding ads to App Store search results for the first time, and going forward, the App Store's ""Featured"" section will not display apps that are already installed. Apple is planning to bring back the Categories tab, and a Share sheet option is being added as a 3D Touch Quick action for all apps to make sharing easier. Search will also be improved. Hardware Ahead of WWDC there were rumors and speculation suggesting Apple could introduce products like an updated MacBook Pro or a new Thunderbolt Display , but it's likely neither of these products are going to be ready for a June debut, instead coming later in 2016. Leading up to WWDC, multiple Apple insiders have said the event will focus on software instead of hardware, and so we are not expecting hardware updates at the event. There's a possibility Apple could surprise us with an announcement, but if there are plans for a hardware product debut, those plans have been kept under wraps. Though we're not expecting any Mac announcements at WWDC, one questionable rumor from Japanese site Mac Otakara suggests Apple is planning to introduce both new MacBook Air and new MacBook Pro models in the month of June, shipping the notebooks in August. This is somewhat unlikely as Apple just bumped the RAM in the 13-inch MacBook Air up to 8GB in April. Previous rumors also indicate Apple may not be planning to introduce further MacBook Air updates, ending development on the machine in favor of the MacBook. As for the MacBook Pro, past information has said it will ship in 4Q 2016 , conflicting with Mac Otakara 's rumor of a June debut. Streaming Details and Release Date Apple is will offer a live stream of the keynote event through its website and through a dedicated events app on the Apple TV . MacRumors will also provide live coverage, on MacRumors.com and on the MacRumorsLive Twitter account . Following the conclusion of the keynote, Apple will likely provide iOS 10, OS X 10.12, watchOS 3, and tvOS 2 to developers, giving them time to adapt their apps to take advantage of new features. The operating systems are likely to be released to the public in the fall alongside new hardware.",en,40
493,1426,1466163036,CONTENT SHARED,1797761064772338938,1120069409160402054,-8767642506802299951,,,,HTML,http://m.mobiletime.com.br/news/442222,mobile time,"A Visa confirmou nesta quinta-feira, 16, que o Centro de Inovação da empresa no Brasil será lançado nos próximos 60 dias. De acordo com Percival Jatobá, diretor de produtos da Visa no Brasil, o novo local de desenvolvimento tecnológico da empresa - o quinto no mundo - ficará instalado na zona oeste de São Paulo.",pt,40
494,2877,1481802989,CONTENT SHARED,7424795099560672335,5127372011815639401,-3863590710909536616,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.75 Safari/537.36",MG,BR,HTML,https://www.userlike.com/en/blog/company-learning-culture,7 tips to create a company learning culture like google - without resources,"""Ordinary people seek entertainment. Extraordinary people seek education and learning."" - Benjamin Hardy Companies with a learning culture are popular employers. In fact, how well a company supports the personal development of its employees is one of the main criteria for making it into 'Fortune's 100 Best Companies to Work For' list. Still, it's a point that most small businesses ignore. Part of the problem is the idea that small businesses can't afford it - that it means sending your employees on MBA's and expensive training programs. We used to think so at Userlike as well, but that's not what a learning culture is about. To use a stiff definition, a learning culture is, ""a collection of organizational conventions, values, practices and processes that encourage employees and organizations develop knowledge and competence."" Sounds fancy enough, but no mention of an MBA in there. It really doesn't have to be expensive - like us, you can start one almost for free. And although it might not get you into Fortune's list right away, it'll bless your business in a number of ways. Boost Productivity In mental labor - and I presume that's what you're doing - productivity doesn't depend so much on working hours. As Gregory Ciotti put it, productivity = focus + energy . Repetitive work drains you of your energy. Learning, on the other hand, releases an energy boost. Daniel Pink shows how motivation is made up out of three building blocks, the so-called trifecta of motivation : Autonomy, Mastery, and Purpose. All these elements are present in the act of learning. When you dedicate some resources to learning, you're actually taking a bit of focus away for the benefit of energy - lifting your overall productivity. Gallup's State of the Global Workplace Report came to the same conclusion, finding that learning leads to employee engagement. Cultivate Best Practices There's an obvious alignment of interests between employee and employer in a steep learning curve. For the company, the more the employee learns, the better she will do her job. The employee, on the other hand, will get more satisfaction out of her work by doing a better job and growing as a person. Paul Jun : ""Learning hones us like wind over sandstone; it sharpens our craft and removes the impurities in our misperceptions."" Grow Your Employees into the Leaders of Tomorrow Working in a fast growing company means you'll be taking in a lot of new people in a short period of time. More people means more hierarchy - perhaps even full time managers. Just like a feeling of injustice will strike when people cut in line at your daily supermarket trip, it's extremely frustrating to see new team members starting out higher on the ladder than you. To prevent this situation, we explained our team members that we see them as the leaders of tomorrow, but that they'll have to invest in themselves to rise up to the task. Rich football clubs like Manchester City or Chelsea spend dozens - sometimes hundreds - of millions each year on adding top players to their teams. But if you're a small club and you're not blessed with an oil sheik sugar daddy, your only option is to invest in training and nurturing your own youth teams into the stars of tomorrow. Attract Talent Harvard Business Review showed that personal growth is an innate desire for 'high potentials' . Therefore, to attract such talent, you need to offer growth opportunities. Back to the football analogy. You often see teenage prodigies choose a modest salary at a smaller club over a royal salary at a top club. Why? At smaller clubs, players are given more time to develop themselves, while in larger clubs the young guns stay out of sight on the bench or in the youth teams. It's a story of performance versus growth culture. At a small club - or startup - talents can be important at the frontline from the start, while at a top club - or corporate - they'll start out as glorified minions. So instead of aiming at their wallets, compete with the corporates for the best talents by addressing their desire to learn and be important. Retain Talent According to Inc , the number one reason for employees leaving a company is not salary, but a lack of advancement. As long as they are growing, it makes sense for your prodigies to stick around. Once they've reached their plateau, however, they'll get bored and leave. Ambitious people want to be important, successful, and make an impact. By keeping the focus on personal growth and challenging projects, you'll retain your star employees for much longer. What's more, by making their personal growth one of your main objectives, you'll secure their loyalty. In the end, your company will have to grow to ensure growth opportunities for your top performers. Now this part, as they would say in academics, is 'outside the scope of this paper'. But there's one sure way you won't ever grow - when you let your best people leave. Stay Up-to-Date with Critical Developments The present-day pace of development is so high that you can't leave its tracking to management only. Britt Andreatta shows that in a learning company, all employees have their eyes open to relevant developments - minimizing the risk of missing the boat on critical developments. Around one year ago we set out to develop a learning culture at Userlike - without resources. Here are our 7 tips. 1. Make Learning a Fixed Point on Your Discussion List Employees interpret the culture and values of a company through the focus points of its leadership. You can tell people to invest in themselves, but if you don't get back to it - if you don't repeat it over and over again - it won't stick. In the words of late Aristotle: ""it is frequent repetition that produces a natural tendency"". So after your upcoming team speech about your move to a company learning culture, include personal learning and growth topics in all of your one-on-one's. Start acting like one of Google's personal development coaches , supporting your employees in realizing their full potential. 2. Set Up Relevant Topics to Learn About One of the reasons why our education system is so darn inefficient is because 90% of what is taught won't be used ever again - or not anytime soon, anyways. To reach deep knowledge, you should deploy what you've learned in the workplace. To get to this stage we discuss upcoming projects with our team members and look for learning topics that relate to them. Some weeks ago, for example, we decided to set up a more structured process for our public relations. So we discussed what would be helpful areas to learn more about - with influencer marketing the first topic to focus on. The knife cuts both ways: the quality of learning improves because of its relevance to the current project, and upcoming projects are executed better. 3. Set Up a Company Library Since a few months we've set up a company library in our office. It started out with the founders bringing in their favorite books related to our business or their field - titles like Behind the Cloud, the Hard Thing About Hard Things, The Pragmatic Programmer, Rework, etc. We encouraged others to read these books as well, and then to take their own relevant books and make them available to the team. What's more, each employee got a budget for buying books related to their specific fields - books that advance their professional development. The library fulfills a practical as well as a symbolic purpose. Practical, because it offers easy access to high quality learning material. Symbolic, because it reflects our culture - our values of personal growth and following best practices. Besides that, having your own collection of field-specific books in the office reinforces a sense of ownership, a certain pride of your craft. 4. Online Course Allowance Not all online courses are free, of course, but on platforms like Coursera there are many great, affordable options out there that your employees can learn from. Most employees would rather spend those few bucks on another beer at their favorite bar than to spend it on an online course, so it's a good idea to set up a yearly allowance for this purpose. 5. Dedicate First 30 - 60 Minutes of the Day to Learning If you ask people to learn 'in their own time', it'll likely get pushed away for family time, sports, Netflix, etc. So we told our team that, even though we recommend investing in themselves outside of working hours, they should start each working day with 30 to 60 minutes of related studies. Some managers will worry about a loss in precious productivity. But remember that productivity doesn't depend so much on the number of hours worked. Productivity = focus + energy. Take away some focus for learning to make motivation and energy levels surge. Another benefit is that what you read about in the morning, your mind will be focused on throughout the day. So if your employees dedicate this morning hour to learning about their craft, they will perform their tasks better throughout the remainder of the day. This is why Benjamin Hardy advises us to read uplifting content in the morning , since ""it puts you in the zone to perform at your highest."" 6. Encourage the Use of a Notebook We encourage our team members to keep notebooks to write down their thoughts and learnings. Paul Jun shows how a world of benefits flows from the habit of regular writing , with wins in clarity, creativity, and better learning. Ask any writer and she'll tell you that her craft is her biggest source of learning. But even though it's a good habit, it's one that is hard to install on others. Our next point will help you with that. 7. Write Blog Posts About your Learnings The best blogs on the web, the ones providing the most value and deepest insights, come from people who write about experiences and learnings very close to them. This is nicely visualized in Teri Watson's post "" How to Find Your Content Marketing 'Sweet Spot' "". Blogs reaching this sweet spot go beyond your standard, brainless listicles. They spit knowledge. Some of my favorite examples from other SaaS companies are Help Scout and Intercom. Their entire teams seem to be activated in the creation of content - with the insights and experiences of the company overlapping with the interests of their target groups. Look around at your teammates. Look at all this potential, all this invaluable knowledge locked up inside their heads. Harness that knowledge and you can create some high value content. We set up quarterly goals for our team members to come up with a number of post topics related to their learnings. These suggestions are then discussed with the content team, turned into a tight and edgy post topic, after which a collaborative blog post is created. Again, the knife cuts on both sides. There is immediate relevance for the learnings of your team members, who need to think of topics for posts while they are reading, and apply them for the content. It's also stimulating, because the completion of a blog post marks the completion of a learning track - a celebration of what they've learned. On the other edge of the knife, the value of your blog will rise immensely. A company learning culture is too valuable to ignore. Guide your employees to success and they'll return the favor. Do you have some more bootstrapped tips for promoting a learning culture? Let us know in the comments below.",en,40
495,900,1462991873,CONTENT SHARED,8402348887626635381,-315113304665694510,-7170575965220567933,,,,HTML,http://www.wired.com/insights/2014/09/e-commerce-to-web-3-0/,from e-commerce to web 3.0: let the bots do the shopping,"Back in 2007, even before the iPhone was launched, giving us a powerful computer in our pockets or handbags, I started outlining a vision for Web 3.0. Tim Berners-Lee, a father of the World Wide Web, talks about the ""Semantic Web,"" a way that computers employ the meaning of words - not just pattern matching - along with logical rules to connect independent nuggets of data and so create more context for information. The formula that makes the most sense to me is this: Web 3.0 results from combining content, commerce, community and context, with personalization and vertical search. Or, to put it in a handy phrase: Web 3.0 = (4C + P + VS). Web 1.0 was all about driving online commerce and trying to find ""anything"" in the tangled jungle of the Web. It produced companies like Yahoo, Amazon.com, eBay, Netflix and Blue Nile. The rush for dollars also resulted in the dot-com meltdown. Even so, people's habits of searching, buying and selling genuinely changed. Today, you will find hundreds of thousands of merchants selling everything from cable organizers, window blinds, clothes, water filters, and everything else under the sun through specialized online stores. Specialty retail, in other words, has made a decisive transition to the web. Web 2.0 has been primarily focused on social networking through online communities. Facebook, Twitter and LinkedIn, have been the most notable companies to emerge. But there are a plethora of others where you can ""meet,"" ""connect"" and ""make friends"" online these days - habits no longer considered weird. At the same time, we've seen a great deal of investment in vertical search companies. If you are looking for a job, you can go to a site like Indeed.com and search across various job portals and career sites. Or go to Kayak if you've got travel questions, or TheFind if you're seeking shopping advice. In each case, the sites have carefully customized search parameters (job seekers, for instance, can search on salary ranges, locations, job levels and so on). Therein lies the big difference with Google, a generic horizontal search engine. Finally, Web 2.0 has brought an onslaught of user-generated content in the form of blogs, podcasts, appending comments at the bottom of articles, posting reviews of restaurants, movies, stores, and hotels. Media has become truly interactive, as opposed to the one-way world we were used to. Many more voices are being raised, and heard. The media industry, as we have known it, has been shaken to its roots. The next wave - Web 3.0 - will organize itself around two different elements: context and the user.By ""context,"" I mean the intent that brings you to the Web, your reason for surfing. Looking for a job is ""context,"" as is planning a trip or shopping for clothes. Fundamental to context is the user. And when you fuse a specific user with genuine context, you wind up with truly personalized service. Imagine this: You are planning a trip to Rome. You are looking for a hotel around Piazza di Spagna, but not something large and impersonal - which rules out the Hassler Villa Medici. You like smaller bed-and-breakfasts, with charm, warmth, character. You want an online travel agent who can understand your needs and preferences, and find you not only the right hotel but really interesting restaurants, boutiques and shows all aligned with your taste. Normally, you use Guide du Routard as your travel guide, but today there is still a gulf between travel guides and online travel-booking sites - in other words, content and commerce are fragmented. In Web 3.0, you will see content and commerce finally come together in a big way, no longer forcing you to hop from site to site to get one job done. On this same trip, you would love to meet local people who share your interests - say, cooking, jazz, opera. In Web 3.0, you will see the community elements of Web 2.0 pulled into context, making it as easy to find new friends with common interests, even in a distant city, as it is to book a hotel room. Some user-generated content is already evolving into an integral part of travel planning today. At TripAdviser, for instance, travelers report back on their experiences at hotels around the world. The missing element, however, is the notion of the individual user and his or her personal needs. You don't want to read reviews from anyone. You want to read reviews by people whose taste and judgment you trust. In a Web 3.0 world, then, a personalized travel agent will help you find and book a highly customized itinerary, leveraging all the power of previous generations of Web technology -searching (both generic and vertical), community building, content and commerce. That's how I get Web 3.0 = (4C+P+VS) - the sum of content, commerce, community and context, with personalization and vertical search. This is complex technology, requiring sophisticated artificial-intelligence algorithms. After all, your Web 3.0 travel agent will not be a ""person"" but a ""bot"", or intelligent agent. But I suspect you will like your travel bot. And your career bot. And your shopping bot. Let's look at a shopping bot, in fact. I am a petite woman, dark-skinned, dark-haired, brown-eyed. I have a distinct personal style, and only certain designers resonate with it (Context). I want my personal SAKS Fifth Avenue which carries clothes by those designers, in my size (Commerce). I want my personal Vogue, which covers articles about that style, those designers, and other emerging ones like them (Content). And I want to exchange notes with others of my size-shape-style-psychographic and discover what else looks good. I also want the recommendation system tell me what they're buying (Community). There are also some basic principles of what looks good based on skin tone, body shape, hair color, eye color ... I want the search engine to be able to filter and match based on an algorithm that builds in this knowledge base (Personalization, Vertical Search). Now, imagine the same for a short, plump, white, red-haired man, who doesn't really have a sense of what to wear. And he doesn't have a wife or a girlfriend. Before Web 3.0, he could go to the personal shopper at Nordstrom. With Web 3.0, the Internet will be his personal shopper. Aided with ubiquitous, mobile presence, these digital shoppers will anticipate all his needs, all of each of our needs. So be patient with the technology entrepreneurs around the world, who are working through these generations of the evolving Web, trying to bring about a dramatically better user experience. After all, they - and their bots - are working for you. Sramana Mitra is the founder of One Million by One Million (1M/1M), a global virtual incubator. This post was excerpted from her new book, From E-Commerce to Web 3.0: Entrepreneur Journeys .",en,40
496,838,1462564019,CONTENT SHARED,5037221949349729844,-1032019229384696495,-5381545755928873151,,,,HTML,https://www.thinkwithgoogle.com/articles/youtube-empowering-ads-engage.html?utm_source=Gplus&utm_medium=social&utm_campaign=Think,susan wojcicki on the effectiveness of empowering ads on youtube,"Ads that empower women don't just generate impressions, they leave them. They are more likely to be seen, shared, and remembered. Susan Wojcicki, CEO of YouTube, explains why this is a such a tremendous opportunity for savvy brands and enlightened creative agencies. The girl says to the doll, ""I'll make believe that I am you."" With those seven words, Mattel introduced the world to Barbie, America's first fashion doll. The original commercial, which aired in 1959 (and of course is up on YouTube), shows Barbie in various outfits, from ball gowns to bathing suits. But then, as the music crescendos, we hear those seven words against a lasting image: Barbie in a wedding dress. That's the kind of advertising both my mother and I grew up with. Women in commercials were portrayed as either mothers or models, wives or waitresses, with few depictions of the diverse lives we could lead outside of our relationships to men or children. If we weren't arm candy, we were eye candy, as scantily clad women promoted everything from beer to cheeseburgers to gym memberships. Women ages 18-34 are twice as likely to think highly of a brand that made an empowering ad. [[callout-headline-1]] But recently, we've seen some brave advertisers portray women and girls in a new light, one focused on breaking down stereotypes, rather than reinforcing them. From Always' #LikeAGirl to Nike's #BetterForIt , women are being encouraged, celebrated, held up not for how they look but for what they can accomplish. And while the latest research from the American Psychological Association shows that sex doesn't actually sell , it's clear on YouTube that empowerment engages. In the past year, the number of empowering advertisements that appeared on our Ads Leaderboard -our monthly tracker of the most watched ads on YouTube-more than doubled. 1 A big reason for that trend is that people are choosing to watch them; the top 10 empowering ads were two-and-a-half times less likely to be skipped than their peers. 2 To highlight this movement, we've collected those top 10 ads into a special Empowering Ads Leaderboard , featuring those that performed best on YouTube. [[inline-video-1]] These ads don't just generate impressions, they leave impressions. Women ages 18-34 are twice as likely to think highly of a brand that made an empowering ad and nearly 80% more likely to like, share, comment, and subscribe after watching one. 3 We also ran ad recall studies on eight of the campaigns on the Empowering Ads Leaderboard, and all performed in the top 25% of their categories, with most in the top 10%. 4 [[callout-headline-2]] So if empowering ads are so effective, why are we only seeing them now? Partly because women are being called upon to advertise to women. Despite the disappointing fact that only 11% of creative directors are women, 5 half of the creatives responsible for the empowering ads on our Leaderboard were women. 6 With women expected to control two-thirds of consumer spending in the U.S. over the next decade, 7 creative agencies would be wise to empower women not just in their ads but in their own ranks. But I believe another big reason for the rise of these ads isn't just the artist, it's the canvas. With YouTube, brands can break free of the 30- or 60-second spot to tell rich, nuanced stories. They can take advantage of the creative freedom our platform allows to tackle complex issues like glass ceilings or gender violence . Dove's Real Beauty Sketches , a video many consider the start of this trend, was a three-minute meditation on women's self-image, something you simply couldn't imagine airing on TV during a commercial break. [[inline-video-2]] And perhaps most importantly, social media gives women viewers a voice and opportunity to talk back. We've seen sexist or regressive ads draw an increased number of critical comments and dislikes from both men and women on our platform, while hashtags like #WomenNotObjects and #NotBuyingIt have led thousands to critique ads and boycott brands. I'm incredibly proud of the role YouTube has played in bringing more textured, inspiring stories to life and I'm prouder still that a community of engaged fans is consistently watching and engaging with them. It goes to show that people are hungry for creative that empowers, rather than objectifies. Savvy brands and enlightened creative agencies have a tremendous amount to gain by satisfying those appetites. As for Barbie, she's got a new look and a new ad. In the campaign, called "" Imagine the Possibilities ,"" we see young girls giving a college lecture to a room full of surprised students, treating a sick animal, taking a business call at the airport and coaching a men's soccer team. At the end of the ad, we realize the girls are imagining their own future as they play with their Barbie dolls. The message is still ""I'll make believe that I am you,"" but this time Barbie's wearing a lab coat, not a wedding dress. It's been watched more than 20 million times. This op-ed originally appeared in the April 2016 Adweek Women's Issue . Sources 1 YouTube data, U.S., 2014-2015. 2 Compared to their peers in similar categories. YouTube Data, U.S., 2013-2015. 3 Google Consumer Survey, U.S., February 2016. (n=1,500 women, ages 18-34) 4 There were ad recall studies featuring eight of the empowering ads from 2015 considered for our Leaderboard. All of the empowering ads were best-in-class and drove significant lift in ad recall, with all performing in the top 25% of their peer set and the majority in the top 10% of their peer set. 5 6 YouTube Data, U.S., 2013-2015. 7 Susan Wojcicki CEO of YouTube",en,40
497,2623,1477134684,CONTENT SHARED,-3071732669882279801,3915038251784681624,-3836398237662463548,Android - Native Mobile App,SP,BR,HTML,http://cio.com.br/opiniao/2016/10/20/qual-e-a-real-diferenca-entre-data-storytelling-e-analise-de-dados/,qual é a real diferença entre data storytelling e análise de dados? - cio,"Armazenar dados é uma prática muito comum nas organizações. Afinal, é preciso registrar tudo o que acontece no ambiente corporativo a fim de criar um histórico consistente que possa estruturar o planejamento futuro da organização. Mas nem sempre ler esses dados e enxergar respostas é uma tarefa fácil. Geralmente, grandes bases de conteúdos são acumuladas pelas empresas, mas nas últimas décadas, com a digitalização de todas as informações das companhias, uma massa gigantesca de informações desconexas foi criada, tornando mais difíceis não só a busca por dados consistentes e estratégicos, mas também a maneira destes serem decifrados. Quando queremos entender os problemas e encontrar as soluções, analisar a base de informações das organizações é extremamente vital. Esses dados ligam os pontos que jamais imaginaríamos que pudessem ser conectados, e nos fornece respostas para perguntas importantes sobre o core business da empresa e as estratégias a serem seguidas para seu crescimento. A partir daí são transformados em infográficos e os indicadores fornecidos por este podem ser contados por meio de uma em uma história simples, cabendo à liderança ou aos gestores a missão de ler, entender e aplicar a ""moral"" da história em decisões estratégicas para a empresa. Eis a origem do fenômeno do Data Storytelling. Mas você deve estar se perguntando: qual a real diferença entre Data Storytelling e análise de dados? Atualmente, há várias ferramentas que cumprem bem o papel de analisar dados, mas com a dinamicidade do mercado e a exigência cada vez mais crescente por respostas rápidas, as empresas têm buscado instrumentos que forneçam uma análise completa, incluindo recursos para a criação de apresentações dinâmicas e de fácil entendimento. O Data Storytelling é o resultado do que chamamos de pacote completo: uma solução que seja capaz de analisar dados, cruzá-los e ainda dispor de uma interface que possibilite a construção de apresentações visuais. Com isso, os tomadores de decisão conseguem utilizar apenas uma plataforma para entender o cenário, fazer análises preditivas e apresentar as informações com embasamento e praticidade. Sendo assim, interpretar as informações e saber como e onde utilizá-las é a parte mais difícil da equação. Entendendo essa dificuldade surge no mercado essa tendência que veio para simplificar a análise e o entendimento das informações por meio de uma história. A ideia é que esta possa ajudar o board da companhia na compreensão do passado e do presente dos negócios, promovendo direcionamentos para estes decidirem como querem que seja o futuro. A conclusão a ser realizada é: tão imprescindível quanto analisar a alta gama de dados atuais é saber interpretá-las e aproveitá-las de maneira eficaz e benéfica, podendo ainda otimizar tempo e reduzir custos. (*) Roberto Guerra é diretor-geral da Inteligência de Negócios (IN)",pt,40
498,1723,1468244860,CONTENT SHARED,-3102446808559723934,3609194402293569455,-493688854152847866,,,,HTML,http://startupi.com.br/2016/07/visa-e-swatch-lancam-relogio-para-pagamentos-contactless-no-brasil/,visa e swatch lançam relógio para pagamentos contactless no brasil - startupi,"Acaba de chegar oficialmente ao Brasil o novo relógio da Swatch Bellamy - um ""bel ami"", bom amigo em francês. Através de sua tecnologia de pagamento sem contato, o produto chega ao mercado para auxiliar quem está com pressa na hora de fazer ou prefere sair de casa sem carregar a carteira no bolso. O Swatch Bellamy é uma alternativa ao dinheiro em espécie: basta aproximar o relógio de um terminal de pagamento sem contato por um instante e a transação será efetuada em questão de segundos. Como a realização de pagamentos sem contato não exige energia, a bateria da pulseira pode durar por anos. Diferentemente dos cartões de pagamento tradicionais, não há nada no relógio que mostre que ele possa ser utilizado para tal finalidade, tornando-o uma forma de pagamento mais discreta que as demais. Cada relógio vem equipado com uma antena NFC (Near Field Communications) embutida e um chip contactless da Visa para comunicação com terminais de pagamento sem contato. Semelhante aos cartões pré-pagos sem contato, os gastos são debitados de créditos previamente carregados por meio de uma função de pagamento bancário associada. Desta forma, mesmo que o usuário perca seu relógio, o crédito estará seguro. Após adquirir um dos quatro modelos já disponíveis, os clientes devem se cadastrar junto à sua instituição bancária e depositar créditos em seu relógio. Os usuários podem fazer compras tanto no Brasil quanto em qualquer parte do mundo que aceite o pagamento sem contato da Visa. Para visualizar saldo e extrato, o usuário deve acessar a página da Swatch para os relógios Bellamy . O Swatch Bellamy possui parceria com a Visa e a Brasil Pré-Pagos. A OT (Oberthur Technologies) foi a empresa selecionada para fornecer a solução de pagamento NFC e a personalização de dados, garantindo aos usuários a realização de pagamentos seguros e sem contato. A Swatch escolheu o Brasil para ser o primeiro país das Américas a começar a usar o Swatch Bellamy. Esta nova forma de pagamento foi lançada na China e na Suíça no início do ano. Os primeiros modelos estão disponíveis no Brasil todo a partir deste mês, no valor de R$ 725.",pt,40
499,1604,1467312509,CONTENT SHARED,3825866166263569974,22763587941636338,-1704524960073432371,,,,HTML,http://tecnologia.uol.com.br/noticias/bbc/2016/06/29/piloto-de-caca-criado-por-inteligencia-artificial-vence-humano-em-combate-simulado.htm,piloto de caça criado por inteligência artificial vence humano em combate simulado,"Um sistema de pilotagem de caças criado por inteligência artificial derrotou dois jatos em uma simulação de combate. O piloto, batizado de Alpha, usou quatro jatos virtuais para defender uma área de litoral dos dois caças - e não sofreu perdas. Desenvolvido por uma equipe de pesquisadores da Universidade de Cincinnati, nos Estados Unidos, o sistema também venceu um piloto aposentado de caças da Força Aérea Americana - logo, bastante experiente. Na simulação descrita no estudo, os dois jatos que atacavam o litoral - chamada de equipe azul - tinham um sistema de armas mais poderoso que os jatos usados pelo Alpha, chamados de equipe vermelha. Mas o sistema criado por inteligência artificial conseguiu se livrar dos inimigos depois de realizar uma série de manobras evasivas. Um especialista em aviação afirmou que os resultados são promissores. 'Adversário letal' Na pesquisa, os cientistas da Universidade de Cincinnati e a companhia especializada em tecnologia Psibernetix chamam o sistema Alpha de um ""adversário letal"". Descrevendo as simulações de combate entre o sistema de inteligência artificial e o piloto aposentado Gene Lee, os pesquisadores escreveram que o americano ""não apenas não conseguiu uma morte contra (o Alpha), mas também foi derrubado pelos vermelhos todas as vezes nos combates prolongados"". O Alpha usa uma forma de inteligência artificial baseada no conceito de lógica difusa (ou ""fuzzy""), na qual um computador analisa uma série ampla de opções antes de tomar a decisão. Devido ao fato de um caça virtual produzir uma quantidade grande de dados para serem interpretados, nem sempre fica óbvio quais as manobras são mais vantajosas e, um combate ou mesmo em que momento uma arma deve ser disparada. Sistemas que usam a lógica difusa podem analisar a importância desses dados individuais antes de tomar uma decisão mais ampla. O grande feito atingido pelos pesquisadores americanos com o sistema Alpha foi a capacidade de tomar essas decisões em tempo real e com a eficiência de um computador. ""Aqui você tem um sistema de inteligência artificial que parece ser capaz de lidar com o ambiente exclusivamente aéreo, é extraordinariamente dinâmico, tem um número extraordinário de parâmetros e, na teoria, consegue enfrentar muito bem um piloto de combate qualificado, capaz e experiente"", disse Doug Barrie, analista aeroespacial militar da consultoria IISS. É como um campeão de xadrez perdendo para um computador Ética Apesar do entusiasmo, Barrie lembrou à BBC que pode não ser tão fácil ou apropriado tentar usar o Alpha em ambientes de combate na vida real. A analista afirmou que se o sistema for usado de verdade e decidir atacar um alvo não militar, por exemplo, os resultados poderão ser terríveis. ""A indignação do público seria imensa."" Barrie acrescentou, porém, que o Alpha tem potencial para se tornar uma ferramenta de simulação de combate ou para ajudar a desenvolver sistemas melhores para uso dos pilotos humanos. Obrigado pela sua inscrição",pt,40
500,1753,1468413328,CONTENT SHARED,5940374562401786524,4918484843075254252,4482638943263754028,,,,HTML,http://www.happymelly.com/visual-thinking-and-learning-3-0-working-together-at-walmart-com/,visual thinking and learning 3.0 working together at walmart.com | happy melly,"As usual, three Scrum Masters and I were drinking a coffee and talking about our daily grind at Walmart.com , when a common problem emerged during our dialogue: How to improve our grooming meetings? In my opinion the grooming meeting is the most important factor in a Sprint. Do it well and your team will know exactly what they have to do, with a low level of uncertainty. However, if the meeting isn't organized properly, it can cause misunderstanding and your Sprint can be flawed. As good Scrum Masters, we decided to continue the conversation in a different way. I proposed to try the Learning Canvas - a Learning 3.0 tool , together with a Visual Thinking Practice created by Pati Dobrowolski: The Big Picture Wall Map. I could talk about Product Grooming Meetings forever, but I'll try to describe just the mix of practices that we used, as this is the main purpose of this post. So, let's go! The Learning Canvas The Learning Canvas is an awesome tool created by Alexandre Magno that promotes learning using an emergent approach. Using a canvas, the asker exposes a specific problem and its symptoms to a team or a group of people who are interested in solving the problem. Afterwards, he explains what he expects as the outcome when the problem is solved. The group begins by sharing their experiences, what they have lived and what makes sense as a problem solution. Everybody is invited to think out of the box and to propose ideas that the asker could try. At this moment, anyone can propose anything despite of their experience. Finally, the asker decides the best solution based on what has been shared. The Canvas is just one part of a great concept for emergent learning. There's so much more to discover! So, after reading this post, please visit Learning 3.0 where you'll find incredible things! Have fun! The Big Picture Wall Map Anybody who knows me understands that for ten themes that I'm in, nine are about visual thinking! Since I was a kid, I have loved to draw! This subject really excites me and I'm always trying to use it in my daily grind. One day, searching my passion on Google, I found Pati Dobrowolski talking about a visual tool that helps us go from a problematic situation, to an ideal one; whilst establishing some bold steps to get there. The method is very simple and you can get there in just four steps: Divide a sheet of paper in a half with a line On the left side, draw the current reality that is bothering you On the right side, draw the desired new reality, how the things will be when you solve what is bothering you Draw three 'big arrows as bridges between the actual situation and the future situation, and write in each one a bold step that you think will help you to get from one side to the other. How we arrange things to get them ""dancing"" together In a reserved room, I put a Learning Canvas on the wall and distributed some sheets of paper for a group of Scrum Masters who came along to discuss our grooming meetings. Here's a step-by-step account of what we did: We divided a sheet of paper in a half with a line On the left side, we wrote ""Current Situation"" In 5 minutes each one drew the actual situation of their grooming meetings on the left side The SMs explained their drawings, and as the facilitator, I supported the discussion, sustaining questions and extracting the symptoms by writing them on post-its We all stopped in front of the Learning Canvas to verify if all symptoms written on the post-its made sense to the group On the right side, we wrote ""Desired new Situation"" In 5 minutes each one drew the desired situation of their grooming meetings on the right side The SMs explained their drawings and as a facilitator, I supported again the discussion, sustaining questions and extracting the expected results by writing them on post-its We all stopped in front of the Learning Canvas to verify if all expected results written on the post-its made sense to the group In 5 minutes, each participant was encouraged to share good and bad experiences about their grooming meetings. We wrote those experiences on some post-its and fixed them to the Learning Canvas After 5 minutes of sharing experiences, each participant was encouraged to share out-of-box ideas about Grooming Meetings. We wrote those ideas on post-its and fixed them on the Learning Canvas Each participant should write the ""Three Bold Steps"" to go from the actual situation to the desired situation (the bridges that I mentioned before)! To do that, they must consider experiences and ideas shared in the steps before Finally, we had a discussion around the bold steps, which each one elected to try, and fixed the drawings on the Learning Canvas Conclusions Visual Thinking helped us to be pragmatic in identifying symptoms and expected results. Sometimes we tend to be wordy when discussing these things, some of us like to talk, and don't pay attention to the others. To draw first is an exercise in thinking before speaking , and observing a drawing made by another person is a good exercise to perceive different point of views. The strict time boxes helped us to stay focused. Visual Thinking can help us to be pragmatic in identifying symptoms and expected results.... Click To Tweet We had no ""formal asker"", so the whole group took on the role. It's a cool idea when you're in situations where the ""Try"" part of Learning Canvas can't be a homogeneous sequence of steps, because there are a lot of different realities. Even if the problem discussed: how to improve our grooming meetings, is something that affects everyone. Each participant has a particular focus in his daily grind and the big picture wall map supported us to visualize that, and to accept more than one way to try. To mix practices is a cool and necessary thing. We have a lot of good ideas blooming around the world, so, why not to put them together to see if they positively affect team discussions? That's just a brief description about a Learning 3.0 Experience that was very helpful to us in Walmart.com. I hope it was useful to you! A special thanks to: Melissa Itimura, Ana Marysa, Rafael Buzon and Eduardo Tardin for the great help! Let me know your perceptions, thoughts and experiences, and we can go make a mojito of ideas! My twitter is @jreisstudio , fell free to get in touch.￼￼￼￼￼￼",en,40
501,1528,1466867555,CONTENT SHARED,3579921471626387620,-6895155480127642372,6182198327666594200,,,,HTML,http://www.segfoco.com.br/mercado/segures/,dicionário de segurês,"O Mercado de Seguros é prodigioso no que se refere aos termos usados no seu dia a dia: o já famoso Segurês ! São tantos os termos técnicos utilizados tão frequentemente pelos profissionais da área, que muitas vezes nos esquecemos que muitas pessoas (a maioria, diga-se de passagem...) não fazem a menor idéia do que significa muitos desses termos. Se você está, ou conhece alguém que esteja, ""meio confuso"" com a sopa de letrinhas do mercado de seguros, a Revista Seguros em Foco ® está aqui para ajudar. Confira, a seguir, alguns dos termos mais utilizados pelo mercado de seguros. Se a palavra que procura não está na lista, então é só mandar sua dúvida para (mencionando no Campo Assunto: Dicionário), que descobriremos seu significado e o mandaremos para você. Aceitação - Aprovação da seguradora da proposta apresentada pelo segurado, e emissão da respectiva apólice de seguro. Acessório - Entende-se como acessório rádios, toca-fitas, CD player, televisões, amplificadores e alto-falantes originais ou não, desde que fixados de forma permanente, relacionados na proposta e com limite máximo de indenização indicado na apólice. Acidente de trânsito - É o evento ocorrido no trânsito e nos pontos de parada e apoio, com data caracterizada, exclusiva e diretamente externo, súbito, involuntário e violento, causador de danos materiais, danos corporais e/ou danos morais. Acidentes Pessoais - Eventos ocorridos a pessoas de forma súbita, involuntária e externa, causadores de lesão física que por si só, e independentemente de toda e qualquer outra causa, tenha como conseqüência direta a morte ou invalidez permanente total ou parcial. Adesão (Contrato de) - A maior parte dos contratos de seguro é, na verdade, composta de contratos de adesão, pois seus termos e condições são elaborados pelo segurador e o segurado simplesmente adere ao contrato. Exatamente por isto, nos contratos em que existe ambigüidade é necessária a intervenção de um juiz. Os contratos de seguros de riscos comerciais, industriais e marítimos e de aeronaves não são mais chamados de contratos de adesão, pois é o próprio segurado que negocia os termos do contrato com a seguradora, o que não acontece com os chamados seguros de massa. Pela mesma razão, os contratos de re-seguro também não são considerados como sendo de adesão, pois ambas as partes pertencem à mesma indústria e, portanto, existe negociação de cláusulas. Aditivo - Condição adicional incluída no contrato de seguro, além das já previstas anteriormente. Agravação de Risco - Também conhecido na indústria de seguros, como hazard. Trata-se das circunstâncias que aumentam a probabilidade, a freqüência, ou magnitude, de que um sinistro ocorra. Estas circunstâncias independem da vontade do segurado e, por isto, levam à uma mudança na taxa ou nas condições oferecidas no seguro. Agravação Moral (do Risco) - quando a seguradora acredita que o segurado tenha alguma razão para intencionalmente acarretar um sinistro. Agravação Física (do Risco) - características tangíveis de risco de uma determinada exposição que aumentem as chances de ocorrer um sinistro. Aleatório - É tudo o que depende de fatores incertos ou casuais. ANS - A Agência Nacional de Saúde Suplementar tem por finalidade institucional promover a defesa do interesse público na assistência suplementar à saúde, regular as operadoras setoriais - inclusive quanto às suas relações com prestadores e consumidores - e contribuir para o desenvolvimento das ações de saúde no País. Apólice - É o documento emitido pela seguradora em função da aceitação do risco apresentado pelo segurado. A apólice contém os dados do segurado, do bem segurado e das coberturas contratadas. São parte integrante da apólice: a proposta de seguro, as condições gerais e particulares que identificam o risco, assim como as modificações que se produzam durante a vigência do seguro, realizadas através de endossos. Assim que receber a proposta, a seguradora tem 15 dias para emitir a apólice, que tem como objetivo exprimir de forma concisa e sucinta os principais tópicos cobertos pelo contrato Apólice coletiva - Contrato de seguro que cobre apenas uma pessoa ou um bem. Apólice individual - Contrato de seguro que cobre apenas uma pessoa ou um bem. Assistência - Garantias adicionais constantes em alguns seguros com serviços emergenciais, tais como: ambulância, coberturas provisórias de telhados, chaveiros, hospedagem, carro-reserva, guincho etc. Avaria Prévia - Danos existentes no veículo antes da contratação do seguro ou antes de um acidente, tais como, ferrugem, amassamentos e riscos que não são cobertos e que serão deduzidos do pagamento em caso de sinistros com perdas parciais. Averbação - Documento que o segurado utiliza para informar à seguradora sobre verbas e objetos a garantir nas apólices abertas. Muito utilizado no seguro de transportes. Permite ao segurado obter agilidade na contratação do seguro. Averbadora - Termo que é usado para denominar a pessoa jurídica (empresa) que contrata um plano de previdência privada, mas que não participa do custeio do mesmo. Nos planos de previdência a empresa pode participar como patrocinadora, e efetuar contribuições ao plano dos seus funcionários, ou simplesmente como averbadora. Aviso de Sinistro - Documento pelo qual o segurado, terceiro ou seu representante legal, comunicam à seguradora a ocorrência do evento, citando dia, hora, circunstâncias da ocorrência etc., cuja característica estão ligadas à circunstâncias previstas na apólice. É documento fundamental para que seja iniciado o processo de indenização. Beneficiário - Pessoa física ou jurídica a quem o segurado reconhece o direito de receber a quantia correspondente à indenização derivada da apólice de seguro, ou seja, é a pessoa que detém legalmente o direito à indenização. Bilhete de seguro - É um documento jurídico, emitido pelo segurador ao segurado, que substitui a apólice de seguro. Foi criado com o objetivo de facilitar a contratação do seguro. Boletim de Ocorrência (BO) - É o documento emitido por órgão competente que relata as circunstâncias do acidente ou registra o roubo/furto dos bens do cidadão. Bônus - Desconto progressivo dado ao segurado na renovação da apólice, reduzindo o preço do seguro dos segurados que não apresentarem reclamação de indenização durante a vigência da apólice. Cancelamento de Apólice - É a baixa do seguro no registro geral de apólice por falta de pagamento do prêmio, anulação do contrato, pelo pagamento de indenização pela perda total do bem segurado, ou por vontade das partes. Capital segurado - Valor máximo de indenização, ou seja, valor máximo que o segurado tem direito a receber em caso de sinistro. Carência - Prazo em que a responsabilidade do segurador em relação ao contrato de seguro fica suspensa. É o período após o qual o segurado pode utilizar o seguro. Casco - É o automóvel propriamente dito, excluídos os acessórios e equipamentos adicionais. Cobertura - Garantia de indenização ao segurado ou aos seus beneficiários do prejuízo decorrente da ocorrência de um dos riscos previstos no contrato de seguro. Coberturas Básicas - São aquelas garantias sem as quais a apólice de seguro não pode ser constituída. Coberturas Adicionais - São aquelas garantias oferecidas ao segurado, cuja contratação é opcional e de acordo com as suas necessidades. Condições gerais - Condições que regem o contrato de seguro, estabelecendo inclusive os direitos e obrigações do segurado e da seguradora, e que definem os riscos cobertos na apólice de seguro, bem como a sua indenização. Condições particulares de um seguro - Conjunto de cláusulas contratuais que obrigam e dão direitos tando ao segurado como ao segurador. Dizem respeito às diferentes modalidades de cobertura que podem existir dentro de um mesmo ramo de seguro. Contrato de Boa-Fé - Conceito que, no Código Civil Brasileiro, obriga o segurado e segurador ""à mais estrita boa-fé e veracidade, tanto a respeito do oobjeto, como das circunstâncias e declarações a ele concernentes"". O segurado corre o risco, inclusive, de perder o direito ao valor do seguro, ""se não fizer declarações verdadeiras e completas, omitindo circunstâncias que possam influir na aceitação da proposta ou na taxa do prêmio"". Corretor de Seguros - Profissional habilitado perante a SUSEP, pessoa física ou jurídica, legalmente autorizado a representar o segurado em um contrato de seguro, conforme Decreto Lei n°. 73 de 21/11/1966. É o intermediário legalmente autorizado a angariar e a promover contratos de seguro entre as seguradoras e pessoas físicas ou jurídicas. Os corretores são remunerados por comissão de corretagem e impedidos de manter qualquer tipo de vínculo de emprego ou sociedade com o segurador. Sua habilitação é concedida através de exame promovido pela Escola Nacional de Seguros (Funenseg). Cabe ao corretor de seguros intermediar o seguro pretendido, bem como orientar e esclarecer o segurado sobre os direitos, obrigações, limites e penalidades da apólice de seguro. Cosseguro - Operação em que mais de um segurador participa diretamente, em uma mesma apólice, de um mesmo risco. Cada segurador é responsável por uma quota ou parte do montante total do seguro. O prêmio pago é dividido na proporção da quota de cada segurador. Cotação - Ato de promover tomada de preços junto a mais de um segurador para a realização do contrato de seguro. Culpa Grave - Grau de culpa que se converteria em dolo se fosse praticada com má-fé. Falta que, por mais desleixado ou medíocre, o indivíduo não poderia cometer em detrimento de seu próprio interesse. Dano - É todo prejuízo material ou pessoal sofrido por um segurado, passível de indenização, de acordo com as condições de cobertura de uma apólice de seguro. Danos Corporais - É todo e qualquer danos causado ao corpo humano. Danos Materiais - É todo e qualquer dano que atinge os bens materiais, objetos e animais. Danos Morais - É todo aquele que, em decorrência de um dano corporal, traz como conseqüência a ofensa à honra, ao afeto, à liberdade, à profissão, ao respeito aos mortos, à psique, à saúde, ao nome, ao crédito, ao bem estar e à vida, ainda sem o advento do prejuízo econômico. Será devida somente quando contratada a cobertura específica de Danos Morais, dentro da garantia de Responsabilidade Civil Facultativa de Veículos. Dolo - Intenção de praticar um mal que é capitulado como crime, seja por ação ou por omissão ou, ainda, vício de consentimento caracterizado pela intenção de prejudicar ou fraudar outrem. Emolumentos - Conjunto de despesas adicionais que a seguradora cobra do segurado, correspondente as parcelas de impostos e outros encargos a que está sujeito o seguro, tal como custo de apólice. Endosso - Documento emitido pela seguradora, durante a vigência da apólice de seguro, comprovando alterações na apólice (aumento da importância segurada, cancelamento do seguro, mudança de endereço, etc). Estipulante - Pessoa física ou jurídica que contrata o seguro e em cujo nome é emitida a apólice e que fica investido dos poderes de representação dos segurados perante o segurador. O estipulante pode acumular a condição de beneficiário ou apenas representar interesse de terceiros, declarados nominalmente no contrato de seguro. Evento - Termo que define o sinistro ou acontecimento previsto e coberto ou não no contrato de seguro, que resulta em dano ao segurado. Franquia - É o valor ou percentual definido na apólice de seguro pelo qual o segurado responde obrigatoriamente pelos prejuízos de um sinistro coberto, e que representa a participação do segurado nos prejuízos, começando a responsabilidade da seguradora apenas e tão somente após alcançado o seu limite. Furto qualificado - Para efeito de cobertura, o furto qualificado é aquele em que há vestígios materiais inequívocos de destruição ou rompimento de obstáculos como portas, fechaduras, cadeados, alarmes. Obs.: A definição de furto qualificado para fins de seguro não se confunde com o disposto no § 40, artigo 155 do Código Penal. Furto Simples - Subtração de um bem, sem ameaça ou violência. Garantia (seguro) - Anteriormente denominado ""seguro garantia de obrigações contratuais"", é utilizado por instituições que necessitam garantir-se contra o risco de descumprimento de contratos firmados com fornecedores ou prestadores de serviços. Apresenta-se em várias modalidades: seguro de garantia do executante construtor, fornecedor ou prestador de serviços, garantia de adiantamento de pagamento, garantia de concorr6encia, garantia do executante e garantia de perfeito funcionamento. Não há termos iniciando por esta letra. Importância segurada - O mesmo que capital segurado. É o valor estabelecido na apólice de seguro para a garantia contratada e estabelece o valor máximo de indenização a ser paga pela seguradora em caso de sinistro. Em seguros de pessoas e responsabilidades, a determinação desse valor é livre entre as partes contratantes. Em se tratando de bens móveis ou imóveis, porém, esse valor fica limitado ao valor real do objeto. Indenização - Valor que a seguradora deve pagar ao segurado, beneficiário ou terceiro quando ocorre um sinistro coberto pelo contrato de seguro, ou seja, previsto na apólice. Em outras palavras, é o valor que a seguradora deverá pagar ao segurado, no caso de ocorrência do risco coberto, previsto no contrato de seguro. Não há termos iniciando por esta letra. Não há termos iniciando por esta letra. Limite máximo de indenização (LMI) - É o limite de responsabilidade da seguradora por sinistro ou série de sinistros para as coberturas contratadas na apólice. Má-Fé - Intenção dolosa. Intenção de prejudicar ou fraudar outrem. Praticar mal que é capitulado como crime. Motorista - Pessoa que, legalmente habilitada e com autorização do segurado, dirige o veículo segurado ou tem sob sua responsabilidade no momento do sinistro. Motorista Principal - Pessoa indicada na proposta de seguro como tal, devidamente habilitada, cujas características determinam o cálculo do seguro. Motorista Ocasional - Pessoa não indicada na proposta de seguro, que ocasionalmente venha a conduzir o veículo segurado, desde que devidamente habilitada. Nota Técnica Atuarial - É o estudo matemático e atuarial, feito por técnico capacitado, que serve para determinar o preço do prêmio de seguro. Objeto Segurado - É o bem material ou pessoa constante na apólice de seguro e para a qual são contratadas as garantias. Oficina Credenciada - Oficinas particulares e concessionárias que, através de contrato, prestam serviços à seguradora. Passageiro - É o usuário legalmente provido de seu bilhete de passagem ou o legalmente beneficiados com isenção de pagamento. Pecúlio - Em previdência social designa um benefício que é pago quando um aposentado volta a trabalhar. Em previdência privada designa a indenização, ou seja, benefício pago de uma só vez, e se contrapõe ao pagamento de benefícios na forma de renda mensal. Perda Parcial - A perda parcial será caracterizada quando os prejuízos indenizáveis, na data da liquidação do sinistro, não atingirem ou ultrapassarem a 75% do valor determinado para o objeto do segurado. Perda Total - Ocorre quando o valor dos prejuízos apurados em um sinistro atinge determinado montante, igual ou superior a 75% do bem segurado, tornando sua recuperação impossível ou economicamente inviável. Período Indenitário - Período compreendido entre a data em que o segurado começa a sofrer as conseqüências da queda de produção, consumo ou de prestação de serviços, provocadas pelo evento coberto, e a data em que retorna às atividades normais. Em geral este termo é usado nos contratos de seguro de locação, pagamento de aluguel, ou perda de emprego. Nos planos de previdência refere-se ao período em que o assistido tem direito ao recebimento de indenização sob a forma de renda vitalícia ou temporária. Perito ou Vistoriador - Profissional técnico especializado que intervém para informar sobre as causas, conseqüências e circunstâncias nos sinistros, e na avaliação dos danos sofridos. PGBL (Plano Gerador de Benefício Livre) - Trata-se de um dos planos de previdência complementar existentes no país. O PGBL ao invés de garantir uma rentabilidade mínima como acontece nos planos tradicionais, oferece a você 100% dos ganhos que o fundo onde os recursos são alocados obtiver no período. Existem basicamente três tipos de PGBL de acordo com o risco e volume aplicado em ações. Todos os investimentos são dedutíveis da base de cálculo do IR até o limite de 12% da renda bruta e o tributo incide sobre total do valor acumulado. As taxas de carregamento variam entre 0% e 3,5%. Ao aplicar em um PGBL o participante estará adquirindo cotas do fundo atrelado ao plano, da mesma forma que ocorre quando aplica num fundo de investimento comum. Os valores das cotas são divulgados diariamente nos jornais de grande circulação Plano Aberto - Termo usado para denominar os planos de previdência vendidos no mercado, que podem ser acessíveis por qualquer pessoa interessada em participar. Em geral são vendidos por seguradoras, bancos e entidades de previdência aberta. Exemplos são os planos PGBL, FAPI, etc... . Plano de Capitalização - Os planos de capitalização combinam a possibilidade de se concorrer a sorteios e prêmios, com uma forma de poupança programada. Nestes títulos a conta de investimento é ajustada pela TR +0,5%, o equivalente ao rendimento da poupança, mas ao contrário da poupança você não está isento do pagamento de imposto de renda. Em caso de sorteio e resgate do prêmio o investidor deverá pagar uma alíquota de 25% de imposto de renda, enquanto se decidir continuar participando do plano a alíquota sobe para 30%. O saque antes do prazo pode implicar na restituição apenas parcial do valor aplicado. Plano Fechado - Plano de previdência complementar criado pelas empresas em benefício de seus funcionários. Para cada R$ 1 em contribuições do funcionário ao fundo, a empresa deve fazer uma contribuição de no mínimo R$ 1. Plano Fechado com Benefício Definido - Plano de previdência complementar criado pelas empresas em benefício de seus funcionários, onde o que está definido é o benefício a ser recebido na aposentadoria. Como o saldo das contribuições não está dividido em contas individuais, pode haver déficit. Plano Fechado com Contribuição Definida - Plano de previdência complementar criado pelas empresas em benefício de seus funcionários, onde o valor do benefício a ser pago durante a aposentadoria equivale ao total acumulado. Ao contrário dos planos de benefício definido, nestes planos o que se determina é o valor da contribuição mensal. Além disso, há uma conta separada para cada empregado, minimizando o risco de déficit. Prejuízos - Perdas econômicas em conseqüência de um dano corporal ou material, sofrido pelo reclamante e indenizável pelo seguro, limitado à importância segurada. Prêmio - É o valor pago pelo segurado ou estipulante à seguradora para que esta assuma um determinado risco, ou um conjunto de riscos, através do contrato de seguro, transferindo para o segurador o risco a que está exposto. Ou seja, ao contrário do que a maioria das pessoas imagina, o termo não se refere ao valor que o segurado recebe, mas sim ao valor que o segurado paga para a seguradora para que esta garanta o pagamento da indenização especificada no contrato. O prêmio é estipulado com base em um percentual do risco que você pretende cobrir, isto é, da importância segurada. Prêmio Adicional - Trata-se de um prêmio suplementar, cobrado em certos casos determinados. Prêmio Fracionado - Trata-se do prêmio anual, que é dividido em parcelas para fins de pagamento. Prêmio Não Ganho - Montante do dinheiro que a seguradora terá que devolver em cada apólice caso a mesma seja cancelada. Prêmio Puro - Prêmio calculado pelo segurador para uma determinada cobertura ou conjunto de coberturas para fazer face ao pagamento de indenização ao segurado. Prescrição - É a perda da ação para reclamar os direitos ou a extinção das obrigações previstas nas apólices de seguro, em razão do transcurso dos prazos fixados em lei. Previdência Privada - A Previdência Privada (ou Complementar) foi implantada no Brasil no final da década de 70. No final dos anos 90, a reforma da previdência tornou a previdência privada mais atrativa, passando a permitir a dedução das contribuições previdenciárias para fins de imposto de renda, além de introduzir novos produtos. A Previdência Complementar pode ser de dois tipos: Fechada e Aberta. Previdência Privada Aberta - Uma das opções de previdência complementar existente no mercado, a outra sendo a de planos fechados. Inclui planos individuais, facultativos, que funcionam como fundos de investimento voltado para a aposentadoria (isto é com uma ótica de longo prazo). Esses fundos são administrados por instituições financeiras como, por exemplo, seguradoras, empresas de previdência privada e bancos, que em troca da administração dos recursos cobram uma comissão (taxa de administração). Previdência Privada Fechada - Uma das opções de previdência complementar existente no mercado, a outra sendo a previdência aberta. Ao contrário do que ocorre na previdência aberta, no caso da previdência fechada somente os funcionários da empresa patrocinadora podem participar do fundo, não sendo acessível à outras pessoas. A previdência fechada é oferecida pelas empresas aos seus funcionários através da constituição de um fundo de pensão para o qual contribuem tanto a empresa, quanto os funcionários. Os benefícios são acessíveis aos empregados ou dirigentes da empresa patrocinadora. Primeiro Risco Absoluto - Seguro em que a companhia seguradora responde por qualquer prejuízo real coberto até o limite da importância segurada e não invoca a regra proporcional. Neste tipo de seguro a regra de rateio nunca é aplicada. Primeiro Risco Relativo - Neste tipo de seguro a companhia seguradora responde somente pelos prejuízos até o limite da importância segurada. Caso o valor supere o montante fixado na apólice o segurado terá que dividir as perdas como se fosse um seguro proporcional. Proposta - Instrumento que representa a vontade do segurado de transferir o risco para o segurador. A proposta contém um questionário detalhado sobre o risco a ser segurado, e pode ser preenchida pelo próprio segurado, por seu representante legal ou pelo corretor de seguros. Após efetivada a apólice de seguro, a proposta torna-se parte integrante da mesma. Pró-rata - Critério utilizado para cálculo de devolução de prêmio ou cobrança de prêmio adicional. Leva em consideração o tempo a decorrer até o término do seguro ou o tempo decorrido desde o início da vigência até o momento da alteração. Provisões Técnicas - São assim chamadas nas empresas de seguros algumas reservas obrigatórias. Formam parte integrante e indispensável do mecanismo do seguro, sendo constituídas mensalmente e independente da existência de lucro na companhia seguradora. Pulverização de Risco - É o ato de distribuir ou dividir as responsabilidades do risco assumido pelo segurador, através do cosseguro e do resseguro, de forma que, assim disseminado, por maior que seja sua importância não venha a constituir perigo à estabilidade da carteira da seguradora. Não há termos iniciando por esta letra. Ramo - Tipo ou classificação das várias modalidades de seguro - vida, acidentes pessoais, automóveis, transportes, saúde. RC (Seguro de) - Responsabilidade Civil. Seguro que reembolsa prejuízos causados pelo segurado a terceiros. É baseado no princípio geral de direito que impõe a quem causa dano a outros o dever de reparar. Reembolso - Restituição total ou parcial de valores pagos pelo segurado a terceiros, referentes a danos cobertos pelo contrato de seguro. Regulação de Sinistro - É o exame executado pelo perito ou vistoriador, na ocorrência de um sinistro, das causas e circunstâncias para a caracterização do risco e, em face dessas verificações, se concluir sobre a cobertura, bem como se o segurado cumpriu todas as suas obrigações legais e contratuais. Reintegração de Cobertura - É a solicitação de recomposição da importância segurada de uma cobertura, na mesma proporção em que foi reduzida em função de um sinistro indenizado. Resgate - Recebimento, pelo segurado, de parte ou do total de valores, determinados para esse fim no seguro contratado. Responsabilidade Civil - Princípio geral de direito que impõe a quem causa dano a outros o dever de reparar. É a garantia que visa cobrir, até o limite máximo de indenização (LMI), o reembolso da indenização pela qual o segurado vier a ser responsabilizado civilmente, em sentença judicial transitada em julgada ou em acordo judicial autorizado pela seguradora, por danos involuntários, corporais, materiais ou morais que tiver causado a terceiros, conforme a cobertura contratada. Resseguro - Operação utilizada pelas companhias seguradoras para transferir a outras companhias - as resseguradoras - o excesso de responsabilidade que ultrapassa o limite de sua capacidade econômica de indenizar. Retrocessão - Operação utilizada pelas companhias resseguradoras para distribuir pelo mercado segurador interno a responsabilidade que ultrapassar os limites de sua capacidade de indenizar as seguradoras. Risco - Elemento fundamental do contrato de seguro, definido como o acontecimento possível, futuro, incerto e aleatório, que independente da vontade das partes contratantes, e de cuja ocorrência decorram prejuízos de ordem econômica. Roubo - Subtração de bem, objeto ou parte dele mediante grave ameaça ou violência à pessoa responsável por sua guarda. Salvados - Bens com valor econômico que escapam, sobram ou se recuperam após um sinistro, passando a pertencer ao segurador, mediante indenização ao segurado. Segundo Risco - É um seguro complementar, cujas coberturas funcionarão somente no caso de os prejuízos ultrapassarem a importância prevista para o primeiro seguro. Neste caso, o segurado faz um seguro com outra seguradora para complementar a cobertura de primeiro risco absoluto. Este tipo de seguro é recomendado nos casos em que o segurado quer se proteger contra a possibilidade de que o sinistro venha a superar a importância segurada na cobertura de primeiro risco absoluto. Segurado - Pessoa física ou jurídica em nome de quem é emitida a apólice de seguro, ou sobre quem deverá recair o risco, conforme as coberturas indicadas na apólice do seguro. Seguradora - É a pessoa jurídica de direito público ou privado, autorizada pela SUSEP (Superintendência de Seguros Privados) a operar mediante a cobrança de prêmio e que assume o risco e garante a indenização em caso de ocorrência de sinistro amparado pela apólice de seguro. Seguro - Contrato mediante o qual uma pessoa (a seguradora) se obriga, mediante a cobrança de um valor estipulado (prêmio), a indenizar outra pessoa (o segurado) do prejuízo resultante da ocorrência dos riscos futuros, previstos no contrato. Seguro de Acidentes Pessoais - Trata-se da modalidade de seguro que garante o pagamento de uma quantia determinada usada para reembolso dos gastos com médicos, hospitais e, no caso de morte ou incapacidade total ou parcial do segurado devido a um acidente. Estes seguros podem ser contratados de forma individual ou coletiva. Seguro Acidente de Trabalho (SAT) - Contribuição que varia entre 1% e 3% sobre a folha de pagamento da empresa. Esta variação depende do risco de vida que a empresa apresenta aos seus funcionários, em decorrência da atividade exercida. Seguro Desemprego - É um beneficio pago através da Caixa Econômica Federal, com o intuito de assegurar o trabalhador que tenha sido demitido sem justa causa, desde que tenha trabalhado no mínimo seis meses registrado em carteira de trabalho. O valor varia de acordo com a faixa salarial, sendo pago em até cinco parcelas, conforme a situação do beneficiário. Seguro de Fidelidade - Modalidade de seguro que garante o empregador por eventuais prejuízos que venha a sofrer decorrentes de furto, roubo ou apropriação indébita, ou outros delitos contra o patrimônio da empresa, que tenham sido cometidos por seus empregados, ou pessoas com vínculo empregatício. Seguro em Grupo - Termo usado para indicar os seguros de vida e acidentes pessoais feitos de forma coletiva, ou seja, envolvendo mais de um segurado. Os termos destes seguros são determinados pelo estipulante, no caso o empregador destas pessoas, sendo que a apólice favorece várias pessoas. Assim sendo nos seguros de grupo, os contratos se repartem em contratos distintos para quantas forem as pessoas seguradas. Seguro Fiança - Modalidade de seguro que protege o segurado caso este não consiga arcar com uma obrigação específica para com o devedor principal ou o afiançado. Usado com freqüência nos contratos de locação de imóveis. Seguro de Lucros Cessantes - Modalidade de seguro contratada por empresas, como indústrias, comércio e prestadoras de serviço, cujo objetivo é preservar os negócios do segurado de forma a garantir sua capacidade operacional e rentabilidade nos mesmos níveis em que se encontravam antes da ocorrência do sinistro. Seguro de Responsabilidade Civil - Modalidade de seguro que pretende garantir o desembolso de despesas pagas a terceiros por danos materiais ou pessoais, que foram involuntariamente causados. Bastante usado por executivos de grandes empresas, advogados, médicos, etc. Seguros de Riscos Diversos - Trata-se de um tipo de seguro que é constituído por várias modalidades com cobertura multirrisco, sendo que sua principal característica é a de cobrir perdas e danos materiais ou pessoais involuntariamente causados ocorridos durante a vigência do contrato de seguros. Seguro de Saúde - Trata-se dos seguros que garantem o pagamento das despesas com assistência médica e hospitalar que garante o pagamento de todos os procedimentos efetuados em nome do segurado diretamente a quem prestou o serviço. Em outros casos, o re-embolso é feito com base na quantia estipulada na apólice. Seguro Social - Termo usado para determinar os seguros que buscam proteger as pessoas que pertencem a classes de menor poder aquisitivo contra certos riscos específicos, como por exemplo, doença, velhice, invalidez e acidentes de trabalho. Sinistro - Termo utilizado para definir, em qualquer ramo ou carteira de seguro, o acontecimento do evento previsto e coberto na apólice de seguro, causando danos materiais ou pessoais ao segurado ou a seus beneficiários. Sub-rogação - É a transferência de direitos e ações do segurado à seguradora, após pagamento de indenização, para que esta possa agir legalmente contra terceiros que tenham causado os prejuízos por ela indenizados. SUSEP - A Superintendência de Seguros Privados é o órgão regulador da atividade, responsável pelo controle e fiscalização dos mercados de seguro, previdência privada aberta, capitalização e resseguro. Tabela de Referência - Publicação especializada com valor de mercado de veículos, atualizada mensalmente, formulada por instituto de pesquisas independente. Terceiro - Qualquer pessoa física ou jurídica que não seja o próprio segurado, seus sócios ou funcionários, bem como, seus cônjuges, pais e filhos e/ou pessoa que dependa economicamente do segurado, além dos diretores e administradores da empresa segurada, culpada ou prejudicada no sinistro. Títulos de Capitalização - Certificados emitidos pelas sociedades de capitalização em favor dos respectivos tomadores. Os portadores dos títulos pagam à sociedade, durante um certo tempo, uma mensalidade correspondente ao valor dos títulos, formando assim um capital que, acrescido dos juros acumulados, será recuperado pelos portadores em prazos previamente fixados. Os títulos de capitalização comportam também a eventualidade de um reembolso antecipado, por sorteio. Não há termos iniciando por esta letra. VAGP (Vida com Atualização Garantida) - Seguro de vida com opção de previdência. Recomendado para quem não se beneficia do incentivo fiscal oferecido pelos PGBLs e planos tradicionais, que permite a dedução dos valores aplicados nestes produtos do imposto a pagar desde que não supere 12% da renda bruta anual do investidor. Os VAGPs buscam uma aplicação que garante rentabilidade mínima e correção da inflação, mas os ganhos financeiros acima disso são divididos com o gestor do plano. Assim como nos VGBLs os impostos incidem sobre os rendimentos financeiros acumulados. O VAGPs deverão ter taxas de carregamento iguais ou pouco maiores do que as do PGBL. Valor Atual - Indenização do bem segurado, roubado ou destruído, pelo valor de um novo bem, deduzida a depreciação pelo uso, idade e estado de conservação. Valor de Mercado - Atualiza o valor da indenização no dia do pagamento de acordo com o preço de mercado. Valor de Mercado Referenciado - Corresponde ao valor em moeda corrente e variável de acordo com a tabela de referência constante na apólice de seguro, garantida ao segurado em caso de perda total. Valor de Novo - Indenização do bem segurado, roubado ou destruído, pelo valor de um novo bem no mercado. Valor determinado - Trata-se de uma cláusula na apólice em que a seguradora garante ao segurado, quando caracterizada a perda total, o pagamento da quantia fixa, expressa em moeda corrente nacional e estipulada pelas partes no ato da contratação. Valor indenizável - Valor a ser pago na ocorrência de sinistro. VGBL (Vida Gerador de Benefício Livre) - Seguro de vida com opção de previdência, desenvolvido com base nos PGBLs. A grande diferença é que ao contrário dos planos de previdência do tipo PGBL ou tradicionais, não é possível abater o valor das contribuições ao VGBL do imposto de renda a pagar durante a fase de acumulação. Em contrapartida, ao contrário dos PGBLs, o imposto no resgate é calculado apenas sobre os rendimentos e não inclui o valor das contribuições, sendo que o imposto é calculado com base na tabela progressiva de imposto de renda. Os VGBLs devem ter taxas de carregamento iguais ou pouco maiores do que as do PGBL. VGBL Empresarial - Plano de previdência complementar do tipo VGBL que é destinado ao mercado corporativo. O VGBL Empresarial pode ser averbado (quando a empresa não contribui para o plano) ou instituído (quando a empresa contribui parcial, ou integralmente ao plano). Vício Intrínseco - Termo usado na indústria de seguros, que determina a condição natural de certas coisas que as torna mais suscetíveis a se destruir ou avariar, sem que seja necessária a intervenção de qualquer causa externa. Vigência - Prazo de duração da apólice de seguro e, conseqüentemente, sua cobertura do risco. Vistoria Prévia - Avaliação, por pessoa autorizada pela seguradora, do estado do bem antes da formalização do contrato de seguro. Vistoria de Sinistro - Visita ao local onde se encontram os bens sinistrados, a fim de apurar, qualificar e quantificar os danos ou prejuízos sofridos pelo bem segurado, decorrentes de evento previsto e cobertos pelo contrato de seguro. VRGP (Vida com Remuneração Garantida) - Seguro de vida com opção de previdência. Recomendado para quem não se beneficia da possibilidade de deduzir as contribuições do plano do imposto de renda a pagar permitida pelos PGBLs e planos tradicionais. Assim destina-se aos contribuintes que estão isentos do IR, ou declaram através do formulário simples, ou já excederam o limite de dedução previsto nos PGBLs que é de 12% da renda bruta anual do contribuinte. Os VRGPs garantem a correção dos valores aplicados pela inflação, sendo que ganhos superiores a este rendimento garantido são divididos com o gestor do plano. Assim como nos VGBLs os impostos incidem sobre os rendimentos financeiros acumulados. O VAGPs deverão ter taxas de carregamento iguais ou pouco maiores do que as do PGBL. Não há termos iniciando por esta letra. Não há termos iniciando por esta letra. Não há termos iniciando por esta letra.",pt,40
502,1961,1470162319,CONTENT SHARED,-908052164352446106,-2820994773540913369,1807329753509412871,,,,HTML,http://hospitalar.com/pt/portal-de-noticias/blog/81-tecnologia-e-inovacao/486-iot-a-favor-do-relacionamento-medico-paciente,iot a favor do relacionamento médico-paciente,"IoT a favor do relacionamento médico-paciente Conceito de Internet das Coisas emergiu em meados dos anos 80 para ilustrar um futuro onde tudo se comunica. Hoje, novos recursos tecnológicos vêm aperfeiçoando a comunicação no setor e oferecendo experiências positivas a médicos e pacientes Por Flávia D'Angelo Quando surgiu, em meados dos anos 80, a comunicação entre máquinas era vista como coisa do futuro e era retratada como ficção científica em filmes como Blade Runner (1982) ou De volta para o futuro II (1989). Quem não se lembra da roupa automatizada de Marty McFly, dos carros voadores ou até mesmo dos garçons holográficos que atendiam no bar em que o personagem visitou em 2015? Embora intangível na época, nascia ali o conceito de Internet das Coisas (Internet of Things - IoT). Ali era um simples exercício de futurologia, mas o que não se previa era que a revolução estava prestes a explodir e, em poucas décadas, o impacto da internet seria tremendo. Hoje, a Internet das Coisas promove mais que uma mudança de paradigma. Em linhas gerais, ela representa a interação de objetos físicos e reais através de uma conexão de Internet e armazenamento em nuvens. Ou seja, com o uso de tecnologias de rastreamento, identificação e troca de informações esse novo padrão de comunicação é capaz de gerar uma avalanche de dados que fica armazenado e disponível ao mundo físico no chamado Big Data. Em um futuro próximo, uma imensa quantidade de informações sobre temperatura, trânsito, consumo de energia, localização, entre outros, será enviada periodicamente por todos os carros, medidores elétricos, torradeiras e celulares para a internet. A IMS Research estima que, em 2020, existirão cerca de 22 bilhões de sistemas embarcados e outros dispositivos portáteis conectados à Internet que produzirão mais de 2,5 quintilhões de bytes de dados novos a cada dia. Segundo a consultoria Gartner, entre 2014 e 2015, houve um aumento de 30% no uso de aparelhos inteligentes, alcançando 4,9 milhões de dispositivos conectados no período, e esse número deve chegar a 23,4 milhões, em 2017, e 25 bilhões, em 2020. Na área de saúde, a contribuição é ainda maior visto que o conceito de IoT engloba não só a conexão de dados em tempo real, mas também a análise e a reação com base no entendimento dos dados capturados. Por exemplo, a conexão de monitores acoplados pode permitir ao médico, por exemplo, acompanhar pacientes e até fazer prescrições de remédios com base nas informações coletadas. A tecnologia RFID, que utiliza código de barras, também permite o monitoramento de medicamentos, localização de arquivos ou movimentação de um paciente no hospital. Basicamente, é a IoT faciltando os processos, atuando a favor da saúde e aprimorando o relacionamento entre médico e paciente. Em estudo sobre os impactos da IoT no mercado de Saúde, a Deloitte aponta que a análise dos dados é fundamental para o sucesso do uso desse conceito. ""Em cinco anos, a maioria dos dados clinicamente relevantes será recolhido fora do ambiente clínico"", aponta estudo do instituto. Ainda, de acordo com o levantamento do instituto, nas próximas décadas, mudanças demográficas aumentarão as oportunidades para aplicar a IoT no apoio aos cuidados de bem-estar e saúde para segmentos específicos da população. No entanto, a Deloitte alerta: é preciso melhorar a infraestrutura e a segurança para a geração e análise desses dados. Devido à natureza distribuída, a IoT multiplica os riscos das organizações, forçando a revisão das políticas de segurança dos sistemas. De todas as indústrias, a de saúde será a mais beneficiada com a IoT. Ela permite a criação, por exemplo, de novos tratamentos, mais precisos e com uma maior riqueza de informações, já que se baseiam em informações obtidas em tempo real. Os diagnósticos passam a ser mais precisos, uma vez que o perfil do paciente é criado com registros de longa duração. Isso pode eliminar a necessidade de rotina check-ups e compromissos, mas permitirá, por outro lado, um relacionamento mais próximo com o médico. Por exemplo, um portador de Alzheimer pode se beneficiar da integração de sensores com smartphones, que rastreiam a intensidade do tremor das mãos e cruzam com o local em que o usuário se encontra. Com esses dados, o médico pode identificar os avanços da doença e o impacto que ela possui na rotina da pessoa. Tudo isso remotamente e em tempo real. Aplicativos móveis podem se conectar a dispositivos diversos para obter e relacionar dados sobre o nosso corpo em busca de insights médicos. Nossos carros, relógios e celulares passarão a serem vistos como uma fonte de registros digitais sobre a nossa vida. Camisetas com sensores para monitoramento da frequência cardíaca e temperatura, além de dispositivos em miniatura embutidos embaixo da pele do paciente para monitorar os níveis de açúcar, ou até sensores ingeríveis, que podem ser consumidos junto com o medicamento para verificar como ele age no organismo, também tendem a revolucionar e promover um relacionamento ainda mais próximo entre médico e paciente.",pt,40
503,1115,1464299862,CONTENT SHARED,-3912939678517879962,-1032019229384696495,-3408618679466961471,,,,HTML,http://techcrunch.com/2016/05/26/jury-finds-googles-implementation-of-java-in-android-was-fair-use/,jury finds google's implementation of java in android was fair use,"Software developers can breathe a massive sigh of relief - a jury found today that Google's implementation of 37 Java APIs in Android qualified as fair use. If the jury had found in favor of Oracle, which owns the Java programming language, it may have scared developers away from using other companies' programming languages in their work - a widespread practice in software development. Judge William Alsup, who also presided over Oracle's copyright case against Google, frequently praised the jury for its thoughtful approach to this case. In a somewhat unusual move, Alsup allowed jurors to take their notes home with them before they heard closing arguments in the case and during their deliberations, so they could study the facts. The ruling could indicate the winding down of a legal battle that has bounced through the courts since 2010 - but Oracle is likely to appeal. This is a developing story and will be updated. Featured Image: corgarashu / Shutterstock",en,40
504,2579,1476785581,CONTENT SHARED,7506614456429888492,1895326251577378793,-3542948564210885159,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36",SP,BR,HTML,http://www.zdnet.com/article/assessing-salesforces-platform-and-ecosystem/,assessing salesforce's platform and ecosystem | zdnet,"Coming out of of the crush and excitement of Dreamforce in San Francisco last week, which has now become the second largest tech event in North America (only after CES), Salesforce sits atop an impressive growth curve, an enviable top 10 position in the industry , and what many regard as one of the more compelling cloud offerings in the marketplace. The organization is often considered one of the leading lights of progress when it comes to enterprise technology as well as business leadership , and its extensive philanthropic pursuits are well known. Overall, the outlook continues to look good for the company: Of the aforementioned top 10 software firms, of the companies that are in the cloud, only Amazon Web Services is growing faster (70% vs. Salesforce's 24% year-over-year growth.) Yet the outlook isn't necessarily entirely rosy. Headwinds are on the horizon as technology continues to change and companies look for new ways to digitize, yet somehow remain in control of their IT systems. The company's strict 100% public cloud policy, which has long been touted by CEO Marc Benioff, has been a long-standing issue based on my conversations with CIOs over the years, even as it offers genuinely compelling economies of scale and a real (though partial) solution to the perennial upgrade hell that plagues most IT organizations today. That's because in theory -- and often in practice -- public cloud largely offloads the substantial maintenance headache of keeping applications up to date, allowing IT to focus more of its time and resources on strategic activities like innovation and digital transformation . Recent industry data also shows that organizations in general are growing more concerned about the control and influence of large cloud suppliers like Salesforce: The typical worries include risks such as the consequences of sole sourcing to one vendor, getting locked into a proprietary platform, and even using new enterprise solutions that are still very much maturing. These vendor issues are valid customer concerns that will likely continue as Salesforce continues expanding its product portfolio into adjacent territories and functional areas well beyond its roots in customer relationship management (CRM.) Salesforce Well Positioned for the IT End Game: Public Cloud Yet despite the concerns, the inexorable march towards public cloud has greatly accelerated in recent years, positioning Salesforce well for continued rapid growth over the next few years. The latest trend data available show that sometime during 2018, over half of all IT spending -- and just over half of the overall share of actual computing workloads -- will have permanently shifted to the public cloud, making it the leading operational model for running our business applications for the first time in IT history. Other leading cloud quasi-competitors, such as Amazon's now vast set of offerings for public cloud , have focused more on the bottom of the cloud computing stack with horizontal technology services like storage and compute power. Amazon has largely let other companies, including now significantly Salesforce itself , build out the top of the cloud stack, where business applications operate. In this regard, Salesforce offers ready-to-go off-the-shelf enterprise business apps today that are usually in the lead positions of the various analyst scoring charts. For example, Gartner's latest Magic Quadrant for CRM lists Salesforce as far and away the market leader in CRM, even though the company has only about a 20% global marketshare in the segment. However, the company has also achieved something else important that's no mean feat: The same industry leadership is also true of the company's newer offerings as well, consistently entering new functional sectors like marketing and customer service and solution spaces like horizontal portals or social software already in the leader segments of Gartner's vendor rankings. It's the same with many other analyst scorecards. This implies that organizations that adopt the platform broadly have ready and straightforward access to best-of-breed solutions across the board by virtue of simply selecting the Salesforce platform, even as it ventures into new territories. This is a compelling and desirable value proposition to IT and business buyers both. Because of Salesforce's reputation in this regard, as steadily offering a wide variety of leading business solutions with the very latest new capabilities baked in on a regular basis -- starting with cloud, and then over time adding capabilities for social, mobile, analytics, and now artificial intelligence -- companies often feel assured that the platform as a whole is taking them towards the future and will stay current. Related: Salesforce Einstein: Dream versus reality A Platform Beats an Application Every Time However, Salesforce hasn't stopped there. Realizing that they could never deliver all the innovation their customers needed on their own, a strategic insight that's often made by digital leaders , Salesforce was one of the very first enterprise cloud companies to open their platform to partners to develop on, well before companies like Apple and Google did something similar with their app stores. Known as AppExchange , the company's bustling online marketplace for 3rd party Salesforce solutions was launched way back in 2005, and currently offers over 3,000 business apps built on top of the company's platform, which have now been installed more than 4 million times. This has resulted in a rich ecosystem for Salesforce that goes well beyond the traditional value-added reseller (VAR) and service provider environment that accumulates around most enterprise software companies. With thousands of software companies (ISVs) building solutions for the Salesforce platform, adding features, functions, software suites, and even entirely new industry capabilities, the company has proactively enabled the external enrichment of its platform such that it can now deliver a wider variety of business functionality than almost any other enterprise cloud company today. Salesforce has even recently quantified the holistic economic value of its ecosystem of partners , claiming that every $1 spent on the company gains $4.14 more for them. This then is the so-called ecosystem game that the largest consumer companies in the world have long excelled at, and turned into scalable competitive advantage. But notably, Salesforce has translated this model successfully to the enterprise like few others, enriching everyone involved in the process including its customers, partners, and company itself. An Apparent Commitment to Future Proofing Salesforce has also not shied away from add new capabilities the market may not be fully ready for today but will be mature by the time it has caught up. Possibly the most challenging addition was the incorporation of social and community features in the platform back in 2011. At the time I wrote that it was a significant move that would position them and their customers well in the future, despite criticisms about social sprawl and stack fatigue : It's safe to say that Salesforce has a very clear vision that is connected as much as they can to ground truth about what businesses are doing and then distilling where they think the world is going to their products. They are making a big investment in time and resources in attempting to understand the strategies, lessons learned, and best practices that businesses can just adopt by using their platform. Access to this insight merely by virtue of using the platform is going to be compelling for many. As I recapped last year -- despite ultimately being early to the game -- Salesforce has steadily continued investment in social and has expanded well beyond its Chatter product into its Community Cloud platform. As digital engagement becomes an ever more critical differentiator for organizations , I believe the company's investment in social will likely bear some of the most fruit long term, by holding stakeholders digitally closer than any other known method. This was a brave commitment by Salesforce at the time when it's now clear that many of its customers still weren't ready for what has now become a key plank in digital experience . All of this is why I place the community layer so prominently in the Salesforce product chart above. The Salesforce Platform: Still Room to Grow However, despite its forward-looking product lines and robust enterprise ecosystem, the company is still far from a one-stop cloud shop. While you can acquire some of the missing capabilities by selecting 3rd party solutions from AppExchange, there are still significant gaps in Salesforce's own product offerings, despite over a decade of acquisitions and internal development of its platform. Notably missing are many of the components of ERP, which competitors like Oracle and SAP are much more focused on. Capabilities such as accounting, human capital management, supply chain management, and production management are all absent from the platform, which instead tends to focus on the market-facing side what organizations do: Marketing, sales, and customer service. What this means is that -- for better or for worse -- Salesforce is not yet a cloud provider that can deliver the majority of what's required to run the average enterprise. Admittedly, it's not even clear if this is a strategic goal of the company. But to sustain the platform as it gets larger, and for it to become the industry's ultimate cloud provider for business applications in general, it will likely be encouraged to expand into these areas and others. So, for organizations that want to move towards the public cloud today, additional partners beyond Salesforce are still required. This then almost certainly highlights potential future directions for the company. It now seems hard to imagine, given its growth into new major corporate functions in recent years, that the company will not eventually expand into the back office and digital workplace when the time is right. For now, however, acquiring one of the the market leaders in ERP, for example, seems unlikely. At least not until the company gets significantly larger and has the resources for such a significant investment. Thus Salesforce will probably bide its time until either a second tier smaller player breaks out, FinancialForce gets major traction in the ERP space (which Salesforce has an equity stake in), or the company is finally in the position to reshape the top of the industry through a merger or acquisition. Looking at last week's announcements at Dreamforce can also give us some indication of where Salesforce will decide to go next. Their acquisition of Quip was the most significant move yet for the company into the digital workplace and workforce productivity tools, showing potential interest in moving into the office software space. The addition of Einstein to put artificial intelligence capabilities into the core of the platform shows how the company is future proofing for capabilities that will have competitive significant in the future. Another interesting highlight is the company's continued investment in Internet of Things via its IoT Cloud offering, another addition to the platform that doesn't seem related to its core CRM mission, though the reality could not be further from the truth . At the end of the day, it's clear that Salesforce appears fully committed to creating the richest, market-leading cloud platform for business software in the industry, out-maneuvering traditional competitors like Oracle and Microsoft when it comes to offering the capabilities that business that will need in the near future. While its market-facing emphasis still means that the platform isn't going to run an entire business -- at least not for a few more years -- it's clear that the company has big plans for the future. In any case, much of what's missing can be sourced in its ecosystem, and as a result has already long become mainstay in many large enterprises. Ultimately, however, it's perhaps an emerging new mantra from the company that is the most compelling. As I noted in my live blogging of the details of Benioff's keynote at Dreamforce last week, simplicity is a new focus for the company in the face of the growing complexity of its market, its customers, and its products. Benioff pointedly said on stage last week in San Francisco, ""We want to take all the complex things in the world and make them easy."" That's a technology vision that most organizations could get behind today. Additional Reading The leading enterprise intranet, portal, and collaboration platforms for 2016 Salesforce poised to grab more spending as customers eye cloud integration",en,39
505,3043,1485949537,CONTENT SHARED,-879594538384021068,801895594717772308,8904143352876001053,"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36",MG,BR,HTML,https://www.linkedin.com/pulse/o-manual-do-debo%C3%ADsta-corporativo-fabio-betti,o manual do deboísta corporativo,"Fabio Betti Follow Following Unfollow Fabio Betti Sign in to follow this author Sócio-consultor da Corall No meio de uma conversa de Facebook, uma amiga referiu-se a mim como deboísta. Procurando disfarçar a ignorância, apenas curti o comentário e fui correndo me consultar com o Dr. Google. Para meu alívio, logo descobri não ser nenhuma doença grave. Pelo contrário, para enfrentar a onda de desesperança que nos abateu e continua nos abatendo com uma frequência e uma intensidade quase que insuportáveis, a salvação pode estar no deboísmo. O movimento não é novo. Consta que surgiu em agosto de 2014, quando uma tanatopraxista (praticamente um trava-lingua!) de nome Jéssica Loli, cansada de ver as colegas feministas se estapearem nas redes, criou, na brincadeira, uma corrente de feminismo que ela apelidou de deboísta. A ideia era fazer todo mundo parar de brigar entre si e conversar. Na página ""Feminismo Deboísta"", Jéssica começou a postar fotos do bicho-preguiça que logo virou o mascote do movimento, com frases do tipo: ""O nordeste é de boa, a xenofobia não é"" e ""pode beijar homem, pode beijar mulher. O importante é ser de boas"". Embora a página de Jessica tenha estacionado nas 40 mil curtidas, os memes com o bicho-preguiça e a cultura ""de boas"" logo espalharam-se pela internet, a ponto de servir de inspiração para uma campanha do Ministério do Trabalho intitulada ""filosofia de vida"". Porém, mesmo que bem intencionada, associar bicho-preguiça a trabalho não foi, digamos, a melhor ideia do mundo, e os internautas detonaram a iniciativa em mais memes criativos e debochados. Por outro lado, quando vemos a quantidade de gente doente nas organizações, que já não aguenta mais esse clima de treta permanente, com áreas ainda trabalhando em silos (tão anos 80!), colegas em clima de guerra, a ameaça permanente de degola e a inovação necessária à sobrevivência cada vez mais comprometida pelo medo, o movimento deboísta, ao pregar uma forma mais leve - o que não quer dizer menos comprometida - para lidar com nossos problemas, surge como uma possibilidade real de reconstrução de vínculos e recuperação da sanidade necessária para lidar com os desafios desse complexo e, às vezes, maluco mundo corporativo. O manifesto da página ""Deboísmo"" indica o caminho: ""Deboísmo é um estilo de vida. Ser de boa não significa ser inerte ao mundo, significa ser ponderado. Calma e paciência e o mundo se tornará um lugar melhor. Ser de boa não significa não debater, não significa se calar perante as injustiças. Significa não ofender as pessoas, argumentar de maneira lógica e sensata, sempre respeitando os direitos humanos. Ser de boa significa deixar uma discussão acabar mesmo sem convencer o inimigo, mesmo se ele partir para ofensas, pois em todo bom debate uma ideia é plantada e eventualmente o sujeito voltará a pensar sobre o assunto. Significa, em suma, ser de boas."" Inspirado no ""Feminismo Deboísta"" e com a missão de tornar as pessoas um pouco mais de boas, a página criada em maio de 2015 também alimenta um blog que só publica algo raríssimo atualmente na imprensa e nas redes sociais: noticias boas, claro. Do aplicativo que propõe árvores em vez de procrastinar nas redes sociais à história do Batman catarinense que visita crianças em hospitais ou as 5 maneiras de ficar de boas com o seu ex-namorado, o blog tem uma produção intensa, de olho em quem está querendo ficar ou que já está de boas. Inspirado nos Mandamentos Deboístas - sim, alguém criou 11 mandamentos, dá uma googleada -, resolvi apresentar aqui um protótipo* do que pode vir a ser o Manual do Deboísta Corporativo. 1. Abra sua cabeça 2. Foque nas ideias Uma opinião diferente da sua não precisa necessariamente ser vivida como contrária. Experimente encará-la como complementar e veja o que acontece. 3. Foque na solução 1 Não critique as pessoas, critique as ideias. E seja explícito ao fazer isso. Ninguém é obrigado a adivinhar suas reais intenções. 4. Foque na solução 2 Não perca tempo procurando argumentos para justificar por que as coisas não estão funcionando. Em sistemas vivos, descobrir por que algo não está funcionando não vai, necessariamente, fazer que volte a funcionar. Pior, só vai lhe criar uma boa desculpa para não resolver o problema. 5. Conecte-se com as forças Quanto mais tempo você se concentrar no erro, mais você se afastará da solução. Em uma cultura que ainda tem dificuldade em encarar erros como etapas naturais de um processo de aprendizagem, pensar, falar, enfim, viver o erro só lhe fará sentir-se culpado. 6. Ressignifique a crise Deu ruim, a tendência é ir direto para os gaps . E lá vamos nós de novo descendo a ladeira do ""ó vida, ó céus"". Deu ruim? Pergunte-se pelos talentos, pelas fortalezas, pelas competências já instaladas. Para superar um desafio, o que é melhor: sentir-se frágil e incompetente ou fortalecido e poderoso? A regra vale - e muito - para uma conversa sobre desempenho. Se você quer alguém da sua equipe performando melhor, ajude-o a contectar-se com o que ela ou ele tem de melhor. 7. Olhe além da crise A história da evolução das espécies é marcada por crises. Algumas espécies sucumbem, enquanto outras, as mais flexíveis e que mais rapidamente se adaptam às mudanças do ambiente em que vivem, evoluem. Em resumo: sem crise, sem energia para mudar. Olhe, portanto, para a crise como a oportunidade que o sistema do qual você faz parte precisa para evoluir. 8. Use sua inteligência: escute! Uma crise é apenas uma dentre tantas descrições possíveis da realidade. O nome técnico dado à fixação por uma única descrição é trauma. Pergunte sobre o que mais além da crise está acontecendo e evite a prisão traumática e paralisante da história única. Não confundir com Síndrome de Pollyanna, que também é uma espécie de trauma, uma vez que só consegue perceber o que está bem. Como naquela música do Barão Vermelho, ""que você descubra que rir é bom, mas que rir de tudo é desespero."" 9. Aprenda a relaxar Se você não é nem onisciente nem onipresente, escutar com atenção e curiosidade genuína o que o outro está vendo não é apenas um gesto de educação, é uma demonstração de inteligência. Quando você está muito tenso, submetido à pressão extrema, seu sistema nervoso interpreta que sua vida está sob ameaça. O sangue migra para as extremidades, para os músculos e seu cérebro fica oxigenado apenas para garantir que você continue respirando. Pensar? Nem pensar! Se a leitura é que sua vida corre perigo, as decisões ficam a cargo do sistema reptiliano, que basicamente suprime sua capacidade de raciocínio e lhe dá apenas 3 opções de ação: lutar, fugir ou se fingir de morto (congelar). A boa notícia é que você pode ajudar seu organismo a operar sem esforço novamente e você não precisa ser nenhum guru para aprender a relaxar. Fique apenas atento aos sinais: coração disparou? Frio na barriga? Respire 3 vezes va-ga-ro-sa-e-pro-fun-da-men-te. Ou tome um copo d'água. Ou coloque uma música tranquila para ouvir. Ou vá dar uma volta no quarteirão. Simplesmente pare esse ciclo vicioso que faz seu organismo trabalhar sob stress intenso e, a longo prazo, vai lhe adoecer, pode crer. Deboístas X Tretaístas: quem vencerá essa batalha? * Se quiser me ajudar no desafio de construir o Manual do Deboísta Corporativo, envie sua contribuição para fabio@corall.net É óbvio que nem tudo são flores no mundo deboísta. Criticado como movimento elitista, os desafetos responderam com o Tretaísmo: ""Não somos contra o deboismo, apenas vamos dar colo pra quem não é privilegiado ou tem condições de ser adepto a ele."" Gente humilde não pode ficar de boas? Sei não... Bem, guerra ou não, o fato é que na batalha por adeptos, os deboístas estão ganhando de lavada: a página acumula mais de 1 milhão e 100 mil curtidas contra reles 30 mil dos colegas da treta. Sócio-consultor da Corall",pt,39
506,2575,1476729218,CONTENT SHARED,-4024778022934140027,3829784524040647339,1283403172134232428,"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36",SP,BR,HTML,https://www.oreilly.com/ideas/using-behavioral-data-to-improve-customer-satisfaction,using behavioral data to improve customer satisfaction,"Why cross-channel analytics are crucial to empowering business teams with a behavioral view of your customer. Bird's eye view. (source: Pixabay ). Today's customer can choose to interact with you in many ways-over the phone, in your stores, on the web, through social media, or likely all of the above. The data from these interactions provides a new level of insight into customer satisfaction, more so than traditional loyalty metrics. Behavioral data is exciting because unlike loyalty metrics, it shows you what the customer does, instead of telling you what they think. It can also shed light on the context of their actions, helping you see areas where you can improve immediately. By using information from every touchpoint, you create a picture of customer behavior that moves like they do-in real-time and in all dimensions. Not only is this level of detail necessary for you to have a clear picture of customer happiness, you need it to satisfy the current demands of business. Ryan Garrett of Teradata works with data teams to improve customer satisfaction. He says that though the way customers interact with companies has changed: ""The goal of the interaction hasn't. Customers still want a simple experience. Except now, that means companies both understanding who you are as well as your intentions. It's the modern version of calling a phone company and the support representative knowing about the recent issues you've experienced."" According to Garrett, overlooking one of the channels in which your customers interact with you is a mistake since without the full picture, it's difficult to understand how events affect customer satisfaction with products and services. Garrett gives the example of in-store visits for telecom companies, which are typically viewed as positive events. A very different picture should emerge here, however, when the customer shows up after two calls to a center with the problem still unresolved. Cross-channel analytics reveal the more likely conclusion of this scenario where analysis of only one channel would have fallen short. ""Once you tie those cross-channel pieces together,"" Garrett says, ""you can more accurately infer the customer's degree of satisfaction or dissatisfaction and respond accordingly."" In some cases, analytics teams have the relevant behavioral data, but are analyzing it in a way that is inactionable-you may learn of an increasing number of customer problems without any detailed information on what types of problems they are, for example. Rather than leading to more efficient business, this abets a notorious enemy of work-information overload. In another pitfall, analysts sometimes build complicated models that confuse the business teams that need to use them. As a result, nothing changes. To solve this problem, Garrett says, ""You need to provide non-engineering users with tools to understand the data behind the events so they can take action to simplify customers' lives."" This allows both teams to play to their strengths and navigates the gap between the two stakeholders. Understanding cross-channel customer behavior will take time, resources, and experience (read: mistakes). You might need to update your organization's strategy and analytics structure, or make deep changes to how you're organizing analytics processes or gathering and preparing data. Developing the most accurate picture of your customer, however, is invaluable. With it, business teams can make better decisions, resulting in smarter customer interactions wherever and whenever they happen. This post is a collaboration between O'Reilly and Teradata. See our statement of editorial independence . Article image: Bird's eye view. (source: Pixabay ). Distributed analytics using Apache Spark, integrated with Python libraries scikit-learn and seaborn to visualize clusters of artwork in the MOMA collection. Smart cities and smart nations run on data. Making search smarter through better human-computer interaction. Python Data Science Handbook: Early Release",en,39
507,1982,1470337037,CONTENT SHARED,6643427232964983922,-7606731662737258050,7886043073449621571,,,,HTML,https://medium.com/@isisAnchalee/ilooklikeanengineer-one-year-later-b599e0cae817,#ilooklikeanengineer: one year later,"#iLookLikeAnEngineer: One Year Later Today I have all the feels. A year and some change ago I was asked to participate in a BART station recruiting campaign as a software engineer for OneLogin... Today in particular, marks the exact date one year ago when I shared the tweet that gave birth to #iLookLikeAnEngineer. Never in a million years would I have guessed that one year later people would still be proudly sharing the hashtag and using it as a source for building community. Because it developed a life of its own, I'm not even aware of the entirety of the extent that it has reached(while Googling I even saw an #iLookLikeAnEngineer event in Munich Germany etc.). So, I'd like to share some of the highlights that have been very special to me: Including Scott Kelly in space: When Goldieblox created this Fast-Forward Girls 2015 video: When this happened, because Emma Watson & The UN: When multiple people have reached out to me and expressing that the #iLookLikeAnEngineer movement gave them the confidence that they needed to change careers and pursue learning how to code. Then when TeeSpring came out with(and was profiting off of)this line of horribly tone-deaf shirts without anyone's permission: And finally(my favorite part) when I got to meet some of the most amazing people I've ever met in my life, like this amazing human right here and the rest of our Mind Makers team: When created these amazing 'I'm an engineer, bitch.' shirts: When Twitter threw an #iLookLikeAnEngineer event to kick off the Twitter Flight Developer Conference: That time they asked me to speak on the main stage at GHC 2016 in front of 6k people: When I was interviewed by Google's Made With Code: When I got to speak at NASA and saw posters of my face there :O :O **giggles like school girl** When #iLookLikeAnEngineer made it into Google's Year in Search compilation: This list doesn't even come close to covering all of the wonderful facets of #iLookLikeAnEngineer, & I'd love to hear your story. I am endlessly grateful to have been able to be a voice in this conversation for those who feel that they need to sacrifice pieces of their identity to be treated with respect. Some of the conversations that I have had with you after speaking at events have forever touched my heart. The stories that I've heard, and the tears that I have seen shed. You are not alone. The system is pretty shitty sometimes, but it takes effort from all of us to make it better. I will continue speaking up. Embodying the principle that anyone(yes anyone;)) can be femme as **** and still kick ass at what they do. Because #iLookLikeAnEngineer, and so do you :) P.S I'm now a software engineer under the Driver Engagement umbrella at Uber and my team is hiring. If you're interested in solving some really interesting challenges with some really passionate/smart/kind humans, send me a resume/cover letter and what position you're interested in at [email protected] ! Xo, Isis Anchalee",en,39
508,1263,1465126521,CONTENT SHARED,-4765711818183276269,-1443636648652872475,-261539783637799977,,,,HTML,http://www.businessinsider.com/google-lags-behind-amazon-and-microsofts-cloud-in-one-important-area-2016-6,google lags behind amazon and microsoft's cloud in one important area,"Thomson Reuters Google's cloud service may want to spent a lot more on hiring enterprise sales people if it wants to catch up to the competition. According to a research note by Evercore's Ken Sena on Thursday, the number of sales job openings at Google Cloud Platform far lags behind its biggest competitors, including Amazon Web Services, Microsoft Azure, and Salesforce. Currently, GCP has only 80 sales job openings, a small fraction compared to AWS, Azure, and Salesforce, all of which have over 300 open spots in sales. Based on Sena's own estimates, GCP's sales cost only accounts for 8% of its total operating expenses, a tiny portion compared to some of the larger enterprise players. And although GCP ramped up its sales hires in recent months, nearly doubling to 50 sales reps in the West Coast alone, Sena notes that Google's investment seems more focused on the technical side than sales - potentially weakening its growth prospects in the enterprise space, which tend to rely heavily on sales people to sign large business customers. ""Google's Cloud investment seems more focused on tools and infrastructure versus the sales headcount needed to help Google garner greater Cloud market share...as a result, we tend to view AWS's business as being under less competitive threat than we did previously, which our new estimates better reflect,"" Sena wrote. Unlike consumer products that could sometimes become major hits without much sales and marketing efforts, enterprise technology is still an area that's largely dictated by a strong sales culture. It's possible to penetrate small businesses and teams within large businesses without any sales people, but to sell to large businesses who drive the big bucks, a large sales team is essential. AWS's recent $400 million deal with Salesforce , for example, wouldn't have happened without a large number of enterprise salespeople involved. Sena's note argues GCP's level of investment - not just in sales, but across data centers and infrastructure - also falls behind competition. GCP's total expenses was $1.9 billion in 2015 and is expected to grow to $3.4 billion in 2016, a fraction of AWS's $6.9 billion in 2015 and $9.1 billion projected in 2016. Evercore Still, it's too early to count GCP out of the picture. Google CEO Sundar Pichai has stated that the company will invest ""significantly"" in GCP this year, and has brought in VMWare cofounder Diane Greene, who's one of the most powerful people in enterprise tech, to lead GCP last year. Plus, as Sena noted, a lot of GCP's investments are flowing into improving its technical capabilities, giving it a niche advantage over other cloud providers. Its pricing is also more competitive than others, he notes. In any case, all this leads to the conclusion that GCP won't be a major threat to the market leading AWS in the near future, Sena writes. ""In sum, while big-name customer additions (notably Snapchat, Spotify, and some of Apple's business) and the hire of Diane Green have created some obvious reasons for concern from an AWS standpoint, our conclusion is that AWS is likely better situated than some may perceive,"" he writes. Disclosure: Jeff Bezos is an investor in Business Insider through his personal investment company Bezos Expeditions.",en,39
509,2050,1471007526,CONTENT SHARED,-4133169317019898816,6013226412048763966,-360142285864220391,,,,VIDEO,https://www.ted.com/talks/robert_waldinger_what_makes_a_good_life_lessons_from_the_longest_study_on_happiness?language=pt-br,robert waldinger: do que é feita uma vida boa? lições do mais longo estudo sobre felicidade,"You have JavaScript disabled O que nos mantêm felizes e saudáveis ao longo da vida? Se você pensa que é fama e dinheiro, você não está sozinho - mas de acordo com o psiquiatra Robert Waldinger, você está equivocado. Como diretor de um estudo de 75 anos sobre desenvolvimento de adultos, Waldinger tem acesso sem precedentes a dados sobre a verdadeira felicidade e satisfação. Nesta palestra, ele compartilha três importantes lições aprendidas com o estudo, assim como alguns conhecimentos práticos e antiquíssimos sobre como construir uma vida longa e plena. This talk was presented to a local audience at TEDxBeaconStreet, an independent event. TED editors featured it among our selections on the home page. Enthusiastically agree? Respectfully beg to differ? Have your say here.",pt,39
510,1389,1465951636,CONTENT SHARED,-1556169727291354289,-1443636648652872475,6340331642204025925,,,,HTML,http://www.businessinsider.com/netflix-recommendation-engine-worth-1-billion-per-year-2016-6,why netflix thinks its personalized recommendation engine is worth $1 billion per year,"Netflix After a long refinement process, Netflix finally released its first ""global"" recommendation system in December. The system takes dozens of algorithms into account, and compares you with similar users in the more than 190 countries where Netflix's service is available. ""If one member in this tiny island expresses an interest for anime, then we're able to map that person to the global anime community,"" Carlos Gomez-Uribe , VP of product innovation at Netflix, told Tech Insider in February . And Netflix thinks it's worth a lot of money: one billion dollars per year, in fact. In an academic paper penned by Gomez-Uribe and Netflix's chief of product Neil Hunt, they assert that "" the combined effect of personalization and recommendations save us more than $1B per year."" That's a significant chunk of change considering that Netflix's total spending on global content this year will be $5 billion. 90 seconds or bust Why does Netflix think it's worth so much? The short answer is because it helps them keep subscribers from canceling. ""Consumer research suggests that a typical Netflix member loses interest after perhaps 60 to 90 seconds of choosing, having reviewed 10 to 20 titles (perhaps 3 in detail) on one or two screens,"" they write. ""The user either finds something of interest or the risk of the user abandoning our service increases substantially."" If Netflix only has 90 seconds to grab a subscriber's attention, it needs to find a good show or movie fast. If people were just typing in what they wanted to see into the search bar, this would be relatively easy. But Netflix estimates that only 20% of its subscriber video choices come from search, with the other 80% being from recommendations. So it's essential that Netflix gets this right. The end goal of the system are ""moments of truth,"" when ""a member starts a session and we help that member find something engaging within a few seconds, preventing abandonment of our service for an alternative entertainment option."" In other words, Netflix believes it could lose $1 billion or more every year from subscribers quitting service if it weren't for its personalized recommendation engine. And that's why Netflix thinks that its recommendation engine, however much it could be improved in the future, is already worth so much money to the company.",en,39
511,1064,1463934373,CONTENT SHARED,6852597772196653540,-1032019229384696495,7846322948118114767,,,,HTML,https://cloudplatform.googleblog.com/2016/05/Firebase-and-Google-Cloud-better-together.html,firebase and google cloud: better together,"In the short time since Firebase joined Google , the passionate community of developers using the backend-as-a-service to handle the heavy lifting of building an app has grown from 110,000 to over 470,000 developers around the world. In that same span, Firebase has come to rely on Google Cloud Platform , leaning on GCP for core infrastructure as well as value-added services. For example, GCP figures prominently in several of the new Firebase features that we're announcing at Google I/O 2016 today. One of the most requested features by Firebase developers is the ability to store images, videos and other large files. The new Firebase Storage is powered by Google Cloud Storage , giving it massive scalability and allowing stored files to be easily accessed by other projects running on Google Cloud Platform. Firebase now uses the same underlying account system as GCP, which means you can use any GCP product with your Firebase app. For example, you can export raw analytics data from the new Firebase Analytics to Google BigQuery to help you surface advanced insights about your application and users. Going forward, we'll continue to build out integrations between Firebase and Google Cloud Platform, giving you the functionality of a full public cloud as you add to your mobile application portfolio. To learn more about Firebase, visit our new site . For all the new features we're announcing at Google I/O today, click on over to the Firebase blog . We're working quickly to close gaps, and we'd love to hear your feedback so we can improve. You can help by requesting a feature .",en,39
512,930,1463087238,CONTENT SHARED,-4050006690189978332,-1443636648652872475,-5528735230097235953,,,,HTML,http://googleresearch.blogspot.com.br/2016/05/announcing-syntaxnet-worlds-most.html,announcing syntaxnet: the world's most accurate parser goes open source,"At Google, we spend a lot of time thinking about how computer systems can read and understand human language in order to process it in intelligent ways . Today, we are excited to share the fruits of our research with the broader community by releasing SyntaxNet , an open-source neural network framework implemented in TensorFlow that provides a foundation for Natural Language Understanding (NLU) systems. Our release includes all the code needed to train new SyntaxNet models on your own data, as well as Parsey McParseface , an English parser that we have trained for you and that you can use to analyze English text. Parsey McParseface is built on powerful machine learning algorithms that learn to analyze the linguistic structure of language, and that can explain the functional role of each word in a given sentence. Because Parsey McParseface is the most accurate such model in the world , we hope that it will be useful to developers and researchers interested in automatic extraction of information, translation, and other core applications of NLU. How does SyntaxNet work? SyntaxNet is a framework for what's known in academic circles as a syntactic parser , which is a key first component in many NLU systems. Given a sentence as input, it tags each word with a part-of-speech (POS) tag that describes the word's syntactic function, and it determines the syntactic relationships between words in the sentence, represented in the dependency parse tree. These syntactic relationships are directly related to the underlying meaning of the sentence in question. To take a very simple example, consider the following dependency tree for Alice saw Bob : This structure encodes that Alice and Bob are nouns and saw is a verb. The main verb saw is the root of the sentence and Alice is the subject (nsubj) of saw , while Bob is its direct object (dobj). As expected, Parsey McParseface analyzes this sentence correctly, but also understands the following more complex example: This structure again encodes the fact that Alice and Bob are the subject and object respectively of saw , in addition that Alice is modified by a relative clause with the verb reading , that saw is modified by the temporal modifier yesterday , and so on. The grammatical relationships encoded in dependency structures allow us to easily recover the answers to various questions, for example whom did Alice see? , who saw Bob? , what had Alice been reading about? or when did Alice see Bob? . Why is Parsing So Hard For Computers to Get Right? One of the main problems that makes parsing so challenging is that human languages show remarkable levels of ambiguity. It is not uncommon for moderate length sentences - say 20 or 30 words in length - to have hundreds, thousands, or even tens of thousands of possible syntactic structures. A natural language parser must somehow search through all of these alternatives, and find the most plausible structure given the context. As a very simple example, the sentence Alice drove down the street in her car has at least two possible dependency parses: The first corresponds to the (correct) interpretation where Alice is driving in her car; the second corresponds to the (absurd, but possible) interpretation where the street is located in her car. The ambiguity arises because the preposition in can either modify drove or street ; this example is an instance of what is called prepositional phrase attachment ambiguity . Humans do a remarkable job of dealing with ambiguity, almost to the point where the problem is unnoticeable; the challenge is for computers to do the same. Multiple ambiguities such as these in longer sentences conspire to give a combinatorial explosion in the number of possible structures for a sentence. Usually the vast majority of these structures are wildly implausible, but are nevertheless possible and must be somehow discarded by a parser. SyntaxNet applies neural networks to the ambiguity problem. An input sentence is processed from left to right, with dependencies between words being incrementally added as each word in the sentence is considered. At each point in processing many decisions may be possible-due to ambiguity-and a neural network gives scores for competing decisions based on their plausibility. For this reason, it is very important to use beam search in the model. Instead of simply taking the first-best decision at each point, multiple partial hypotheses are kept at each step, with hypotheses only being discarded when there are several other higher-ranked hypotheses under consideration. An example of a left-to-right sequence of decisions that produces a simple parse is shown below for the sentence I booked a ticket to Google . Furthermore, as described in our paper , it is critical to tightly integrate learning and search in order to achieve the highest prediction accuracy. Parsey McParseface and other SyntaxNet models are some of the most complex networks that we have trained with the TensorFlow framework at Google. Given some data from the Google supported Universal Treebanks project, you can train a parsing model on your own machine. So How Accurate is Parsey McParseface? On a standard benchmark consisting of randomly drawn English newswire sentences (the 20 year old Penn Treebank ), Parsey McParseface recovers individual dependencies between words with over 94% accuracy, beating our own previous state-of-the-art results, which were already better than any previous approach . While there are no explicit studies in the literature about human performance, we know from our in-house annotation projects that linguists trained for this task agree in 96-97% of the cases. This suggests that we are approaching human performance-but only on well-formed text. Sentences drawn from the web are a lot harder to analyze, as we learned from the Google WebTreebank (released in 2011). Parsey McParseface achieves just over 90% of parse accuracy on this dataset. While the accuracy is not perfect, it's certainly high enough to be useful in many applications. The major source of errors at this point are examples such as the prepositional phrase attachment ambiguity described above, which require real world knowledge (e.g. that a street is not likely to be located in a car) and deep contextual reasoning. Machine learning (and in particular, neural networks) have made significant progress in resolving these ambiguities. But our work is still cut out for us: we would like to develop methods that can learn world knowledge and enable equal understanding of natural language across all languages and contexts. To get started, see the SyntaxNet code and download the Parsey McParseface parser model. Happy parsing from the main developers, Chris Alberti, David Weiss, Daniel Andor, Michael Collins & Slav Petrov.",en,39
513,645,1461853408,CONTENT SHARED,-1444481301861872291,6735372008307093370,7211147496209308956,,,,HTML,http://www.brasilpost.com.br/2016/04/26/drag-queen-pabllo-vittar-_n_9780744.html,nova campanha da avon tem drag queen como estrela,"Publicado: Entrando na onda da representatividade, a marca de cosméticos Avon divulgou sua nova garota propaganda: a drag queen Pabllo Vittar . Logo após a postagem, na última segunda-feira, 25, a página da marca recebeu centenas de comentários positivos sobre a iniciativa. O ""mundo precisa de mais empresas como a Avon"", escreveu um usuário. A Avon respondeu alguns comentários, enfatizando que ""representatividade importa"": ""O mundo tá evoluindo e é importante a gente crescer com ele. Claro que assim como todo mundo, temos muito a desconstruir, mas esse já é o primeiro passo em direção a um mundo mais justo!"" (Com informações da Estadão Conteúdo) Pabllo Vittar ficou conhecida em rede nacional por participar da banda do programa 'Amor & Sexo', da Rede Globo. Antes disso, ela já fazia muito sucesso por seus videoclipes no YouTube, em especial o hit 'Open Bar'. Anteriormente, a marca já havia incluído a cantora transexual da Banda Uó, Mel Gonçalves, numa campanha publicitária. No exterior, empresas como MAC e Make Up Forever também têm apostado em modelos transgênero. LEIA MAIS: - Menino de 8 anos realiza sonho de aprender a se maquiar como uma drag queen - Drag Queen Deena Love rouba a cena na estreia da segunda edição do 'The Voice Brasil' (VÍDEO)",pt,39
514,475,1461021707,CONTENT SHARED,6850500272809381909,-1032019229384696495,1310371857892872479,,,,HTML,https://www.techinasia.com/wechat-slack-enterprise-chat-app-launch,wechat's competitor to slack has arrived,"Today, WeChat launched version 1.0 of its hotly-anticipated office chat app, WeChat Enterprise . WeSlack (not its real name, but it does have a nice ring to it) is free, is only available in Chinese via apps for iOS, Android, Windows, and OS X. But using the program isn't quite as simple as just downloading and logging in - as we found out below. Business casual Like we said at the app's announcement , WeChat Enterprise includes a number of features that are familiar territory to those who have used business chat apps. But it also has some welcome innovations. The service can essentially replace group email, and make quick chats between colleagues easier. It has useful add-ons like the ability for an employee to mark when they are on a break, hold group voice/text chats, and allows for companies to build in specific features like automated forms for reimbursements, vacations, and the like. Employees can even make calls via WeChat Enterprise that are charged to their company's bill - an offer that we haven't seen in similar office chat programs. WeSlack (sorry, but that is a catchy name) also allows for users to embrace their less formal side, with support for the personal chat client's emoticons and animated stickers. According to a post on QQ Tech - a website owned by WeChat's parent company, Tencent - the service has been in a closed beta since early March. To register on WeSlack, you need an official Chinese business license. While today technically marks the service's launch, it's not quite as easy to use WeChat Enterprise as it is to start up a random Slack group. Companies that already have official business WeChat accounts (as many Chinese companies do) are already registered, and just need to go through a few steps on the WeChat Enterprise website . But if you're a small startup - or not in China - you may be out of luck for now. To register on WeChat Enterprise, you need an official Chinese business license. There is no word on if/when the service will be available outside of the country. If you work for a Chinese company and are determined to take WeChat Enterprise for a spin, now is the time to message your boss and tell them to register with Tencent. If they agree, then that may be one of the last work emails you'll need to send for a while.",en,39
515,1379,1465918747,CONTENT SHARED,9136323715291453594,-7711052404720939396,4374244126221895679,,,,HTML,http://www.fluentu.com/japanese/blog/how-to-improve-your-japanese/,how to improve 8 major problem areas for japanese learners of all levels,"Have you hit a big ol' wall while learning Japanese? Do you feel like you've collided head-on with a giant pile of bricks? Or maybe, instead of bricks, that obtrusive wall is built out of and complex Japanese grammar rules. Don't worry. You're not alone. Most learners eventually reach a point where learning Japanese becomes much more difficult and your progress seems to slow to a crawl. Boredom or frustration sets in. It can happen at different times for different people, but the reaction is often the same: You start making excuses not to learn. You're suddenly too busy. You have better (easier) things to do with your time. It's at this point most people will take a break or quit altogether. Well, if you want to take a giant sledgehammer to this wall or just avoid it in the future, here are some tips to get you back in the game! 1. ""I don't know where to start."" First, calm yourself down. Take a breath. Perhaps meditate for a little while. The key here is to avoid becoming overwhelmed. Now go a little deeper into your existing Japanese skills and knowledge. You know what you know, and you know what you don't know. Take a moment to seriously analyze your weaknesses. Be honest with yourself. Once you identify some key areas in which you can improve, you can target those areas to continue improving. Let's have a look at how we can identify and correct our problems. This post will address a few different key issues you may be having in learning Japanese. Vocabulary Grammar Pronunciation Sounding like a native speaker Speaking without fear Motivation (or lack thereof) Once you've identified your major problem areas, proceed to read the rest of this post and choose some methods of improvement from our given recommendations. 2. ""I'm missing a lot of vocabulary."" Maybe vocabulary is a weakness for you. Do you find yourself able to name simple objects but unable to thoroughly describe them? For example, maybe you know the word ""table"" but you don't know words for ""legs,"" ""surface,"" ""wood,"" ""heavy"" or other things related to tables. Maybe you know a basic word for an object but not different types of those objects. For example, let's take a look at the word ""food,"" 食べ物 (tabemono) . Japanese food is 和食 (washoku) Italian food is イタリア料理 (Itaria ryouri) Appetizer is 前菜 (zensai) A traditional Japanese multi-course meal is called 懐石料理 (kaiseki ryouri) If you know the word ""food"" but don't know any types of Japanese food or any other words about food, you may want to focus on building some food vocabulary. Here's a trick that works well. Learn vocabulary by situation or theme Try to learn words following a theme or situation you might encounter. For example, one theme might be ""at the restaurant"" so you would study a lot of vocabulary related to restaurants. That includes food words, ordering and paying of course. You can even study about different types of restaurants. By learning these words together, you'll be able to remember them more easily since they all have a strong thematic link. This will also assist you later when you want to practice conversations involving that situation or theme. 3. ""I'm kinda bored."" Boredom can really get the best of you when trying to study a language-but luckily we live in a world with unlimited resources for fun. You just need to shift your perspective a little. Play a game! There's a cool Japanese language game you can play with yourself in your everyday life to increase your vocabulary. Practice describing something random you come across during your regular routine. If you walk past a park, try to describe the park in excruciating detail. What's in the park? What plants? What animals do you see? Do you see people? Describe them. Is there a bench? What color is it? What material? What does it feel like? Translate these descriptions into Japanese, or try to think them up directly in Japanese. By describing everything in extreme detail, you can increase your vocabulary quickly. By providing context, you can also have an easier time remembering. You'll also start to see the gaps in your vocabulary, which allows you to take the steps to fill them in. If you don't know a word, take note of it on paper or on your smartphone using your favorite note-taking app. Go back later and translate the words and study them. Denshi Jisho is a great free online dictionary to look up words.Add them to your favorite flashcard app like to continue your study. 4. ""I can't get through grammar lessons."" You might be stuck on some grammar lessons at your current Japanese level, or you might simply be dreading the next one you'll need to face to advance. Or perhaps grammar lessons bore you to tears and you can't focus enough to soldier through. For all these issues, the solution is the same. Read grammar sites and blogs. This is a great, free way to improve your grammar. Grammar sites and blogs often provide an engaging narrative to guide you along and make learning feel effortless-even entertaining! Simply read and read and read about it online. Choose a grammar point. Search for it. Study the explanations and examples. Try to form your own examples. Keep trying to use the grammar point in content until you can use it without problem. Continue to practice. If you don't, you'll forget! Need a place to get started? Here are some great free resources you can use to practice your grammar. 5. ""My pronunciation doesn't sound native."" Perhaps you really still sound too darn foreign when the time comes to speak Japanese-not that there's anything wrong with sounding foreign, accents are awfully attractive after all. Even so, you've been dying to learn fluent, natural-sounding Japanese. Maybe you can't quite nail down the accent. Maybe you're still having issues with some pronunciation. Either way, that's a problem that needs to be fixed. If you can't match native intonation, it may be difficult for them to understand you when you speak. Mimic your favorite TV character. If you like watching TV shows or , this is the tip for you. Here's how you can use your favorite TV show or anime series to help you learn Japanese. You might not want to use action-packed shows for this. Pick something with a bit of dialogue to it. Pick a character. This should be a character that you can identify with in a show you really like. The character might be your own gender, have a similar profession or express a similar outlook on life with their spoken words. Try to pick someone who speaks the way you'd ideally like to speak in Japanese. Play it back . I know, it's hard to listen to yourself isn't it? Replay the original clip . Compare and contrast. Find points that you need to improve on and keep trying. In addition to this, make sure you're saying borrowed English words with the Japanese accent. Don't say ""cake."" Say ケーキ (ke-ki). Japanese people will have a lot of trouble understanding you unless you say it with a katakana pronunciation. Even if the word is borrowed from English, it often sounds different enough that they won't always catch it. 6. ""I don't understand half of what native speakers say."" This can be one of the biggest challenges to overcome and certainly one of the most frustrating. This is the ""walking textbook"" problem. You always sound robotic , and you have trouble deciphering casual Japanese speech , because you're operating how your textbook or class taught you to speak and listen. You don't necessarily learn how modern people speak in everyday conversations-this is too often pushed aside in favor or more formal instruction. This is especially true if you're going somewhere outside of Tokyo. Those Japanese dialects open up a whole new can of worms! I live in Osaka, where Osaka dialect is spoken heavily even in some business situations. It's a bit more casual than the standard dialect spoken in Tokyo, and some of the words are different. It definitely has taken me a while to get adjusted and pick up new vocabulary, grammar twists and slang. So, how do you get started if you want to better understand native Japanese speakers? Mimic a character once more! This is a seriously great way to help you out when learning anything to do with speaking Japanese. This time we're going to be looking for some different things, though. Pay attention to the situation. Is it formal? Casual? How does the language differ in each situation? What words are they using? Look at how vocab words are used. For example, even though they have words for ""say"" and ""tell,"" Japanese people use the verb for ""to teach,"" 教える (oshieru), when saying ""someone told me something."" Learn some idioms. Make note of any strange expressions that don't seem to make any sense in context. It's probably an idiomatic expression. One of my favorite in Japanese is 頭に来る (atama ni kuru) or ""coming to my head."" This means to get very angry. It makes no sense reading it literally, right? Try FluentU! FluentU takes real-world Japanese videos-like music videos, movie trailers, news and inspiring talks-and turns them into personalized language learning lessons. Use the website on your computer or tablet or, better yet, download the FluentU app to your iPhone. The Android app is in the works! FluentU gives you the access you need to authentic Japanese content, along with the ability to utilize it to enhance your studying. You'll quickly see that we offer a broad range of contemporary videos-just take a look at one small sample: You'll get reading practice too, as every video is subtitled. FluentU makes these native Japanese videos approachable through interactive captions. These interactive captions will show you the definition of a word (and simultaneously pause the video) whenever you hover your mouse over it. Interactive subtitles allow you to immediately understand what's being said. All definitions have multiple in-context usage examples, and they're written for Japanese learners like you. You'll also find audio pronunciations, synonyms, helpful images and more. Tap again to add words you'd like to review later to your running vocab list. And that's not all. FluentU's learn mode lets you learn Japanese even better by turning your selected videos into personalized language lessons. You'll go through exercises that show the video clips as the prompts, multimedia flashcards, quizzes and fun activities like ""fill in the blank."" The best part? Every time you use FluentU, the site keeps track of the grammar and vocab you've learned and the words you struggle with, personalizing video suggestions and learning sessions based on your unique set of knowledge. It'll then recommend the natural next step in the progression of your learning. You're delivered a 100% personalized experience. 7. ""I can understand Japanese, but I can't say much."" In this case, you probably spend a lot of time studying the language but not using it. You need to actually practice using the language verbally in order to build communication skills. There are a couple things you can try to get your speaking game up to snuff. Get a one-on-one teacher It might not be a free option, but it's probably the fastest and most effective way to practice your conversation as well as learn about Japanese language. Especially if you don't have any Japanese friends to talk to. You can search for teachers in your area or take to Skype options to find a quick, convenient solution. Take to the Internet The Internet is a great resource for many things. It has provided us a way to communicate across the vast oceans that may separate us. Lucky for us, there are some great tools out there to help us learners meet native Japanese speakers with whom we can practice speaking. Lang-8 is an online service where you can post content and have it corrected by a native speaker. This isn't verbal communication, but it will allow you to make contacts which you could then possibly chat up on Skype or other chat service. HelloTalk is a relatively new entry on the scene. It's an Android / iOS app that basically expands a bit on Lang-8. It's a language exchange in an app where you can search for contacts to engage. People usually seem to be pretty good about responding too. I've tried it and had a pretty good response rate. Native speakers can offer real-time corrections to anything you write or record-oh yeah, you can also send recorded voice clips! This is another great way to meet people and bring the conversation to Skype or a similar medium. 8. ""I've completely lost motivation."" Maybe you've just completely lost the motivation to study Japanese. It happens. In that case, we need to get you back on track! Here's a tip. Find your reason. Maybe you don't really remember why you're studying in the first place. Get back that spark that pushed you to learn Japanese in the first place and you'll be well on the path to recovery. Have you always wanted to be able to watch anime without subtitles? Great! Give your Japanese learning a little kickstart by taking the time to watch your favorite anime shows every day. Once you're on a roll, ease yourself into learning mode. Try watching the shows with a stronger focus on learning and understanding what's being said. If you're at an intermediate level, try watching clips or shows twice-only once with subtitles, noting what you don't understand. Then try it without the subtitles. See how much you can pick up. You might be surprised. Incorporate what you love into your learning and make it fun again! All you need to do now is say ""no"" to excuses and get back in the Japanese learning zone. If you liked this post, something tells me that you'll love FluentU, the best way to learn Japanese with real-world videos. Experience Japanese immersion online!",en,39
516,2676,1477661749,CONTENT SHARED,-4831310174172854034,7645894863578715801,3480723906734623856,Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:49.0) Gecko/20100101 Firefox/49.0,SP,BR,HTML,https://medium.com/built-to-adapt/standing-on-the-shoulders-of-giants-59762fb0c155,standing on the shoulders of giants - built to adapt,"Back in 1869 when Washington Roebling started work on the Brooklyn Bridge, there wasn't a tradition of collaboration among fellow bridge builders. Engineers were encouraged to mind their own business and rarely offered up insight that would be deemed ""valuable"" to rivals. This meant that information about a mysterious illness crippling workers building a bridge in St. Louis was not socialized with the professional community. If Roebling had access to this information, or the findings from Europe about this affliction later labeled ""the bends,"" the lives of many men would have been spared. What does this have to do with microservices? As it turns out, a lot. We're now in a period of unprecedented openness and collaboration among technical colleagues. Instead of building systems made up of proprietary technology and closely-guarded local knowledge, companies are constructing modern microservices with the help of open source software and patterns evangelized by forward-thinking members of the community. This sharing of assets helps developers build more reliable software, faster. And almost no one shares their software assets quite like Netflix. Instead of building systems made up of proprietary technology and closely-guarded local knowledge, companies are constructing modern microservices with the help of open source software and patterns evangelized by forward-thinking members of the community. Netflix is a media giant, with over 80 million global customers who watch more than 125 million hours of streaming content everyday. During primetime hours, Netflix consumption constitutes 35% of all Internet traffic in North America. In 2008, Netflix started their journey to the cloud and microservices for three reasons: availability, scale, and speed. As a 24x7 service, Netflix needs to be ""always on"" and their monolithic code base made troubleshooting problems difficult. Back then, a single missing semicolon took the entire Netflix site down for hours! The company also faced scaling challenges that were hard to address within a single monolithic application. As Netflix added more types of user interfaces to their service and expanded into more geographies, they couldn't easily scale individual parts of their application. It was all or nothing. Finally, Netflix needed to optimize for speed. In a competitive market, companies like Netflix have to help teams deliver more software, faster, and that's not easily achievable when everyone steps all over themselves in a single monolithic system. Netflix is a software-driven business, so their engineers constructed an impressive array of software that builds upon foundational components from Amazon Web Services, their chosen cloud provider. As an early adopter of microservices and highly-resilient cloud systems, Netflix solved a number of these complex problems that most companies are just starting to face. They've shared these hard-fought lessons in the form of open source software that anyone can use to build cloud- scale systems. Microservices: Opportunities and Challenges With a microservices architecture, you have many individual components, each associated with a specific responsibility, loosely coupled to each other, continuously delivered by independent teams through automation. Let's unpack that sentence, and compare these attributes to those of a monolithic software solution. ... many individual components ... Traditional business systems are often monolithic in nature. All of the business logic, user experience, and integration interfaces are part of a single code-base or software installation. It's often fairly self-contained, but not modular in a way that supports component upgrades. Conversely, a microservices architecture has lots of little pieces that may be dedicated to a given application, or shared by many systems. ""Micro"" is meant to refer to the scope of the service, so don't get caught up in ""physical size"" or ""lines-of-code"" as meaningful attributes. ... each associated with a specific responsibility ... In his O'Reilly book titled Migrating to Cloud-Native Application Architectures , Matt Stine says that it's about creating: ""... independently deployable services that do 'one thing well.' That one thing usually represents a business capability, or the smallest 'atomic' unit of a service that delivers business value."" A hallmark of microservices is building discrete services that don't encroach on the domain of others. As a reference, the principles of ""domain driven design"" help teams successfully define their boundaries. Because services should ""own"" whatever they need to deliver a given business capability, experts recommend that microservices have their own data store. This concept requires a major shift in how we've traditionally design software, but offers a compelling approach to those have spent years building or maintaining dense, tangled systems. ... loosely coupled to each other ... One of the major challenges in a monolithic business system is the interconnectedness of all the components. Are you changing the database structure, swapping out a web server, or altering some business logic? Often this requires an update to numerous components because of the uncomfortable closeness of all the pieces. As a result, teams shy away from making (necessary) updates because of the many ways that things could go wrong. Conversely, microservices encourage loose coupling. Services interact with each other through contracts or messaging buses, and have zero knowledge about the implementation details. The owner of a particular microservice can responsibly iterate on the service without fear of shattering a fragile relationship with its ecosystem. Thus, encouraging teams to make frequent changes. ... continuously delivered by independent teams .... If one believes in Conway's Law - the observation that systems reflect the organizational communication paths of those who build them - then a transition to microservices must be accompanied by a corresponding change in the team. The same project teams and line-of-business hierarchies that created a data center full of monolithic business systems cannot successfully create a microservices architecture. Instead, microservices often bring about a major change in team structure and tooling. If the goal is to continuously deliver business logic in the form of microservices, then software teams need the autonomy and directive to build and deploy focused services all on their own. This means that short-lived project teams give way to long-lived, independent product teams. These teams don't file tickets to do deployments; they directly push to production early and often. The team makeup consists of all the skills needed to design, build, and deploy the service. Such teams have the personnel, permission, and technology needed to iterate constantly and get the results of each iteration into production immediately. The same project teams and line-of-business hierarchies that created a data center full of monolithic business systems cannot successfully create a microservices architecture. ... through automation. Pivotal's Casey West says that microservices are about ""automating the path to production."" It's virtually impossible to ship software constantly if the delivery pipeline is manual, as it has historically been with monolithic software. Automation is a core part of the microservices experience, enabled via continuous integration, continuous delivery, and continuous deployment. Continuous integration refers to the practice of developers merging all their code multiple times per day. Instead of slogging through the ""integration testing"" phase of a project where countless unforeseen mismatches occur between code modules, continuous integration ensures that issues bubble up faster. There are a lot of logistics required to constantly execute tests, so modern teams rely on event-driven automation to retrieve source code, stand up infrastructure, and execute tests. If all the tests pass, code could be considered production ready. Continuous delivery happens when code is packaged and capable of being deployed to production. Capable being the key word here. There could be many reasons why the pipeline is completely automated, but code is purposely gated before actually getting deployed, usually related to business timing. Continuous deployment goes one step further and involves every change automatically moving to a production environment. All of these techniques require a careful assessment of current deployment procedures, and a ruthless focus on eliminating waste and automating manual steps. The result? Repeatable, timely deployments that greatly reduce the amount of time needed to get valuable software into the hands of your customers. In this paper, we take a look at the architectural challenges Netflix faced, what they've released as open source software, why this software matters to you, and how Pivotal makes it easy to consume Netflix software as part of Spring Cloud. To learn more, download the free white paper here .",en,39
517,285,1460120825,CONTENT SHARED,-4625438649010928449,6650574433856141080,3145020730587339693,,,,HTML,http://www.camcode.com/asset-tags/top-asset-tracking-software/,top asset tracking software: the 52 best tools and software solutions to track your company's vital physical assets,"Assets are a critical component of any business or organization, but tracking those assets can be one of the most time-consuming tasks of the entire work day. The good news is, barcodes, scanners, asset tracking software, and other asset tracking tools can streamline your work day and help to make your employees work more efficiently and productively. The best asset tracking software solutions and tools also enable organizations to track repair and maintenance schedules, asset locations, and other crucial asset information. We at Camcode understand the value of assets and the importance of utilizing high quality software and tools to track them. That's why we have compiled our list of the top asset tracking software solutions and tools. We have searched asset tracking software and tool reviews, top technology sites and blogs, and expert opinions to build this list of top-rated asset tracking software solutions and tools. Our asset tracking software and tool picks cover a range of industries and include the most useful features, such as automated check in and check out; asset tracking by site, location, user, serial number, and other criteria; alerts and notifications of overdue assets, maintenance needs, expiring warranties; multi-user and multi-location support; and more. To meet all of your asset tracking needs, our list of top asset tracking software solutions and tools also includes handheld scanners and mobile apps. Please note, our top 52 asset tracking software solutions and tools are listed here, in no particular order . Wasp Asset Software provides ""asset management benefits for your business."" The asset software saves valuable time and eliminates the need for spreadsheets, while ensuring proper inventory management for your organization. With Wasp Asset Software, you instantly know who has which assets, when they are due back, the date of purchase